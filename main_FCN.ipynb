{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce5e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58eded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a3116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/final_format/train_set.csv\",header=None).to_numpy()\n",
    "train_label = pd.read_csv(\"data/final_format/train_label.csv\",header=None).to_numpy()\n",
    "test_set = pd.read_csv(\"data/final_format/test_set.csv\",header=None).to_numpy()\n",
    "test_label = pd.read_csv(\"data/final_format/test_label.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f64e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14393, 4096) (14393, 1) (3599, 4096) (3599, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d522cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 4096) (14392, 1) (3598, 4096) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "#delet first row data\n",
    "train_set = train_set[1:]\n",
    "train_label = train_label[1:]\n",
    "test_set = test_set[1:]\n",
    "test_label = test_label[1:]\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a84a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 1, 64, 64) (14392, 1) (3598, 1, 64, 64) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.reshape((-1,1,64,64))\n",
    "test_set = test_set.reshape((-1,1,64,64))\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e23a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 1, 64, 64) (14392,) (3598, 1, 64, 64) (3598,)\n"
     ]
    }
   ],
   "source": [
    "train_label = train_label.reshape(-1)\n",
    "test_label = test_label.reshape(-1)\n",
    "\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f62253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 300\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d66b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tensor = Tensor(train_set) \n",
    "train_label_tensor = Tensor(train_label).type(torch.LongTensor)\n",
    "\n",
    "train_dataset = TensorDataset(train_set_tensor,train_label_tensor) \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size) \n",
    "\n",
    "test_set_tensor = Tensor(test_set) \n",
    "test_label_tensor = Tensor(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_dataset = TensorDataset(test_set_tensor,test_label_tensor) \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33820b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6f6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(FCN, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 32, kernel_size=3, padding='same')\n",
    "        self.c2 = nn.Conv2d(32, 32, kernel_size=3,padding='same')\n",
    "        self.c3 = nn.Conv2d(64, 64, kernel_size=3,padding='same') \n",
    "        self.c4 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "        self.c5 = nn.Conv2d(64, 64, kernel_size=3,padding='same')\n",
    "        self.c6 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
    "        self.m1 = nn.Conv2d(64, 64, kernel_size=3,stride=2,padding='valid')\n",
    "        self.m2 = nn.Conv2d(128, 128, kernel_size=3,stride=2,padding='valid')\n",
    "        \n",
    "        self.d1 = nn.Dropout(p=0.25)\n",
    "        self.d2 = nn.Dropout(p=0.25)\n",
    "        self.d3 = nn.Dropout(p=0.25)\n",
    "        self.d4 = nn.Dropout(p=0.25)\n",
    "        self.d5 = nn.Dropout(p=0.25)\n",
    "     \n",
    "        self.bn1 =  nn.BatchNorm2d(32)\n",
    "        self.bn2 =  nn.BatchNorm2d(64)\n",
    "        self.bn3 =  nn.BatchNorm2d(64)\n",
    "        self.bn4 =  nn.BatchNorm1d(512)\n",
    "       \n",
    "        self.fc1 = nn.Linear(128*15*15, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        C1 = self.c1(x)\n",
    "        C1 = F.leaky_relu(C1,0.2)\n",
    "        C1 = self.d1(C1)\n",
    "        C1 = self.bn1(C1)\n",
    "        C2 = self.c2(C1)\n",
    "        C2 = F.leaky_relu(C2,0.2)\n",
    "        sum1 = torch.cat((C1, C2), dim=1)\n",
    "        C3 = self.c3(sum1)\n",
    "        C3 = F.leaky_relu(C3,0.2)\n",
    "        C3 = self.d2(C3)\n",
    "        M1 = self.m1(C3)\n",
    "        \n",
    "        C4 = self.c4(M1)\n",
    "        C4 = self.bn2(C4)\n",
    "        C4 = F.leaky_relu(C4,0.2)\n",
    "        C4 = self.d3(C4)\n",
    "        C5 = self.c5(C4)\n",
    "        C5 = self.bn3(C5)\n",
    "        C5 = F.leaky_relu(C5,0.2)\n",
    "        sum2 = torch.cat((C4, C5), dim=1)\n",
    "        C6 = self.c6(sum2)\n",
    "        C6 = F.leaky_relu(C6,0.2)\n",
    "        C6 = self.d4(C6)\n",
    "        M2 = self.m2(C6)\n",
    "\n",
    "        F1 = M2.reshape(M2.size(0), -1)\n",
    "        Fc1 = self.fc1(F1)\n",
    "        Fc1 = self.bn4(Fc1)\n",
    "        Fc1 = F.leaky_relu(Fc1,0.2)\n",
    "        Fc1 = self.d5(Fc1)\n",
    "        Fc2 = self.fc2(Fc1)\n",
    "       \n",
    "        return Fc2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901366a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ae0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3) \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "milestones = [50,100,150,200,250]\n",
    "milestones = [a * len(train_loader) for a in milestones]\n",
    "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b03775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [1/225], Training Accuracy: 26.5625%, Training Loss: 1.4490%\n",
      "Epoch [1/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: 2.0552%\n",
      "Epoch [1/300], Step [3/225], Training Accuracy: 27.6042%, Training Loss: 1.9193%\n",
      "Epoch [1/300], Step [4/225], Training Accuracy: 30.4688%, Training Loss: 1.8943%\n",
      "Epoch [1/300], Step [5/225], Training Accuracy: 30.3125%, Training Loss: 1.8225%\n",
      "Epoch [1/300], Step [6/225], Training Accuracy: 29.9479%, Training Loss: 1.7859%\n",
      "Epoch [1/300], Step [7/225], Training Accuracy: 29.4643%, Training Loss: 1.7332%\n",
      "Epoch [1/300], Step [8/225], Training Accuracy: 29.1016%, Training Loss: 1.6994%\n",
      "Epoch [1/300], Step [9/225], Training Accuracy: 28.9931%, Training Loss: 1.6685%\n",
      "Epoch [1/300], Step [10/225], Training Accuracy: 28.1250%, Training Loss: 1.6538%\n",
      "Epoch [1/300], Step [11/225], Training Accuracy: 27.9830%, Training Loss: 1.6354%\n",
      "Epoch [1/300], Step [12/225], Training Accuracy: 27.7344%, Training Loss: 1.6115%\n",
      "Epoch [1/300], Step [13/225], Training Accuracy: 27.7644%, Training Loss: 1.5960%\n",
      "Epoch [1/300], Step [14/225], Training Accuracy: 28.2366%, Training Loss: 1.5801%\n",
      "Epoch [1/300], Step [15/225], Training Accuracy: 28.1250%, Training Loss: 1.5717%\n",
      "Epoch [1/300], Step [16/225], Training Accuracy: 27.5391%, Training Loss: 1.5649%\n",
      "Epoch [1/300], Step [17/225], Training Accuracy: 27.9412%, Training Loss: 1.5553%\n",
      "Epoch [1/300], Step [18/225], Training Accuracy: 27.6042%, Training Loss: 1.5505%\n",
      "Epoch [1/300], Step [19/225], Training Accuracy: 27.2204%, Training Loss: 1.5457%\n",
      "Epoch [1/300], Step [20/225], Training Accuracy: 27.4219%, Training Loss: 1.5421%\n",
      "Epoch [1/300], Step [21/225], Training Accuracy: 27.5298%, Training Loss: 1.5361%\n",
      "Epoch [1/300], Step [22/225], Training Accuracy: 27.4858%, Training Loss: 1.5301%\n",
      "Epoch [1/300], Step [23/225], Training Accuracy: 27.6495%, Training Loss: 1.5259%\n",
      "Epoch [1/300], Step [24/225], Training Accuracy: 27.4089%, Training Loss: 1.5234%\n",
      "Epoch [1/300], Step [25/225], Training Accuracy: 27.6250%, Training Loss: 1.5194%\n",
      "Epoch [1/300], Step [26/225], Training Accuracy: 27.3438%, Training Loss: 1.5156%\n",
      "Epoch [1/300], Step [27/225], Training Accuracy: 27.3148%, Training Loss: 1.5125%\n",
      "Epoch [1/300], Step [28/225], Training Accuracy: 27.1763%, Training Loss: 1.5094%\n",
      "Epoch [1/300], Step [29/225], Training Accuracy: 27.4246%, Training Loss: 1.5057%\n",
      "Epoch [1/300], Step [30/225], Training Accuracy: 27.5000%, Training Loss: 1.5010%\n",
      "Epoch [1/300], Step [31/225], Training Accuracy: 27.2177%, Training Loss: 1.4997%\n",
      "Epoch [1/300], Step [32/225], Training Accuracy: 27.6367%, Training Loss: 1.4944%\n",
      "Epoch [1/300], Step [33/225], Training Accuracy: 27.6042%, Training Loss: 1.4898%\n",
      "Epoch [1/300], Step [34/225], Training Accuracy: 27.5276%, Training Loss: 1.4874%\n",
      "Epoch [1/300], Step [35/225], Training Accuracy: 27.5893%, Training Loss: 1.4834%\n",
      "Epoch [1/300], Step [36/225], Training Accuracy: 27.5608%, Training Loss: 1.4816%\n",
      "Epoch [1/300], Step [37/225], Training Accuracy: 27.7027%, Training Loss: 1.4815%\n",
      "Epoch [1/300], Step [38/225], Training Accuracy: 27.9605%, Training Loss: 1.4767%\n",
      "Epoch [1/300], Step [39/225], Training Accuracy: 27.9647%, Training Loss: 1.4733%\n",
      "Epoch [1/300], Step [40/225], Training Accuracy: 28.0859%, Training Loss: 1.4699%\n",
      "Epoch [1/300], Step [41/225], Training Accuracy: 28.0488%, Training Loss: 1.4687%\n",
      "Epoch [1/300], Step [42/225], Training Accuracy: 27.9762%, Training Loss: 1.4669%\n",
      "Epoch [1/300], Step [43/225], Training Accuracy: 27.9797%, Training Loss: 1.4669%\n",
      "Epoch [1/300], Step [44/225], Training Accuracy: 28.1250%, Training Loss: 1.4672%\n",
      "Epoch [1/300], Step [45/225], Training Accuracy: 28.1597%, Training Loss: 1.4656%\n",
      "Epoch [1/300], Step [46/225], Training Accuracy: 28.2609%, Training Loss: 1.4629%\n",
      "Epoch [1/300], Step [47/225], Training Accuracy: 28.2580%, Training Loss: 1.4616%\n",
      "Epoch [1/300], Step [48/225], Training Accuracy: 28.3203%, Training Loss: 1.4598%\n",
      "Epoch [1/300], Step [49/225], Training Accuracy: 28.4120%, Training Loss: 1.4583%\n",
      "Epoch [1/300], Step [50/225], Training Accuracy: 28.2812%, Training Loss: 1.4581%\n",
      "Epoch [1/300], Step [51/225], Training Accuracy: 28.0637%, Training Loss: 1.4583%\n",
      "Epoch [1/300], Step [52/225], Training Accuracy: 28.1250%, Training Loss: 1.4560%\n",
      "Epoch [1/300], Step [53/225], Training Accuracy: 28.1840%, Training Loss: 1.4527%\n",
      "Epoch [1/300], Step [54/225], Training Accuracy: 28.2118%, Training Loss: 1.4507%\n",
      "Epoch [1/300], Step [55/225], Training Accuracy: 28.3239%, Training Loss: 1.4494%\n",
      "Epoch [1/300], Step [56/225], Training Accuracy: 28.4319%, Training Loss: 1.4466%\n",
      "Epoch [1/300], Step [57/225], Training Accuracy: 28.5088%, Training Loss: 1.4448%\n",
      "Epoch [1/300], Step [58/225], Training Accuracy: 28.6369%, Training Loss: 1.4431%\n",
      "Epoch [1/300], Step [59/225], Training Accuracy: 28.6282%, Training Loss: 1.4414%\n",
      "Epoch [1/300], Step [60/225], Training Accuracy: 28.8021%, Training Loss: 1.4388%\n",
      "Epoch [1/300], Step [61/225], Training Accuracy: 28.8422%, Training Loss: 1.4371%\n",
      "Epoch [1/300], Step [62/225], Training Accuracy: 28.8054%, Training Loss: 1.4368%\n",
      "Epoch [1/300], Step [63/225], Training Accuracy: 28.7698%, Training Loss: 1.4360%\n",
      "Epoch [1/300], Step [64/225], Training Accuracy: 28.7598%, Training Loss: 1.4357%\n",
      "Epoch [1/300], Step [65/225], Training Accuracy: 28.9183%, Training Loss: 1.4336%\n",
      "Epoch [1/300], Step [66/225], Training Accuracy: 28.8826%, Training Loss: 1.4332%\n",
      "Epoch [1/300], Step [67/225], Training Accuracy: 29.0112%, Training Loss: 1.4319%\n",
      "Epoch [1/300], Step [68/225], Training Accuracy: 29.1131%, Training Loss: 1.4310%\n",
      "Epoch [1/300], Step [69/225], Training Accuracy: 29.0308%, Training Loss: 1.4309%\n",
      "Epoch [1/300], Step [70/225], Training Accuracy: 29.1964%, Training Loss: 1.4295%\n",
      "Epoch [1/300], Step [71/225], Training Accuracy: 29.1593%, Training Loss: 1.4291%\n",
      "Epoch [1/300], Step [72/225], Training Accuracy: 29.0582%, Training Loss: 1.4297%\n",
      "Epoch [1/300], Step [73/225], Training Accuracy: 29.0240%, Training Loss: 1.4291%\n",
      "Epoch [1/300], Step [74/225], Training Accuracy: 29.1385%, Training Loss: 1.4271%\n",
      "Epoch [1/300], Step [75/225], Training Accuracy: 29.2083%, Training Loss: 1.4257%\n",
      "Epoch [1/300], Step [76/225], Training Accuracy: 29.2352%, Training Loss: 1.4237%\n",
      "Epoch [1/300], Step [77/225], Training Accuracy: 29.1396%, Training Loss: 1.4251%\n",
      "Epoch [1/300], Step [78/225], Training Accuracy: 29.1667%, Training Loss: 1.4247%\n",
      "Epoch [1/300], Step [79/225], Training Accuracy: 29.3117%, Training Loss: 1.4244%\n",
      "Epoch [1/300], Step [80/225], Training Accuracy: 29.2578%, Training Loss: 1.4232%\n",
      "Epoch [1/300], Step [81/225], Training Accuracy: 29.3210%, Training Loss: 1.4225%\n",
      "Epoch [1/300], Step [82/225], Training Accuracy: 29.3826%, Training Loss: 1.4223%\n",
      "Epoch [1/300], Step [83/225], Training Accuracy: 29.4804%, Training Loss: 1.4207%\n",
      "Epoch [1/300], Step [84/225], Training Accuracy: 29.4829%, Training Loss: 1.4200%\n",
      "Epoch [1/300], Step [85/225], Training Accuracy: 29.5221%, Training Loss: 1.4196%\n",
      "Epoch [1/300], Step [86/225], Training Accuracy: 29.5967%, Training Loss: 1.4184%\n",
      "Epoch [1/300], Step [87/225], Training Accuracy: 29.6516%, Training Loss: 1.4174%\n",
      "Epoch [1/300], Step [88/225], Training Accuracy: 29.6520%, Training Loss: 1.4174%\n",
      "Epoch [1/300], Step [89/225], Training Accuracy: 29.6348%, Training Loss: 1.4178%\n",
      "Epoch [1/300], Step [90/225], Training Accuracy: 29.7569%, Training Loss: 1.4173%\n",
      "Epoch [1/300], Step [91/225], Training Accuracy: 29.7047%, Training Loss: 1.4182%\n",
      "Epoch [1/300], Step [92/225], Training Accuracy: 29.7385%, Training Loss: 1.4177%\n",
      "Epoch [1/300], Step [93/225], Training Accuracy: 29.8051%, Training Loss: 1.4169%\n",
      "Epoch [1/300], Step [94/225], Training Accuracy: 29.8870%, Training Loss: 1.4156%\n",
      "Epoch [1/300], Step [95/225], Training Accuracy: 29.9178%, Training Loss: 1.4164%\n",
      "Epoch [1/300], Step [96/225], Training Accuracy: 29.9479%, Training Loss: 1.4153%\n",
      "Epoch [1/300], Step [97/225], Training Accuracy: 30.0097%, Training Loss: 1.4147%\n",
      "Epoch [1/300], Step [98/225], Training Accuracy: 29.9745%, Training Loss: 1.4144%\n",
      "Epoch [1/300], Step [99/225], Training Accuracy: 30.0505%, Training Loss: 1.4138%\n",
      "Epoch [1/300], Step [100/225], Training Accuracy: 30.1562%, Training Loss: 1.4123%\n",
      "Epoch [1/300], Step [101/225], Training Accuracy: 30.2908%, Training Loss: 1.4111%\n",
      "Epoch [1/300], Step [102/225], Training Accuracy: 30.4381%, Training Loss: 1.4100%\n",
      "Epoch [1/300], Step [103/225], Training Accuracy: 30.5218%, Training Loss: 1.4095%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [104/225], Training Accuracy: 30.6190%, Training Loss: 1.4085%\n",
      "Epoch [1/300], Step [105/225], Training Accuracy: 30.5804%, Training Loss: 1.4086%\n",
      "Epoch [1/300], Step [106/225], Training Accuracy: 30.6751%, Training Loss: 1.4076%\n",
      "Epoch [1/300], Step [107/225], Training Accuracy: 30.7243%, Training Loss: 1.4072%\n",
      "Epoch [1/300], Step [108/225], Training Accuracy: 30.8304%, Training Loss: 1.4067%\n",
      "Epoch [1/300], Step [109/225], Training Accuracy: 30.7626%, Training Loss: 1.4064%\n",
      "Epoch [1/300], Step [110/225], Training Accuracy: 30.8381%, Training Loss: 1.4052%\n",
      "Epoch [1/300], Step [111/225], Training Accuracy: 30.8136%, Training Loss: 1.4051%\n",
      "Epoch [1/300], Step [112/225], Training Accuracy: 30.8454%, Training Loss: 1.4045%\n",
      "Epoch [1/300], Step [113/225], Training Accuracy: 30.8075%, Training Loss: 1.4055%\n",
      "Epoch [1/300], Step [114/225], Training Accuracy: 30.9073%, Training Loss: 1.4042%\n",
      "Epoch [1/300], Step [115/225], Training Accuracy: 30.9783%, Training Loss: 1.4032%\n",
      "Epoch [1/300], Step [116/225], Training Accuracy: 30.9941%, Training Loss: 1.4027%\n",
      "Epoch [1/300], Step [117/225], Training Accuracy: 30.9829%, Training Loss: 1.4034%\n",
      "Epoch [1/300], Step [118/225], Training Accuracy: 30.9719%, Training Loss: 1.4028%\n",
      "Epoch [1/300], Step [119/225], Training Accuracy: 30.9611%, Training Loss: 1.4022%\n",
      "Epoch [1/300], Step [120/225], Training Accuracy: 30.9766%, Training Loss: 1.4014%\n",
      "Epoch [1/300], Step [121/225], Training Accuracy: 30.9917%, Training Loss: 1.4011%\n",
      "Epoch [1/300], Step [122/225], Training Accuracy: 30.9682%, Training Loss: 1.4007%\n",
      "Epoch [1/300], Step [123/225], Training Accuracy: 30.9197%, Training Loss: 1.4013%\n",
      "Epoch [1/300], Step [124/225], Training Accuracy: 31.0106%, Training Loss: 1.4005%\n",
      "Epoch [1/300], Step [125/225], Training Accuracy: 30.9250%, Training Loss: 1.4012%\n",
      "Epoch [1/300], Step [126/225], Training Accuracy: 30.9648%, Training Loss: 1.4004%\n",
      "Epoch [1/300], Step [127/225], Training Accuracy: 30.9424%, Training Loss: 1.4003%\n",
      "Epoch [1/300], Step [128/225], Training Accuracy: 30.9570%, Training Loss: 1.4009%\n",
      "Epoch [1/300], Step [129/225], Training Accuracy: 30.9593%, Training Loss: 1.4003%\n",
      "Epoch [1/300], Step [130/225], Training Accuracy: 30.9135%, Training Loss: 1.4003%\n",
      "Epoch [1/300], Step [131/225], Training Accuracy: 30.9041%, Training Loss: 1.3997%\n",
      "Epoch [1/300], Step [132/225], Training Accuracy: 30.9659%, Training Loss: 1.3990%\n",
      "Epoch [1/300], Step [133/225], Training Accuracy: 30.9915%, Training Loss: 1.3985%\n",
      "Epoch [1/300], Step [134/225], Training Accuracy: 31.0518%, Training Loss: 1.3978%\n",
      "Epoch [1/300], Step [135/225], Training Accuracy: 30.9838%, Training Loss: 1.3981%\n",
      "Epoch [1/300], Step [136/225], Training Accuracy: 31.0087%, Training Loss: 1.3978%\n",
      "Epoch [1/300], Step [137/225], Training Accuracy: 30.9877%, Training Loss: 1.3974%\n",
      "Epoch [1/300], Step [138/225], Training Accuracy: 31.0349%, Training Loss: 1.3972%\n",
      "Epoch [1/300], Step [139/225], Training Accuracy: 30.9915%, Training Loss: 1.3970%\n",
      "Epoch [1/300], Step [140/225], Training Accuracy: 30.9821%, Training Loss: 1.3970%\n",
      "Epoch [1/300], Step [141/225], Training Accuracy: 30.9730%, Training Loss: 1.3965%\n",
      "Epoch [1/300], Step [142/225], Training Accuracy: 30.9859%, Training Loss: 1.3961%\n",
      "Epoch [1/300], Step [143/225], Training Accuracy: 31.0315%, Training Loss: 1.3956%\n",
      "Epoch [1/300], Step [144/225], Training Accuracy: 31.0872%, Training Loss: 1.3949%\n",
      "Epoch [1/300], Step [145/225], Training Accuracy: 31.0668%, Training Loss: 1.3947%\n",
      "Epoch [1/300], Step [146/225], Training Accuracy: 31.1537%, Training Loss: 1.3936%\n",
      "Epoch [1/300], Step [147/225], Training Accuracy: 31.2075%, Training Loss: 1.3929%\n",
      "Epoch [1/300], Step [148/225], Training Accuracy: 31.1972%, Training Loss: 1.3925%\n",
      "Epoch [1/300], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 1.3922%\n",
      "Epoch [1/300], Step [150/225], Training Accuracy: 31.1875%, Training Loss: 1.3922%\n",
      "Epoch [1/300], Step [151/225], Training Accuracy: 31.2500%, Training Loss: 1.3917%\n",
      "Epoch [1/300], Step [152/225], Training Accuracy: 31.2603%, Training Loss: 1.3912%\n",
      "Epoch [1/300], Step [153/225], Training Accuracy: 31.3011%, Training Loss: 1.3906%\n",
      "Epoch [1/300], Step [154/225], Training Accuracy: 31.3007%, Training Loss: 1.3901%\n",
      "Epoch [1/300], Step [155/225], Training Accuracy: 31.2903%, Training Loss: 1.3898%\n",
      "Epoch [1/300], Step [156/225], Training Accuracy: 31.3201%, Training Loss: 1.3893%\n",
      "Epoch [1/300], Step [157/225], Training Accuracy: 31.3296%, Training Loss: 1.3891%\n",
      "Epoch [1/300], Step [158/225], Training Accuracy: 31.3390%, Training Loss: 1.3888%\n",
      "Epoch [1/300], Step [159/225], Training Accuracy: 31.3483%, Training Loss: 1.3882%\n",
      "Epoch [1/300], Step [160/225], Training Accuracy: 31.3574%, Training Loss: 1.3876%\n",
      "Epoch [1/300], Step [161/225], Training Accuracy: 31.3762%, Training Loss: 1.3872%\n",
      "Epoch [1/300], Step [162/225], Training Accuracy: 31.4140%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [163/225], Training Accuracy: 31.4321%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [164/225], Training Accuracy: 31.4596%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [165/225], Training Accuracy: 31.4678%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [166/225], Training Accuracy: 31.5324%, Training Loss: 1.3844%\n",
      "Epoch [1/300], Step [167/225], Training Accuracy: 31.5681%, Training Loss: 1.3839%\n",
      "Epoch [1/300], Step [168/225], Training Accuracy: 31.6313%, Training Loss: 1.3831%\n",
      "Epoch [1/300], Step [169/225], Training Accuracy: 31.6198%, Training Loss: 1.3840%\n",
      "Epoch [1/300], Step [170/225], Training Accuracy: 31.6636%, Training Loss: 1.3839%\n",
      "Epoch [1/300], Step [171/225], Training Accuracy: 31.6977%, Training Loss: 1.3834%\n",
      "Epoch [1/300], Step [172/225], Training Accuracy: 31.7496%, Training Loss: 1.3829%\n",
      "Epoch [1/300], Step [173/225], Training Accuracy: 31.8009%, Training Loss: 1.3822%\n",
      "Epoch [1/300], Step [174/225], Training Accuracy: 31.8786%, Training Loss: 1.3814%\n",
      "Epoch [1/300], Step [175/225], Training Accuracy: 31.9107%, Training Loss: 1.3813%\n",
      "Epoch [1/300], Step [176/225], Training Accuracy: 31.9070%, Training Loss: 1.3814%\n",
      "Epoch [1/300], Step [177/225], Training Accuracy: 31.8768%, Training Loss: 1.3814%\n",
      "Epoch [1/300], Step [178/225], Training Accuracy: 31.8469%, Training Loss: 1.3811%\n",
      "Epoch [1/300], Step [179/225], Training Accuracy: 31.7999%, Training Loss: 1.3806%\n",
      "Epoch [1/300], Step [180/225], Training Accuracy: 31.8490%, Training Loss: 1.3802%\n",
      "Epoch [1/300], Step [181/225], Training Accuracy: 31.9233%, Training Loss: 1.3798%\n",
      "Epoch [1/300], Step [182/225], Training Accuracy: 31.9025%, Training Loss: 1.3792%\n",
      "Epoch [1/300], Step [183/225], Training Accuracy: 31.9074%, Training Loss: 1.3786%\n",
      "Epoch [1/300], Step [184/225], Training Accuracy: 31.9124%, Training Loss: 1.3785%\n",
      "Epoch [1/300], Step [185/225], Training Accuracy: 32.0017%, Training Loss: 1.3776%\n",
      "Epoch [1/300], Step [186/225], Training Accuracy: 32.0144%, Training Loss: 1.3772%\n",
      "Epoch [1/300], Step [187/225], Training Accuracy: 32.0354%, Training Loss: 1.3771%\n",
      "Epoch [1/300], Step [188/225], Training Accuracy: 32.0479%, Training Loss: 1.3765%\n",
      "Epoch [1/300], Step [189/225], Training Accuracy: 32.0354%, Training Loss: 1.3763%\n",
      "Epoch [1/300], Step [190/225], Training Accuracy: 32.0559%, Training Loss: 1.3764%\n",
      "Epoch [1/300], Step [191/225], Training Accuracy: 32.1008%, Training Loss: 1.3757%\n",
      "Epoch [1/300], Step [192/225], Training Accuracy: 32.1208%, Training Loss: 1.3753%\n",
      "Epoch [1/300], Step [193/225], Training Accuracy: 32.1567%, Training Loss: 1.3751%\n",
      "Epoch [1/300], Step [194/225], Training Accuracy: 32.1923%, Training Loss: 1.3747%\n",
      "Epoch [1/300], Step [195/225], Training Accuracy: 32.2115%, Training Loss: 1.3745%\n",
      "Epoch [1/300], Step [196/225], Training Accuracy: 32.1349%, Training Loss: 1.3748%\n",
      "Epoch [1/300], Step [197/225], Training Accuracy: 32.0907%, Training Loss: 1.3749%\n",
      "Epoch [1/300], Step [198/225], Training Accuracy: 32.1654%, Training Loss: 1.3741%\n",
      "Epoch [1/300], Step [199/225], Training Accuracy: 32.2158%, Training Loss: 1.3741%\n",
      "Epoch [1/300], Step [200/225], Training Accuracy: 32.2188%, Training Loss: 1.3738%\n",
      "Epoch [1/300], Step [201/225], Training Accuracy: 32.2139%, Training Loss: 1.3736%\n",
      "Epoch [1/300], Step [202/225], Training Accuracy: 32.2092%, Training Loss: 1.3731%\n",
      "Epoch [1/300], Step [203/225], Training Accuracy: 32.2044%, Training Loss: 1.3732%\n",
      "Epoch [1/300], Step [204/225], Training Accuracy: 32.2074%, Training Loss: 1.3726%\n",
      "Epoch [1/300], Step [205/225], Training Accuracy: 32.2637%, Training Loss: 1.3721%\n",
      "Epoch [1/300], Step [206/225], Training Accuracy: 32.2967%, Training Loss: 1.3718%\n",
      "Epoch [1/300], Step [207/225], Training Accuracy: 32.2766%, Training Loss: 1.3718%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [208/225], Training Accuracy: 32.3017%, Training Loss: 1.3711%\n",
      "Epoch [1/300], Step [209/225], Training Accuracy: 32.3041%, Training Loss: 1.3712%\n",
      "Epoch [1/300], Step [210/225], Training Accuracy: 32.2991%, Training Loss: 1.3711%\n",
      "Epoch [1/300], Step [211/225], Training Accuracy: 32.2793%, Training Loss: 1.3706%\n",
      "Epoch [1/300], Step [212/225], Training Accuracy: 32.2745%, Training Loss: 1.3701%\n",
      "Epoch [1/300], Step [213/225], Training Accuracy: 32.2917%, Training Loss: 1.3699%\n",
      "Epoch [1/300], Step [214/225], Training Accuracy: 32.3379%, Training Loss: 1.3695%\n",
      "Epoch [1/300], Step [215/225], Training Accuracy: 32.3038%, Training Loss: 1.3695%\n",
      "Epoch [1/300], Step [216/225], Training Accuracy: 32.2772%, Training Loss: 1.3696%\n",
      "Epoch [1/300], Step [217/225], Training Accuracy: 32.3085%, Training Loss: 1.3687%\n",
      "Epoch [1/300], Step [218/225], Training Accuracy: 32.3179%, Training Loss: 1.3686%\n",
      "Epoch [1/300], Step [219/225], Training Accuracy: 32.3701%, Training Loss: 1.3679%\n",
      "Epoch [1/300], Step [220/225], Training Accuracy: 32.3793%, Training Loss: 1.3677%\n",
      "Epoch [1/300], Step [221/225], Training Accuracy: 32.3388%, Training Loss: 1.3680%\n",
      "Epoch [1/300], Step [222/225], Training Accuracy: 32.3620%, Training Loss: 1.3677%\n",
      "Epoch [1/300], Step [223/225], Training Accuracy: 32.3711%, Training Loss: 1.3677%\n",
      "Epoch [1/300], Step [224/225], Training Accuracy: 32.3730%, Training Loss: 1.3673%\n",
      "Epoch [1/300], Step [225/225], Training Accuracy: 32.3722%, Training Loss: 1.3672%\n",
      "Epoch [2/300], Step [1/225], Training Accuracy: 40.6250%, Training Loss: 1.2490%\n",
      "Epoch [2/300], Step [2/225], Training Accuracy: 39.0625%, Training Loss: 1.2718%\n",
      "Epoch [2/300], Step [3/225], Training Accuracy: 38.0208%, Training Loss: 1.2995%\n",
      "Epoch [2/300], Step [4/225], Training Accuracy: 35.9375%, Training Loss: 1.3397%\n",
      "Epoch [2/300], Step [5/225], Training Accuracy: 36.5625%, Training Loss: 1.3302%\n",
      "Epoch [2/300], Step [6/225], Training Accuracy: 35.4167%, Training Loss: 1.3344%\n",
      "Epoch [2/300], Step [7/225], Training Accuracy: 34.8214%, Training Loss: 1.3561%\n",
      "Epoch [2/300], Step [8/225], Training Accuracy: 34.1797%, Training Loss: 1.3535%\n",
      "Epoch [2/300], Step [9/225], Training Accuracy: 33.3333%, Training Loss: 1.3518%\n",
      "Epoch [2/300], Step [10/225], Training Accuracy: 32.6562%, Training Loss: 1.3465%\n",
      "Epoch [2/300], Step [11/225], Training Accuracy: 32.9545%, Training Loss: 1.3463%\n",
      "Epoch [2/300], Step [12/225], Training Accuracy: 33.5938%, Training Loss: 1.3353%\n",
      "Epoch [2/300], Step [13/225], Training Accuracy: 34.3750%, Training Loss: 1.3343%\n",
      "Epoch [2/300], Step [14/225], Training Accuracy: 33.8170%, Training Loss: 1.3368%\n",
      "Epoch [2/300], Step [15/225], Training Accuracy: 33.9583%, Training Loss: 1.3366%\n",
      "Epoch [2/300], Step [16/225], Training Accuracy: 33.7891%, Training Loss: 1.3340%\n",
      "Epoch [2/300], Step [17/225], Training Accuracy: 34.0993%, Training Loss: 1.3309%\n",
      "Epoch [2/300], Step [18/225], Training Accuracy: 33.7674%, Training Loss: 1.3314%\n",
      "Epoch [2/300], Step [19/225], Training Accuracy: 33.4704%, Training Loss: 1.3304%\n",
      "Epoch [2/300], Step [20/225], Training Accuracy: 33.5156%, Training Loss: 1.3294%\n",
      "Epoch [2/300], Step [21/225], Training Accuracy: 33.6310%, Training Loss: 1.3280%\n",
      "Epoch [2/300], Step [22/225], Training Accuracy: 33.4517%, Training Loss: 1.3254%\n",
      "Epoch [2/300], Step [23/225], Training Accuracy: 33.6957%, Training Loss: 1.3221%\n",
      "Epoch [2/300], Step [24/225], Training Accuracy: 33.7240%, Training Loss: 1.3237%\n",
      "Epoch [2/300], Step [25/225], Training Accuracy: 34.1250%, Training Loss: 1.3239%\n",
      "Epoch [2/300], Step [26/225], Training Accuracy: 33.9543%, Training Loss: 1.3232%\n",
      "Epoch [2/300], Step [27/225], Training Accuracy: 34.0278%, Training Loss: 1.3232%\n",
      "Epoch [2/300], Step [28/225], Training Accuracy: 34.2634%, Training Loss: 1.3202%\n",
      "Epoch [2/300], Step [29/225], Training Accuracy: 34.7522%, Training Loss: 1.3150%\n",
      "Epoch [2/300], Step [30/225], Training Accuracy: 34.8958%, Training Loss: 1.3162%\n",
      "Epoch [2/300], Step [31/225], Training Accuracy: 35.0302%, Training Loss: 1.3173%\n",
      "Epoch [2/300], Step [32/225], Training Accuracy: 35.4004%, Training Loss: 1.3154%\n",
      "Epoch [2/300], Step [33/225], Training Accuracy: 35.3220%, Training Loss: 1.3146%\n",
      "Epoch [2/300], Step [34/225], Training Accuracy: 35.2482%, Training Loss: 1.3142%\n",
      "Epoch [2/300], Step [35/225], Training Accuracy: 35.2232%, Training Loss: 1.3141%\n",
      "Epoch [2/300], Step [36/225], Training Accuracy: 35.3733%, Training Loss: 1.3123%\n",
      "Epoch [2/300], Step [37/225], Training Accuracy: 35.4307%, Training Loss: 1.3099%\n",
      "Epoch [2/300], Step [38/225], Training Accuracy: 35.4852%, Training Loss: 1.3075%\n",
      "Epoch [2/300], Step [39/225], Training Accuracy: 35.6971%, Training Loss: 1.3039%\n",
      "Epoch [2/300], Step [40/225], Training Accuracy: 35.6641%, Training Loss: 1.3031%\n",
      "Epoch [2/300], Step [41/225], Training Accuracy: 35.5564%, Training Loss: 1.3039%\n",
      "Epoch [2/300], Step [42/225], Training Accuracy: 35.7143%, Training Loss: 1.3022%\n",
      "Epoch [2/300], Step [43/225], Training Accuracy: 35.6831%, Training Loss: 1.3015%\n",
      "Epoch [2/300], Step [44/225], Training Accuracy: 35.8665%, Training Loss: 1.2988%\n",
      "Epoch [2/300], Step [45/225], Training Accuracy: 36.0417%, Training Loss: 1.2981%\n",
      "Epoch [2/300], Step [46/225], Training Accuracy: 36.1073%, Training Loss: 1.2969%\n",
      "Epoch [2/300], Step [47/225], Training Accuracy: 36.1037%, Training Loss: 1.2978%\n",
      "Epoch [2/300], Step [48/225], Training Accuracy: 36.1328%, Training Loss: 1.2965%\n",
      "Epoch [2/300], Step [49/225], Training Accuracy: 36.2245%, Training Loss: 1.2961%\n",
      "Epoch [2/300], Step [50/225], Training Accuracy: 36.0312%, Training Loss: 1.2989%\n",
      "Epoch [2/300], Step [51/225], Training Accuracy: 36.1826%, Training Loss: 1.2987%\n",
      "Epoch [2/300], Step [52/225], Training Accuracy: 36.0877%, Training Loss: 1.3002%\n",
      "Epoch [2/300], Step [53/225], Training Accuracy: 36.2028%, Training Loss: 1.2995%\n",
      "Epoch [2/300], Step [54/225], Training Accuracy: 36.3137%, Training Loss: 1.3001%\n",
      "Epoch [2/300], Step [55/225], Training Accuracy: 36.4205%, Training Loss: 1.2993%\n",
      "Epoch [2/300], Step [56/225], Training Accuracy: 36.3839%, Training Loss: 1.2991%\n",
      "Epoch [2/300], Step [57/225], Training Accuracy: 36.3761%, Training Loss: 1.2984%\n",
      "Epoch [2/300], Step [58/225], Training Accuracy: 36.4494%, Training Loss: 1.2984%\n",
      "Epoch [2/300], Step [59/225], Training Accuracy: 36.4672%, Training Loss: 1.2983%\n",
      "Epoch [2/300], Step [60/225], Training Accuracy: 36.5625%, Training Loss: 1.2970%\n",
      "Epoch [2/300], Step [61/225], Training Accuracy: 36.6291%, Training Loss: 1.2960%\n",
      "Epoch [2/300], Step [62/225], Training Accuracy: 36.6431%, Training Loss: 1.2959%\n",
      "Epoch [2/300], Step [63/225], Training Accuracy: 36.4831%, Training Loss: 1.2969%\n",
      "Epoch [2/300], Step [64/225], Training Accuracy: 36.4014%, Training Loss: 1.2973%\n",
      "Epoch [2/300], Step [65/225], Training Accuracy: 36.4423%, Training Loss: 1.2967%\n",
      "Epoch [2/300], Step [66/225], Training Accuracy: 36.4820%, Training Loss: 1.2963%\n",
      "Epoch [2/300], Step [67/225], Training Accuracy: 36.3806%, Training Loss: 1.2963%\n",
      "Epoch [2/300], Step [68/225], Training Accuracy: 36.3971%, Training Loss: 1.2952%\n",
      "Epoch [2/300], Step [69/225], Training Accuracy: 36.3451%, Training Loss: 1.2945%\n",
      "Epoch [2/300], Step [70/225], Training Accuracy: 36.2277%, Training Loss: 1.2934%\n",
      "Epoch [2/300], Step [71/225], Training Accuracy: 36.3776%, Training Loss: 1.2936%\n",
      "Epoch [2/300], Step [72/225], Training Accuracy: 36.2196%, Training Loss: 1.2964%\n",
      "Epoch [2/300], Step [73/225], Training Accuracy: 36.2586%, Training Loss: 1.2967%\n",
      "Epoch [2/300], Step [74/225], Training Accuracy: 36.3598%, Training Loss: 1.2964%\n",
      "Epoch [2/300], Step [75/225], Training Accuracy: 36.4583%, Training Loss: 1.2957%\n",
      "Epoch [2/300], Step [76/225], Training Accuracy: 36.4720%, Training Loss: 1.2954%\n",
      "Epoch [2/300], Step [77/225], Training Accuracy: 36.4448%, Training Loss: 1.2944%\n",
      "Epoch [2/300], Step [78/225], Training Accuracy: 36.4383%, Training Loss: 1.2930%\n",
      "Epoch [2/300], Step [79/225], Training Accuracy: 36.3331%, Training Loss: 1.2940%\n",
      "Epoch [2/300], Step [80/225], Training Accuracy: 36.3867%, Training Loss: 1.2943%\n",
      "Epoch [2/300], Step [81/225], Training Accuracy: 36.4390%, Training Loss: 1.2940%\n",
      "Epoch [2/300], Step [82/225], Training Accuracy: 36.3948%, Training Loss: 1.2956%\n",
      "Epoch [2/300], Step [83/225], Training Accuracy: 36.5023%, Training Loss: 1.2946%\n",
      "Epoch [2/300], Step [84/225], Training Accuracy: 36.6257%, Training Loss: 1.2948%\n",
      "Epoch [2/300], Step [85/225], Training Accuracy: 36.7096%, Training Loss: 1.2938%\n",
      "Epoch [2/300], Step [86/225], Training Accuracy: 36.7914%, Training Loss: 1.2930%\n",
      "Epoch [2/300], Step [87/225], Training Accuracy: 36.7816%, Training Loss: 1.2927%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [88/225], Training Accuracy: 36.7543%, Training Loss: 1.2924%\n",
      "Epoch [2/300], Step [89/225], Training Accuracy: 36.6397%, Training Loss: 1.2929%\n",
      "Epoch [2/300], Step [90/225], Training Accuracy: 36.7361%, Training Loss: 1.2926%\n",
      "Epoch [2/300], Step [91/225], Training Accuracy: 36.7960%, Training Loss: 1.2919%\n",
      "Epoch [2/300], Step [92/225], Training Accuracy: 36.8376%, Training Loss: 1.2918%\n",
      "Epoch [2/300], Step [93/225], Training Accuracy: 36.9792%, Training Loss: 1.2908%\n",
      "Epoch [2/300], Step [94/225], Training Accuracy: 37.0346%, Training Loss: 1.2890%\n",
      "Epoch [2/300], Step [95/225], Training Accuracy: 36.9737%, Training Loss: 1.2903%\n",
      "Epoch [2/300], Step [96/225], Training Accuracy: 37.0605%, Training Loss: 1.2888%\n",
      "Epoch [2/300], Step [97/225], Training Accuracy: 37.1295%, Training Loss: 1.2888%\n",
      "Epoch [2/300], Step [98/225], Training Accuracy: 37.0376%, Training Loss: 1.2902%\n",
      "Epoch [2/300], Step [99/225], Training Accuracy: 37.0265%, Training Loss: 1.2899%\n",
      "Epoch [2/300], Step [100/225], Training Accuracy: 36.9688%, Training Loss: 1.2897%\n",
      "Epoch [2/300], Step [101/225], Training Accuracy: 37.0204%, Training Loss: 1.2893%\n",
      "Epoch [2/300], Step [102/225], Training Accuracy: 37.0711%, Training Loss: 1.2886%\n",
      "Epoch [2/300], Step [103/225], Training Accuracy: 37.1814%, Training Loss: 1.2882%\n",
      "Epoch [2/300], Step [104/225], Training Accuracy: 37.1845%, Training Loss: 1.2876%\n",
      "Epoch [2/300], Step [105/225], Training Accuracy: 37.2470%, Training Loss: 1.2871%\n",
      "Epoch [2/300], Step [106/225], Training Accuracy: 37.2199%, Training Loss: 1.2869%\n",
      "Epoch [2/300], Step [107/225], Training Accuracy: 37.2225%, Training Loss: 1.2866%\n",
      "Epoch [2/300], Step [108/225], Training Accuracy: 37.2830%, Training Loss: 1.2860%\n",
      "Epoch [2/300], Step [109/225], Training Accuracy: 37.2563%, Training Loss: 1.2858%\n",
      "Epoch [2/300], Step [110/225], Training Accuracy: 37.3295%, Training Loss: 1.2852%\n",
      "Epoch [2/300], Step [111/225], Training Accuracy: 37.4015%, Training Loss: 1.2853%\n",
      "Epoch [2/300], Step [112/225], Training Accuracy: 37.4721%, Training Loss: 1.2837%\n",
      "Epoch [2/300], Step [113/225], Training Accuracy: 37.4170%, Training Loss: 1.2850%\n",
      "Epoch [2/300], Step [114/225], Training Accuracy: 37.5411%, Training Loss: 1.2834%\n",
      "Epoch [2/300], Step [115/225], Training Accuracy: 37.6359%, Training Loss: 1.2826%\n",
      "Epoch [2/300], Step [116/225], Training Accuracy: 37.5943%, Training Loss: 1.2831%\n",
      "Epoch [2/300], Step [117/225], Training Accuracy: 37.4199%, Training Loss: 1.2851%\n",
      "Epoch [2/300], Step [118/225], Training Accuracy: 37.3941%, Training Loss: 1.2846%\n",
      "Epoch [2/300], Step [119/225], Training Accuracy: 37.3424%, Training Loss: 1.2846%\n",
      "Epoch [2/300], Step [120/225], Training Accuracy: 37.4089%, Training Loss: 1.2838%\n",
      "Epoch [2/300], Step [121/225], Training Accuracy: 37.4096%, Training Loss: 1.2839%\n",
      "Epoch [2/300], Step [122/225], Training Accuracy: 37.5000%, Training Loss: 1.2831%\n",
      "Epoch [2/300], Step [123/225], Training Accuracy: 37.4873%, Training Loss: 1.2828%\n",
      "Epoch [2/300], Step [124/225], Training Accuracy: 37.5126%, Training Loss: 1.2826%\n",
      "Epoch [2/300], Step [125/225], Training Accuracy: 37.5000%, Training Loss: 1.2824%\n",
      "Epoch [2/300], Step [126/225], Training Accuracy: 37.4380%, Training Loss: 1.2820%\n",
      "Epoch [2/300], Step [127/225], Training Accuracy: 37.4016%, Training Loss: 1.2821%\n",
      "Epoch [2/300], Step [128/225], Training Accuracy: 37.3657%, Training Loss: 1.2828%\n",
      "Epoch [2/300], Step [129/225], Training Accuracy: 37.3789%, Training Loss: 1.2825%\n",
      "Epoch [2/300], Step [130/225], Training Accuracy: 37.2837%, Training Loss: 1.2827%\n",
      "Epoch [2/300], Step [131/225], Training Accuracy: 37.2495%, Training Loss: 1.2829%\n",
      "Epoch [2/300], Step [132/225], Training Accuracy: 37.2869%, Training Loss: 1.2821%\n",
      "Epoch [2/300], Step [133/225], Training Accuracy: 37.3003%, Training Loss: 1.2818%\n",
      "Epoch [2/300], Step [134/225], Training Accuracy: 37.2085%, Training Loss: 1.2813%\n",
      "Epoch [2/300], Step [135/225], Training Accuracy: 37.2454%, Training Loss: 1.2808%\n",
      "Epoch [2/300], Step [136/225], Training Accuracy: 37.2817%, Training Loss: 1.2808%\n",
      "Epoch [2/300], Step [137/225], Training Accuracy: 37.2605%, Training Loss: 1.2804%\n",
      "Epoch [2/300], Step [138/225], Training Accuracy: 37.3415%, Training Loss: 1.2800%\n",
      "Epoch [2/300], Step [139/225], Training Accuracy: 37.3201%, Training Loss: 1.2795%\n",
      "Epoch [2/300], Step [140/225], Training Accuracy: 37.2768%, Training Loss: 1.2799%\n",
      "Epoch [2/300], Step [141/225], Training Accuracy: 37.3005%, Training Loss: 1.2796%\n",
      "Epoch [2/300], Step [142/225], Training Accuracy: 37.2689%, Training Loss: 1.2792%\n",
      "Epoch [2/300], Step [143/225], Training Accuracy: 37.2815%, Training Loss: 1.2787%\n",
      "Epoch [2/300], Step [144/225], Training Accuracy: 37.3372%, Training Loss: 1.2782%\n",
      "Epoch [2/300], Step [145/225], Training Accuracy: 37.4569%, Training Loss: 1.2776%\n",
      "Epoch [2/300], Step [146/225], Training Accuracy: 37.4465%, Training Loss: 1.2779%\n",
      "Epoch [2/300], Step [147/225], Training Accuracy: 37.4150%, Training Loss: 1.2780%\n",
      "Epoch [2/300], Step [148/225], Training Accuracy: 37.5211%, Training Loss: 1.2772%\n",
      "Epoch [2/300], Step [149/225], Training Accuracy: 37.5629%, Training Loss: 1.2768%\n",
      "Epoch [2/300], Step [150/225], Training Accuracy: 37.5938%, Training Loss: 1.2762%\n",
      "Epoch [2/300], Step [151/225], Training Accuracy: 37.6449%, Training Loss: 1.2754%\n",
      "Epoch [2/300], Step [152/225], Training Accuracy: 37.6439%, Training Loss: 1.2752%\n",
      "Epoch [2/300], Step [153/225], Training Accuracy: 37.6430%, Training Loss: 1.2748%\n",
      "Epoch [2/300], Step [154/225], Training Accuracy: 37.6420%, Training Loss: 1.2749%\n",
      "Epoch [2/300], Step [155/225], Training Accuracy: 37.6411%, Training Loss: 1.2745%\n",
      "Epoch [2/300], Step [156/225], Training Accuracy: 37.6302%, Training Loss: 1.2744%\n",
      "Epoch [2/300], Step [157/225], Training Accuracy: 37.6592%, Training Loss: 1.2739%\n",
      "Epoch [2/300], Step [158/225], Training Accuracy: 37.6088%, Training Loss: 1.2743%\n",
      "Epoch [2/300], Step [159/225], Training Accuracy: 37.6474%, Training Loss: 1.2735%\n",
      "Epoch [2/300], Step [160/225], Training Accuracy: 37.6660%, Training Loss: 1.2730%\n",
      "Epoch [2/300], Step [161/225], Training Accuracy: 37.6165%, Training Loss: 1.2726%\n",
      "Epoch [2/300], Step [162/225], Training Accuracy: 37.7025%, Training Loss: 1.2717%\n",
      "Epoch [2/300], Step [163/225], Training Accuracy: 37.7301%, Training Loss: 1.2712%\n",
      "Epoch [2/300], Step [164/225], Training Accuracy: 37.7763%, Training Loss: 1.2704%\n",
      "Epoch [2/300], Step [165/225], Training Accuracy: 37.7841%, Training Loss: 1.2703%\n",
      "Epoch [2/300], Step [166/225], Training Accuracy: 37.8012%, Training Loss: 1.2700%\n",
      "Epoch [2/300], Step [167/225], Training Accuracy: 37.8462%, Training Loss: 1.2690%\n",
      "Epoch [2/300], Step [168/225], Training Accuracy: 37.8441%, Training Loss: 1.2686%\n",
      "Epoch [2/300], Step [169/225], Training Accuracy: 37.8606%, Training Loss: 1.2687%\n",
      "Epoch [2/300], Step [170/225], Training Accuracy: 37.8493%, Training Loss: 1.2686%\n",
      "Epoch [2/300], Step [171/225], Training Accuracy: 37.8655%, Training Loss: 1.2680%\n",
      "Epoch [2/300], Step [172/225], Training Accuracy: 37.8906%, Training Loss: 1.2672%\n",
      "Epoch [2/300], Step [173/225], Training Accuracy: 37.8703%, Training Loss: 1.2671%\n",
      "Epoch [2/300], Step [174/225], Training Accuracy: 37.9041%, Training Loss: 1.2670%\n",
      "Epoch [2/300], Step [175/225], Training Accuracy: 37.9375%, Training Loss: 1.2665%\n",
      "Epoch [2/300], Step [176/225], Training Accuracy: 37.9350%, Training Loss: 1.2667%\n",
      "Epoch [2/300], Step [177/225], Training Accuracy: 37.9502%, Training Loss: 1.2663%\n",
      "Epoch [2/300], Step [178/225], Training Accuracy: 37.9477%, Training Loss: 1.2662%\n",
      "Epoch [2/300], Step [179/225], Training Accuracy: 38.0150%, Training Loss: 1.2657%\n",
      "Epoch [2/300], Step [180/225], Training Accuracy: 38.0903%, Training Loss: 1.2650%\n",
      "Epoch [2/300], Step [181/225], Training Accuracy: 38.1302%, Training Loss: 1.2647%\n",
      "Epoch [2/300], Step [182/225], Training Accuracy: 38.1954%, Training Loss: 1.2643%\n",
      "Epoch [2/300], Step [183/225], Training Accuracy: 38.1916%, Training Loss: 1.2639%\n",
      "Epoch [2/300], Step [184/225], Training Accuracy: 38.1878%, Training Loss: 1.2636%\n",
      "Epoch [2/300], Step [185/225], Training Accuracy: 38.2432%, Training Loss: 1.2627%\n",
      "Epoch [2/300], Step [186/225], Training Accuracy: 38.2560%, Training Loss: 1.2621%\n",
      "Epoch [2/300], Step [187/225], Training Accuracy: 38.2687%, Training Loss: 1.2622%\n",
      "Epoch [2/300], Step [188/225], Training Accuracy: 38.2896%, Training Loss: 1.2620%\n",
      "Epoch [2/300], Step [189/225], Training Accuracy: 38.3185%, Training Loss: 1.2613%\n",
      "Epoch [2/300], Step [190/225], Training Accuracy: 38.3059%, Training Loss: 1.2616%\n",
      "Epoch [2/300], Step [191/225], Training Accuracy: 38.3426%, Training Loss: 1.2612%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [192/225], Training Accuracy: 38.4277%, Training Loss: 1.2604%\n",
      "Epoch [2/300], Step [193/225], Training Accuracy: 38.3986%, Training Loss: 1.2603%\n",
      "Epoch [2/300], Step [194/225], Training Accuracy: 38.4101%, Training Loss: 1.2598%\n",
      "Epoch [2/300], Step [195/225], Training Accuracy: 38.4535%, Training Loss: 1.2595%\n",
      "Epoch [2/300], Step [196/225], Training Accuracy: 38.4327%, Training Loss: 1.2596%\n",
      "Epoch [2/300], Step [197/225], Training Accuracy: 38.4280%, Training Loss: 1.2595%\n",
      "Epoch [2/300], Step [198/225], Training Accuracy: 38.5417%, Training Loss: 1.2580%\n",
      "Epoch [2/300], Step [199/225], Training Accuracy: 38.5914%, Training Loss: 1.2569%\n",
      "Epoch [2/300], Step [200/225], Training Accuracy: 38.6016%, Training Loss: 1.2566%\n",
      "Epoch [2/300], Step [201/225], Training Accuracy: 38.5805%, Training Loss: 1.2566%\n",
      "Epoch [2/300], Step [202/225], Training Accuracy: 38.5907%, Training Loss: 1.2567%\n",
      "Epoch [2/300], Step [203/225], Training Accuracy: 38.5622%, Training Loss: 1.2564%\n",
      "Epoch [2/300], Step [204/225], Training Accuracy: 38.5876%, Training Loss: 1.2555%\n",
      "Epoch [2/300], Step [205/225], Training Accuracy: 38.6585%, Training Loss: 1.2546%\n",
      "Epoch [2/300], Step [206/225], Training Accuracy: 38.6757%, Training Loss: 1.2546%\n",
      "Epoch [2/300], Step [207/225], Training Accuracy: 38.6700%, Training Loss: 1.2544%\n",
      "Epoch [2/300], Step [208/225], Training Accuracy: 38.7245%, Training Loss: 1.2537%\n",
      "Epoch [2/300], Step [209/225], Training Accuracy: 38.7635%, Training Loss: 1.2533%\n",
      "Epoch [2/300], Step [210/225], Training Accuracy: 38.7277%, Training Loss: 1.2534%\n",
      "Epoch [2/300], Step [211/225], Training Accuracy: 38.8107%, Training Loss: 1.2524%\n",
      "Epoch [2/300], Step [212/225], Training Accuracy: 38.8561%, Training Loss: 1.2524%\n",
      "Epoch [2/300], Step [213/225], Training Accuracy: 38.8644%, Training Loss: 1.2525%\n",
      "Epoch [2/300], Step [214/225], Training Accuracy: 38.9092%, Training Loss: 1.2520%\n",
      "Epoch [2/300], Step [215/225], Training Accuracy: 38.9026%, Training Loss: 1.2524%\n",
      "Epoch [2/300], Step [216/225], Training Accuracy: 38.9251%, Training Loss: 1.2522%\n",
      "Epoch [2/300], Step [217/225], Training Accuracy: 38.9257%, Training Loss: 1.2517%\n",
      "Epoch [2/300], Step [218/225], Training Accuracy: 38.9192%, Training Loss: 1.2516%\n",
      "Epoch [2/300], Step [219/225], Training Accuracy: 38.9555%, Training Loss: 1.2506%\n",
      "Epoch [2/300], Step [220/225], Training Accuracy: 38.9702%, Training Loss: 1.2503%\n",
      "Epoch [2/300], Step [221/225], Training Accuracy: 38.9777%, Training Loss: 1.2503%\n",
      "Epoch [2/300], Step [222/225], Training Accuracy: 39.0343%, Training Loss: 1.2498%\n",
      "Epoch [2/300], Step [223/225], Training Accuracy: 39.0765%, Training Loss: 1.2496%\n",
      "Epoch [2/300], Step [224/225], Training Accuracy: 39.1044%, Training Loss: 1.2490%\n",
      "Epoch [2/300], Step [225/225], Training Accuracy: 39.0981%, Training Loss: 1.2487%\n",
      "Epoch [3/300], Step [1/225], Training Accuracy: 48.4375%, Training Loss: 1.0644%\n",
      "Epoch [3/300], Step [2/225], Training Accuracy: 46.8750%, Training Loss: 1.1403%\n",
      "Epoch [3/300], Step [3/225], Training Accuracy: 42.7083%, Training Loss: 1.2057%\n",
      "Epoch [3/300], Step [4/225], Training Accuracy: 42.5781%, Training Loss: 1.1941%\n",
      "Epoch [3/300], Step [5/225], Training Accuracy: 43.1250%, Training Loss: 1.1827%\n",
      "Epoch [3/300], Step [6/225], Training Accuracy: 42.1875%, Training Loss: 1.1889%\n",
      "Epoch [3/300], Step [7/225], Training Accuracy: 42.4107%, Training Loss: 1.1955%\n",
      "Epoch [3/300], Step [8/225], Training Accuracy: 41.7969%, Training Loss: 1.1965%\n",
      "Epoch [3/300], Step [9/225], Training Accuracy: 41.4931%, Training Loss: 1.1946%\n",
      "Epoch [3/300], Step [10/225], Training Accuracy: 40.6250%, Training Loss: 1.1970%\n",
      "Epoch [3/300], Step [11/225], Training Accuracy: 40.6250%, Training Loss: 1.2026%\n",
      "Epoch [3/300], Step [12/225], Training Accuracy: 41.5365%, Training Loss: 1.1914%\n",
      "Epoch [3/300], Step [13/225], Training Accuracy: 42.5481%, Training Loss: 1.1846%\n",
      "Epoch [3/300], Step [14/225], Training Accuracy: 42.8571%, Training Loss: 1.1914%\n",
      "Epoch [3/300], Step [15/225], Training Accuracy: 43.2292%, Training Loss: 1.1904%\n",
      "Epoch [3/300], Step [16/225], Training Accuracy: 43.1641%, Training Loss: 1.1856%\n",
      "Epoch [3/300], Step [17/225], Training Accuracy: 44.1176%, Training Loss: 1.1762%\n",
      "Epoch [3/300], Step [18/225], Training Accuracy: 43.9236%, Training Loss: 1.1767%\n",
      "Epoch [3/300], Step [19/225], Training Accuracy: 43.6678%, Training Loss: 1.1768%\n",
      "Epoch [3/300], Step [20/225], Training Accuracy: 43.6719%, Training Loss: 1.1769%\n",
      "Epoch [3/300], Step [21/225], Training Accuracy: 43.6012%, Training Loss: 1.1791%\n",
      "Epoch [3/300], Step [22/225], Training Accuracy: 43.1108%, Training Loss: 1.1806%\n",
      "Epoch [3/300], Step [23/225], Training Accuracy: 43.2065%, Training Loss: 1.1769%\n",
      "Epoch [3/300], Step [24/225], Training Accuracy: 42.9036%, Training Loss: 1.1797%\n",
      "Epoch [3/300], Step [25/225], Training Accuracy: 43.4375%, Training Loss: 1.1750%\n",
      "Epoch [3/300], Step [26/225], Training Accuracy: 43.6899%, Training Loss: 1.1705%\n",
      "Epoch [3/300], Step [27/225], Training Accuracy: 43.9236%, Training Loss: 1.1663%\n",
      "Epoch [3/300], Step [28/225], Training Accuracy: 44.1406%, Training Loss: 1.1633%\n",
      "Epoch [3/300], Step [29/225], Training Accuracy: 44.8276%, Training Loss: 1.1563%\n",
      "Epoch [3/300], Step [30/225], Training Accuracy: 44.4792%, Training Loss: 1.1603%\n",
      "Epoch [3/300], Step [31/225], Training Accuracy: 44.2036%, Training Loss: 1.1670%\n",
      "Epoch [3/300], Step [32/225], Training Accuracy: 44.4336%, Training Loss: 1.1644%\n",
      "Epoch [3/300], Step [33/225], Training Accuracy: 44.3655%, Training Loss: 1.1630%\n",
      "Epoch [3/300], Step [34/225], Training Accuracy: 44.3015%, Training Loss: 1.1645%\n",
      "Epoch [3/300], Step [35/225], Training Accuracy: 44.2411%, Training Loss: 1.1677%\n",
      "Epoch [3/300], Step [36/225], Training Accuracy: 44.2274%, Training Loss: 1.1670%\n",
      "Epoch [3/300], Step [37/225], Training Accuracy: 44.1723%, Training Loss: 1.1672%\n",
      "Epoch [3/300], Step [38/225], Training Accuracy: 44.1612%, Training Loss: 1.1657%\n",
      "Epoch [3/300], Step [39/225], Training Accuracy: 44.3109%, Training Loss: 1.1637%\n",
      "Epoch [3/300], Step [40/225], Training Accuracy: 44.1797%, Training Loss: 1.1658%\n",
      "Epoch [3/300], Step [41/225], Training Accuracy: 43.9787%, Training Loss: 1.1660%\n",
      "Epoch [3/300], Step [42/225], Training Accuracy: 44.0848%, Training Loss: 1.1649%\n",
      "Epoch [3/300], Step [43/225], Training Accuracy: 44.0044%, Training Loss: 1.1645%\n",
      "Epoch [3/300], Step [44/225], Training Accuracy: 44.1406%, Training Loss: 1.1628%\n",
      "Epoch [3/300], Step [45/225], Training Accuracy: 44.3056%, Training Loss: 1.1603%\n",
      "Epoch [3/300], Step [46/225], Training Accuracy: 44.3614%, Training Loss: 1.1591%\n",
      "Epoch [3/300], Step [47/225], Training Accuracy: 44.3484%, Training Loss: 1.1581%\n",
      "Epoch [3/300], Step [48/225], Training Accuracy: 44.3359%, Training Loss: 1.1564%\n",
      "Epoch [3/300], Step [49/225], Training Accuracy: 44.3878%, Training Loss: 1.1563%\n",
      "Epoch [3/300], Step [50/225], Training Accuracy: 44.3750%, Training Loss: 1.1558%\n",
      "Epoch [3/300], Step [51/225], Training Accuracy: 44.6078%, Training Loss: 1.1541%\n",
      "Epoch [3/300], Step [52/225], Training Accuracy: 44.5312%, Training Loss: 1.1554%\n",
      "Epoch [3/300], Step [53/225], Training Accuracy: 44.5165%, Training Loss: 1.1551%\n",
      "Epoch [3/300], Step [54/225], Training Accuracy: 44.5891%, Training Loss: 1.1538%\n",
      "Epoch [3/300], Step [55/225], Training Accuracy: 44.4602%, Training Loss: 1.1544%\n",
      "Epoch [3/300], Step [56/225], Training Accuracy: 44.3359%, Training Loss: 1.1542%\n",
      "Epoch [3/300], Step [57/225], Training Accuracy: 44.4079%, Training Loss: 1.1520%\n",
      "Epoch [3/300], Step [58/225], Training Accuracy: 44.1541%, Training Loss: 1.1536%\n",
      "Epoch [3/300], Step [59/225], Training Accuracy: 44.2532%, Training Loss: 1.1525%\n",
      "Epoch [3/300], Step [60/225], Training Accuracy: 44.4792%, Training Loss: 1.1502%\n",
      "Epoch [3/300], Step [61/225], Training Accuracy: 44.5953%, Training Loss: 1.1486%\n",
      "Epoch [3/300], Step [62/225], Training Accuracy: 44.4556%, Training Loss: 1.1482%\n",
      "Epoch [3/300], Step [63/225], Training Accuracy: 44.2460%, Training Loss: 1.1489%\n",
      "Epoch [3/300], Step [64/225], Training Accuracy: 44.1162%, Training Loss: 1.1488%\n",
      "Epoch [3/300], Step [65/225], Training Accuracy: 44.0865%, Training Loss: 1.1480%\n",
      "Epoch [3/300], Step [66/225], Training Accuracy: 44.2472%, Training Loss: 1.1463%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [67/225], Training Accuracy: 44.2397%, Training Loss: 1.1461%\n",
      "Epoch [3/300], Step [68/225], Training Accuracy: 44.1176%, Training Loss: 1.1471%\n",
      "Epoch [3/300], Step [69/225], Training Accuracy: 44.0444%, Training Loss: 1.1461%\n",
      "Epoch [3/300], Step [70/225], Training Accuracy: 43.9955%, Training Loss: 1.1463%\n",
      "Epoch [3/300], Step [71/225], Training Accuracy: 44.1901%, Training Loss: 1.1460%\n",
      "Epoch [3/300], Step [72/225], Training Accuracy: 44.1623%, Training Loss: 1.1461%\n",
      "Epoch [3/300], Step [73/225], Training Accuracy: 44.1139%, Training Loss: 1.1471%\n",
      "Epoch [3/300], Step [74/225], Training Accuracy: 44.2990%, Training Loss: 1.1443%\n",
      "Epoch [3/300], Step [75/225], Training Accuracy: 44.3958%, Training Loss: 1.1427%\n",
      "Epoch [3/300], Step [76/225], Training Accuracy: 44.3873%, Training Loss: 1.1426%\n",
      "Epoch [3/300], Step [77/225], Training Accuracy: 44.4399%, Training Loss: 1.1418%\n",
      "Epoch [3/300], Step [78/225], Training Accuracy: 44.4111%, Training Loss: 1.1409%\n",
      "Epoch [3/300], Step [79/225], Training Accuracy: 44.3631%, Training Loss: 1.1418%\n",
      "Epoch [3/300], Step [80/225], Training Accuracy: 44.3945%, Training Loss: 1.1408%\n",
      "Epoch [3/300], Step [81/225], Training Accuracy: 44.5216%, Training Loss: 1.1398%\n",
      "Epoch [3/300], Step [82/225], Training Accuracy: 44.5503%, Training Loss: 1.1387%\n",
      "Epoch [3/300], Step [83/225], Training Accuracy: 44.6348%, Training Loss: 1.1373%\n",
      "Epoch [3/300], Step [84/225], Training Accuracy: 44.7359%, Training Loss: 1.1361%\n",
      "Epoch [3/300], Step [85/225], Training Accuracy: 44.7426%, Training Loss: 1.1355%\n",
      "Epoch [3/300], Step [86/225], Training Accuracy: 44.7856%, Training Loss: 1.1355%\n",
      "Epoch [3/300], Step [87/225], Training Accuracy: 44.8455%, Training Loss: 1.1345%\n",
      "Epoch [3/300], Step [88/225], Training Accuracy: 44.8153%, Training Loss: 1.1354%\n",
      "Epoch [3/300], Step [89/225], Training Accuracy: 44.7331%, Training Loss: 1.1374%\n",
      "Epoch [3/300], Step [90/225], Training Accuracy: 44.7222%, Training Loss: 1.1379%\n",
      "Epoch [3/300], Step [91/225], Training Accuracy: 44.6085%, Training Loss: 1.1386%\n",
      "Epoch [3/300], Step [92/225], Training Accuracy: 44.5822%, Training Loss: 1.1393%\n",
      "Epoch [3/300], Step [93/225], Training Accuracy: 44.6237%, Training Loss: 1.1392%\n",
      "Epoch [3/300], Step [94/225], Training Accuracy: 44.7806%, Training Loss: 1.1371%\n",
      "Epoch [3/300], Step [95/225], Training Accuracy: 44.7533%, Training Loss: 1.1382%\n",
      "Epoch [3/300], Step [96/225], Training Accuracy: 44.9219%, Training Loss: 1.1372%\n",
      "Epoch [3/300], Step [97/225], Training Accuracy: 44.9581%, Training Loss: 1.1364%\n",
      "Epoch [3/300], Step [98/225], Training Accuracy: 44.9139%, Training Loss: 1.1358%\n",
      "Epoch [3/300], Step [99/225], Training Accuracy: 44.9968%, Training Loss: 1.1355%\n",
      "Epoch [3/300], Step [100/225], Training Accuracy: 45.0000%, Training Loss: 1.1355%\n",
      "Epoch [3/300], Step [101/225], Training Accuracy: 45.0186%, Training Loss: 1.1343%\n",
      "Epoch [3/300], Step [102/225], Training Accuracy: 45.0061%, Training Loss: 1.1337%\n",
      "Epoch [3/300], Step [103/225], Training Accuracy: 45.0091%, Training Loss: 1.1335%\n",
      "Epoch [3/300], Step [104/225], Training Accuracy: 44.9219%, Training Loss: 1.1334%\n",
      "Epoch [3/300], Step [105/225], Training Accuracy: 44.9107%, Training Loss: 1.1338%\n",
      "Epoch [3/300], Step [106/225], Training Accuracy: 44.8703%, Training Loss: 1.1336%\n",
      "Epoch [3/300], Step [107/225], Training Accuracy: 44.8306%, Training Loss: 1.1342%\n",
      "Epoch [3/300], Step [108/225], Training Accuracy: 44.7917%, Training Loss: 1.1346%\n",
      "Epoch [3/300], Step [109/225], Training Accuracy: 44.7248%, Training Loss: 1.1346%\n",
      "Epoch [3/300], Step [110/225], Training Accuracy: 44.7869%, Training Loss: 1.1339%\n",
      "Epoch [3/300], Step [111/225], Training Accuracy: 44.7494%, Training Loss: 1.1346%\n",
      "Epoch [3/300], Step [112/225], Training Accuracy: 44.7824%, Training Loss: 1.1343%\n",
      "Epoch [3/300], Step [113/225], Training Accuracy: 44.8009%, Training Loss: 1.1343%\n",
      "Epoch [3/300], Step [114/225], Training Accuracy: 44.8191%, Training Loss: 1.1334%\n",
      "Epoch [3/300], Step [115/225], Training Accuracy: 44.9049%, Training Loss: 1.1323%\n",
      "Epoch [3/300], Step [116/225], Training Accuracy: 44.9353%, Training Loss: 1.1320%\n",
      "Epoch [3/300], Step [117/225], Training Accuracy: 44.8451%, Training Loss: 1.1329%\n",
      "Epoch [3/300], Step [118/225], Training Accuracy: 44.8623%, Training Loss: 1.1333%\n",
      "Epoch [3/300], Step [119/225], Training Accuracy: 44.8923%, Training Loss: 1.1328%\n",
      "Epoch [3/300], Step [120/225], Training Accuracy: 44.9219%, Training Loss: 1.1331%\n",
      "Epoch [3/300], Step [121/225], Training Accuracy: 44.9251%, Training Loss: 1.1335%\n",
      "Epoch [3/300], Step [122/225], Training Accuracy: 44.9411%, Training Loss: 1.1327%\n",
      "Epoch [3/300], Step [123/225], Training Accuracy: 44.9314%, Training Loss: 1.1324%\n",
      "Epoch [3/300], Step [124/225], Training Accuracy: 44.9471%, Training Loss: 1.1322%\n",
      "Epoch [3/300], Step [125/225], Training Accuracy: 44.8750%, Training Loss: 1.1328%\n",
      "Epoch [3/300], Step [126/225], Training Accuracy: 44.8661%, Training Loss: 1.1328%\n",
      "Epoch [3/300], Step [127/225], Training Accuracy: 44.7958%, Training Loss: 1.1331%\n",
      "Epoch [3/300], Step [128/225], Training Accuracy: 44.7510%, Training Loss: 1.1330%\n",
      "Epoch [3/300], Step [129/225], Training Accuracy: 44.7432%, Training Loss: 1.1326%\n",
      "Epoch [3/300], Step [130/225], Training Accuracy: 44.6995%, Training Loss: 1.1328%\n",
      "Epoch [3/300], Step [131/225], Training Accuracy: 44.6923%, Training Loss: 1.1328%\n",
      "Epoch [3/300], Step [132/225], Training Accuracy: 44.7206%, Training Loss: 1.1323%\n",
      "Epoch [3/300], Step [133/225], Training Accuracy: 44.7368%, Training Loss: 1.1322%\n",
      "Epoch [3/300], Step [134/225], Training Accuracy: 44.7295%, Training Loss: 1.1331%\n",
      "Epoch [3/300], Step [135/225], Training Accuracy: 44.7917%, Training Loss: 1.1325%\n",
      "Epoch [3/300], Step [136/225], Training Accuracy: 44.7955%, Training Loss: 1.1316%\n",
      "Epoch [3/300], Step [137/225], Training Accuracy: 44.8221%, Training Loss: 1.1310%\n",
      "Epoch [3/300], Step [138/225], Training Accuracy: 44.8936%, Training Loss: 1.1301%\n",
      "Epoch [3/300], Step [139/225], Training Accuracy: 44.9640%, Training Loss: 1.1296%\n",
      "Epoch [3/300], Step [140/225], Training Accuracy: 44.9554%, Training Loss: 1.1297%\n",
      "Epoch [3/300], Step [141/225], Training Accuracy: 44.9579%, Training Loss: 1.1295%\n",
      "Epoch [3/300], Step [142/225], Training Accuracy: 44.9494%, Training Loss: 1.1295%\n",
      "Epoch [3/300], Step [143/225], Training Accuracy: 44.9519%, Training Loss: 1.1287%\n",
      "Epoch [3/300], Step [144/225], Training Accuracy: 44.9436%, Training Loss: 1.1286%\n",
      "Epoch [3/300], Step [145/225], Training Accuracy: 45.0216%, Training Loss: 1.1277%\n",
      "Epoch [3/300], Step [146/225], Training Accuracy: 44.9807%, Training Loss: 1.1281%\n",
      "Epoch [3/300], Step [147/225], Training Accuracy: 44.9405%, Training Loss: 1.1281%\n",
      "Epoch [3/300], Step [148/225], Training Accuracy: 44.9641%, Training Loss: 1.1272%\n",
      "Epoch [3/300], Step [149/225], Training Accuracy: 44.9455%, Training Loss: 1.1270%\n",
      "Epoch [3/300], Step [150/225], Training Accuracy: 44.9583%, Training Loss: 1.1263%\n",
      "Epoch [3/300], Step [151/225], Training Accuracy: 44.9710%, Training Loss: 1.1254%\n",
      "Epoch [3/300], Step [152/225], Training Accuracy: 45.0452%, Training Loss: 1.1243%\n",
      "Epoch [3/300], Step [153/225], Training Accuracy: 45.0776%, Training Loss: 1.1233%\n",
      "Epoch [3/300], Step [154/225], Training Accuracy: 45.0588%, Training Loss: 1.1235%\n",
      "Epoch [3/300], Step [155/225], Training Accuracy: 45.0302%, Training Loss: 1.1236%\n",
      "Epoch [3/300], Step [156/225], Training Accuracy: 45.1022%, Training Loss: 1.1229%\n",
      "Epoch [3/300], Step [157/225], Training Accuracy: 45.0537%, Training Loss: 1.1225%\n",
      "Epoch [3/300], Step [158/225], Training Accuracy: 44.9763%, Training Loss: 1.1233%\n",
      "Epoch [3/300], Step [159/225], Training Accuracy: 44.9587%, Training Loss: 1.1231%\n",
      "Epoch [3/300], Step [160/225], Training Accuracy: 44.9609%, Training Loss: 1.1232%\n",
      "Epoch [3/300], Step [161/225], Training Accuracy: 44.9631%, Training Loss: 1.1227%\n",
      "Epoch [3/300], Step [162/225], Training Accuracy: 45.0039%, Training Loss: 1.1223%\n",
      "Epoch [3/300], Step [163/225], Training Accuracy: 44.9674%, Training Loss: 1.1221%\n",
      "Epoch [3/300], Step [164/225], Training Accuracy: 44.9981%, Training Loss: 1.1212%\n",
      "Epoch [3/300], Step [165/225], Training Accuracy: 45.0095%, Training Loss: 1.1209%\n",
      "Epoch [3/300], Step [166/225], Training Accuracy: 45.0301%, Training Loss: 1.1206%\n",
      "Epoch [3/300], Step [167/225], Training Accuracy: 45.0131%, Training Loss: 1.1202%\n",
      "Epoch [3/300], Step [168/225], Training Accuracy: 45.0242%, Training Loss: 1.1200%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [169/225], Training Accuracy: 45.0259%, Training Loss: 1.1200%\n",
      "Epoch [3/300], Step [170/225], Training Accuracy: 45.0368%, Training Loss: 1.1196%\n",
      "Epoch [3/300], Step [171/225], Training Accuracy: 45.0749%, Training Loss: 1.1189%\n",
      "Epoch [3/300], Step [172/225], Training Accuracy: 45.1126%, Training Loss: 1.1185%\n",
      "Epoch [3/300], Step [173/225], Training Accuracy: 45.1319%, Training Loss: 1.1182%\n",
      "Epoch [3/300], Step [174/225], Training Accuracy: 45.0970%, Training Loss: 1.1188%\n",
      "Epoch [3/300], Step [175/225], Training Accuracy: 45.1518%, Training Loss: 1.1184%\n",
      "Epoch [3/300], Step [176/225], Training Accuracy: 45.0817%, Training Loss: 1.1186%\n",
      "Epoch [3/300], Step [177/225], Training Accuracy: 45.1183%, Training Loss: 1.1189%\n",
      "Epoch [3/300], Step [178/225], Training Accuracy: 45.1018%, Training Loss: 1.1189%\n",
      "Epoch [3/300], Step [179/225], Training Accuracy: 45.1466%, Training Loss: 1.1184%\n",
      "Epoch [3/300], Step [180/225], Training Accuracy: 45.2083%, Training Loss: 1.1179%\n",
      "Epoch [3/300], Step [181/225], Training Accuracy: 45.2089%, Training Loss: 1.1180%\n",
      "Epoch [3/300], Step [182/225], Training Accuracy: 45.2438%, Training Loss: 1.1177%\n",
      "Epoch [3/300], Step [183/225], Training Accuracy: 45.2357%, Training Loss: 1.1170%\n",
      "Epoch [3/300], Step [184/225], Training Accuracy: 45.2446%, Training Loss: 1.1168%\n",
      "Epoch [3/300], Step [185/225], Training Accuracy: 45.2872%, Training Loss: 1.1163%\n",
      "Epoch [3/300], Step [186/225], Training Accuracy: 45.3041%, Training Loss: 1.1159%\n",
      "Epoch [3/300], Step [187/225], Training Accuracy: 45.3292%, Training Loss: 1.1156%\n",
      "Epoch [3/300], Step [188/225], Training Accuracy: 45.3374%, Training Loss: 1.1149%\n",
      "Epoch [3/300], Step [189/225], Training Accuracy: 45.3952%, Training Loss: 1.1138%\n",
      "Epoch [3/300], Step [190/225], Training Accuracy: 45.3865%, Training Loss: 1.1137%\n",
      "Epoch [3/300], Step [191/225], Training Accuracy: 45.4352%, Training Loss: 1.1131%\n",
      "Epoch [3/300], Step [192/225], Training Accuracy: 45.5566%, Training Loss: 1.1123%\n",
      "Epoch [3/300], Step [193/225], Training Accuracy: 45.5554%, Training Loss: 1.1125%\n",
      "Epoch [3/300], Step [194/225], Training Accuracy: 45.5944%, Training Loss: 1.1118%\n",
      "Epoch [3/300], Step [195/225], Training Accuracy: 45.5849%, Training Loss: 1.1113%\n",
      "Epoch [3/300], Step [196/225], Training Accuracy: 45.5596%, Training Loss: 1.1114%\n",
      "Epoch [3/300], Step [197/225], Training Accuracy: 45.5346%, Training Loss: 1.1123%\n",
      "Epoch [3/300], Step [198/225], Training Accuracy: 45.6045%, Training Loss: 1.1112%\n",
      "Epoch [3/300], Step [199/225], Training Accuracy: 45.6737%, Training Loss: 1.1103%\n",
      "Epoch [3/300], Step [200/225], Training Accuracy: 45.7109%, Training Loss: 1.1102%\n",
      "Epoch [3/300], Step [201/225], Training Accuracy: 45.7090%, Training Loss: 1.1101%\n",
      "Epoch [3/300], Step [202/225], Training Accuracy: 45.7070%, Training Loss: 1.1102%\n",
      "Epoch [3/300], Step [203/225], Training Accuracy: 45.6897%, Training Loss: 1.1104%\n",
      "Epoch [3/300], Step [204/225], Training Accuracy: 45.7031%, Training Loss: 1.1101%\n",
      "Epoch [3/300], Step [205/225], Training Accuracy: 45.7241%, Training Loss: 1.1104%\n",
      "Epoch [3/300], Step [206/225], Training Accuracy: 45.7145%, Training Loss: 1.1107%\n",
      "Epoch [3/300], Step [207/225], Training Accuracy: 45.7277%, Training Loss: 1.1106%\n",
      "Epoch [3/300], Step [208/225], Training Accuracy: 45.7482%, Training Loss: 1.1100%\n",
      "Epoch [3/300], Step [209/225], Training Accuracy: 45.7611%, Training Loss: 1.1096%\n",
      "Epoch [3/300], Step [210/225], Training Accuracy: 45.7738%, Training Loss: 1.1092%\n",
      "Epoch [3/300], Step [211/225], Training Accuracy: 45.8309%, Training Loss: 1.1085%\n",
      "Epoch [3/300], Step [212/225], Training Accuracy: 45.8358%, Training Loss: 1.1085%\n",
      "Epoch [3/300], Step [213/225], Training Accuracy: 45.8260%, Training Loss: 1.1088%\n",
      "Epoch [3/300], Step [214/225], Training Accuracy: 45.8674%, Training Loss: 1.1082%\n",
      "Epoch [3/300], Step [215/225], Training Accuracy: 45.8285%, Training Loss: 1.1086%\n",
      "Epoch [3/300], Step [216/225], Training Accuracy: 45.8261%, Training Loss: 1.1084%\n",
      "Epoch [3/300], Step [217/225], Training Accuracy: 45.8165%, Training Loss: 1.1082%\n",
      "Epoch [3/300], Step [218/225], Training Accuracy: 45.7927%, Training Loss: 1.1083%\n",
      "Epoch [3/300], Step [219/225], Training Accuracy: 45.8119%, Training Loss: 1.1078%\n",
      "Epoch [3/300], Step [220/225], Training Accuracy: 45.8239%, Training Loss: 1.1076%\n",
      "Epoch [3/300], Step [221/225], Training Accuracy: 45.8286%, Training Loss: 1.1076%\n",
      "Epoch [3/300], Step [222/225], Training Accuracy: 45.8756%, Training Loss: 1.1073%\n",
      "Epoch [3/300], Step [223/225], Training Accuracy: 45.8590%, Training Loss: 1.1073%\n",
      "Epoch [3/300], Step [224/225], Training Accuracy: 45.8915%, Training Loss: 1.1065%\n",
      "Epoch [3/300], Step [225/225], Training Accuracy: 45.8866%, Training Loss: 1.1066%\n",
      "Epoch [4/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.9525%\n",
      "Epoch [4/300], Step [2/225], Training Accuracy: 53.1250%, Training Loss: 1.0193%\n",
      "Epoch [4/300], Step [3/225], Training Accuracy: 49.4792%, Training Loss: 1.0985%\n",
      "Epoch [4/300], Step [4/225], Training Accuracy: 48.4375%, Training Loss: 1.1078%\n",
      "Epoch [4/300], Step [5/225], Training Accuracy: 50.0000%, Training Loss: 1.0729%\n",
      "Epoch [4/300], Step [6/225], Training Accuracy: 50.0000%, Training Loss: 1.0841%\n",
      "Epoch [4/300], Step [7/225], Training Accuracy: 50.8929%, Training Loss: 1.0770%\n",
      "Epoch [4/300], Step [8/225], Training Accuracy: 51.3672%, Training Loss: 1.0834%\n",
      "Epoch [4/300], Step [9/225], Training Accuracy: 51.2153%, Training Loss: 1.0952%\n",
      "Epoch [4/300], Step [10/225], Training Accuracy: 50.6250%, Training Loss: 1.1011%\n",
      "Epoch [4/300], Step [11/225], Training Accuracy: 49.2898%, Training Loss: 1.1047%\n",
      "Epoch [4/300], Step [12/225], Training Accuracy: 50.1302%, Training Loss: 1.1009%\n",
      "Epoch [4/300], Step [13/225], Training Accuracy: 50.9615%, Training Loss: 1.0965%\n",
      "Epoch [4/300], Step [14/225], Training Accuracy: 50.7812%, Training Loss: 1.0926%\n",
      "Epoch [4/300], Step [15/225], Training Accuracy: 50.2083%, Training Loss: 1.0933%\n",
      "Epoch [4/300], Step [16/225], Training Accuracy: 50.2930%, Training Loss: 1.0873%\n",
      "Epoch [4/300], Step [17/225], Training Accuracy: 50.7353%, Training Loss: 1.0818%\n",
      "Epoch [4/300], Step [18/225], Training Accuracy: 50.0000%, Training Loss: 1.0800%\n",
      "Epoch [4/300], Step [19/225], Training Accuracy: 49.8355%, Training Loss: 1.0804%\n",
      "Epoch [4/300], Step [20/225], Training Accuracy: 49.5312%, Training Loss: 1.0790%\n",
      "Epoch [4/300], Step [21/225], Training Accuracy: 49.7024%, Training Loss: 1.0781%\n",
      "Epoch [4/300], Step [22/225], Training Accuracy: 49.5739%, Training Loss: 1.0828%\n",
      "Epoch [4/300], Step [23/225], Training Accuracy: 50.0679%, Training Loss: 1.0765%\n",
      "Epoch [4/300], Step [24/225], Training Accuracy: 49.5443%, Training Loss: 1.0789%\n",
      "Epoch [4/300], Step [25/225], Training Accuracy: 49.4375%, Training Loss: 1.0756%\n",
      "Epoch [4/300], Step [26/225], Training Accuracy: 49.0986%, Training Loss: 1.0737%\n",
      "Epoch [4/300], Step [27/225], Training Accuracy: 49.0741%, Training Loss: 1.0715%\n",
      "Epoch [4/300], Step [28/225], Training Accuracy: 49.6094%, Training Loss: 1.0680%\n",
      "Epoch [4/300], Step [29/225], Training Accuracy: 50.2155%, Training Loss: 1.0624%\n",
      "Epoch [4/300], Step [30/225], Training Accuracy: 50.2083%, Training Loss: 1.0612%\n",
      "Epoch [4/300], Step [31/225], Training Accuracy: 50.0000%, Training Loss: 1.0665%\n",
      "Epoch [4/300], Step [32/225], Training Accuracy: 50.0488%, Training Loss: 1.0647%\n",
      "Epoch [4/300], Step [33/225], Training Accuracy: 50.0000%, Training Loss: 1.0626%\n",
      "Epoch [4/300], Step [34/225], Training Accuracy: 49.6783%, Training Loss: 1.0658%\n",
      "Epoch [4/300], Step [35/225], Training Accuracy: 49.4643%, Training Loss: 1.0681%\n",
      "Epoch [4/300], Step [36/225], Training Accuracy: 49.4358%, Training Loss: 1.0685%\n",
      "Epoch [4/300], Step [37/225], Training Accuracy: 49.3243%, Training Loss: 1.0682%\n",
      "Epoch [4/300], Step [38/225], Training Accuracy: 49.3832%, Training Loss: 1.0664%\n",
      "Epoch [4/300], Step [39/225], Training Accuracy: 49.3590%, Training Loss: 1.0667%\n",
      "Epoch [4/300], Step [40/225], Training Accuracy: 49.1016%, Training Loss: 1.0713%\n",
      "Epoch [4/300], Step [41/225], Training Accuracy: 48.8567%, Training Loss: 1.0729%\n",
      "Epoch [4/300], Step [42/225], Training Accuracy: 48.8095%, Training Loss: 1.0710%\n",
      "Epoch [4/300], Step [43/225], Training Accuracy: 48.9462%, Training Loss: 1.0701%\n",
      "Epoch [4/300], Step [44/225], Training Accuracy: 49.2898%, Training Loss: 1.0689%\n",
      "Epoch [4/300], Step [45/225], Training Accuracy: 49.1319%, Training Loss: 1.0682%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [46/225], Training Accuracy: 49.3207%, Training Loss: 1.0664%\n",
      "Epoch [4/300], Step [47/225], Training Accuracy: 49.3351%, Training Loss: 1.0650%\n",
      "Epoch [4/300], Step [48/225], Training Accuracy: 49.4792%, Training Loss: 1.0619%\n",
      "Epoch [4/300], Step [49/225], Training Accuracy: 49.4260%, Training Loss: 1.0623%\n",
      "Epoch [4/300], Step [50/225], Training Accuracy: 49.3125%, Training Loss: 1.0616%\n",
      "Epoch [4/300], Step [51/225], Training Accuracy: 49.3566%, Training Loss: 1.0603%\n",
      "Epoch [4/300], Step [52/225], Training Accuracy: 49.3389%, Training Loss: 1.0615%\n",
      "Epoch [4/300], Step [53/225], Training Accuracy: 49.4104%, Training Loss: 1.0612%\n",
      "Epoch [4/300], Step [54/225], Training Accuracy: 49.4502%, Training Loss: 1.0603%\n",
      "Epoch [4/300], Step [55/225], Training Accuracy: 49.2898%, Training Loss: 1.0609%\n",
      "Epoch [4/300], Step [56/225], Training Accuracy: 49.3862%, Training Loss: 1.0599%\n",
      "Epoch [4/300], Step [57/225], Training Accuracy: 49.5614%, Training Loss: 1.0568%\n",
      "Epoch [4/300], Step [58/225], Training Accuracy: 49.4343%, Training Loss: 1.0586%\n",
      "Epoch [4/300], Step [59/225], Training Accuracy: 49.4968%, Training Loss: 1.0581%\n",
      "Epoch [4/300], Step [60/225], Training Accuracy: 49.6354%, Training Loss: 1.0555%\n",
      "Epoch [4/300], Step [61/225], Training Accuracy: 49.6670%, Training Loss: 1.0546%\n",
      "Epoch [4/300], Step [62/225], Training Accuracy: 49.6220%, Training Loss: 1.0545%\n",
      "Epoch [4/300], Step [63/225], Training Accuracy: 49.4792%, Training Loss: 1.0552%\n",
      "Epoch [4/300], Step [64/225], Training Accuracy: 49.4873%, Training Loss: 1.0556%\n",
      "Epoch [4/300], Step [65/225], Training Accuracy: 49.3750%, Training Loss: 1.0561%\n",
      "Epoch [4/300], Step [66/225], Training Accuracy: 49.5265%, Training Loss: 1.0550%\n",
      "Epoch [4/300], Step [67/225], Training Accuracy: 49.5802%, Training Loss: 1.0564%\n",
      "Epoch [4/300], Step [68/225], Training Accuracy: 49.5175%, Training Loss: 1.0568%\n",
      "Epoch [4/300], Step [69/225], Training Accuracy: 49.3659%, Training Loss: 1.0560%\n",
      "Epoch [4/300], Step [70/225], Training Accuracy: 49.3973%, Training Loss: 1.0551%\n",
      "Epoch [4/300], Step [71/225], Training Accuracy: 49.4058%, Training Loss: 1.0555%\n",
      "Epoch [4/300], Step [72/225], Training Accuracy: 49.2622%, Training Loss: 1.0565%\n",
      "Epoch [4/300], Step [73/225], Training Accuracy: 49.3151%, Training Loss: 1.0571%\n",
      "Epoch [4/300], Step [74/225], Training Accuracy: 49.4932%, Training Loss: 1.0547%\n",
      "Epoch [4/300], Step [75/225], Training Accuracy: 49.6042%, Training Loss: 1.0527%\n",
      "Epoch [4/300], Step [76/225], Training Accuracy: 49.5271%, Training Loss: 1.0528%\n",
      "Epoch [4/300], Step [77/225], Training Accuracy: 49.5942%, Training Loss: 1.0514%\n",
      "Epoch [4/300], Step [78/225], Training Accuracy: 49.5593%, Training Loss: 1.0506%\n",
      "Epoch [4/300], Step [79/225], Training Accuracy: 49.5451%, Training Loss: 1.0507%\n",
      "Epoch [4/300], Step [80/225], Training Accuracy: 49.5312%, Training Loss: 1.0503%\n",
      "Epoch [4/300], Step [81/225], Training Accuracy: 49.4599%, Training Loss: 1.0505%\n",
      "Epoch [4/300], Step [82/225], Training Accuracy: 49.5046%, Training Loss: 1.0502%\n",
      "Epoch [4/300], Step [83/225], Training Accuracy: 49.5482%, Training Loss: 1.0487%\n",
      "Epoch [4/300], Step [84/225], Training Accuracy: 49.4978%, Training Loss: 1.0484%\n",
      "Epoch [4/300], Step [85/225], Training Accuracy: 49.4853%, Training Loss: 1.0480%\n",
      "Epoch [4/300], Step [86/225], Training Accuracy: 49.5094%, Training Loss: 1.0477%\n",
      "Epoch [4/300], Step [87/225], Training Accuracy: 49.4971%, Training Loss: 1.0473%\n",
      "Epoch [4/300], Step [88/225], Training Accuracy: 49.4673%, Training Loss: 1.0483%\n",
      "Epoch [4/300], Step [89/225], Training Accuracy: 49.4382%, Training Loss: 1.0507%\n",
      "Epoch [4/300], Step [90/225], Training Accuracy: 49.4444%, Training Loss: 1.0510%\n",
      "Epoch [4/300], Step [91/225], Training Accuracy: 49.3132%, Training Loss: 1.0513%\n",
      "Epoch [4/300], Step [92/225], Training Accuracy: 49.2697%, Training Loss: 1.0516%\n",
      "Epoch [4/300], Step [93/225], Training Accuracy: 49.3448%, Training Loss: 1.0506%\n",
      "Epoch [4/300], Step [94/225], Training Accuracy: 49.4182%, Training Loss: 1.0490%\n",
      "Epoch [4/300], Step [95/225], Training Accuracy: 49.3750%, Training Loss: 1.0498%\n",
      "Epoch [4/300], Step [96/225], Training Accuracy: 49.4792%, Training Loss: 1.0488%\n",
      "Epoch [4/300], Step [97/225], Training Accuracy: 49.3557%, Training Loss: 1.0486%\n",
      "Epoch [4/300], Step [98/225], Training Accuracy: 49.2985%, Training Loss: 1.0494%\n",
      "Epoch [4/300], Step [99/225], Training Accuracy: 49.3529%, Training Loss: 1.0500%\n",
      "Epoch [4/300], Step [100/225], Training Accuracy: 49.3281%, Training Loss: 1.0504%\n",
      "Epoch [4/300], Step [101/225], Training Accuracy: 49.3812%, Training Loss: 1.0496%\n",
      "Epoch [4/300], Step [102/225], Training Accuracy: 49.4026%, Training Loss: 1.0492%\n",
      "Epoch [4/300], Step [103/225], Training Accuracy: 49.4539%, Training Loss: 1.0492%\n",
      "Epoch [4/300], Step [104/225], Training Accuracy: 49.3389%, Training Loss: 1.0491%\n",
      "Epoch [4/300], Step [105/225], Training Accuracy: 49.3304%, Training Loss: 1.0486%\n",
      "Epoch [4/300], Step [106/225], Training Accuracy: 49.2630%, Training Loss: 1.0485%\n",
      "Epoch [4/300], Step [107/225], Training Accuracy: 49.1968%, Training Loss: 1.0496%\n",
      "Epoch [4/300], Step [108/225], Training Accuracy: 49.2477%, Training Loss: 1.0492%\n",
      "Epoch [4/300], Step [109/225], Training Accuracy: 49.2259%, Training Loss: 1.0491%\n",
      "Epoch [4/300], Step [110/225], Training Accuracy: 49.2330%, Training Loss: 1.0493%\n",
      "Epoch [4/300], Step [111/225], Training Accuracy: 49.2258%, Training Loss: 1.0491%\n",
      "Epoch [4/300], Step [112/225], Training Accuracy: 49.2327%, Training Loss: 1.0483%\n",
      "Epoch [4/300], Step [113/225], Training Accuracy: 49.2533%, Training Loss: 1.0481%\n",
      "Epoch [4/300], Step [114/225], Training Accuracy: 49.2736%, Training Loss: 1.0472%\n",
      "Epoch [4/300], Step [115/225], Training Accuracy: 49.4158%, Training Loss: 1.0458%\n",
      "Epoch [4/300], Step [116/225], Training Accuracy: 49.5016%, Training Loss: 1.0449%\n",
      "Epoch [4/300], Step [117/225], Training Accuracy: 49.3990%, Training Loss: 1.0460%\n",
      "Epoch [4/300], Step [118/225], Training Accuracy: 49.3379%, Training Loss: 1.0466%\n",
      "Epoch [4/300], Step [119/225], Training Accuracy: 49.3697%, Training Loss: 1.0465%\n",
      "Epoch [4/300], Step [120/225], Training Accuracy: 49.3880%, Training Loss: 1.0473%\n",
      "Epoch [4/300], Step [121/225], Training Accuracy: 49.3543%, Training Loss: 1.0472%\n",
      "Epoch [4/300], Step [122/225], Training Accuracy: 49.3468%, Training Loss: 1.0467%\n",
      "Epoch [4/300], Step [123/225], Training Accuracy: 49.3521%, Training Loss: 1.0463%\n",
      "Epoch [4/300], Step [124/225], Training Accuracy: 49.3952%, Training Loss: 1.0457%\n",
      "Epoch [4/300], Step [125/225], Training Accuracy: 49.3125%, Training Loss: 1.0465%\n",
      "Epoch [4/300], Step [126/225], Training Accuracy: 49.3180%, Training Loss: 1.0466%\n",
      "Epoch [4/300], Step [127/225], Training Accuracy: 49.2495%, Training Loss: 1.0470%\n",
      "Epoch [4/300], Step [128/225], Training Accuracy: 49.1699%, Training Loss: 1.0474%\n",
      "Epoch [4/300], Step [129/225], Training Accuracy: 49.1642%, Training Loss: 1.0469%\n",
      "Epoch [4/300], Step [130/225], Training Accuracy: 49.1226%, Training Loss: 1.0471%\n",
      "Epoch [4/300], Step [131/225], Training Accuracy: 49.0697%, Training Loss: 1.0473%\n",
      "Epoch [4/300], Step [132/225], Training Accuracy: 49.0767%, Training Loss: 1.0472%\n",
      "Epoch [4/300], Step [133/225], Training Accuracy: 49.0484%, Training Loss: 1.0472%\n",
      "Epoch [4/300], Step [134/225], Training Accuracy: 49.0089%, Training Loss: 1.0481%\n",
      "Epoch [4/300], Step [135/225], Training Accuracy: 49.0394%, Training Loss: 1.0480%\n",
      "Epoch [4/300], Step [136/225], Training Accuracy: 49.0924%, Training Loss: 1.0474%\n",
      "Epoch [4/300], Step [137/225], Training Accuracy: 49.1560%, Training Loss: 1.0467%\n",
      "Epoch [4/300], Step [138/225], Training Accuracy: 49.2074%, Training Loss: 1.0459%\n",
      "Epoch [4/300], Step [139/225], Training Accuracy: 49.1906%, Training Loss: 1.0453%\n",
      "Epoch [4/300], Step [140/225], Training Accuracy: 49.2188%, Training Loss: 1.0452%\n",
      "Epoch [4/300], Step [141/225], Training Accuracy: 49.2132%, Training Loss: 1.0449%\n",
      "Epoch [4/300], Step [142/225], Training Accuracy: 49.1857%, Training Loss: 1.0447%\n",
      "Epoch [4/300], Step [143/225], Training Accuracy: 49.1477%, Training Loss: 1.0441%\n",
      "Epoch [4/300], Step [144/225], Training Accuracy: 49.1319%, Training Loss: 1.0438%\n",
      "Epoch [4/300], Step [145/225], Training Accuracy: 49.1918%, Training Loss: 1.0428%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [146/225], Training Accuracy: 49.1652%, Training Loss: 1.0440%\n",
      "Epoch [4/300], Step [147/225], Training Accuracy: 49.1390%, Training Loss: 1.0439%\n",
      "Epoch [4/300], Step [148/225], Training Accuracy: 49.1660%, Training Loss: 1.0435%\n",
      "Epoch [4/300], Step [149/225], Training Accuracy: 49.0772%, Training Loss: 1.0437%\n",
      "Epoch [4/300], Step [150/225], Training Accuracy: 49.1250%, Training Loss: 1.0425%\n",
      "Epoch [4/300], Step [151/225], Training Accuracy: 49.1308%, Training Loss: 1.0418%\n",
      "Epoch [4/300], Step [152/225], Training Accuracy: 49.1571%, Training Loss: 1.0413%\n",
      "Epoch [4/300], Step [153/225], Training Accuracy: 49.2136%, Training Loss: 1.0406%\n",
      "Epoch [4/300], Step [154/225], Training Accuracy: 49.1985%, Training Loss: 1.0407%\n",
      "Epoch [4/300], Step [155/225], Training Accuracy: 49.1835%, Training Loss: 1.0410%\n",
      "Epoch [4/300], Step [156/225], Training Accuracy: 49.2288%, Training Loss: 1.0407%\n",
      "Epoch [4/300], Step [157/225], Training Accuracy: 49.2138%, Training Loss: 1.0402%\n",
      "Epoch [4/300], Step [158/225], Training Accuracy: 49.1594%, Training Loss: 1.0410%\n",
      "Epoch [4/300], Step [159/225], Training Accuracy: 49.1254%, Training Loss: 1.0410%\n",
      "Epoch [4/300], Step [160/225], Training Accuracy: 49.1406%, Training Loss: 1.0409%\n",
      "Epoch [4/300], Step [161/225], Training Accuracy: 49.1945%, Training Loss: 1.0405%\n",
      "Epoch [4/300], Step [162/225], Training Accuracy: 49.3056%, Training Loss: 1.0399%\n",
      "Epoch [4/300], Step [163/225], Training Accuracy: 49.3002%, Training Loss: 1.0397%\n",
      "Epoch [4/300], Step [164/225], Training Accuracy: 49.3807%, Training Loss: 1.0386%\n",
      "Epoch [4/300], Step [165/225], Training Accuracy: 49.3655%, Training Loss: 1.0392%\n",
      "Epoch [4/300], Step [166/225], Training Accuracy: 49.3788%, Training Loss: 1.0387%\n",
      "Epoch [4/300], Step [167/225], Training Accuracy: 49.3918%, Training Loss: 1.0385%\n",
      "Epoch [4/300], Step [168/225], Training Accuracy: 49.3583%, Training Loss: 1.0383%\n",
      "Epoch [4/300], Step [169/225], Training Accuracy: 49.3158%, Training Loss: 1.0381%\n",
      "Epoch [4/300], Step [170/225], Training Accuracy: 49.3199%, Training Loss: 1.0380%\n",
      "Epoch [4/300], Step [171/225], Training Accuracy: 49.3238%, Training Loss: 1.0375%\n",
      "Epoch [4/300], Step [172/225], Training Accuracy: 49.3641%, Training Loss: 1.0374%\n",
      "Epoch [4/300], Step [173/225], Training Accuracy: 49.3678%, Training Loss: 1.0371%\n",
      "Epoch [4/300], Step [174/225], Training Accuracy: 49.3355%, Training Loss: 1.0377%\n",
      "Epoch [4/300], Step [175/225], Training Accuracy: 49.4107%, Training Loss: 1.0374%\n",
      "Epoch [4/300], Step [176/225], Training Accuracy: 49.3608%, Training Loss: 1.0376%\n",
      "Epoch [4/300], Step [177/225], Training Accuracy: 49.4174%, Training Loss: 1.0377%\n",
      "Epoch [4/300], Step [178/225], Training Accuracy: 49.3943%, Training Loss: 1.0380%\n",
      "Epoch [4/300], Step [179/225], Training Accuracy: 49.4588%, Training Loss: 1.0373%\n",
      "Epoch [4/300], Step [180/225], Training Accuracy: 49.5226%, Training Loss: 1.0365%\n",
      "Epoch [4/300], Step [181/225], Training Accuracy: 49.5166%, Training Loss: 1.0365%\n",
      "Epoch [4/300], Step [182/225], Training Accuracy: 49.5536%, Training Loss: 1.0362%\n",
      "Epoch [4/300], Step [183/225], Training Accuracy: 49.5560%, Training Loss: 1.0357%\n",
      "Epoch [4/300], Step [184/225], Training Accuracy: 49.5584%, Training Loss: 1.0354%\n",
      "Epoch [4/300], Step [185/225], Training Accuracy: 49.5439%, Training Loss: 1.0352%\n",
      "Epoch [4/300], Step [186/225], Training Accuracy: 49.5968%, Training Loss: 1.0344%\n",
      "Epoch [4/300], Step [187/225], Training Accuracy: 49.6156%, Training Loss: 1.0339%\n",
      "Epoch [4/300], Step [188/225], Training Accuracy: 49.6509%, Training Loss: 1.0333%\n",
      "Epoch [4/300], Step [189/225], Training Accuracy: 49.6693%, Training Loss: 1.0325%\n",
      "Epoch [4/300], Step [190/225], Training Accuracy: 49.6628%, Training Loss: 1.0326%\n",
      "Epoch [4/300], Step [191/225], Training Accuracy: 49.6973%, Training Loss: 1.0321%\n",
      "Epoch [4/300], Step [192/225], Training Accuracy: 49.7559%, Training Loss: 1.0313%\n",
      "Epoch [4/300], Step [193/225], Training Accuracy: 49.7085%, Training Loss: 1.0316%\n",
      "Epoch [4/300], Step [194/225], Training Accuracy: 49.7101%, Training Loss: 1.0311%\n",
      "Epoch [4/300], Step [195/225], Training Accuracy: 49.7035%, Training Loss: 1.0307%\n",
      "Epoch [4/300], Step [196/225], Training Accuracy: 49.6732%, Training Loss: 1.0309%\n",
      "Epoch [4/300], Step [197/225], Training Accuracy: 49.6669%, Training Loss: 1.0313%\n",
      "Epoch [4/300], Step [198/225], Training Accuracy: 49.6922%, Training Loss: 1.0304%\n",
      "Epoch [4/300], Step [199/225], Training Accuracy: 49.7095%, Training Loss: 1.0299%\n",
      "Epoch [4/300], Step [200/225], Training Accuracy: 49.7266%, Training Loss: 1.0300%\n",
      "Epoch [4/300], Step [201/225], Training Accuracy: 49.6813%, Training Loss: 1.0308%\n",
      "Epoch [4/300], Step [202/225], Training Accuracy: 49.6674%, Training Loss: 1.0308%\n",
      "Epoch [4/300], Step [203/225], Training Accuracy: 49.6536%, Training Loss: 1.0311%\n",
      "Epoch [4/300], Step [204/225], Training Accuracy: 49.6630%, Training Loss: 1.0309%\n",
      "Epoch [4/300], Step [205/225], Training Accuracy: 49.7027%, Training Loss: 1.0305%\n",
      "Epoch [4/300], Step [206/225], Training Accuracy: 49.6587%, Training Loss: 1.0307%\n",
      "Epoch [4/300], Step [207/225], Training Accuracy: 49.6528%, Training Loss: 1.0306%\n",
      "Epoch [4/300], Step [208/225], Training Accuracy: 49.6695%, Training Loss: 1.0303%\n",
      "Epoch [4/300], Step [209/225], Training Accuracy: 49.6935%, Training Loss: 1.0300%\n",
      "Epoch [4/300], Step [210/225], Training Accuracy: 49.7098%, Training Loss: 1.0301%\n",
      "Epoch [4/300], Step [211/225], Training Accuracy: 49.7482%, Training Loss: 1.0295%\n",
      "Epoch [4/300], Step [212/225], Training Accuracy: 49.7568%, Training Loss: 1.0295%\n",
      "Epoch [4/300], Step [213/225], Training Accuracy: 49.7139%, Training Loss: 1.0302%\n",
      "Epoch [4/300], Step [214/225], Training Accuracy: 49.7664%, Training Loss: 1.0297%\n",
      "Epoch [4/300], Step [215/225], Training Accuracy: 49.7384%, Training Loss: 1.0301%\n",
      "Epoch [4/300], Step [216/225], Training Accuracy: 49.7323%, Training Loss: 1.0302%\n",
      "Epoch [4/300], Step [217/225], Training Accuracy: 49.7336%, Training Loss: 1.0301%\n",
      "Epoch [4/300], Step [218/225], Training Accuracy: 49.7133%, Training Loss: 1.0303%\n",
      "Epoch [4/300], Step [219/225], Training Accuracy: 49.7289%, Training Loss: 1.0298%\n",
      "Epoch [4/300], Step [220/225], Training Accuracy: 49.7301%, Training Loss: 1.0294%\n",
      "Epoch [4/300], Step [221/225], Training Accuracy: 49.6889%, Training Loss: 1.0296%\n",
      "Epoch [4/300], Step [222/225], Training Accuracy: 49.7044%, Training Loss: 1.0292%\n",
      "Epoch [4/300], Step [223/225], Training Accuracy: 49.6777%, Training Loss: 1.0296%\n",
      "Epoch [4/300], Step [224/225], Training Accuracy: 49.6791%, Training Loss: 1.0289%\n",
      "Epoch [4/300], Step [225/225], Training Accuracy: 49.6595%, Training Loss: 1.0291%\n",
      "Epoch [5/300], Step [1/225], Training Accuracy: 56.2500%, Training Loss: 0.8333%\n",
      "Epoch [5/300], Step [2/225], Training Accuracy: 56.2500%, Training Loss: 0.9013%\n",
      "Epoch [5/300], Step [3/225], Training Accuracy: 53.1250%, Training Loss: 0.9410%\n",
      "Epoch [5/300], Step [4/225], Training Accuracy: 52.3438%, Training Loss: 0.9562%\n",
      "Epoch [5/300], Step [5/225], Training Accuracy: 53.4375%, Training Loss: 0.9486%\n",
      "Epoch [5/300], Step [6/225], Training Accuracy: 52.8646%, Training Loss: 0.9786%\n",
      "Epoch [5/300], Step [7/225], Training Accuracy: 53.7946%, Training Loss: 0.9775%\n",
      "Epoch [5/300], Step [8/225], Training Accuracy: 52.5391%, Training Loss: 0.9811%\n",
      "Epoch [5/300], Step [9/225], Training Accuracy: 52.0833%, Training Loss: 0.9884%\n",
      "Epoch [5/300], Step [10/225], Training Accuracy: 51.0938%, Training Loss: 1.0120%\n",
      "Epoch [5/300], Step [11/225], Training Accuracy: 50.7102%, Training Loss: 1.0139%\n",
      "Epoch [5/300], Step [12/225], Training Accuracy: 50.5208%, Training Loss: 1.0126%\n",
      "Epoch [5/300], Step [13/225], Training Accuracy: 51.4423%, Training Loss: 1.0055%\n",
      "Epoch [5/300], Step [14/225], Training Accuracy: 51.0045%, Training Loss: 1.0081%\n",
      "Epoch [5/300], Step [15/225], Training Accuracy: 50.7292%, Training Loss: 1.0141%\n",
      "Epoch [5/300], Step [16/225], Training Accuracy: 50.5859%, Training Loss: 1.0099%\n",
      "Epoch [5/300], Step [17/225], Training Accuracy: 51.0110%, Training Loss: 1.0078%\n",
      "Epoch [5/300], Step [18/225], Training Accuracy: 50.5208%, Training Loss: 1.0072%\n",
      "Epoch [5/300], Step [19/225], Training Accuracy: 50.3289%, Training Loss: 1.0057%\n",
      "Epoch [5/300], Step [20/225], Training Accuracy: 50.3125%, Training Loss: 1.0060%\n",
      "Epoch [5/300], Step [21/225], Training Accuracy: 50.9673%, Training Loss: 1.0000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [22/225], Training Accuracy: 50.5682%, Training Loss: 1.0069%\n",
      "Epoch [5/300], Step [23/225], Training Accuracy: 51.2228%, Training Loss: 1.0021%\n",
      "Epoch [5/300], Step [24/225], Training Accuracy: 50.7812%, Training Loss: 1.0055%\n",
      "Epoch [5/300], Step [25/225], Training Accuracy: 50.6250%, Training Loss: 1.0047%\n",
      "Epoch [5/300], Step [26/225], Training Accuracy: 50.3005%, Training Loss: 1.0063%\n",
      "Epoch [5/300], Step [27/225], Training Accuracy: 50.0000%, Training Loss: 1.0084%\n",
      "Epoch [5/300], Step [28/225], Training Accuracy: 50.2232%, Training Loss: 1.0077%\n",
      "Epoch [5/300], Step [29/225], Training Accuracy: 50.2694%, Training Loss: 1.0027%\n",
      "Epoch [5/300], Step [30/225], Training Accuracy: 50.4688%, Training Loss: 1.0013%\n",
      "Epoch [5/300], Step [31/225], Training Accuracy: 50.3024%, Training Loss: 1.0051%\n",
      "Epoch [5/300], Step [32/225], Training Accuracy: 50.5371%, Training Loss: 1.0031%\n",
      "Epoch [5/300], Step [33/225], Training Accuracy: 50.7102%, Training Loss: 0.9998%\n",
      "Epoch [5/300], Step [34/225], Training Accuracy: 50.3217%, Training Loss: 1.0048%\n",
      "Epoch [5/300], Step [35/225], Training Accuracy: 50.4018%, Training Loss: 1.0039%\n",
      "Epoch [5/300], Step [36/225], Training Accuracy: 50.3906%, Training Loss: 1.0054%\n",
      "Epoch [5/300], Step [37/225], Training Accuracy: 50.5490%, Training Loss: 1.0037%\n",
      "Epoch [5/300], Step [38/225], Training Accuracy: 50.6990%, Training Loss: 1.0022%\n",
      "Epoch [5/300], Step [39/225], Training Accuracy: 50.6410%, Training Loss: 1.0019%\n",
      "Epoch [5/300], Step [40/225], Training Accuracy: 50.3516%, Training Loss: 1.0067%\n",
      "Epoch [5/300], Step [41/225], Training Accuracy: 50.0762%, Training Loss: 1.0083%\n",
      "Epoch [5/300], Step [42/225], Training Accuracy: 50.0000%, Training Loss: 1.0063%\n",
      "Epoch [5/300], Step [43/225], Training Accuracy: 50.0000%, Training Loss: 1.0058%\n",
      "Epoch [5/300], Step [44/225], Training Accuracy: 50.1065%, Training Loss: 1.0066%\n",
      "Epoch [5/300], Step [45/225], Training Accuracy: 50.0000%, Training Loss: 1.0054%\n",
      "Epoch [5/300], Step [46/225], Training Accuracy: 50.1698%, Training Loss: 1.0042%\n",
      "Epoch [5/300], Step [47/225], Training Accuracy: 50.0997%, Training Loss: 1.0025%\n",
      "Epoch [5/300], Step [48/225], Training Accuracy: 50.3581%, Training Loss: 1.0003%\n",
      "Epoch [5/300], Step [49/225], Training Accuracy: 50.3827%, Training Loss: 0.9994%\n",
      "Epoch [5/300], Step [50/225], Training Accuracy: 50.4062%, Training Loss: 0.9978%\n",
      "Epoch [5/300], Step [51/225], Training Accuracy: 50.5821%, Training Loss: 0.9961%\n",
      "Epoch [5/300], Step [52/225], Training Accuracy: 50.6911%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [53/225], Training Accuracy: 50.7960%, Training Loss: 0.9963%\n",
      "Epoch [5/300], Step [54/225], Training Accuracy: 50.8681%, Training Loss: 0.9973%\n",
      "Epoch [5/300], Step [55/225], Training Accuracy: 50.8523%, Training Loss: 0.9973%\n",
      "Epoch [5/300], Step [56/225], Training Accuracy: 51.0324%, Training Loss: 0.9956%\n",
      "Epoch [5/300], Step [57/225], Training Accuracy: 51.1787%, Training Loss: 0.9932%\n",
      "Epoch [5/300], Step [58/225], Training Accuracy: 51.0776%, Training Loss: 0.9944%\n",
      "Epoch [5/300], Step [59/225], Training Accuracy: 51.0328%, Training Loss: 0.9929%\n",
      "Epoch [5/300], Step [60/225], Training Accuracy: 51.0417%, Training Loss: 0.9911%\n",
      "Epoch [5/300], Step [61/225], Training Accuracy: 51.0758%, Training Loss: 0.9900%\n",
      "Epoch [5/300], Step [62/225], Training Accuracy: 50.9829%, Training Loss: 0.9907%\n",
      "Epoch [5/300], Step [63/225], Training Accuracy: 50.8185%, Training Loss: 0.9923%\n",
      "Epoch [5/300], Step [64/225], Training Accuracy: 50.8301%, Training Loss: 0.9919%\n",
      "Epoch [5/300], Step [65/225], Training Accuracy: 50.6731%, Training Loss: 0.9925%\n",
      "Epoch [5/300], Step [66/225], Training Accuracy: 50.8286%, Training Loss: 0.9913%\n",
      "Epoch [5/300], Step [67/225], Training Accuracy: 50.8162%, Training Loss: 0.9930%\n",
      "Epoch [5/300], Step [68/225], Training Accuracy: 50.8272%, Training Loss: 0.9940%\n",
      "Epoch [5/300], Step [69/225], Training Accuracy: 50.7020%, Training Loss: 0.9949%\n",
      "Epoch [5/300], Step [70/225], Training Accuracy: 50.6250%, Training Loss: 0.9945%\n",
      "Epoch [5/300], Step [71/225], Training Accuracy: 50.6602%, Training Loss: 0.9955%\n",
      "Epoch [5/300], Step [72/225], Training Accuracy: 50.6293%, Training Loss: 0.9965%\n",
      "Epoch [5/300], Step [73/225], Training Accuracy: 50.5351%, Training Loss: 0.9982%\n",
      "Epoch [5/300], Step [74/225], Training Accuracy: 50.5912%, Training Loss: 0.9964%\n",
      "Epoch [5/300], Step [75/225], Training Accuracy: 50.7500%, Training Loss: 0.9954%\n",
      "Epoch [5/300], Step [76/225], Training Accuracy: 50.7196%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [77/225], Training Accuracy: 50.8320%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [78/225], Training Accuracy: 50.6811%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [79/225], Training Accuracy: 50.5934%, Training Loss: 0.9966%\n",
      "Epoch [5/300], Step [80/225], Training Accuracy: 50.5078%, Training Loss: 0.9975%\n",
      "Epoch [5/300], Step [81/225], Training Accuracy: 50.4823%, Training Loss: 0.9975%\n",
      "Epoch [5/300], Step [82/225], Training Accuracy: 50.4192%, Training Loss: 0.9968%\n",
      "Epoch [5/300], Step [83/225], Training Accuracy: 50.5459%, Training Loss: 0.9957%\n",
      "Epoch [5/300], Step [84/225], Training Accuracy: 50.5580%, Training Loss: 0.9949%\n",
      "Epoch [5/300], Step [85/225], Training Accuracy: 50.4596%, Training Loss: 0.9956%\n",
      "Epoch [5/300], Step [86/225], Training Accuracy: 50.4724%, Training Loss: 0.9965%\n",
      "Epoch [5/300], Step [87/225], Training Accuracy: 50.4490%, Training Loss: 0.9968%\n",
      "Epoch [5/300], Step [88/225], Training Accuracy: 50.4261%, Training Loss: 0.9978%\n",
      "Epoch [5/300], Step [89/225], Training Accuracy: 50.2633%, Training Loss: 1.0006%\n",
      "Epoch [5/300], Step [90/225], Training Accuracy: 50.2431%, Training Loss: 1.0016%\n",
      "Epoch [5/300], Step [91/225], Training Accuracy: 50.2060%, Training Loss: 1.0019%\n",
      "Epoch [5/300], Step [92/225], Training Accuracy: 50.1529%, Training Loss: 1.0018%\n",
      "Epoch [5/300], Step [93/225], Training Accuracy: 50.2184%, Training Loss: 1.0012%\n",
      "Epoch [5/300], Step [94/225], Training Accuracy: 50.3989%, Training Loss: 0.9997%\n",
      "Epoch [5/300], Step [95/225], Training Accuracy: 50.3618%, Training Loss: 0.9998%\n",
      "Epoch [5/300], Step [96/225], Training Accuracy: 50.5371%, Training Loss: 0.9984%\n",
      "Epoch [5/300], Step [97/225], Training Accuracy: 50.5316%, Training Loss: 0.9991%\n",
      "Epoch [5/300], Step [98/225], Training Accuracy: 50.4624%, Training Loss: 0.9989%\n",
      "Epoch [5/300], Step [99/225], Training Accuracy: 50.4893%, Training Loss: 0.9987%\n",
      "Epoch [5/300], Step [100/225], Training Accuracy: 50.3906%, Training Loss: 0.9995%\n",
      "Epoch [5/300], Step [101/225], Training Accuracy: 50.4022%, Training Loss: 0.9989%\n",
      "Epoch [5/300], Step [102/225], Training Accuracy: 50.3830%, Training Loss: 0.9985%\n",
      "Epoch [5/300], Step [103/225], Training Accuracy: 50.4248%, Training Loss: 0.9980%\n",
      "Epoch [5/300], Step [104/225], Training Accuracy: 50.3456%, Training Loss: 0.9980%\n",
      "Epoch [5/300], Step [105/225], Training Accuracy: 50.3571%, Training Loss: 0.9977%\n",
      "Epoch [5/300], Step [106/225], Training Accuracy: 50.3833%, Training Loss: 0.9972%\n",
      "Epoch [5/300], Step [107/225], Training Accuracy: 50.3359%, Training Loss: 0.9979%\n",
      "Epoch [5/300], Step [108/225], Training Accuracy: 50.2894%, Training Loss: 0.9979%\n",
      "Epoch [5/300], Step [109/225], Training Accuracy: 50.3154%, Training Loss: 0.9974%\n",
      "Epoch [5/300], Step [110/225], Training Accuracy: 50.3125%, Training Loss: 0.9969%\n",
      "Epoch [5/300], Step [111/225], Training Accuracy: 50.2675%, Training Loss: 0.9967%\n",
      "Epoch [5/300], Step [112/225], Training Accuracy: 50.2372%, Training Loss: 0.9963%\n",
      "Epoch [5/300], Step [113/225], Training Accuracy: 50.3180%, Training Loss: 0.9956%\n",
      "Epoch [5/300], Step [114/225], Training Accuracy: 50.3975%, Training Loss: 0.9943%\n",
      "Epoch [5/300], Step [115/225], Training Accuracy: 50.4755%, Training Loss: 0.9933%\n",
      "Epoch [5/300], Step [116/225], Training Accuracy: 50.4849%, Training Loss: 0.9926%\n",
      "Epoch [5/300], Step [117/225], Training Accuracy: 50.4140%, Training Loss: 0.9945%\n",
      "Epoch [5/300], Step [118/225], Training Accuracy: 50.4370%, Training Loss: 0.9947%\n",
      "Epoch [5/300], Step [119/225], Training Accuracy: 50.4596%, Training Loss: 0.9940%\n",
      "Epoch [5/300], Step [120/225], Training Accuracy: 50.4427%, Training Loss: 0.9947%\n",
      "Epoch [5/300], Step [121/225], Training Accuracy: 50.3357%, Training Loss: 0.9949%\n",
      "Epoch [5/300], Step [122/225], Training Accuracy: 50.3202%, Training Loss: 0.9944%\n",
      "Epoch [5/300], Step [123/225], Training Accuracy: 50.2795%, Training Loss: 0.9941%\n",
      "Epoch [5/300], Step [124/225], Training Accuracy: 50.3276%, Training Loss: 0.9938%\n",
      "Epoch [5/300], Step [125/225], Training Accuracy: 50.3125%, Training Loss: 0.9945%\n",
      "Epoch [5/300], Step [126/225], Training Accuracy: 50.2852%, Training Loss: 0.9948%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [127/225], Training Accuracy: 50.2338%, Training Loss: 0.9953%\n",
      "Epoch [5/300], Step [128/225], Training Accuracy: 50.1709%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [129/225], Training Accuracy: 50.1817%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [130/225], Training Accuracy: 50.0962%, Training Loss: 0.9970%\n",
      "Epoch [5/300], Step [131/225], Training Accuracy: 50.0596%, Training Loss: 0.9976%\n",
      "Epoch [5/300], Step [132/225], Training Accuracy: 50.0000%, Training Loss: 0.9977%\n",
      "Epoch [5/300], Step [133/225], Training Accuracy: 50.0000%, Training Loss: 0.9980%\n",
      "Epoch [5/300], Step [134/225], Training Accuracy: 49.9417%, Training Loss: 0.9996%\n",
      "Epoch [5/300], Step [135/225], Training Accuracy: 49.9421%, Training Loss: 0.9997%\n",
      "Epoch [5/300], Step [136/225], Training Accuracy: 49.9540%, Training Loss: 0.9994%\n",
      "Epoch [5/300], Step [137/225], Training Accuracy: 50.0000%, Training Loss: 0.9989%\n",
      "Epoch [5/300], Step [138/225], Training Accuracy: 50.0340%, Training Loss: 0.9982%\n",
      "Epoch [5/300], Step [139/225], Training Accuracy: 50.0112%, Training Loss: 0.9980%\n",
      "Epoch [5/300], Step [140/225], Training Accuracy: 50.0223%, Training Loss: 0.9979%\n",
      "Epoch [5/300], Step [141/225], Training Accuracy: 50.0000%, Training Loss: 0.9974%\n",
      "Epoch [5/300], Step [142/225], Training Accuracy: 50.0000%, Training Loss: 0.9972%\n",
      "Epoch [5/300], Step [143/225], Training Accuracy: 50.0000%, Training Loss: 0.9969%\n",
      "Epoch [5/300], Step [144/225], Training Accuracy: 49.9783%, Training Loss: 0.9968%\n",
      "Epoch [5/300], Step [145/225], Training Accuracy: 50.0108%, Training Loss: 0.9962%\n",
      "Epoch [5/300], Step [146/225], Training Accuracy: 49.9251%, Training Loss: 0.9969%\n",
      "Epoch [5/300], Step [147/225], Training Accuracy: 49.8512%, Training Loss: 0.9972%\n",
      "Epoch [5/300], Step [148/225], Training Accuracy: 49.9050%, Training Loss: 0.9972%\n",
      "Epoch [5/300], Step [149/225], Training Accuracy: 49.8742%, Training Loss: 0.9979%\n",
      "Epoch [5/300], Step [150/225], Training Accuracy: 49.9167%, Training Loss: 0.9972%\n",
      "Epoch [5/300], Step [151/225], Training Accuracy: 49.9069%, Training Loss: 0.9969%\n",
      "Epoch [5/300], Step [152/225], Training Accuracy: 49.9692%, Training Loss: 0.9962%\n",
      "Epoch [5/300], Step [153/225], Training Accuracy: 50.0000%, Training Loss: 0.9963%\n",
      "Epoch [5/300], Step [154/225], Training Accuracy: 50.0101%, Training Loss: 0.9964%\n",
      "Epoch [5/300], Step [155/225], Training Accuracy: 49.9294%, Training Loss: 0.9968%\n",
      "Epoch [5/300], Step [156/225], Training Accuracy: 49.9499%, Training Loss: 0.9970%\n",
      "Epoch [5/300], Step [157/225], Training Accuracy: 50.0000%, Training Loss: 0.9965%\n",
      "Epoch [5/300], Step [158/225], Training Accuracy: 49.9407%, Training Loss: 0.9974%\n",
      "Epoch [5/300], Step [159/225], Training Accuracy: 49.8919%, Training Loss: 0.9976%\n",
      "Epoch [5/300], Step [160/225], Training Accuracy: 49.8730%, Training Loss: 0.9974%\n",
      "Epoch [5/300], Step [161/225], Training Accuracy: 49.9030%, Training Loss: 0.9973%\n",
      "Epoch [5/300], Step [162/225], Training Accuracy: 49.9518%, Training Loss: 0.9969%\n",
      "Epoch [5/300], Step [163/225], Training Accuracy: 50.0000%, Training Loss: 0.9965%\n",
      "Epoch [5/300], Step [164/225], Training Accuracy: 50.0572%, Training Loss: 0.9959%\n",
      "Epoch [5/300], Step [165/225], Training Accuracy: 50.0379%, Training Loss: 0.9960%\n",
      "Epoch [5/300], Step [166/225], Training Accuracy: 50.0094%, Training Loss: 0.9957%\n",
      "Epoch [5/300], Step [167/225], Training Accuracy: 50.0000%, Training Loss: 0.9954%\n",
      "Epoch [5/300], Step [168/225], Training Accuracy: 49.9907%, Training Loss: 0.9951%\n",
      "Epoch [5/300], Step [169/225], Training Accuracy: 49.9908%, Training Loss: 0.9948%\n",
      "Epoch [5/300], Step [170/225], Training Accuracy: 50.0000%, Training Loss: 0.9945%\n",
      "Epoch [5/300], Step [171/225], Training Accuracy: 50.0457%, Training Loss: 0.9942%\n",
      "Epoch [5/300], Step [172/225], Training Accuracy: 50.0636%, Training Loss: 0.9947%\n",
      "Epoch [5/300], Step [173/225], Training Accuracy: 50.0723%, Training Loss: 0.9941%\n",
      "Epoch [5/300], Step [174/225], Training Accuracy: 50.0718%, Training Loss: 0.9949%\n",
      "Epoch [5/300], Step [175/225], Training Accuracy: 50.1429%, Training Loss: 0.9945%\n",
      "Epoch [5/300], Step [176/225], Training Accuracy: 50.1065%, Training Loss: 0.9945%\n",
      "Epoch [5/300], Step [177/225], Training Accuracy: 50.1589%, Training Loss: 0.9941%\n",
      "Epoch [5/300], Step [178/225], Training Accuracy: 50.1053%, Training Loss: 0.9942%\n",
      "Epoch [5/300], Step [179/225], Training Accuracy: 50.1484%, Training Loss: 0.9933%\n",
      "Epoch [5/300], Step [180/225], Training Accuracy: 50.2170%, Training Loss: 0.9927%\n",
      "Epoch [5/300], Step [181/225], Training Accuracy: 50.1381%, Training Loss: 0.9933%\n",
      "Epoch [5/300], Step [182/225], Training Accuracy: 50.2060%, Training Loss: 0.9928%\n",
      "Epoch [5/300], Step [183/225], Training Accuracy: 50.1878%, Training Loss: 0.9928%\n",
      "Epoch [5/300], Step [184/225], Training Accuracy: 50.1783%, Training Loss: 0.9925%\n",
      "Epoch [5/300], Step [185/225], Training Accuracy: 50.2196%, Training Loss: 0.9922%\n",
      "Epoch [5/300], Step [186/225], Training Accuracy: 50.2856%, Training Loss: 0.9914%\n",
      "Epoch [5/300], Step [187/225], Training Accuracy: 50.3259%, Training Loss: 0.9909%\n",
      "Epoch [5/300], Step [188/225], Training Accuracy: 50.3241%, Training Loss: 0.9905%\n",
      "Epoch [5/300], Step [189/225], Training Accuracy: 50.3390%, Training Loss: 0.9899%\n",
      "Epoch [5/300], Step [190/225], Training Accuracy: 50.3372%, Training Loss: 0.9901%\n",
      "Epoch [5/300], Step [191/225], Training Accuracy: 50.3681%, Training Loss: 0.9896%\n",
      "Epoch [5/300], Step [192/225], Training Accuracy: 50.4557%, Training Loss: 0.9889%\n",
      "Epoch [5/300], Step [193/225], Training Accuracy: 50.4534%, Training Loss: 0.9890%\n",
      "Epoch [5/300], Step [194/225], Training Accuracy: 50.4510%, Training Loss: 0.9888%\n",
      "Epoch [5/300], Step [195/225], Training Accuracy: 50.4327%, Training Loss: 0.9885%\n",
      "Epoch [5/300], Step [196/225], Training Accuracy: 50.3747%, Training Loss: 0.9889%\n",
      "Epoch [5/300], Step [197/225], Training Accuracy: 50.4204%, Training Loss: 0.9887%\n",
      "Epoch [5/300], Step [198/225], Training Accuracy: 50.4498%, Training Loss: 0.9883%\n",
      "Epoch [5/300], Step [199/225], Training Accuracy: 50.4947%, Training Loss: 0.9875%\n",
      "Epoch [5/300], Step [200/225], Training Accuracy: 50.5156%, Training Loss: 0.9876%\n",
      "Epoch [5/300], Step [201/225], Training Accuracy: 50.4975%, Training Loss: 0.9880%\n",
      "Epoch [5/300], Step [202/225], Training Accuracy: 50.5105%, Training Loss: 0.9878%\n",
      "Epoch [5/300], Step [203/225], Training Accuracy: 50.5003%, Training Loss: 0.9881%\n",
      "Epoch [5/300], Step [204/225], Training Accuracy: 50.5208%, Training Loss: 0.9875%\n",
      "Epoch [5/300], Step [205/225], Training Accuracy: 50.5335%, Training Loss: 0.9874%\n",
      "Epoch [5/300], Step [206/225], Training Accuracy: 50.5006%, Training Loss: 0.9876%\n",
      "Epoch [5/300], Step [207/225], Training Accuracy: 50.4680%, Training Loss: 0.9876%\n",
      "Epoch [5/300], Step [208/225], Training Accuracy: 50.4808%, Training Loss: 0.9874%\n",
      "Epoch [5/300], Step [209/225], Training Accuracy: 50.4560%, Training Loss: 0.9873%\n",
      "Epoch [5/300], Step [210/225], Training Accuracy: 50.4688%, Training Loss: 0.9871%\n",
      "Epoch [5/300], Step [211/225], Training Accuracy: 50.5184%, Training Loss: 0.9867%\n",
      "Epoch [5/300], Step [212/225], Training Accuracy: 50.5307%, Training Loss: 0.9867%\n",
      "Epoch [5/300], Step [213/225], Training Accuracy: 50.5062%, Training Loss: 0.9872%\n",
      "Epoch [5/300], Step [214/225], Training Accuracy: 50.5403%, Training Loss: 0.9869%\n",
      "Epoch [5/300], Step [215/225], Training Accuracy: 50.5305%, Training Loss: 0.9873%\n",
      "Epoch [5/300], Step [216/225], Training Accuracy: 50.5570%, Training Loss: 0.9874%\n",
      "Epoch [5/300], Step [217/225], Training Accuracy: 50.5328%, Training Loss: 0.9875%\n",
      "Epoch [5/300], Step [218/225], Training Accuracy: 50.5161%, Training Loss: 0.9880%\n",
      "Epoch [5/300], Step [219/225], Training Accuracy: 50.5137%, Training Loss: 0.9875%\n",
      "Epoch [5/300], Step [220/225], Training Accuracy: 50.5185%, Training Loss: 0.9874%\n",
      "Epoch [5/300], Step [221/225], Training Accuracy: 50.5232%, Training Loss: 0.9876%\n",
      "Epoch [5/300], Step [222/225], Training Accuracy: 50.5701%, Training Loss: 0.9870%\n",
      "Epoch [5/300], Step [223/225], Training Accuracy: 50.5675%, Training Loss: 0.9870%\n",
      "Epoch [5/300], Step [224/225], Training Accuracy: 50.5859%, Training Loss: 0.9864%\n",
      "Epoch [5/300], Step [225/225], Training Accuracy: 50.5628%, Training Loss: 0.9869%\n",
      "Epoch [6/300], Step [1/225], Training Accuracy: 60.9375%, Training Loss: 0.8638%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [2/225], Training Accuracy: 60.1562%, Training Loss: 0.9199%\n",
      "Epoch [6/300], Step [3/225], Training Accuracy: 57.2917%, Training Loss: 0.9507%\n",
      "Epoch [6/300], Step [4/225], Training Accuracy: 57.0312%, Training Loss: 0.9403%\n",
      "Epoch [6/300], Step [5/225], Training Accuracy: 58.1250%, Training Loss: 0.9447%\n",
      "Epoch [6/300], Step [6/225], Training Accuracy: 56.5104%, Training Loss: 0.9858%\n",
      "Epoch [6/300], Step [7/225], Training Accuracy: 56.0268%, Training Loss: 0.9857%\n",
      "Epoch [6/300], Step [8/225], Training Accuracy: 55.2734%, Training Loss: 0.9933%\n",
      "Epoch [6/300], Step [9/225], Training Accuracy: 53.8194%, Training Loss: 0.9965%\n",
      "Epoch [6/300], Step [10/225], Training Accuracy: 53.2812%, Training Loss: 1.0122%\n",
      "Epoch [6/300], Step [11/225], Training Accuracy: 52.5568%, Training Loss: 1.0155%\n",
      "Epoch [6/300], Step [12/225], Training Accuracy: 53.3854%, Training Loss: 1.0030%\n",
      "Epoch [6/300], Step [13/225], Training Accuracy: 54.5673%, Training Loss: 0.9911%\n",
      "Epoch [6/300], Step [14/225], Training Accuracy: 54.1295%, Training Loss: 0.9923%\n",
      "Epoch [6/300], Step [15/225], Training Accuracy: 53.5417%, Training Loss: 0.9951%\n",
      "Epoch [6/300], Step [16/225], Training Accuracy: 53.3203%, Training Loss: 0.9914%\n",
      "Epoch [6/300], Step [17/225], Training Accuracy: 53.6765%, Training Loss: 0.9894%\n",
      "Epoch [6/300], Step [18/225], Training Accuracy: 53.2986%, Training Loss: 0.9896%\n",
      "Epoch [6/300], Step [19/225], Training Accuracy: 53.2072%, Training Loss: 0.9876%\n",
      "Epoch [6/300], Step [20/225], Training Accuracy: 53.0469%, Training Loss: 0.9844%\n",
      "Epoch [6/300], Step [21/225], Training Accuracy: 53.6458%, Training Loss: 0.9775%\n",
      "Epoch [6/300], Step [22/225], Training Accuracy: 53.1250%, Training Loss: 0.9832%\n",
      "Epoch [6/300], Step [23/225], Training Accuracy: 53.6685%, Training Loss: 0.9774%\n",
      "Epoch [6/300], Step [24/225], Training Accuracy: 53.2552%, Training Loss: 0.9782%\n",
      "Epoch [6/300], Step [25/225], Training Accuracy: 53.1875%, Training Loss: 0.9742%\n",
      "Epoch [6/300], Step [26/225], Training Accuracy: 52.9447%, Training Loss: 0.9716%\n",
      "Epoch [6/300], Step [27/225], Training Accuracy: 52.5463%, Training Loss: 0.9724%\n",
      "Epoch [6/300], Step [28/225], Training Accuracy: 52.8460%, Training Loss: 0.9700%\n",
      "Epoch [6/300], Step [29/225], Training Accuracy: 53.1789%, Training Loss: 0.9655%\n",
      "Epoch [6/300], Step [30/225], Training Accuracy: 53.5417%, Training Loss: 0.9612%\n",
      "Epoch [6/300], Step [31/225], Training Accuracy: 53.3770%, Training Loss: 0.9664%\n",
      "Epoch [6/300], Step [32/225], Training Accuracy: 53.3691%, Training Loss: 0.9633%\n",
      "Epoch [6/300], Step [33/225], Training Accuracy: 53.5985%, Training Loss: 0.9596%\n",
      "Epoch [6/300], Step [34/225], Training Accuracy: 53.4467%, Training Loss: 0.9653%\n",
      "Epoch [6/300], Step [35/225], Training Accuracy: 53.7054%, Training Loss: 0.9641%\n",
      "Epoch [6/300], Step [36/225], Training Accuracy: 53.7326%, Training Loss: 0.9641%\n",
      "Epoch [6/300], Step [37/225], Training Accuracy: 53.5051%, Training Loss: 0.9637%\n",
      "Epoch [6/300], Step [38/225], Training Accuracy: 53.5362%, Training Loss: 0.9624%\n",
      "Epoch [6/300], Step [39/225], Training Accuracy: 53.3654%, Training Loss: 0.9629%\n",
      "Epoch [6/300], Step [40/225], Training Accuracy: 53.0469%, Training Loss: 0.9663%\n",
      "Epoch [6/300], Step [41/225], Training Accuracy: 52.8582%, Training Loss: 0.9677%\n",
      "Epoch [6/300], Step [42/225], Training Accuracy: 52.5670%, Training Loss: 0.9677%\n",
      "Epoch [6/300], Step [43/225], Training Accuracy: 52.5436%, Training Loss: 0.9667%\n",
      "Epoch [6/300], Step [44/225], Training Accuracy: 52.6989%, Training Loss: 0.9648%\n",
      "Epoch [6/300], Step [45/225], Training Accuracy: 52.9167%, Training Loss: 0.9621%\n",
      "Epoch [6/300], Step [46/225], Training Accuracy: 53.0231%, Training Loss: 0.9611%\n",
      "Epoch [6/300], Step [47/225], Training Accuracy: 53.0253%, Training Loss: 0.9595%\n",
      "Epoch [6/300], Step [48/225], Training Accuracy: 53.0273%, Training Loss: 0.9585%\n",
      "Epoch [6/300], Step [49/225], Training Accuracy: 53.0931%, Training Loss: 0.9570%\n",
      "Epoch [6/300], Step [50/225], Training Accuracy: 53.2500%, Training Loss: 0.9554%\n",
      "Epoch [6/300], Step [51/225], Training Accuracy: 53.5233%, Training Loss: 0.9542%\n",
      "Epoch [6/300], Step [52/225], Training Accuracy: 53.6659%, Training Loss: 0.9529%\n",
      "Epoch [6/300], Step [53/225], Training Accuracy: 53.6262%, Training Loss: 0.9519%\n",
      "Epoch [6/300], Step [54/225], Training Accuracy: 53.4722%, Training Loss: 0.9527%\n",
      "Epoch [6/300], Step [55/225], Training Accuracy: 53.3807%, Training Loss: 0.9532%\n",
      "Epoch [6/300], Step [56/225], Training Accuracy: 53.3761%, Training Loss: 0.9528%\n",
      "Epoch [6/300], Step [57/225], Training Accuracy: 53.5088%, Training Loss: 0.9517%\n",
      "Epoch [6/300], Step [58/225], Training Accuracy: 53.3944%, Training Loss: 0.9539%\n",
      "Epoch [6/300], Step [59/225], Training Accuracy: 53.4693%, Training Loss: 0.9520%\n",
      "Epoch [6/300], Step [60/225], Training Accuracy: 53.3333%, Training Loss: 0.9506%\n",
      "Epoch [6/300], Step [61/225], Training Accuracy: 53.3555%, Training Loss: 0.9501%\n",
      "Epoch [6/300], Step [62/225], Training Accuracy: 53.3014%, Training Loss: 0.9506%\n",
      "Epoch [6/300], Step [63/225], Training Accuracy: 53.1002%, Training Loss: 0.9515%\n",
      "Epoch [6/300], Step [64/225], Training Accuracy: 53.2715%, Training Loss: 0.9506%\n",
      "Epoch [6/300], Step [65/225], Training Accuracy: 53.2933%, Training Loss: 0.9504%\n",
      "Epoch [6/300], Step [66/225], Training Accuracy: 53.4328%, Training Loss: 0.9491%\n",
      "Epoch [6/300], Step [67/225], Training Accuracy: 53.4515%, Training Loss: 0.9509%\n",
      "Epoch [6/300], Step [68/225], Training Accuracy: 53.4697%, Training Loss: 0.9522%\n",
      "Epoch [6/300], Step [69/225], Training Accuracy: 53.4420%, Training Loss: 0.9520%\n",
      "Epoch [6/300], Step [70/225], Training Accuracy: 53.3259%, Training Loss: 0.9513%\n",
      "Epoch [6/300], Step [71/225], Training Accuracy: 53.3011%, Training Loss: 0.9541%\n",
      "Epoch [6/300], Step [72/225], Training Accuracy: 53.1467%, Training Loss: 0.9555%\n",
      "Epoch [6/300], Step [73/225], Training Accuracy: 52.9538%, Training Loss: 0.9574%\n",
      "Epoch [6/300], Step [74/225], Training Accuracy: 52.9772%, Training Loss: 0.9556%\n",
      "Epoch [6/300], Step [75/225], Training Accuracy: 53.0417%, Training Loss: 0.9544%\n",
      "Epoch [6/300], Step [76/225], Training Accuracy: 53.0016%, Training Loss: 0.9551%\n",
      "Epoch [6/300], Step [77/225], Training Accuracy: 53.0032%, Training Loss: 0.9557%\n",
      "Epoch [6/300], Step [78/225], Training Accuracy: 53.0248%, Training Loss: 0.9556%\n",
      "Epoch [6/300], Step [79/225], Training Accuracy: 53.0459%, Training Loss: 0.9561%\n",
      "Epoch [6/300], Step [80/225], Training Accuracy: 52.9492%, Training Loss: 0.9574%\n",
      "Epoch [6/300], Step [81/225], Training Accuracy: 52.9900%, Training Loss: 0.9579%\n",
      "Epoch [6/300], Step [82/225], Training Accuracy: 53.0869%, Training Loss: 0.9572%\n",
      "Epoch [6/300], Step [83/225], Training Accuracy: 53.1250%, Training Loss: 0.9560%\n",
      "Epoch [6/300], Step [84/225], Training Accuracy: 53.1622%, Training Loss: 0.9555%\n",
      "Epoch [6/300], Step [85/225], Training Accuracy: 53.2537%, Training Loss: 0.9547%\n",
      "Epoch [6/300], Step [86/225], Training Accuracy: 53.1977%, Training Loss: 0.9552%\n",
      "Epoch [6/300], Step [87/225], Training Accuracy: 53.1789%, Training Loss: 0.9564%\n",
      "Epoch [6/300], Step [88/225], Training Accuracy: 53.1250%, Training Loss: 0.9576%\n",
      "Epoch [6/300], Step [89/225], Training Accuracy: 52.9670%, Training Loss: 0.9605%\n",
      "Epoch [6/300], Step [90/225], Training Accuracy: 52.8646%, Training Loss: 0.9621%\n",
      "Epoch [6/300], Step [91/225], Training Accuracy: 52.8159%, Training Loss: 0.9628%\n",
      "Epoch [6/300], Step [92/225], Training Accuracy: 52.8193%, Training Loss: 0.9619%\n",
      "Epoch [6/300], Step [93/225], Training Accuracy: 52.8730%, Training Loss: 0.9612%\n",
      "Epoch [6/300], Step [94/225], Training Accuracy: 52.9089%, Training Loss: 0.9598%\n",
      "Epoch [6/300], Step [95/225], Training Accuracy: 52.9112%, Training Loss: 0.9604%\n",
      "Epoch [6/300], Step [96/225], Training Accuracy: 53.0599%, Training Loss: 0.9592%\n",
      "Epoch [6/300], Step [97/225], Training Accuracy: 53.0122%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [98/225], Training Accuracy: 52.9656%, Training Loss: 0.9589%\n",
      "Epoch [6/300], Step [99/225], Training Accuracy: 53.0145%, Training Loss: 0.9585%\n",
      "Epoch [6/300], Step [100/225], Training Accuracy: 52.9375%, Training Loss: 0.9594%\n",
      "Epoch [6/300], Step [101/225], Training Accuracy: 52.9084%, Training Loss: 0.9595%\n",
      "Epoch [6/300], Step [102/225], Training Accuracy: 52.9105%, Training Loss: 0.9587%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [103/225], Training Accuracy: 52.9733%, Training Loss: 0.9583%\n",
      "Epoch [6/300], Step [104/225], Training Accuracy: 52.8546%, Training Loss: 0.9587%\n",
      "Epoch [6/300], Step [105/225], Training Accuracy: 52.8125%, Training Loss: 0.9595%\n",
      "Epoch [6/300], Step [106/225], Training Accuracy: 52.8449%, Training Loss: 0.9594%\n",
      "Epoch [6/300], Step [107/225], Training Accuracy: 52.8329%, Training Loss: 0.9603%\n",
      "Epoch [6/300], Step [108/225], Training Accuracy: 52.8501%, Training Loss: 0.9600%\n",
      "Epoch [6/300], Step [109/225], Training Accuracy: 52.8240%, Training Loss: 0.9601%\n",
      "Epoch [6/300], Step [110/225], Training Accuracy: 52.8267%, Training Loss: 0.9603%\n",
      "Epoch [6/300], Step [111/225], Training Accuracy: 52.7449%, Training Loss: 0.9602%\n",
      "Epoch [6/300], Step [112/225], Training Accuracy: 52.7483%, Training Loss: 0.9599%\n",
      "Epoch [6/300], Step [113/225], Training Accuracy: 52.7931%, Training Loss: 0.9601%\n",
      "Epoch [6/300], Step [114/225], Training Accuracy: 52.8235%, Training Loss: 0.9594%\n",
      "Epoch [6/300], Step [115/225], Training Accuracy: 52.8940%, Training Loss: 0.9585%\n",
      "Epoch [6/300], Step [116/225], Training Accuracy: 52.8825%, Training Loss: 0.9584%\n",
      "Epoch [6/300], Step [117/225], Training Accuracy: 52.8446%, Training Loss: 0.9601%\n",
      "Epoch [6/300], Step [118/225], Training Accuracy: 52.8072%, Training Loss: 0.9607%\n",
      "Epoch [6/300], Step [119/225], Training Accuracy: 52.7967%, Training Loss: 0.9610%\n",
      "Epoch [6/300], Step [120/225], Training Accuracy: 52.7474%, Training Loss: 0.9622%\n",
      "Epoch [6/300], Step [121/225], Training Accuracy: 52.6343%, Training Loss: 0.9630%\n",
      "Epoch [6/300], Step [122/225], Training Accuracy: 52.6767%, Training Loss: 0.9622%\n",
      "Epoch [6/300], Step [123/225], Training Accuracy: 52.6804%, Training Loss: 0.9618%\n",
      "Epoch [6/300], Step [124/225], Training Accuracy: 52.7092%, Training Loss: 0.9618%\n",
      "Epoch [6/300], Step [125/225], Training Accuracy: 52.6625%, Training Loss: 0.9632%\n",
      "Epoch [6/300], Step [126/225], Training Accuracy: 52.6538%, Training Loss: 0.9641%\n",
      "Epoch [6/300], Step [127/225], Training Accuracy: 52.6329%, Training Loss: 0.9649%\n",
      "Epoch [6/300], Step [128/225], Training Accuracy: 52.6367%, Training Loss: 0.9656%\n",
      "Epoch [6/300], Step [129/225], Training Accuracy: 52.6890%, Training Loss: 0.9661%\n",
      "Epoch [6/300], Step [130/225], Training Accuracy: 52.5962%, Training Loss: 0.9670%\n",
      "Epoch [6/300], Step [131/225], Training Accuracy: 52.5763%, Training Loss: 0.9680%\n",
      "Epoch [6/300], Step [132/225], Training Accuracy: 52.4976%, Training Loss: 0.9690%\n",
      "Epoch [6/300], Step [133/225], Training Accuracy: 52.4906%, Training Loss: 0.9692%\n",
      "Epoch [6/300], Step [134/225], Training Accuracy: 52.4254%, Training Loss: 0.9701%\n",
      "Epoch [6/300], Step [135/225], Training Accuracy: 52.4653%, Training Loss: 0.9701%\n",
      "Epoch [6/300], Step [136/225], Training Accuracy: 52.5046%, Training Loss: 0.9698%\n",
      "Epoch [6/300], Step [137/225], Training Accuracy: 52.5319%, Training Loss: 0.9692%\n",
      "Epoch [6/300], Step [138/225], Training Accuracy: 52.5702%, Training Loss: 0.9686%\n",
      "Epoch [6/300], Step [139/225], Training Accuracy: 52.5629%, Training Loss: 0.9681%\n",
      "Epoch [6/300], Step [140/225], Training Accuracy: 52.6116%, Training Loss: 0.9675%\n",
      "Epoch [6/300], Step [141/225], Training Accuracy: 52.5931%, Training Loss: 0.9672%\n",
      "Epoch [6/300], Step [142/225], Training Accuracy: 52.6078%, Training Loss: 0.9670%\n",
      "Epoch [6/300], Step [143/225], Training Accuracy: 52.6224%, Training Loss: 0.9666%\n",
      "Epoch [6/300], Step [144/225], Training Accuracy: 52.6693%, Training Loss: 0.9661%\n",
      "Epoch [6/300], Step [145/225], Training Accuracy: 52.7155%, Training Loss: 0.9654%\n",
      "Epoch [6/300], Step [146/225], Training Accuracy: 52.6648%, Training Loss: 0.9662%\n",
      "Epoch [6/300], Step [147/225], Training Accuracy: 52.5829%, Training Loss: 0.9667%\n",
      "Epoch [6/300], Step [148/225], Training Accuracy: 52.5971%, Training Loss: 0.9667%\n",
      "Epoch [6/300], Step [149/225], Training Accuracy: 52.4958%, Training Loss: 0.9678%\n",
      "Epoch [6/300], Step [150/225], Training Accuracy: 52.5521%, Training Loss: 0.9669%\n",
      "Epoch [6/300], Step [151/225], Training Accuracy: 52.5559%, Training Loss: 0.9666%\n",
      "Epoch [6/300], Step [152/225], Training Accuracy: 52.5082%, Training Loss: 0.9671%\n",
      "Epoch [6/300], Step [153/225], Training Accuracy: 52.5123%, Training Loss: 0.9668%\n",
      "Epoch [6/300], Step [154/225], Training Accuracy: 52.4655%, Training Loss: 0.9670%\n",
      "Epoch [6/300], Step [155/225], Training Accuracy: 52.4294%, Training Loss: 0.9668%\n",
      "Epoch [6/300], Step [156/225], Training Accuracy: 52.4539%, Training Loss: 0.9670%\n",
      "Epoch [6/300], Step [157/225], Training Accuracy: 52.4781%, Training Loss: 0.9668%\n",
      "Epoch [6/300], Step [158/225], Training Accuracy: 52.3833%, Training Loss: 0.9680%\n",
      "Epoch [6/300], Step [159/225], Training Accuracy: 52.3388%, Training Loss: 0.9684%\n",
      "Epoch [6/300], Step [160/225], Training Accuracy: 52.2949%, Training Loss: 0.9683%\n",
      "Epoch [6/300], Step [161/225], Training Accuracy: 52.3001%, Training Loss: 0.9681%\n",
      "Epoch [6/300], Step [162/225], Training Accuracy: 52.4016%, Training Loss: 0.9676%\n",
      "Epoch [6/300], Step [163/225], Training Accuracy: 52.3965%, Training Loss: 0.9675%\n",
      "Epoch [6/300], Step [164/225], Training Accuracy: 52.4581%, Training Loss: 0.9668%\n",
      "Epoch [6/300], Step [165/225], Training Accuracy: 52.4905%, Training Loss: 0.9670%\n",
      "Epoch [6/300], Step [166/225], Training Accuracy: 52.4849%, Training Loss: 0.9667%\n",
      "Epoch [6/300], Step [167/225], Training Accuracy: 52.4701%, Training Loss: 0.9664%\n",
      "Epoch [6/300], Step [168/225], Training Accuracy: 52.4647%, Training Loss: 0.9661%\n",
      "Epoch [6/300], Step [169/225], Training Accuracy: 52.4501%, Training Loss: 0.9660%\n",
      "Epoch [6/300], Step [170/225], Training Accuracy: 52.4265%, Training Loss: 0.9659%\n",
      "Epoch [6/300], Step [171/225], Training Accuracy: 52.4762%, Training Loss: 0.9655%\n",
      "Epoch [6/300], Step [172/225], Training Accuracy: 52.4709%, Training Loss: 0.9656%\n",
      "Epoch [6/300], Step [173/225], Training Accuracy: 52.4747%, Training Loss: 0.9654%\n",
      "Epoch [6/300], Step [174/225], Training Accuracy: 52.4335%, Training Loss: 0.9664%\n",
      "Epoch [6/300], Step [175/225], Training Accuracy: 52.5268%, Training Loss: 0.9661%\n",
      "Epoch [6/300], Step [176/225], Training Accuracy: 52.5124%, Training Loss: 0.9663%\n",
      "Epoch [6/300], Step [177/225], Training Accuracy: 52.5335%, Training Loss: 0.9658%\n",
      "Epoch [6/300], Step [178/225], Training Accuracy: 52.4842%, Training Loss: 0.9660%\n",
      "Epoch [6/300], Step [179/225], Training Accuracy: 52.5576%, Training Loss: 0.9651%\n",
      "Epoch [6/300], Step [180/225], Training Accuracy: 52.6042%, Training Loss: 0.9649%\n",
      "Epoch [6/300], Step [181/225], Training Accuracy: 52.5466%, Training Loss: 0.9657%\n",
      "Epoch [6/300], Step [182/225], Training Accuracy: 52.5498%, Training Loss: 0.9652%\n",
      "Epoch [6/300], Step [183/225], Training Accuracy: 52.5615%, Training Loss: 0.9650%\n",
      "Epoch [6/300], Step [184/225], Training Accuracy: 52.5730%, Training Loss: 0.9647%\n",
      "Epoch [6/300], Step [185/225], Training Accuracy: 52.6182%, Training Loss: 0.9647%\n",
      "Epoch [6/300], Step [186/225], Training Accuracy: 52.6294%, Training Loss: 0.9641%\n",
      "Epoch [6/300], Step [187/225], Training Accuracy: 52.6654%, Training Loss: 0.9634%\n",
      "Epoch [6/300], Step [188/225], Training Accuracy: 52.6513%, Training Loss: 0.9627%\n",
      "Epoch [6/300], Step [189/225], Training Accuracy: 52.6703%, Training Loss: 0.9619%\n",
      "Epoch [6/300], Step [190/225], Training Accuracy: 52.7138%, Training Loss: 0.9619%\n",
      "Epoch [6/300], Step [191/225], Training Accuracy: 52.7241%, Training Loss: 0.9615%\n",
      "Epoch [6/300], Step [192/225], Training Accuracy: 52.7751%, Training Loss: 0.9609%\n",
      "Epoch [6/300], Step [193/225], Training Accuracy: 52.7850%, Training Loss: 0.9613%\n",
      "Epoch [6/300], Step [194/225], Training Accuracy: 52.8270%, Training Loss: 0.9608%\n",
      "Epoch [6/300], Step [195/225], Training Accuracy: 52.8606%, Training Loss: 0.9602%\n",
      "Epoch [6/300], Step [196/225], Training Accuracy: 52.8141%, Training Loss: 0.9607%\n",
      "Epoch [6/300], Step [197/225], Training Accuracy: 52.8315%, Training Loss: 0.9608%\n",
      "Epoch [6/300], Step [198/225], Training Accuracy: 52.8804%, Training Loss: 0.9598%\n",
      "Epoch [6/300], Step [199/225], Training Accuracy: 52.9287%, Training Loss: 0.9589%\n",
      "Epoch [6/300], Step [200/225], Training Accuracy: 52.9375%, Training Loss: 0.9594%\n",
      "Epoch [6/300], Step [201/225], Training Accuracy: 52.8996%, Training Loss: 0.9599%\n",
      "Epoch [6/300], Step [202/225], Training Accuracy: 52.9316%, Training Loss: 0.9599%\n",
      "Epoch [6/300], Step [203/225], Training Accuracy: 52.9249%, Training Loss: 0.9600%\n",
      "Epoch [6/300], Step [204/225], Training Accuracy: 52.9718%, Training Loss: 0.9598%\n",
      "Epoch [6/300], Step [205/225], Training Accuracy: 53.0259%, Training Loss: 0.9598%\n",
      "Epoch [6/300], Step [206/225], Training Accuracy: 52.9885%, Training Loss: 0.9601%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [207/225], Training Accuracy: 52.9891%, Training Loss: 0.9601%\n",
      "Epoch [6/300], Step [208/225], Training Accuracy: 52.9898%, Training Loss: 0.9598%\n",
      "Epoch [6/300], Step [209/225], Training Accuracy: 53.0129%, Training Loss: 0.9595%\n",
      "Epoch [6/300], Step [210/225], Training Accuracy: 52.9985%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [211/225], Training Accuracy: 53.0435%, Training Loss: 0.9589%\n",
      "Epoch [6/300], Step [212/225], Training Accuracy: 53.0660%, Training Loss: 0.9588%\n",
      "Epoch [6/300], Step [213/225], Training Accuracy: 53.0443%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [214/225], Training Accuracy: 53.0739%, Training Loss: 0.9591%\n",
      "Epoch [6/300], Step [215/225], Training Accuracy: 53.0596%, Training Loss: 0.9594%\n",
      "Epoch [6/300], Step [216/225], Training Accuracy: 53.1105%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [217/225], Training Accuracy: 53.1034%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [218/225], Training Accuracy: 53.0748%, Training Loss: 0.9597%\n",
      "Epoch [6/300], Step [219/225], Training Accuracy: 53.1036%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [220/225], Training Accuracy: 53.1037%, Training Loss: 0.9589%\n",
      "Epoch [6/300], Step [221/225], Training Accuracy: 53.0684%, Training Loss: 0.9593%\n",
      "Epoch [6/300], Step [222/225], Training Accuracy: 53.0757%, Training Loss: 0.9591%\n",
      "Epoch [6/300], Step [223/225], Training Accuracy: 53.0689%, Training Loss: 0.9592%\n",
      "Epoch [6/300], Step [224/225], Training Accuracy: 53.0483%, Training Loss: 0.9588%\n",
      "Epoch [6/300], Step [225/225], Training Accuracy: 53.0225%, Training Loss: 0.9590%\n",
      "Epoch [7/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8233%\n",
      "Epoch [7/300], Step [2/225], Training Accuracy: 59.3750%, Training Loss: 0.8981%\n",
      "Epoch [7/300], Step [3/225], Training Accuracy: 56.7708%, Training Loss: 0.9265%\n",
      "Epoch [7/300], Step [4/225], Training Accuracy: 56.2500%, Training Loss: 0.9346%\n",
      "Epoch [7/300], Step [5/225], Training Accuracy: 57.5000%, Training Loss: 0.9363%\n",
      "Epoch [7/300], Step [6/225], Training Accuracy: 55.2083%, Training Loss: 0.9864%\n",
      "Epoch [7/300], Step [7/225], Training Accuracy: 56.0268%, Training Loss: 0.9747%\n",
      "Epoch [7/300], Step [8/225], Training Accuracy: 55.2734%, Training Loss: 0.9708%\n",
      "Epoch [7/300], Step [9/225], Training Accuracy: 53.6458%, Training Loss: 0.9756%\n",
      "Epoch [7/300], Step [10/225], Training Accuracy: 53.4375%, Training Loss: 0.9869%\n",
      "Epoch [7/300], Step [11/225], Training Accuracy: 53.6932%, Training Loss: 0.9901%\n",
      "Epoch [7/300], Step [12/225], Training Accuracy: 53.9062%, Training Loss: 0.9835%\n",
      "Epoch [7/300], Step [13/225], Training Accuracy: 54.6875%, Training Loss: 0.9686%\n",
      "Epoch [7/300], Step [14/225], Training Accuracy: 54.9107%, Training Loss: 0.9664%\n",
      "Epoch [7/300], Step [15/225], Training Accuracy: 54.6875%, Training Loss: 0.9681%\n",
      "Epoch [7/300], Step [16/225], Training Accuracy: 54.2969%, Training Loss: 0.9652%\n",
      "Epoch [7/300], Step [17/225], Training Accuracy: 54.5956%, Training Loss: 0.9633%\n",
      "Epoch [7/300], Step [18/225], Training Accuracy: 54.6875%, Training Loss: 0.9620%\n",
      "Epoch [7/300], Step [19/225], Training Accuracy: 54.5230%, Training Loss: 0.9606%\n",
      "Epoch [7/300], Step [20/225], Training Accuracy: 54.4531%, Training Loss: 0.9583%\n",
      "Epoch [7/300], Step [21/225], Training Accuracy: 55.1339%, Training Loss: 0.9528%\n",
      "Epoch [7/300], Step [22/225], Training Accuracy: 54.6875%, Training Loss: 0.9589%\n",
      "Epoch [7/300], Step [23/225], Training Accuracy: 54.9592%, Training Loss: 0.9540%\n",
      "Epoch [7/300], Step [24/225], Training Accuracy: 54.5573%, Training Loss: 0.9545%\n",
      "Epoch [7/300], Step [25/225], Training Accuracy: 54.6875%, Training Loss: 0.9489%\n",
      "Epoch [7/300], Step [26/225], Training Accuracy: 54.6875%, Training Loss: 0.9467%\n",
      "Epoch [7/300], Step [27/225], Training Accuracy: 54.6875%, Training Loss: 0.9463%\n",
      "Epoch [7/300], Step [28/225], Training Accuracy: 54.9665%, Training Loss: 0.9431%\n",
      "Epoch [7/300], Step [29/225], Training Accuracy: 54.8491%, Training Loss: 0.9413%\n",
      "Epoch [7/300], Step [30/225], Training Accuracy: 55.2604%, Training Loss: 0.9380%\n",
      "Epoch [7/300], Step [31/225], Training Accuracy: 54.8891%, Training Loss: 0.9420%\n",
      "Epoch [7/300], Step [32/225], Training Accuracy: 55.0781%, Training Loss: 0.9384%\n",
      "Epoch [7/300], Step [33/225], Training Accuracy: 55.3504%, Training Loss: 0.9339%\n",
      "Epoch [7/300], Step [34/225], Training Accuracy: 55.0092%, Training Loss: 0.9370%\n",
      "Epoch [7/300], Step [35/225], Training Accuracy: 55.0000%, Training Loss: 0.9395%\n",
      "Epoch [7/300], Step [36/225], Training Accuracy: 55.2083%, Training Loss: 0.9389%\n",
      "Epoch [7/300], Step [37/225], Training Accuracy: 55.0676%, Training Loss: 0.9379%\n",
      "Epoch [7/300], Step [38/225], Training Accuracy: 55.0987%, Training Loss: 0.9388%\n",
      "Epoch [7/300], Step [39/225], Training Accuracy: 55.0080%, Training Loss: 0.9391%\n",
      "Epoch [7/300], Step [40/225], Training Accuracy: 54.9219%, Training Loss: 0.9424%\n",
      "Epoch [7/300], Step [41/225], Training Accuracy: 54.7256%, Training Loss: 0.9451%\n",
      "Epoch [7/300], Step [42/225], Training Accuracy: 54.5387%, Training Loss: 0.9448%\n",
      "Epoch [7/300], Step [43/225], Training Accuracy: 54.6148%, Training Loss: 0.9435%\n",
      "Epoch [7/300], Step [44/225], Training Accuracy: 54.9361%, Training Loss: 0.9432%\n",
      "Epoch [7/300], Step [45/225], Training Accuracy: 54.9653%, Training Loss: 0.9407%\n",
      "Epoch [7/300], Step [46/225], Training Accuracy: 55.0951%, Training Loss: 0.9401%\n",
      "Epoch [7/300], Step [47/225], Training Accuracy: 55.2527%, Training Loss: 0.9382%\n",
      "Epoch [7/300], Step [48/225], Training Accuracy: 55.1432%, Training Loss: 0.9373%\n",
      "Epoch [7/300], Step [49/225], Training Accuracy: 55.1339%, Training Loss: 0.9353%\n",
      "Epoch [7/300], Step [50/225], Training Accuracy: 55.1875%, Training Loss: 0.9338%\n",
      "Epoch [7/300], Step [51/225], Training Accuracy: 55.3615%, Training Loss: 0.9322%\n",
      "Epoch [7/300], Step [52/225], Training Accuracy: 55.4387%, Training Loss: 0.9306%\n",
      "Epoch [7/300], Step [53/225], Training Accuracy: 55.4245%, Training Loss: 0.9301%\n",
      "Epoch [7/300], Step [54/225], Training Accuracy: 55.3241%, Training Loss: 0.9305%\n",
      "Epoch [7/300], Step [55/225], Training Accuracy: 55.1705%, Training Loss: 0.9312%\n",
      "Epoch [7/300], Step [56/225], Training Accuracy: 55.1897%, Training Loss: 0.9300%\n",
      "Epoch [7/300], Step [57/225], Training Accuracy: 55.3454%, Training Loss: 0.9281%\n",
      "Epoch [7/300], Step [58/225], Training Accuracy: 55.2263%, Training Loss: 0.9301%\n",
      "Epoch [7/300], Step [59/225], Training Accuracy: 55.3496%, Training Loss: 0.9280%\n",
      "Epoch [7/300], Step [60/225], Training Accuracy: 55.3906%, Training Loss: 0.9270%\n",
      "Epoch [7/300], Step [61/225], Training Accuracy: 55.3279%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [62/225], Training Accuracy: 55.1663%, Training Loss: 0.9278%\n",
      "Epoch [7/300], Step [63/225], Training Accuracy: 54.9107%, Training Loss: 0.9298%\n",
      "Epoch [7/300], Step [64/225], Training Accuracy: 54.9316%, Training Loss: 0.9285%\n",
      "Epoch [7/300], Step [65/225], Training Accuracy: 54.8317%, Training Loss: 0.9288%\n",
      "Epoch [7/300], Step [66/225], Training Accuracy: 54.9716%, Training Loss: 0.9277%\n",
      "Epoch [7/300], Step [67/225], Training Accuracy: 54.9440%, Training Loss: 0.9297%\n",
      "Epoch [7/300], Step [68/225], Training Accuracy: 54.9632%, Training Loss: 0.9302%\n",
      "Epoch [7/300], Step [69/225], Training Accuracy: 54.8687%, Training Loss: 0.9295%\n",
      "Epoch [7/300], Step [70/225], Training Accuracy: 54.8214%, Training Loss: 0.9287%\n",
      "Epoch [7/300], Step [71/225], Training Accuracy: 54.8415%, Training Loss: 0.9294%\n",
      "Epoch [7/300], Step [72/225], Training Accuracy: 54.6875%, Training Loss: 0.9295%\n",
      "Epoch [7/300], Step [73/225], Training Accuracy: 54.6233%, Training Loss: 0.9299%\n",
      "Epoch [7/300], Step [74/225], Training Accuracy: 54.7297%, Training Loss: 0.9283%\n",
      "Epoch [7/300], Step [75/225], Training Accuracy: 54.8125%, Training Loss: 0.9272%\n",
      "Epoch [7/300], Step [76/225], Training Accuracy: 54.7903%, Training Loss: 0.9282%\n",
      "Epoch [7/300], Step [77/225], Training Accuracy: 54.8295%, Training Loss: 0.9278%\n",
      "Epoch [7/300], Step [78/225], Training Accuracy: 54.8277%, Training Loss: 0.9274%\n",
      "Epoch [7/300], Step [79/225], Training Accuracy: 54.7864%, Training Loss: 0.9277%\n",
      "Epoch [7/300], Step [80/225], Training Accuracy: 54.7070%, Training Loss: 0.9288%\n",
      "Epoch [7/300], Step [81/225], Training Accuracy: 54.7454%, Training Loss: 0.9289%\n",
      "Epoch [7/300], Step [82/225], Training Accuracy: 54.8209%, Training Loss: 0.9270%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Step [83/225], Training Accuracy: 54.8569%, Training Loss: 0.9257%\n",
      "Epoch [7/300], Step [84/225], Training Accuracy: 54.8549%, Training Loss: 0.9252%\n",
      "Epoch [7/300], Step [85/225], Training Accuracy: 54.8713%, Training Loss: 0.9247%\n",
      "Epoch [7/300], Step [86/225], Training Accuracy: 54.7602%, Training Loss: 0.9251%\n",
      "Epoch [7/300], Step [87/225], Training Accuracy: 54.7593%, Training Loss: 0.9252%\n",
      "Epoch [7/300], Step [88/225], Training Accuracy: 54.6342%, Training Loss: 0.9262%\n",
      "Epoch [7/300], Step [89/225], Training Accuracy: 54.4417%, Training Loss: 0.9288%\n",
      "Epoch [7/300], Step [90/225], Training Accuracy: 54.3924%, Training Loss: 0.9311%\n",
      "Epoch [7/300], Step [91/225], Training Accuracy: 54.2582%, Training Loss: 0.9319%\n",
      "Epoch [7/300], Step [92/225], Training Accuracy: 54.2629%, Training Loss: 0.9315%\n",
      "Epoch [7/300], Step [93/225], Training Accuracy: 54.2843%, Training Loss: 0.9309%\n",
      "Epoch [7/300], Step [94/225], Training Accuracy: 54.3883%, Training Loss: 0.9297%\n",
      "Epoch [7/300], Step [95/225], Training Accuracy: 54.3586%, Training Loss: 0.9304%\n",
      "Epoch [7/300], Step [96/225], Training Accuracy: 54.4271%, Training Loss: 0.9290%\n",
      "Epoch [7/300], Step [97/225], Training Accuracy: 54.4137%, Training Loss: 0.9290%\n",
      "Epoch [7/300], Step [98/225], Training Accuracy: 54.3846%, Training Loss: 0.9300%\n",
      "Epoch [7/300], Step [99/225], Training Accuracy: 54.4034%, Training Loss: 0.9303%\n",
      "Epoch [7/300], Step [100/225], Training Accuracy: 54.3281%, Training Loss: 0.9314%\n",
      "Epoch [7/300], Step [101/225], Training Accuracy: 54.3162%, Training Loss: 0.9318%\n",
      "Epoch [7/300], Step [102/225], Training Accuracy: 54.2892%, Training Loss: 0.9314%\n",
      "Epoch [7/300], Step [103/225], Training Accuracy: 54.3993%, Training Loss: 0.9304%\n",
      "Epoch [7/300], Step [104/225], Training Accuracy: 54.2819%, Training Loss: 0.9305%\n",
      "Epoch [7/300], Step [105/225], Training Accuracy: 54.2560%, Training Loss: 0.9305%\n",
      "Epoch [7/300], Step [106/225], Training Accuracy: 54.2895%, Training Loss: 0.9301%\n",
      "Epoch [7/300], Step [107/225], Training Accuracy: 54.2786%, Training Loss: 0.9311%\n",
      "Epoch [7/300], Step [108/225], Training Accuracy: 54.2679%, Training Loss: 0.9310%\n",
      "Epoch [7/300], Step [109/225], Training Accuracy: 54.2431%, Training Loss: 0.9307%\n",
      "Epoch [7/300], Step [110/225], Training Accuracy: 54.2756%, Training Loss: 0.9304%\n",
      "Epoch [7/300], Step [111/225], Training Accuracy: 54.2370%, Training Loss: 0.9311%\n",
      "Epoch [7/300], Step [112/225], Training Accuracy: 54.3387%, Training Loss: 0.9304%\n",
      "Epoch [7/300], Step [113/225], Training Accuracy: 54.4524%, Training Loss: 0.9301%\n",
      "Epoch [7/300], Step [114/225], Training Accuracy: 54.4682%, Training Loss: 0.9300%\n",
      "Epoch [7/300], Step [115/225], Training Accuracy: 54.5652%, Training Loss: 0.9286%\n",
      "Epoch [7/300], Step [116/225], Training Accuracy: 54.6336%, Training Loss: 0.9280%\n",
      "Epoch [7/300], Step [117/225], Training Accuracy: 54.5940%, Training Loss: 0.9296%\n",
      "Epoch [7/300], Step [118/225], Training Accuracy: 54.5286%, Training Loss: 0.9298%\n",
      "Epoch [7/300], Step [119/225], Training Accuracy: 54.5431%, Training Loss: 0.9296%\n",
      "Epoch [7/300], Step [120/225], Training Accuracy: 54.4661%, Training Loss: 0.9306%\n",
      "Epoch [7/300], Step [121/225], Training Accuracy: 54.4680%, Training Loss: 0.9303%\n",
      "Epoch [7/300], Step [122/225], Training Accuracy: 54.5338%, Training Loss: 0.9295%\n",
      "Epoch [7/300], Step [123/225], Training Accuracy: 54.5097%, Training Loss: 0.9292%\n",
      "Epoch [7/300], Step [124/225], Training Accuracy: 54.5363%, Training Loss: 0.9286%\n",
      "Epoch [7/300], Step [125/225], Training Accuracy: 54.5875%, Training Loss: 0.9297%\n",
      "Epoch [7/300], Step [126/225], Training Accuracy: 54.5511%, Training Loss: 0.9301%\n",
      "Epoch [7/300], Step [127/225], Training Accuracy: 54.5153%, Training Loss: 0.9305%\n",
      "Epoch [7/300], Step [128/225], Training Accuracy: 54.4067%, Training Loss: 0.9319%\n",
      "Epoch [7/300], Step [129/225], Training Accuracy: 54.3605%, Training Loss: 0.9327%\n",
      "Epoch [7/300], Step [130/225], Training Accuracy: 54.2668%, Training Loss: 0.9336%\n",
      "Epoch [7/300], Step [131/225], Training Accuracy: 54.2343%, Training Loss: 0.9341%\n",
      "Epoch [7/300], Step [132/225], Training Accuracy: 54.1785%, Training Loss: 0.9348%\n",
      "Epoch [7/300], Step [133/225], Training Accuracy: 54.1823%, Training Loss: 0.9355%\n",
      "Epoch [7/300], Step [134/225], Training Accuracy: 54.1045%, Training Loss: 0.9373%\n",
      "Epoch [7/300], Step [135/225], Training Accuracy: 54.1435%, Training Loss: 0.9374%\n",
      "Epoch [7/300], Step [136/225], Training Accuracy: 54.0786%, Training Loss: 0.9369%\n",
      "Epoch [7/300], Step [137/225], Training Accuracy: 54.1286%, Training Loss: 0.9363%\n",
      "Epoch [7/300], Step [138/225], Training Accuracy: 54.1667%, Training Loss: 0.9355%\n",
      "Epoch [7/300], Step [139/225], Training Accuracy: 54.1704%, Training Loss: 0.9352%\n",
      "Epoch [7/300], Step [140/225], Training Accuracy: 54.2076%, Training Loss: 0.9351%\n",
      "Epoch [7/300], Step [141/225], Training Accuracy: 54.1999%, Training Loss: 0.9350%\n",
      "Epoch [7/300], Step [142/225], Training Accuracy: 54.2254%, Training Loss: 0.9345%\n",
      "Epoch [7/300], Step [143/225], Training Accuracy: 54.2067%, Training Loss: 0.9341%\n",
      "Epoch [7/300], Step [144/225], Training Accuracy: 54.2426%, Training Loss: 0.9342%\n",
      "Epoch [7/300], Step [145/225], Training Accuracy: 54.2672%, Training Loss: 0.9336%\n",
      "Epoch [7/300], Step [146/225], Training Accuracy: 54.2059%, Training Loss: 0.9343%\n",
      "Epoch [7/300], Step [147/225], Training Accuracy: 54.1667%, Training Loss: 0.9347%\n",
      "Epoch [7/300], Step [148/225], Training Accuracy: 54.1807%, Training Loss: 0.9348%\n",
      "Epoch [7/300], Step [149/225], Training Accuracy: 54.0793%, Training Loss: 0.9355%\n",
      "Epoch [7/300], Step [150/225], Training Accuracy: 54.1354%, Training Loss: 0.9347%\n",
      "Epoch [7/300], Step [151/225], Training Accuracy: 54.0977%, Training Loss: 0.9346%\n",
      "Epoch [7/300], Step [152/225], Training Accuracy: 54.0604%, Training Loss: 0.9354%\n",
      "Epoch [7/300], Step [153/225], Training Accuracy: 54.0952%, Training Loss: 0.9349%\n",
      "Epoch [7/300], Step [154/225], Training Accuracy: 54.0584%, Training Loss: 0.9349%\n",
      "Epoch [7/300], Step [155/225], Training Accuracy: 54.0222%, Training Loss: 0.9349%\n",
      "Epoch [7/300], Step [156/225], Training Accuracy: 53.9864%, Training Loss: 0.9353%\n",
      "Epoch [7/300], Step [157/225], Training Accuracy: 53.9610%, Training Loss: 0.9352%\n",
      "Epoch [7/300], Step [158/225], Training Accuracy: 53.9260%, Training Loss: 0.9364%\n",
      "Epoch [7/300], Step [159/225], Training Accuracy: 53.8522%, Training Loss: 0.9367%\n",
      "Epoch [7/300], Step [160/225], Training Accuracy: 53.8281%, Training Loss: 0.9369%\n",
      "Epoch [7/300], Step [161/225], Training Accuracy: 53.8529%, Training Loss: 0.9370%\n",
      "Epoch [7/300], Step [162/225], Training Accuracy: 53.9159%, Training Loss: 0.9364%\n",
      "Epoch [7/300], Step [163/225], Training Accuracy: 53.9398%, Training Loss: 0.9363%\n",
      "Epoch [7/300], Step [164/225], Training Accuracy: 53.9920%, Training Loss: 0.9357%\n",
      "Epoch [7/300], Step [165/225], Training Accuracy: 54.0057%, Training Loss: 0.9360%\n",
      "Epoch [7/300], Step [166/225], Training Accuracy: 53.9910%, Training Loss: 0.9359%\n",
      "Epoch [7/300], Step [167/225], Training Accuracy: 53.9858%, Training Loss: 0.9360%\n",
      "Epoch [7/300], Step [168/225], Training Accuracy: 53.9528%, Training Loss: 0.9360%\n",
      "Epoch [7/300], Step [169/225], Training Accuracy: 53.9294%, Training Loss: 0.9360%\n",
      "Epoch [7/300], Step [170/225], Training Accuracy: 53.8971%, Training Loss: 0.9360%\n",
      "Epoch [7/300], Step [171/225], Training Accuracy: 53.9382%, Training Loss: 0.9359%\n",
      "Epoch [7/300], Step [172/225], Training Accuracy: 53.9244%, Training Loss: 0.9358%\n",
      "Epoch [7/300], Step [173/225], Training Accuracy: 53.9830%, Training Loss: 0.9353%\n",
      "Epoch [7/300], Step [174/225], Training Accuracy: 53.9781%, Training Loss: 0.9356%\n",
      "Epoch [7/300], Step [175/225], Training Accuracy: 54.0000%, Training Loss: 0.9353%\n",
      "Epoch [7/300], Step [176/225], Training Accuracy: 54.0128%, Training Loss: 0.9350%\n",
      "Epoch [7/300], Step [177/225], Training Accuracy: 54.0607%, Training Loss: 0.9349%\n",
      "Epoch [7/300], Step [178/225], Training Accuracy: 54.0555%, Training Loss: 0.9348%\n",
      "Epoch [7/300], Step [179/225], Training Accuracy: 54.1288%, Training Loss: 0.9339%\n",
      "Epoch [7/300], Step [180/225], Training Accuracy: 54.1667%, Training Loss: 0.9336%\n",
      "Epoch [7/300], Step [181/225], Training Accuracy: 54.1350%, Training Loss: 0.9341%\n",
      "Epoch [7/300], Step [182/225], Training Accuracy: 54.1724%, Training Loss: 0.9334%\n",
      "Epoch [7/300], Step [183/225], Training Accuracy: 54.1752%, Training Loss: 0.9330%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Step [184/225], Training Accuracy: 54.1950%, Training Loss: 0.9327%\n",
      "Epoch [7/300], Step [185/225], Training Accuracy: 54.2061%, Training Loss: 0.9324%\n",
      "Epoch [7/300], Step [186/225], Training Accuracy: 54.2675%, Training Loss: 0.9317%\n",
      "Epoch [7/300], Step [187/225], Training Accuracy: 54.2948%, Training Loss: 0.9310%\n",
      "Epoch [7/300], Step [188/225], Training Accuracy: 54.3052%, Training Loss: 0.9307%\n",
      "Epoch [7/300], Step [189/225], Training Accuracy: 54.3155%, Training Loss: 0.9300%\n",
      "Epoch [7/300], Step [190/225], Training Accuracy: 54.3257%, Training Loss: 0.9302%\n",
      "Epoch [7/300], Step [191/225], Training Accuracy: 54.3030%, Training Loss: 0.9299%\n",
      "Epoch [7/300], Step [192/225], Training Accuracy: 54.3294%, Training Loss: 0.9292%\n",
      "Epoch [7/300], Step [193/225], Training Accuracy: 54.3232%, Training Loss: 0.9297%\n",
      "Epoch [7/300], Step [194/225], Training Accuracy: 54.3492%, Training Loss: 0.9293%\n",
      "Epoch [7/300], Step [195/225], Training Accuracy: 54.3590%, Training Loss: 0.9286%\n",
      "Epoch [7/300], Step [196/225], Training Accuracy: 54.3208%, Training Loss: 0.9289%\n",
      "Epoch [7/300], Step [197/225], Training Accuracy: 54.3464%, Training Loss: 0.9287%\n",
      "Epoch [7/300], Step [198/225], Training Accuracy: 54.3876%, Training Loss: 0.9278%\n",
      "Epoch [7/300], Step [199/225], Training Accuracy: 54.4519%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [200/225], Training Accuracy: 54.4844%, Training Loss: 0.9270%\n",
      "Epoch [7/300], Step [201/225], Training Accuracy: 54.4465%, Training Loss: 0.9273%\n",
      "Epoch [7/300], Step [202/225], Training Accuracy: 54.4477%, Training Loss: 0.9272%\n",
      "Epoch [7/300], Step [203/225], Training Accuracy: 54.4951%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [204/225], Training Accuracy: 54.4884%, Training Loss: 0.9269%\n",
      "Epoch [7/300], Step [205/225], Training Accuracy: 54.5274%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [206/225], Training Accuracy: 54.5206%, Training Loss: 0.9273%\n",
      "Epoch [7/300], Step [207/225], Training Accuracy: 54.5063%, Training Loss: 0.9274%\n",
      "Epoch [7/300], Step [208/225], Training Accuracy: 54.5147%, Training Loss: 0.9270%\n",
      "Epoch [7/300], Step [209/225], Training Accuracy: 54.5529%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [210/225], Training Accuracy: 54.5312%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [211/225], Training Accuracy: 54.5024%, Training Loss: 0.9269%\n",
      "Epoch [7/300], Step [212/225], Training Accuracy: 54.5032%, Training Loss: 0.9268%\n",
      "Epoch [7/300], Step [213/225], Training Accuracy: 54.5041%, Training Loss: 0.9274%\n",
      "Epoch [7/300], Step [214/225], Training Accuracy: 54.5488%, Training Loss: 0.9270%\n",
      "Epoch [7/300], Step [215/225], Training Accuracy: 54.5349%, Training Loss: 0.9275%\n",
      "Epoch [7/300], Step [216/225], Training Accuracy: 54.4922%, Training Loss: 0.9279%\n",
      "Epoch [7/300], Step [217/225], Training Accuracy: 54.4859%, Training Loss: 0.9280%\n",
      "Epoch [7/300], Step [218/225], Training Accuracy: 54.4653%, Training Loss: 0.9283%\n",
      "Epoch [7/300], Step [219/225], Training Accuracy: 54.4521%, Training Loss: 0.9281%\n",
      "Epoch [7/300], Step [220/225], Training Accuracy: 54.4673%, Training Loss: 0.9278%\n",
      "Epoch [7/300], Step [221/225], Training Accuracy: 54.4330%, Training Loss: 0.9280%\n",
      "Epoch [7/300], Step [222/225], Training Accuracy: 54.3989%, Training Loss: 0.9279%\n",
      "Epoch [7/300], Step [223/225], Training Accuracy: 54.3582%, Training Loss: 0.9281%\n",
      "Epoch [7/300], Step [224/225], Training Accuracy: 54.3457%, Training Loss: 0.9279%\n",
      "Epoch [7/300], Step [225/225], Training Accuracy: 54.2941%, Training Loss: 0.9284%\n",
      "Epoch [8/300], Step [1/225], Training Accuracy: 51.5625%, Training Loss: 0.9241%\n",
      "Epoch [8/300], Step [2/225], Training Accuracy: 53.1250%, Training Loss: 0.9555%\n",
      "Epoch [8/300], Step [3/225], Training Accuracy: 54.6875%, Training Loss: 0.9444%\n",
      "Epoch [8/300], Step [4/225], Training Accuracy: 56.6406%, Training Loss: 0.9240%\n",
      "Epoch [8/300], Step [5/225], Training Accuracy: 58.1250%, Training Loss: 0.9094%\n",
      "Epoch [8/300], Step [6/225], Training Accuracy: 56.2500%, Training Loss: 0.9406%\n",
      "Epoch [8/300], Step [7/225], Training Accuracy: 56.2500%, Training Loss: 0.9404%\n",
      "Epoch [8/300], Step [8/225], Training Accuracy: 56.0547%, Training Loss: 0.9320%\n",
      "Epoch [8/300], Step [9/225], Training Accuracy: 55.3819%, Training Loss: 0.9368%\n",
      "Epoch [8/300], Step [10/225], Training Accuracy: 54.3750%, Training Loss: 0.9522%\n",
      "Epoch [8/300], Step [11/225], Training Accuracy: 54.5455%, Training Loss: 0.9522%\n",
      "Epoch [8/300], Step [12/225], Training Accuracy: 55.0781%, Training Loss: 0.9420%\n",
      "Epoch [8/300], Step [13/225], Training Accuracy: 56.0096%, Training Loss: 0.9260%\n",
      "Epoch [8/300], Step [14/225], Training Accuracy: 56.0268%, Training Loss: 0.9246%\n",
      "Epoch [8/300], Step [15/225], Training Accuracy: 56.2500%, Training Loss: 0.9233%\n",
      "Epoch [8/300], Step [16/225], Training Accuracy: 56.6406%, Training Loss: 0.9165%\n",
      "Epoch [8/300], Step [17/225], Training Accuracy: 56.9853%, Training Loss: 0.9139%\n",
      "Epoch [8/300], Step [18/225], Training Accuracy: 57.0312%, Training Loss: 0.9110%\n",
      "Epoch [8/300], Step [19/225], Training Accuracy: 56.7434%, Training Loss: 0.9120%\n",
      "Epoch [8/300], Step [20/225], Training Accuracy: 56.7969%, Training Loss: 0.9090%\n",
      "Epoch [8/300], Step [21/225], Training Accuracy: 57.3661%, Training Loss: 0.8996%\n",
      "Epoch [8/300], Step [22/225], Training Accuracy: 56.8892%, Training Loss: 0.9037%\n",
      "Epoch [8/300], Step [23/225], Training Accuracy: 57.2690%, Training Loss: 0.8968%\n",
      "Epoch [8/300], Step [24/225], Training Accuracy: 56.8359%, Training Loss: 0.9004%\n",
      "Epoch [8/300], Step [25/225], Training Accuracy: 57.1250%, Training Loss: 0.8974%\n",
      "Epoch [8/300], Step [26/225], Training Accuracy: 57.1514%, Training Loss: 0.8952%\n",
      "Epoch [8/300], Step [27/225], Training Accuracy: 57.0602%, Training Loss: 0.8958%\n",
      "Epoch [8/300], Step [28/225], Training Accuracy: 57.5335%, Training Loss: 0.8928%\n",
      "Epoch [8/300], Step [29/225], Training Accuracy: 57.5970%, Training Loss: 0.8901%\n",
      "Epoch [8/300], Step [30/225], Training Accuracy: 57.5521%, Training Loss: 0.8880%\n",
      "Epoch [8/300], Step [31/225], Training Accuracy: 57.2581%, Training Loss: 0.8932%\n",
      "Epoch [8/300], Step [32/225], Training Accuracy: 57.2266%, Training Loss: 0.8904%\n",
      "Epoch [8/300], Step [33/225], Training Accuracy: 57.5758%, Training Loss: 0.8866%\n",
      "Epoch [8/300], Step [34/225], Training Accuracy: 57.3989%, Training Loss: 0.8902%\n",
      "Epoch [8/300], Step [35/225], Training Accuracy: 57.1429%, Training Loss: 0.8929%\n",
      "Epoch [8/300], Step [36/225], Training Accuracy: 57.2917%, Training Loss: 0.8906%\n",
      "Epoch [8/300], Step [37/225], Training Accuracy: 57.3902%, Training Loss: 0.8882%\n",
      "Epoch [8/300], Step [38/225], Training Accuracy: 57.5247%, Training Loss: 0.8861%\n",
      "Epoch [8/300], Step [39/225], Training Accuracy: 57.4119%, Training Loss: 0.8872%\n",
      "Epoch [8/300], Step [40/225], Training Accuracy: 57.3828%, Training Loss: 0.8900%\n",
      "Epoch [8/300], Step [41/225], Training Accuracy: 57.3171%, Training Loss: 0.8925%\n",
      "Epoch [8/300], Step [42/225], Training Accuracy: 57.0312%, Training Loss: 0.8917%\n",
      "Epoch [8/300], Step [43/225], Training Accuracy: 56.9041%, Training Loss: 0.8922%\n",
      "Epoch [8/300], Step [44/225], Training Accuracy: 57.1023%, Training Loss: 0.8905%\n",
      "Epoch [8/300], Step [45/225], Training Accuracy: 57.1875%, Training Loss: 0.8885%\n",
      "Epoch [8/300], Step [46/225], Training Accuracy: 57.2351%, Training Loss: 0.8893%\n",
      "Epoch [8/300], Step [47/225], Training Accuracy: 57.0811%, Training Loss: 0.8886%\n",
      "Epoch [8/300], Step [48/225], Training Accuracy: 56.9987%, Training Loss: 0.8887%\n",
      "Epoch [8/300], Step [49/225], Training Accuracy: 56.9515%, Training Loss: 0.8884%\n",
      "Epoch [8/300], Step [50/225], Training Accuracy: 57.0000%, Training Loss: 0.8870%\n",
      "Epoch [8/300], Step [51/225], Training Accuracy: 57.1998%, Training Loss: 0.8857%\n",
      "Epoch [8/300], Step [52/225], Training Accuracy: 57.2115%, Training Loss: 0.8855%\n",
      "Epoch [8/300], Step [53/225], Training Accuracy: 57.2524%, Training Loss: 0.8852%\n",
      "Epoch [8/300], Step [54/225], Training Accuracy: 57.0891%, Training Loss: 0.8855%\n",
      "Epoch [8/300], Step [55/225], Training Accuracy: 57.0739%, Training Loss: 0.8870%\n",
      "Epoch [8/300], Step [56/225], Training Accuracy: 56.8359%, Training Loss: 0.8875%\n",
      "Epoch [8/300], Step [57/225], Training Accuracy: 56.9627%, Training Loss: 0.8854%\n",
      "Epoch [8/300], Step [58/225], Training Accuracy: 56.8427%, Training Loss: 0.8880%\n",
      "Epoch [8/300], Step [59/225], Training Accuracy: 56.9650%, Training Loss: 0.8862%\n",
      "Epoch [8/300], Step [60/225], Training Accuracy: 57.0052%, Training Loss: 0.8847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [61/225], Training Accuracy: 57.1209%, Training Loss: 0.8839%\n",
      "Epoch [8/300], Step [62/225], Training Accuracy: 56.9556%, Training Loss: 0.8843%\n",
      "Epoch [8/300], Step [63/225], Training Accuracy: 56.8452%, Training Loss: 0.8857%\n",
      "Epoch [8/300], Step [64/225], Training Accuracy: 56.8848%, Training Loss: 0.8848%\n",
      "Epoch [8/300], Step [65/225], Training Accuracy: 56.8269%, Training Loss: 0.8849%\n",
      "Epoch [8/300], Step [66/225], Training Accuracy: 56.9366%, Training Loss: 0.8834%\n",
      "Epoch [8/300], Step [67/225], Training Accuracy: 56.9496%, Training Loss: 0.8846%\n",
      "Epoch [8/300], Step [68/225], Training Accuracy: 56.9393%, Training Loss: 0.8859%\n",
      "Epoch [8/300], Step [69/225], Training Accuracy: 56.8841%, Training Loss: 0.8855%\n",
      "Epoch [8/300], Step [70/225], Training Accuracy: 56.8973%, Training Loss: 0.8852%\n",
      "Epoch [8/300], Step [71/225], Training Accuracy: 56.8442%, Training Loss: 0.8853%\n",
      "Epoch [8/300], Step [72/225], Training Accuracy: 56.7491%, Training Loss: 0.8867%\n",
      "Epoch [8/300], Step [73/225], Training Accuracy: 56.6353%, Training Loss: 0.8877%\n",
      "Epoch [8/300], Step [74/225], Training Accuracy: 56.7356%, Training Loss: 0.8858%\n",
      "Epoch [8/300], Step [75/225], Training Accuracy: 56.8542%, Training Loss: 0.8849%\n",
      "Epoch [8/300], Step [76/225], Training Accuracy: 56.7434%, Training Loss: 0.8861%\n",
      "Epoch [8/300], Step [77/225], Training Accuracy: 56.7979%, Training Loss: 0.8855%\n",
      "Epoch [8/300], Step [78/225], Training Accuracy: 56.8510%, Training Loss: 0.8861%\n",
      "Epoch [8/300], Step [79/225], Training Accuracy: 56.7247%, Training Loss: 0.8868%\n",
      "Epoch [8/300], Step [80/225], Training Accuracy: 56.6406%, Training Loss: 0.8881%\n",
      "Epoch [8/300], Step [81/225], Training Accuracy: 56.5586%, Training Loss: 0.8888%\n",
      "Epoch [8/300], Step [82/225], Training Accuracy: 56.6120%, Training Loss: 0.8880%\n",
      "Epoch [8/300], Step [83/225], Training Accuracy: 56.6265%, Training Loss: 0.8868%\n",
      "Epoch [8/300], Step [84/225], Training Accuracy: 56.6778%, Training Loss: 0.8866%\n",
      "Epoch [8/300], Step [85/225], Training Accuracy: 56.6544%, Training Loss: 0.8870%\n",
      "Epoch [8/300], Step [86/225], Training Accuracy: 56.5044%, Training Loss: 0.8881%\n",
      "Epoch [8/300], Step [87/225], Training Accuracy: 56.5553%, Training Loss: 0.8882%\n",
      "Epoch [8/300], Step [88/225], Training Accuracy: 56.5341%, Training Loss: 0.8889%\n",
      "Epoch [8/300], Step [89/225], Training Accuracy: 56.4431%, Training Loss: 0.8914%\n",
      "Epoch [8/300], Step [90/225], Training Accuracy: 56.3021%, Training Loss: 0.8936%\n",
      "Epoch [8/300], Step [91/225], Training Accuracy: 56.1813%, Training Loss: 0.8952%\n",
      "Epoch [8/300], Step [92/225], Training Accuracy: 56.1990%, Training Loss: 0.8950%\n",
      "Epoch [8/300], Step [93/225], Training Accuracy: 56.2332%, Training Loss: 0.8947%\n",
      "Epoch [8/300], Step [94/225], Training Accuracy: 56.2666%, Training Loss: 0.8938%\n",
      "Epoch [8/300], Step [95/225], Training Accuracy: 56.2664%, Training Loss: 0.8947%\n",
      "Epoch [8/300], Step [96/225], Training Accuracy: 56.3639%, Training Loss: 0.8929%\n",
      "Epoch [8/300], Step [97/225], Training Accuracy: 56.2983%, Training Loss: 0.8939%\n",
      "Epoch [8/300], Step [98/225], Training Accuracy: 56.1862%, Training Loss: 0.8948%\n",
      "Epoch [8/300], Step [99/225], Training Accuracy: 56.0922%, Training Loss: 0.8955%\n",
      "Epoch [8/300], Step [100/225], Training Accuracy: 56.0156%, Training Loss: 0.8968%\n",
      "Epoch [8/300], Step [101/225], Training Accuracy: 56.0489%, Training Loss: 0.8971%\n",
      "Epoch [8/300], Step [102/225], Training Accuracy: 56.0815%, Training Loss: 0.8962%\n",
      "Epoch [8/300], Step [103/225], Training Accuracy: 56.1438%, Training Loss: 0.8958%\n",
      "Epoch [8/300], Step [104/225], Training Accuracy: 55.9946%, Training Loss: 0.8966%\n",
      "Epoch [8/300], Step [105/225], Training Accuracy: 56.0119%, Training Loss: 0.8967%\n",
      "Epoch [8/300], Step [106/225], Training Accuracy: 55.9847%, Training Loss: 0.8968%\n",
      "Epoch [8/300], Step [107/225], Training Accuracy: 55.9433%, Training Loss: 0.8976%\n",
      "Epoch [8/300], Step [108/225], Training Accuracy: 55.9462%, Training Loss: 0.8977%\n",
      "Epoch [8/300], Step [109/225], Training Accuracy: 55.9060%, Training Loss: 0.8982%\n",
      "Epoch [8/300], Step [110/225], Training Accuracy: 55.8949%, Training Loss: 0.8979%\n",
      "Epoch [8/300], Step [111/225], Training Accuracy: 55.7995%, Training Loss: 0.8983%\n",
      "Epoch [8/300], Step [112/225], Training Accuracy: 55.8454%, Training Loss: 0.8979%\n",
      "Epoch [8/300], Step [113/225], Training Accuracy: 55.9181%, Training Loss: 0.8974%\n",
      "Epoch [8/300], Step [114/225], Training Accuracy: 55.9348%, Training Loss: 0.8968%\n",
      "Epoch [8/300], Step [115/225], Training Accuracy: 55.9783%, Training Loss: 0.8964%\n",
      "Epoch [8/300], Step [116/225], Training Accuracy: 56.0480%, Training Loss: 0.8962%\n",
      "Epoch [8/300], Step [117/225], Training Accuracy: 55.9829%, Training Loss: 0.8975%\n",
      "Epoch [8/300], Step [118/225], Training Accuracy: 55.9454%, Training Loss: 0.8980%\n",
      "Epoch [8/300], Step [119/225], Training Accuracy: 55.9611%, Training Loss: 0.8980%\n",
      "Epoch [8/300], Step [120/225], Training Accuracy: 55.9505%, Training Loss: 0.8983%\n",
      "Epoch [8/300], Step [121/225], Training Accuracy: 55.8626%, Training Loss: 0.8983%\n",
      "Epoch [8/300], Step [122/225], Training Accuracy: 55.8658%, Training Loss: 0.8976%\n",
      "Epoch [8/300], Step [123/225], Training Accuracy: 55.8562%, Training Loss: 0.8974%\n",
      "Epoch [8/300], Step [124/225], Training Accuracy: 55.8594%, Training Loss: 0.8975%\n",
      "Epoch [8/300], Step [125/225], Training Accuracy: 55.8375%, Training Loss: 0.8983%\n",
      "Epoch [8/300], Step [126/225], Training Accuracy: 55.8284%, Training Loss: 0.8988%\n",
      "Epoch [8/300], Step [127/225], Training Accuracy: 55.7825%, Training Loss: 0.8998%\n",
      "Epoch [8/300], Step [128/225], Training Accuracy: 55.6763%, Training Loss: 0.9009%\n",
      "Epoch [8/300], Step [129/225], Training Accuracy: 55.7413%, Training Loss: 0.9012%\n",
      "Epoch [8/300], Step [130/225], Training Accuracy: 55.6851%, Training Loss: 0.9022%\n",
      "Epoch [8/300], Step [131/225], Training Accuracy: 55.6298%, Training Loss: 0.9028%\n",
      "Epoch [8/300], Step [132/225], Training Accuracy: 55.5516%, Training Loss: 0.9035%\n",
      "Epoch [8/300], Step [133/225], Training Accuracy: 55.5216%, Training Loss: 0.9045%\n",
      "Epoch [8/300], Step [134/225], Training Accuracy: 55.4221%, Training Loss: 0.9061%\n",
      "Epoch [8/300], Step [135/225], Training Accuracy: 55.4745%, Training Loss: 0.9059%\n",
      "Epoch [8/300], Step [136/225], Training Accuracy: 55.4228%, Training Loss: 0.9058%\n",
      "Epoch [8/300], Step [137/225], Training Accuracy: 55.4859%, Training Loss: 0.9050%\n",
      "Epoch [8/300], Step [138/225], Training Accuracy: 55.4914%, Training Loss: 0.9045%\n",
      "Epoch [8/300], Step [139/225], Training Accuracy: 55.5193%, Training Loss: 0.9041%\n",
      "Epoch [8/300], Step [140/225], Training Accuracy: 55.5357%, Training Loss: 0.9034%\n",
      "Epoch [8/300], Step [141/225], Training Accuracy: 55.5519%, Training Loss: 0.9032%\n",
      "Epoch [8/300], Step [142/225], Training Accuracy: 55.5678%, Training Loss: 0.9026%\n",
      "Epoch [8/300], Step [143/225], Training Accuracy: 55.5288%, Training Loss: 0.9027%\n",
      "Epoch [8/300], Step [144/225], Training Accuracy: 55.6098%, Training Loss: 0.9022%\n",
      "Epoch [8/300], Step [145/225], Training Accuracy: 55.6358%, Training Loss: 0.9018%\n",
      "Epoch [8/300], Step [146/225], Training Accuracy: 55.6079%, Training Loss: 0.9025%\n",
      "Epoch [8/300], Step [147/225], Training Accuracy: 55.5272%, Training Loss: 0.9032%\n",
      "Epoch [8/300], Step [148/225], Training Accuracy: 55.5954%, Training Loss: 0.9027%\n",
      "Epoch [8/300], Step [149/225], Training Accuracy: 55.5055%, Training Loss: 0.9031%\n",
      "Epoch [8/300], Step [150/225], Training Accuracy: 55.5833%, Training Loss: 0.9019%\n",
      "Epoch [8/300], Step [151/225], Training Accuracy: 55.5464%, Training Loss: 0.9019%\n",
      "Epoch [8/300], Step [152/225], Training Accuracy: 55.4996%, Training Loss: 0.9024%\n",
      "Epoch [8/300], Step [153/225], Training Accuracy: 55.5351%, Training Loss: 0.9020%\n",
      "Epoch [8/300], Step [154/225], Training Accuracy: 55.4992%, Training Loss: 0.9019%\n",
      "Epoch [8/300], Step [155/225], Training Accuracy: 55.4435%, Training Loss: 0.9021%\n",
      "Epoch [8/300], Step [156/225], Training Accuracy: 55.4387%, Training Loss: 0.9026%\n",
      "Epoch [8/300], Step [157/225], Training Accuracy: 55.4538%, Training Loss: 0.9023%\n",
      "Epoch [8/300], Step [158/225], Training Accuracy: 55.3896%, Training Loss: 0.9032%\n",
      "Epoch [8/300], Step [159/225], Training Accuracy: 55.3164%, Training Loss: 0.9037%\n",
      "Epoch [8/300], Step [160/225], Training Accuracy: 55.2930%, Training Loss: 0.9035%\n",
      "Epoch [8/300], Step [161/225], Training Accuracy: 55.2892%, Training Loss: 0.9037%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [162/225], Training Accuracy: 55.3627%, Training Loss: 0.9030%\n",
      "Epoch [8/300], Step [163/225], Training Accuracy: 55.4160%, Training Loss: 0.9024%\n",
      "Epoch [8/300], Step [164/225], Training Accuracy: 55.4878%, Training Loss: 0.9015%\n",
      "Epoch [8/300], Step [165/225], Training Accuracy: 55.5114%, Training Loss: 0.9013%\n",
      "Epoch [8/300], Step [166/225], Training Accuracy: 55.5158%, Training Loss: 0.9008%\n",
      "Epoch [8/300], Step [167/225], Training Accuracy: 55.4641%, Training Loss: 0.9009%\n",
      "Epoch [8/300], Step [168/225], Training Accuracy: 55.4408%, Training Loss: 0.9008%\n",
      "Epoch [8/300], Step [169/225], Training Accuracy: 55.3902%, Training Loss: 0.9008%\n",
      "Epoch [8/300], Step [170/225], Training Accuracy: 55.3493%, Training Loss: 0.9010%\n",
      "Epoch [8/300], Step [171/225], Training Accuracy: 55.3363%, Training Loss: 0.9011%\n",
      "Epoch [8/300], Step [172/225], Training Accuracy: 55.3416%, Training Loss: 0.9012%\n",
      "Epoch [8/300], Step [173/225], Training Accuracy: 55.3649%, Training Loss: 0.9011%\n",
      "Epoch [8/300], Step [174/225], Training Accuracy: 55.3251%, Training Loss: 0.9018%\n",
      "Epoch [8/300], Step [175/225], Training Accuracy: 55.3750%, Training Loss: 0.9012%\n",
      "Epoch [8/300], Step [176/225], Training Accuracy: 55.3711%, Training Loss: 0.9008%\n",
      "Epoch [8/300], Step [177/225], Training Accuracy: 55.3761%, Training Loss: 0.9007%\n",
      "Epoch [8/300], Step [178/225], Training Accuracy: 55.3195%, Training Loss: 0.9010%\n",
      "Epoch [8/300], Step [179/225], Training Accuracy: 55.3858%, Training Loss: 0.9004%\n",
      "Epoch [8/300], Step [180/225], Training Accuracy: 55.4253%, Training Loss: 0.8998%\n",
      "Epoch [8/300], Step [181/225], Training Accuracy: 55.3867%, Training Loss: 0.9001%\n",
      "Epoch [8/300], Step [182/225], Training Accuracy: 55.4001%, Training Loss: 0.8995%\n",
      "Epoch [8/300], Step [183/225], Training Accuracy: 55.4218%, Training Loss: 0.8995%\n",
      "Epoch [8/300], Step [184/225], Training Accuracy: 55.4263%, Training Loss: 0.8993%\n",
      "Epoch [8/300], Step [185/225], Training Accuracy: 55.4561%, Training Loss: 0.8990%\n",
      "Epoch [8/300], Step [186/225], Training Accuracy: 55.4772%, Training Loss: 0.8985%\n",
      "Epoch [8/300], Step [187/225], Training Accuracy: 55.4813%, Training Loss: 0.8979%\n",
      "Epoch [8/300], Step [188/225], Training Accuracy: 55.5020%, Training Loss: 0.8975%\n",
      "Epoch [8/300], Step [189/225], Training Accuracy: 55.5308%, Training Loss: 0.8968%\n",
      "Epoch [8/300], Step [190/225], Training Accuracy: 55.5428%, Training Loss: 0.8971%\n",
      "Epoch [8/300], Step [191/225], Training Accuracy: 55.5383%, Training Loss: 0.8970%\n",
      "Epoch [8/300], Step [192/225], Training Accuracy: 55.5745%, Training Loss: 0.8967%\n",
      "Epoch [8/300], Step [193/225], Training Accuracy: 55.5538%, Training Loss: 0.8973%\n",
      "Epoch [8/300], Step [194/225], Training Accuracy: 55.5654%, Training Loss: 0.8969%\n",
      "Epoch [8/300], Step [195/225], Training Accuracy: 55.6090%, Training Loss: 0.8964%\n",
      "Epoch [8/300], Step [196/225], Training Accuracy: 55.5724%, Training Loss: 0.8971%\n",
      "Epoch [8/300], Step [197/225], Training Accuracy: 55.5679%, Training Loss: 0.8972%\n",
      "Epoch [8/300], Step [198/225], Training Accuracy: 55.6108%, Training Loss: 0.8966%\n",
      "Epoch [8/300], Step [199/225], Training Accuracy: 55.7004%, Training Loss: 0.8958%\n",
      "Epoch [8/300], Step [200/225], Training Accuracy: 55.7188%, Training Loss: 0.8959%\n",
      "Epoch [8/300], Step [201/225], Training Accuracy: 55.6903%, Training Loss: 0.8963%\n",
      "Epoch [8/300], Step [202/225], Training Accuracy: 55.6699%, Training Loss: 0.8964%\n",
      "Epoch [8/300], Step [203/225], Training Accuracy: 55.7189%, Training Loss: 0.8959%\n",
      "Epoch [8/300], Step [204/225], Training Accuracy: 55.7368%, Training Loss: 0.8961%\n",
      "Epoch [8/300], Step [205/225], Training Accuracy: 55.8079%, Training Loss: 0.8958%\n",
      "Epoch [8/300], Step [206/225], Training Accuracy: 55.8328%, Training Loss: 0.8960%\n",
      "Epoch [8/300], Step [207/225], Training Accuracy: 55.8046%, Training Loss: 0.8961%\n",
      "Epoch [8/300], Step [208/225], Training Accuracy: 55.8143%, Training Loss: 0.8958%\n",
      "Epoch [8/300], Step [209/225], Training Accuracy: 55.8164%, Training Loss: 0.8960%\n",
      "Epoch [8/300], Step [210/225], Training Accuracy: 55.7887%, Training Loss: 0.8961%\n",
      "Epoch [8/300], Step [211/225], Training Accuracy: 55.7761%, Training Loss: 0.8962%\n",
      "Epoch [8/300], Step [212/225], Training Accuracy: 55.8004%, Training Loss: 0.8963%\n",
      "Epoch [8/300], Step [213/225], Training Accuracy: 55.7879%, Training Loss: 0.8972%\n",
      "Epoch [8/300], Step [214/225], Training Accuracy: 55.8338%, Training Loss: 0.8971%\n",
      "Epoch [8/300], Step [215/225], Training Accuracy: 55.8212%, Training Loss: 0.8972%\n",
      "Epoch [8/300], Step [216/225], Training Accuracy: 55.7943%, Training Loss: 0.8975%\n",
      "Epoch [8/300], Step [217/225], Training Accuracy: 55.7892%, Training Loss: 0.8981%\n",
      "Epoch [8/300], Step [218/225], Training Accuracy: 55.7554%, Training Loss: 0.8985%\n",
      "Epoch [8/300], Step [219/225], Training Accuracy: 55.7292%, Training Loss: 0.8982%\n",
      "Epoch [8/300], Step [220/225], Training Accuracy: 55.6960%, Training Loss: 0.8981%\n",
      "Epoch [8/300], Step [221/225], Training Accuracy: 55.6632%, Training Loss: 0.8985%\n",
      "Epoch [8/300], Step [222/225], Training Accuracy: 55.6588%, Training Loss: 0.8983%\n",
      "Epoch [8/300], Step [223/225], Training Accuracy: 55.6334%, Training Loss: 0.8988%\n",
      "Epoch [8/300], Step [224/225], Training Accuracy: 55.6083%, Training Loss: 0.8985%\n",
      "Epoch [8/300], Step [225/225], Training Accuracy: 55.5447%, Training Loss: 0.8993%\n",
      "Epoch [9/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.8073%\n",
      "Epoch [9/300], Step [2/225], Training Accuracy: 64.8438%, Training Loss: 0.8471%\n",
      "Epoch [9/300], Step [3/225], Training Accuracy: 60.4167%, Training Loss: 0.8771%\n",
      "Epoch [9/300], Step [4/225], Training Accuracy: 59.3750%, Training Loss: 0.8856%\n",
      "Epoch [9/300], Step [5/225], Training Accuracy: 60.3125%, Training Loss: 0.8697%\n",
      "Epoch [9/300], Step [6/225], Training Accuracy: 58.5938%, Training Loss: 0.8939%\n",
      "Epoch [9/300], Step [7/225], Training Accuracy: 60.0446%, Training Loss: 0.8846%\n",
      "Epoch [9/300], Step [8/225], Training Accuracy: 59.3750%, Training Loss: 0.8808%\n",
      "Epoch [9/300], Step [9/225], Training Accuracy: 58.6806%, Training Loss: 0.8907%\n",
      "Epoch [9/300], Step [10/225], Training Accuracy: 57.8125%, Training Loss: 0.9072%\n",
      "Epoch [9/300], Step [11/225], Training Accuracy: 58.2386%, Training Loss: 0.9066%\n",
      "Epoch [9/300], Step [12/225], Training Accuracy: 58.2031%, Training Loss: 0.9029%\n",
      "Epoch [9/300], Step [13/225], Training Accuracy: 58.4135%, Training Loss: 0.8974%\n",
      "Epoch [9/300], Step [14/225], Training Accuracy: 58.2589%, Training Loss: 0.9026%\n",
      "Epoch [9/300], Step [15/225], Training Accuracy: 57.9167%, Training Loss: 0.9041%\n",
      "Epoch [9/300], Step [16/225], Training Accuracy: 57.9102%, Training Loss: 0.8981%\n",
      "Epoch [9/300], Step [17/225], Training Accuracy: 58.2721%, Training Loss: 0.8938%\n",
      "Epoch [9/300], Step [18/225], Training Accuracy: 58.0729%, Training Loss: 0.8943%\n",
      "Epoch [9/300], Step [19/225], Training Accuracy: 57.8947%, Training Loss: 0.8967%\n",
      "Epoch [9/300], Step [20/225], Training Accuracy: 58.2031%, Training Loss: 0.8930%\n",
      "Epoch [9/300], Step [21/225], Training Accuracy: 58.8542%, Training Loss: 0.8843%\n",
      "Epoch [9/300], Step [22/225], Training Accuracy: 58.5227%, Training Loss: 0.8913%\n",
      "Epoch [9/300], Step [23/225], Training Accuracy: 58.6957%, Training Loss: 0.8881%\n",
      "Epoch [9/300], Step [24/225], Training Accuracy: 58.1380%, Training Loss: 0.8901%\n",
      "Epoch [9/300], Step [25/225], Training Accuracy: 58.1250%, Training Loss: 0.8887%\n",
      "Epoch [9/300], Step [26/225], Training Accuracy: 58.1731%, Training Loss: 0.8880%\n",
      "Epoch [9/300], Step [27/225], Training Accuracy: 57.9861%, Training Loss: 0.8868%\n",
      "Epoch [9/300], Step [28/225], Training Accuracy: 58.3147%, Training Loss: 0.8846%\n",
      "Epoch [9/300], Step [29/225], Training Accuracy: 58.3513%, Training Loss: 0.8838%\n",
      "Epoch [9/300], Step [30/225], Training Accuracy: 58.3854%, Training Loss: 0.8811%\n",
      "Epoch [9/300], Step [31/225], Training Accuracy: 58.1149%, Training Loss: 0.8858%\n",
      "Epoch [9/300], Step [32/225], Training Accuracy: 57.9590%, Training Loss: 0.8842%\n",
      "Epoch [9/300], Step [33/225], Training Accuracy: 58.1913%, Training Loss: 0.8796%\n",
      "Epoch [9/300], Step [34/225], Training Accuracy: 57.9504%, Training Loss: 0.8824%\n",
      "Epoch [9/300], Step [35/225], Training Accuracy: 57.8125%, Training Loss: 0.8856%\n",
      "Epoch [9/300], Step [36/225], Training Accuracy: 57.9427%, Training Loss: 0.8846%\n",
      "Epoch [9/300], Step [37/225], Training Accuracy: 57.7280%, Training Loss: 0.8841%\n",
      "Epoch [9/300], Step [38/225], Training Accuracy: 57.7714%, Training Loss: 0.8839%\n",
      "Epoch [9/300], Step [39/225], Training Accuracy: 57.6522%, Training Loss: 0.8849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [40/225], Training Accuracy: 57.4609%, Training Loss: 0.8878%\n",
      "Epoch [9/300], Step [41/225], Training Accuracy: 57.3933%, Training Loss: 0.8901%\n",
      "Epoch [9/300], Step [42/225], Training Accuracy: 57.3661%, Training Loss: 0.8893%\n",
      "Epoch [9/300], Step [43/225], Training Accuracy: 57.3038%, Training Loss: 0.8888%\n",
      "Epoch [9/300], Step [44/225], Training Accuracy: 57.4219%, Training Loss: 0.8868%\n",
      "Epoch [9/300], Step [45/225], Training Accuracy: 57.5694%, Training Loss: 0.8848%\n",
      "Epoch [9/300], Step [46/225], Training Accuracy: 57.7785%, Training Loss: 0.8824%\n",
      "Epoch [9/300], Step [47/225], Training Accuracy: 57.7793%, Training Loss: 0.8809%\n",
      "Epoch [9/300], Step [48/225], Training Accuracy: 57.6497%, Training Loss: 0.8815%\n",
      "Epoch [9/300], Step [49/225], Training Accuracy: 57.5893%, Training Loss: 0.8814%\n",
      "Epoch [9/300], Step [50/225], Training Accuracy: 57.6562%, Training Loss: 0.8810%\n",
      "Epoch [9/300], Step [51/225], Training Accuracy: 57.7512%, Training Loss: 0.8793%\n",
      "Epoch [9/300], Step [52/225], Training Accuracy: 57.7224%, Training Loss: 0.8782%\n",
      "Epoch [9/300], Step [53/225], Training Accuracy: 57.6356%, Training Loss: 0.8785%\n",
      "Epoch [9/300], Step [54/225], Training Accuracy: 57.5521%, Training Loss: 0.8782%\n",
      "Epoch [9/300], Step [55/225], Training Accuracy: 57.5000%, Training Loss: 0.8791%\n",
      "Epoch [9/300], Step [56/225], Training Accuracy: 57.3382%, Training Loss: 0.8798%\n",
      "Epoch [9/300], Step [57/225], Training Accuracy: 57.2917%, Training Loss: 0.8778%\n",
      "Epoch [9/300], Step [58/225], Training Accuracy: 57.3006%, Training Loss: 0.8790%\n",
      "Epoch [9/300], Step [59/225], Training Accuracy: 57.3623%, Training Loss: 0.8773%\n",
      "Epoch [9/300], Step [60/225], Training Accuracy: 57.4219%, Training Loss: 0.8759%\n",
      "Epoch [9/300], Step [61/225], Training Accuracy: 57.4283%, Training Loss: 0.8757%\n",
      "Epoch [9/300], Step [62/225], Training Accuracy: 57.4597%, Training Loss: 0.8751%\n",
      "Epoch [9/300], Step [63/225], Training Accuracy: 57.3165%, Training Loss: 0.8763%\n",
      "Epoch [9/300], Step [64/225], Training Accuracy: 57.3486%, Training Loss: 0.8762%\n",
      "Epoch [9/300], Step [65/225], Training Accuracy: 57.3077%, Training Loss: 0.8770%\n",
      "Epoch [9/300], Step [66/225], Training Accuracy: 57.4574%, Training Loss: 0.8754%\n",
      "Epoch [9/300], Step [67/225], Training Accuracy: 57.4160%, Training Loss: 0.8759%\n",
      "Epoch [9/300], Step [68/225], Training Accuracy: 57.3529%, Training Loss: 0.8769%\n",
      "Epoch [9/300], Step [69/225], Training Accuracy: 57.2237%, Training Loss: 0.8774%\n",
      "Epoch [9/300], Step [70/225], Training Accuracy: 57.2321%, Training Loss: 0.8763%\n",
      "Epoch [9/300], Step [71/225], Training Accuracy: 57.2183%, Training Loss: 0.8764%\n",
      "Epoch [9/300], Step [72/225], Training Accuracy: 57.1181%, Training Loss: 0.8782%\n",
      "Epoch [9/300], Step [73/225], Training Accuracy: 57.1062%, Training Loss: 0.8786%\n",
      "Epoch [9/300], Step [74/225], Training Accuracy: 57.2213%, Training Loss: 0.8774%\n",
      "Epoch [9/300], Step [75/225], Training Accuracy: 57.3333%, Training Loss: 0.8776%\n",
      "Epoch [9/300], Step [76/225], Training Accuracy: 57.2368%, Training Loss: 0.8790%\n",
      "Epoch [9/300], Step [77/225], Training Accuracy: 57.3052%, Training Loss: 0.8786%\n",
      "Epoch [9/300], Step [78/225], Training Accuracy: 57.2716%, Training Loss: 0.8786%\n",
      "Epoch [9/300], Step [79/225], Training Accuracy: 57.1994%, Training Loss: 0.8791%\n",
      "Epoch [9/300], Step [80/225], Training Accuracy: 57.1875%, Training Loss: 0.8806%\n",
      "Epoch [9/300], Step [81/225], Training Accuracy: 57.1759%, Training Loss: 0.8807%\n",
      "Epoch [9/300], Step [82/225], Training Accuracy: 57.2409%, Training Loss: 0.8792%\n",
      "Epoch [9/300], Step [83/225], Training Accuracy: 57.2101%, Training Loss: 0.8779%\n",
      "Epoch [9/300], Step [84/225], Training Accuracy: 57.1801%, Training Loss: 0.8785%\n",
      "Epoch [9/300], Step [85/225], Training Accuracy: 57.1875%, Training Loss: 0.8773%\n",
      "Epoch [9/300], Step [86/225], Training Accuracy: 57.1221%, Training Loss: 0.8777%\n",
      "Epoch [9/300], Step [87/225], Training Accuracy: 57.1300%, Training Loss: 0.8782%\n",
      "Epoch [9/300], Step [88/225], Training Accuracy: 57.1555%, Training Loss: 0.8789%\n",
      "Epoch [9/300], Step [89/225], Training Accuracy: 57.1278%, Training Loss: 0.8811%\n",
      "Epoch [9/300], Step [90/225], Training Accuracy: 57.0486%, Training Loss: 0.8832%\n",
      "Epoch [9/300], Step [91/225], Training Accuracy: 56.8853%, Training Loss: 0.8838%\n",
      "Epoch [9/300], Step [92/225], Training Accuracy: 56.8954%, Training Loss: 0.8834%\n",
      "Epoch [9/300], Step [93/225], Training Accuracy: 57.0228%, Training Loss: 0.8821%\n",
      "Epoch [9/300], Step [94/225], Training Accuracy: 57.0479%, Training Loss: 0.8811%\n",
      "Epoch [9/300], Step [95/225], Training Accuracy: 57.0230%, Training Loss: 0.8819%\n",
      "Epoch [9/300], Step [96/225], Training Accuracy: 57.1289%, Training Loss: 0.8805%\n",
      "Epoch [9/300], Step [97/225], Training Accuracy: 57.0715%, Training Loss: 0.8804%\n",
      "Epoch [9/300], Step [98/225], Training Accuracy: 57.0631%, Training Loss: 0.8809%\n",
      "Epoch [9/300], Step [99/225], Training Accuracy: 57.0549%, Training Loss: 0.8820%\n",
      "Epoch [9/300], Step [100/225], Training Accuracy: 56.9531%, Training Loss: 0.8826%\n",
      "Epoch [9/300], Step [101/225], Training Accuracy: 56.9462%, Training Loss: 0.8830%\n",
      "Epoch [9/300], Step [102/225], Training Accuracy: 57.0159%, Training Loss: 0.8826%\n",
      "Epoch [9/300], Step [103/225], Training Accuracy: 57.0995%, Training Loss: 0.8819%\n",
      "Epoch [9/300], Step [104/225], Training Accuracy: 56.9862%, Training Loss: 0.8824%\n",
      "Epoch [9/300], Step [105/225], Training Accuracy: 57.0089%, Training Loss: 0.8820%\n",
      "Epoch [9/300], Step [106/225], Training Accuracy: 56.9281%, Training Loss: 0.8818%\n",
      "Epoch [9/300], Step [107/225], Training Accuracy: 56.8779%, Training Loss: 0.8829%\n",
      "Epoch [9/300], Step [108/225], Training Accuracy: 56.8866%, Training Loss: 0.8830%\n",
      "Epoch [9/300], Step [109/225], Training Accuracy: 56.8951%, Training Loss: 0.8830%\n",
      "Epoch [9/300], Step [110/225], Training Accuracy: 56.9034%, Training Loss: 0.8825%\n",
      "Epoch [9/300], Step [111/225], Training Accuracy: 56.7849%, Training Loss: 0.8837%\n",
      "Epoch [9/300], Step [112/225], Training Accuracy: 56.8220%, Training Loss: 0.8835%\n",
      "Epoch [9/300], Step [113/225], Training Accuracy: 56.9137%, Training Loss: 0.8828%\n",
      "Epoch [9/300], Step [114/225], Training Accuracy: 56.8668%, Training Loss: 0.8835%\n",
      "Epoch [9/300], Step [115/225], Training Accuracy: 56.9837%, Training Loss: 0.8825%\n",
      "Epoch [9/300], Step [116/225], Training Accuracy: 56.9774%, Training Loss: 0.8818%\n",
      "Epoch [9/300], Step [117/225], Training Accuracy: 56.9177%, Training Loss: 0.8827%\n",
      "Epoch [9/300], Step [118/225], Training Accuracy: 56.8459%, Training Loss: 0.8837%\n",
      "Epoch [9/300], Step [119/225], Training Accuracy: 56.8146%, Training Loss: 0.8840%\n",
      "Epoch [9/300], Step [120/225], Training Accuracy: 56.7969%, Training Loss: 0.8845%\n",
      "Epoch [9/300], Step [121/225], Training Accuracy: 56.7924%, Training Loss: 0.8849%\n",
      "Epoch [9/300], Step [122/225], Training Accuracy: 56.8391%, Training Loss: 0.8842%\n",
      "Epoch [9/300], Step [123/225], Training Accuracy: 56.8089%, Training Loss: 0.8843%\n",
      "Epoch [9/300], Step [124/225], Training Accuracy: 56.7918%, Training Loss: 0.8838%\n",
      "Epoch [9/300], Step [125/225], Training Accuracy: 56.7625%, Training Loss: 0.8846%\n",
      "Epoch [9/300], Step [126/225], Training Accuracy: 56.7088%, Training Loss: 0.8851%\n",
      "Epoch [9/300], Step [127/225], Training Accuracy: 56.6314%, Training Loss: 0.8861%\n",
      "Epoch [9/300], Step [128/225], Training Accuracy: 56.6040%, Training Loss: 0.8873%\n",
      "Epoch [9/300], Step [129/225], Training Accuracy: 56.5649%, Training Loss: 0.8878%\n",
      "Epoch [9/300], Step [130/225], Training Accuracy: 56.4904%, Training Loss: 0.8885%\n",
      "Epoch [9/300], Step [131/225], Training Accuracy: 56.4289%, Training Loss: 0.8890%\n",
      "Epoch [9/300], Step [132/225], Training Accuracy: 56.3565%, Training Loss: 0.8901%\n",
      "Epoch [9/300], Step [133/225], Training Accuracy: 56.3792%, Training Loss: 0.8899%\n",
      "Epoch [9/300], Step [134/225], Training Accuracy: 56.2733%, Training Loss: 0.8912%\n",
      "Epoch [9/300], Step [135/225], Training Accuracy: 56.2963%, Training Loss: 0.8910%\n",
      "Epoch [9/300], Step [136/225], Training Accuracy: 56.3189%, Training Loss: 0.8906%\n",
      "Epoch [9/300], Step [137/225], Training Accuracy: 56.3755%, Training Loss: 0.8897%\n",
      "Epoch [9/300], Step [138/225], Training Accuracy: 56.4085%, Training Loss: 0.8890%\n",
      "Epoch [9/300], Step [139/225], Training Accuracy: 56.4074%, Training Loss: 0.8886%\n",
      "Epoch [9/300], Step [140/225], Training Accuracy: 56.4509%, Training Loss: 0.8879%\n",
      "Epoch [9/300], Step [141/225], Training Accuracy: 56.4716%, Training Loss: 0.8876%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [142/225], Training Accuracy: 56.5361%, Training Loss: 0.8867%\n",
      "Epoch [9/300], Step [143/225], Training Accuracy: 56.5232%, Training Loss: 0.8862%\n",
      "Epoch [9/300], Step [144/225], Training Accuracy: 56.5213%, Training Loss: 0.8858%\n",
      "Epoch [9/300], Step [145/225], Training Accuracy: 56.5194%, Training Loss: 0.8852%\n",
      "Epoch [9/300], Step [146/225], Training Accuracy: 56.5390%, Training Loss: 0.8853%\n",
      "Epoch [9/300], Step [147/225], Training Accuracy: 56.4626%, Training Loss: 0.8859%\n",
      "Epoch [9/300], Step [148/225], Training Accuracy: 56.4506%, Training Loss: 0.8861%\n",
      "Epoch [9/300], Step [149/225], Training Accuracy: 56.3863%, Training Loss: 0.8867%\n",
      "Epoch [9/300], Step [150/225], Training Accuracy: 56.4479%, Training Loss: 0.8856%\n",
      "Epoch [9/300], Step [151/225], Training Accuracy: 56.4880%, Training Loss: 0.8852%\n",
      "Epoch [9/300], Step [152/225], Training Accuracy: 56.4145%, Training Loss: 0.8863%\n",
      "Epoch [9/300], Step [153/225], Training Accuracy: 56.4338%, Training Loss: 0.8860%\n",
      "Epoch [9/300], Step [154/225], Training Accuracy: 56.4326%, Training Loss: 0.8861%\n",
      "Epoch [9/300], Step [155/225], Training Accuracy: 56.4617%, Training Loss: 0.8863%\n",
      "Epoch [9/300], Step [156/225], Training Accuracy: 56.4603%, Training Loss: 0.8869%\n",
      "Epoch [9/300], Step [157/225], Training Accuracy: 56.4291%, Training Loss: 0.8870%\n",
      "Epoch [9/300], Step [158/225], Training Accuracy: 56.3786%, Training Loss: 0.8877%\n",
      "Epoch [9/300], Step [159/225], Training Accuracy: 56.3188%, Training Loss: 0.8884%\n",
      "Epoch [9/300], Step [160/225], Training Accuracy: 56.3184%, Training Loss: 0.8885%\n",
      "Epoch [9/300], Step [161/225], Training Accuracy: 56.3373%, Training Loss: 0.8885%\n",
      "Epoch [9/300], Step [162/225], Training Accuracy: 56.3657%, Training Loss: 0.8882%\n",
      "Epoch [9/300], Step [163/225], Training Accuracy: 56.3554%, Training Loss: 0.8881%\n",
      "Epoch [9/300], Step [164/225], Training Accuracy: 56.4215%, Training Loss: 0.8874%\n",
      "Epoch [9/300], Step [165/225], Training Accuracy: 56.4205%, Training Loss: 0.8873%\n",
      "Epoch [9/300], Step [166/225], Training Accuracy: 56.4288%, Training Loss: 0.8870%\n",
      "Epoch [9/300], Step [167/225], Training Accuracy: 56.4184%, Training Loss: 0.8869%\n",
      "Epoch [9/300], Step [168/225], Training Accuracy: 56.4267%, Training Loss: 0.8866%\n",
      "Epoch [9/300], Step [169/225], Training Accuracy: 56.3517%, Training Loss: 0.8871%\n",
      "Epoch [9/300], Step [170/225], Training Accuracy: 56.3327%, Training Loss: 0.8872%\n",
      "Epoch [9/300], Step [171/225], Training Accuracy: 56.3322%, Training Loss: 0.8876%\n",
      "Epoch [9/300], Step [172/225], Training Accuracy: 56.3318%, Training Loss: 0.8879%\n",
      "Epoch [9/300], Step [173/225], Training Accuracy: 56.3313%, Training Loss: 0.8879%\n",
      "Epoch [9/300], Step [174/225], Training Accuracy: 56.3218%, Training Loss: 0.8886%\n",
      "Epoch [9/300], Step [175/225], Training Accuracy: 56.3929%, Training Loss: 0.8882%\n",
      "Epoch [9/300], Step [176/225], Training Accuracy: 56.4098%, Training Loss: 0.8876%\n",
      "Epoch [9/300], Step [177/225], Training Accuracy: 56.4266%, Training Loss: 0.8877%\n",
      "Epoch [9/300], Step [178/225], Training Accuracy: 56.3729%, Training Loss: 0.8878%\n",
      "Epoch [9/300], Step [179/225], Training Accuracy: 56.4333%, Training Loss: 0.8872%\n",
      "Epoch [9/300], Step [180/225], Training Accuracy: 56.4497%, Training Loss: 0.8868%\n",
      "Epoch [9/300], Step [181/225], Training Accuracy: 56.4313%, Training Loss: 0.8870%\n",
      "Epoch [9/300], Step [182/225], Training Accuracy: 56.4475%, Training Loss: 0.8865%\n",
      "Epoch [9/300], Step [183/225], Training Accuracy: 56.4549%, Training Loss: 0.8863%\n",
      "Epoch [9/300], Step [184/225], Training Accuracy: 56.4453%, Training Loss: 0.8861%\n",
      "Epoch [9/300], Step [185/225], Training Accuracy: 56.4696%, Training Loss: 0.8857%\n",
      "Epoch [9/300], Step [186/225], Training Accuracy: 56.5020%, Training Loss: 0.8850%\n",
      "Epoch [9/300], Step [187/225], Training Accuracy: 56.5424%, Training Loss: 0.8844%\n",
      "Epoch [9/300], Step [188/225], Training Accuracy: 56.5741%, Training Loss: 0.8840%\n",
      "Epoch [9/300], Step [189/225], Training Accuracy: 56.5807%, Training Loss: 0.8834%\n",
      "Epoch [9/300], Step [190/225], Training Accuracy: 56.5954%, Training Loss: 0.8833%\n",
      "Epoch [9/300], Step [191/225], Training Accuracy: 56.5854%, Training Loss: 0.8831%\n",
      "Epoch [9/300], Step [192/225], Training Accuracy: 56.6243%, Training Loss: 0.8826%\n",
      "Epoch [9/300], Step [193/225], Training Accuracy: 56.6224%, Training Loss: 0.8829%\n",
      "Epoch [9/300], Step [194/225], Training Accuracy: 56.6366%, Training Loss: 0.8823%\n",
      "Epoch [9/300], Step [195/225], Training Accuracy: 56.6667%, Training Loss: 0.8817%\n",
      "Epoch [9/300], Step [196/225], Training Accuracy: 56.6486%, Training Loss: 0.8821%\n",
      "Epoch [9/300], Step [197/225], Training Accuracy: 56.6783%, Training Loss: 0.8821%\n",
      "Epoch [9/300], Step [198/225], Training Accuracy: 56.7866%, Training Loss: 0.8810%\n",
      "Epoch [9/300], Step [199/225], Training Accuracy: 56.8153%, Training Loss: 0.8802%\n",
      "Epoch [9/300], Step [200/225], Training Accuracy: 56.8125%, Training Loss: 0.8801%\n",
      "Epoch [9/300], Step [201/225], Training Accuracy: 56.7786%, Training Loss: 0.8806%\n",
      "Epoch [9/300], Step [202/225], Training Accuracy: 56.7528%, Training Loss: 0.8808%\n",
      "Epoch [9/300], Step [203/225], Training Accuracy: 56.7888%, Training Loss: 0.8806%\n",
      "Epoch [9/300], Step [204/225], Training Accuracy: 56.7862%, Training Loss: 0.8811%\n",
      "Epoch [9/300], Step [205/225], Training Accuracy: 56.8445%, Training Loss: 0.8808%\n",
      "Epoch [9/300], Step [206/225], Training Accuracy: 56.8340%, Training Loss: 0.8810%\n",
      "Epoch [9/300], Step [207/225], Training Accuracy: 56.8237%, Training Loss: 0.8810%\n",
      "Epoch [9/300], Step [208/225], Training Accuracy: 56.8134%, Training Loss: 0.8806%\n",
      "Epoch [9/300], Step [209/225], Training Accuracy: 56.8630%, Training Loss: 0.8802%\n",
      "Epoch [9/300], Step [210/225], Training Accuracy: 56.8378%, Training Loss: 0.8804%\n",
      "Epoch [9/300], Step [211/225], Training Accuracy: 56.8424%, Training Loss: 0.8802%\n",
      "Epoch [9/300], Step [212/225], Training Accuracy: 56.8249%, Training Loss: 0.8802%\n",
      "Epoch [9/300], Step [213/225], Training Accuracy: 56.8002%, Training Loss: 0.8810%\n",
      "Epoch [9/300], Step [214/225], Training Accuracy: 56.8122%, Training Loss: 0.8806%\n",
      "Epoch [9/300], Step [215/225], Training Accuracy: 56.8096%, Training Loss: 0.8807%\n",
      "Epoch [9/300], Step [216/225], Training Accuracy: 56.7998%, Training Loss: 0.8809%\n",
      "Epoch [9/300], Step [217/225], Training Accuracy: 56.7612%, Training Loss: 0.8813%\n",
      "Epoch [9/300], Step [218/225], Training Accuracy: 56.7231%, Training Loss: 0.8815%\n",
      "Epoch [9/300], Step [219/225], Training Accuracy: 56.7423%, Training Loss: 0.8812%\n",
      "Epoch [9/300], Step [220/225], Training Accuracy: 56.7401%, Training Loss: 0.8813%\n",
      "Epoch [9/300], Step [221/225], Training Accuracy: 56.7166%, Training Loss: 0.8816%\n",
      "Epoch [9/300], Step [222/225], Training Accuracy: 56.7216%, Training Loss: 0.8817%\n",
      "Epoch [9/300], Step [223/225], Training Accuracy: 56.6844%, Training Loss: 0.8818%\n",
      "Epoch [9/300], Step [224/225], Training Accuracy: 56.6406%, Training Loss: 0.8815%\n",
      "Epoch [9/300], Step [225/225], Training Accuracy: 56.5870%, Training Loss: 0.8824%\n",
      "Epoch [10/300], Step [1/225], Training Accuracy: 65.6250%, Training Loss: 0.8809%\n",
      "Epoch [10/300], Step [2/225], Training Accuracy: 64.8438%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [3/225], Training Accuracy: 60.9375%, Training Loss: 0.8743%\n",
      "Epoch [10/300], Step [4/225], Training Accuracy: 60.1562%, Training Loss: 0.8826%\n",
      "Epoch [10/300], Step [5/225], Training Accuracy: 60.3125%, Training Loss: 0.8561%\n",
      "Epoch [10/300], Step [6/225], Training Accuracy: 59.1146%, Training Loss: 0.8803%\n",
      "Epoch [10/300], Step [7/225], Training Accuracy: 60.2679%, Training Loss: 0.8731%\n",
      "Epoch [10/300], Step [8/225], Training Accuracy: 59.9609%, Training Loss: 0.8706%\n",
      "Epoch [10/300], Step [9/225], Training Accuracy: 58.5069%, Training Loss: 0.8831%\n",
      "Epoch [10/300], Step [10/225], Training Accuracy: 57.5000%, Training Loss: 0.9008%\n",
      "Epoch [10/300], Step [11/225], Training Accuracy: 57.6705%, Training Loss: 0.8999%\n",
      "Epoch [10/300], Step [12/225], Training Accuracy: 58.0729%, Training Loss: 0.8955%\n",
      "Epoch [10/300], Step [13/225], Training Accuracy: 58.6538%, Training Loss: 0.8860%\n",
      "Epoch [10/300], Step [14/225], Training Accuracy: 58.4821%, Training Loss: 0.8815%\n",
      "Epoch [10/300], Step [15/225], Training Accuracy: 58.8542%, Training Loss: 0.8784%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [16/225], Training Accuracy: 58.5938%, Training Loss: 0.8786%\n",
      "Epoch [10/300], Step [17/225], Training Accuracy: 59.0074%, Training Loss: 0.8743%\n",
      "Epoch [10/300], Step [18/225], Training Accuracy: 59.1146%, Training Loss: 0.8767%\n",
      "Epoch [10/300], Step [19/225], Training Accuracy: 59.1283%, Training Loss: 0.8751%\n",
      "Epoch [10/300], Step [20/225], Training Accuracy: 58.9062%, Training Loss: 0.8733%\n",
      "Epoch [10/300], Step [21/225], Training Accuracy: 59.4494%, Training Loss: 0.8649%\n",
      "Epoch [10/300], Step [22/225], Training Accuracy: 58.8068%, Training Loss: 0.8712%\n",
      "Epoch [10/300], Step [23/225], Training Accuracy: 59.2391%, Training Loss: 0.8648%\n",
      "Epoch [10/300], Step [24/225], Training Accuracy: 58.7240%, Training Loss: 0.8696%\n",
      "Epoch [10/300], Step [25/225], Training Accuracy: 58.7500%, Training Loss: 0.8654%\n",
      "Epoch [10/300], Step [26/225], Training Accuracy: 58.5938%, Training Loss: 0.8669%\n",
      "Epoch [10/300], Step [27/225], Training Accuracy: 58.5069%, Training Loss: 0.8675%\n",
      "Epoch [10/300], Step [28/225], Training Accuracy: 58.8170%, Training Loss: 0.8657%\n",
      "Epoch [10/300], Step [29/225], Training Accuracy: 58.5668%, Training Loss: 0.8662%\n",
      "Epoch [10/300], Step [30/225], Training Accuracy: 58.6458%, Training Loss: 0.8641%\n",
      "Epoch [10/300], Step [31/225], Training Accuracy: 58.3165%, Training Loss: 0.8688%\n",
      "Epoch [10/300], Step [32/225], Training Accuracy: 58.3496%, Training Loss: 0.8667%\n",
      "Epoch [10/300], Step [33/225], Training Accuracy: 58.5227%, Training Loss: 0.8633%\n",
      "Epoch [10/300], Step [34/225], Training Accuracy: 58.3180%, Training Loss: 0.8670%\n",
      "Epoch [10/300], Step [35/225], Training Accuracy: 58.3482%, Training Loss: 0.8713%\n",
      "Epoch [10/300], Step [36/225], Training Accuracy: 58.4201%, Training Loss: 0.8700%\n",
      "Epoch [10/300], Step [37/225], Training Accuracy: 58.4459%, Training Loss: 0.8680%\n",
      "Epoch [10/300], Step [38/225], Training Accuracy: 58.4704%, Training Loss: 0.8664%\n",
      "Epoch [10/300], Step [39/225], Training Accuracy: 58.4535%, Training Loss: 0.8659%\n",
      "Epoch [10/300], Step [40/225], Training Accuracy: 58.3984%, Training Loss: 0.8672%\n",
      "Epoch [10/300], Step [41/225], Training Accuracy: 58.1555%, Training Loss: 0.8705%\n",
      "Epoch [10/300], Step [42/225], Training Accuracy: 58.0729%, Training Loss: 0.8692%\n",
      "Epoch [10/300], Step [43/225], Training Accuracy: 57.8488%, Training Loss: 0.8695%\n",
      "Epoch [10/300], Step [44/225], Training Accuracy: 58.2741%, Training Loss: 0.8666%\n",
      "Epoch [10/300], Step [45/225], Training Accuracy: 58.4722%, Training Loss: 0.8644%\n",
      "Epoch [10/300], Step [46/225], Training Accuracy: 58.6277%, Training Loss: 0.8622%\n",
      "Epoch [10/300], Step [47/225], Training Accuracy: 58.5106%, Training Loss: 0.8608%\n",
      "Epoch [10/300], Step [48/225], Training Accuracy: 58.4310%, Training Loss: 0.8614%\n",
      "Epoch [10/300], Step [49/225], Training Accuracy: 58.4503%, Training Loss: 0.8607%\n",
      "Epoch [10/300], Step [50/225], Training Accuracy: 58.4688%, Training Loss: 0.8605%\n",
      "Epoch [10/300], Step [51/225], Training Accuracy: 58.5784%, Training Loss: 0.8583%\n",
      "Epoch [10/300], Step [52/225], Training Accuracy: 58.5938%, Training Loss: 0.8577%\n",
      "Epoch [10/300], Step [53/225], Training Accuracy: 58.4316%, Training Loss: 0.8577%\n",
      "Epoch [10/300], Step [54/225], Training Accuracy: 58.3623%, Training Loss: 0.8582%\n",
      "Epoch [10/300], Step [55/225], Training Accuracy: 58.1818%, Training Loss: 0.8603%\n",
      "Epoch [10/300], Step [56/225], Training Accuracy: 58.1752%, Training Loss: 0.8606%\n",
      "Epoch [10/300], Step [57/225], Training Accuracy: 58.2511%, Training Loss: 0.8588%\n",
      "Epoch [10/300], Step [58/225], Training Accuracy: 58.2435%, Training Loss: 0.8607%\n",
      "Epoch [10/300], Step [59/225], Training Accuracy: 58.4216%, Training Loss: 0.8594%\n",
      "Epoch [10/300], Step [60/225], Training Accuracy: 58.4635%, Training Loss: 0.8580%\n",
      "Epoch [10/300], Step [61/225], Training Accuracy: 58.4016%, Training Loss: 0.8577%\n",
      "Epoch [10/300], Step [62/225], Training Accuracy: 58.3669%, Training Loss: 0.8575%\n",
      "Epoch [10/300], Step [63/225], Training Accuracy: 58.1845%, Training Loss: 0.8592%\n",
      "Epoch [10/300], Step [64/225], Training Accuracy: 58.3252%, Training Loss: 0.8585%\n",
      "Epoch [10/300], Step [65/225], Training Accuracy: 58.2212%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [66/225], Training Accuracy: 58.3807%, Training Loss: 0.8573%\n",
      "Epoch [10/300], Step [67/225], Training Accuracy: 58.2789%, Training Loss: 0.8580%\n",
      "Epoch [10/300], Step [68/225], Training Accuracy: 58.2721%, Training Loss: 0.8592%\n",
      "Epoch [10/300], Step [69/225], Training Accuracy: 58.3107%, Training Loss: 0.8581%\n",
      "Epoch [10/300], Step [70/225], Training Accuracy: 58.2143%, Training Loss: 0.8574%\n",
      "Epoch [10/300], Step [71/225], Training Accuracy: 58.2526%, Training Loss: 0.8572%\n",
      "Epoch [10/300], Step [72/225], Training Accuracy: 58.1814%, Training Loss: 0.8585%\n",
      "Epoch [10/300], Step [73/225], Training Accuracy: 58.1336%, Training Loss: 0.8587%\n",
      "Epoch [10/300], Step [74/225], Training Accuracy: 58.1503%, Training Loss: 0.8575%\n",
      "Epoch [10/300], Step [75/225], Training Accuracy: 58.2292%, Training Loss: 0.8564%\n",
      "Epoch [10/300], Step [76/225], Training Accuracy: 58.1003%, Training Loss: 0.8576%\n",
      "Epoch [10/300], Step [77/225], Training Accuracy: 58.1372%, Training Loss: 0.8566%\n",
      "Epoch [10/300], Step [78/225], Training Accuracy: 58.1130%, Training Loss: 0.8565%\n",
      "Epoch [10/300], Step [79/225], Training Accuracy: 58.0696%, Training Loss: 0.8577%\n",
      "Epoch [10/300], Step [80/225], Training Accuracy: 58.0664%, Training Loss: 0.8584%\n",
      "Epoch [10/300], Step [81/225], Training Accuracy: 58.1019%, Training Loss: 0.8581%\n",
      "Epoch [10/300], Step [82/225], Training Accuracy: 58.1174%, Training Loss: 0.8573%\n",
      "Epoch [10/300], Step [83/225], Training Accuracy: 58.1137%, Training Loss: 0.8567%\n",
      "Epoch [10/300], Step [84/225], Training Accuracy: 58.1845%, Training Loss: 0.8556%\n",
      "Epoch [10/300], Step [85/225], Training Accuracy: 58.2537%, Training Loss: 0.8537%\n",
      "Epoch [10/300], Step [86/225], Training Accuracy: 58.2485%, Training Loss: 0.8539%\n",
      "Epoch [10/300], Step [87/225], Training Accuracy: 58.2435%, Training Loss: 0.8544%\n",
      "Epoch [10/300], Step [88/225], Training Accuracy: 58.2564%, Training Loss: 0.8542%\n",
      "Epoch [10/300], Step [89/225], Training Accuracy: 58.1812%, Training Loss: 0.8559%\n",
      "Epoch [10/300], Step [90/225], Training Accuracy: 58.1250%, Training Loss: 0.8582%\n",
      "Epoch [10/300], Step [91/225], Training Accuracy: 58.0014%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [92/225], Training Accuracy: 57.9993%, Training Loss: 0.8593%\n",
      "Epoch [10/300], Step [93/225], Training Accuracy: 58.0645%, Training Loss: 0.8586%\n",
      "Epoch [10/300], Step [94/225], Training Accuracy: 58.1117%, Training Loss: 0.8577%\n",
      "Epoch [10/300], Step [95/225], Training Accuracy: 58.0921%, Training Loss: 0.8583%\n",
      "Epoch [10/300], Step [96/225], Training Accuracy: 58.1706%, Training Loss: 0.8564%\n",
      "Epoch [10/300], Step [97/225], Training Accuracy: 58.1024%, Training Loss: 0.8566%\n",
      "Epoch [10/300], Step [98/225], Training Accuracy: 58.0038%, Training Loss: 0.8573%\n",
      "Epoch [10/300], Step [99/225], Training Accuracy: 57.9072%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [100/225], Training Accuracy: 57.8594%, Training Loss: 0.8599%\n",
      "Epoch [10/300], Step [101/225], Training Accuracy: 57.8434%, Training Loss: 0.8600%\n",
      "Epoch [10/300], Step [102/225], Training Accuracy: 57.9197%, Training Loss: 0.8595%\n",
      "Epoch [10/300], Step [103/225], Training Accuracy: 57.9945%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [104/225], Training Accuracy: 57.8726%, Training Loss: 0.8596%\n",
      "Epoch [10/300], Step [105/225], Training Accuracy: 57.9464%, Training Loss: 0.8597%\n",
      "Epoch [10/300], Step [106/225], Training Accuracy: 57.9746%, Training Loss: 0.8597%\n",
      "Epoch [10/300], Step [107/225], Training Accuracy: 57.9585%, Training Loss: 0.8600%\n",
      "Epoch [10/300], Step [108/225], Training Accuracy: 57.9138%, Training Loss: 0.8597%\n",
      "Epoch [10/300], Step [109/225], Training Accuracy: 57.8985%, Training Loss: 0.8594%\n",
      "Epoch [10/300], Step [110/225], Training Accuracy: 57.9119%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [111/225], Training Accuracy: 57.8547%, Training Loss: 0.8588%\n",
      "Epoch [10/300], Step [112/225], Training Accuracy: 57.9381%, Training Loss: 0.8581%\n",
      "Epoch [10/300], Step [113/225], Training Accuracy: 57.9784%, Training Loss: 0.8578%\n",
      "Epoch [10/300], Step [114/225], Training Accuracy: 57.9770%, Training Loss: 0.8576%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [115/225], Training Accuracy: 58.0571%, Training Loss: 0.8570%\n",
      "Epoch [10/300], Step [116/225], Training Accuracy: 58.0819%, Training Loss: 0.8565%\n",
      "Epoch [10/300], Step [117/225], Training Accuracy: 58.0262%, Training Loss: 0.8573%\n",
      "Epoch [10/300], Step [118/225], Training Accuracy: 57.9714%, Training Loss: 0.8580%\n",
      "Epoch [10/300], Step [119/225], Training Accuracy: 58.0226%, Training Loss: 0.8581%\n",
      "Epoch [10/300], Step [120/225], Training Accuracy: 57.9948%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [121/225], Training Accuracy: 57.9416%, Training Loss: 0.8589%\n",
      "Epoch [10/300], Step [122/225], Training Accuracy: 58.0174%, Training Loss: 0.8584%\n",
      "Epoch [10/300], Step [123/225], Training Accuracy: 58.0285%, Training Loss: 0.8578%\n",
      "Epoch [10/300], Step [124/225], Training Accuracy: 58.0267%, Training Loss: 0.8575%\n",
      "Epoch [10/300], Step [125/225], Training Accuracy: 57.9750%, Training Loss: 0.8579%\n",
      "Epoch [10/300], Step [126/225], Training Accuracy: 57.9613%, Training Loss: 0.8588%\n",
      "Epoch [10/300], Step [127/225], Training Accuracy: 57.9232%, Training Loss: 0.8600%\n",
      "Epoch [10/300], Step [128/225], Training Accuracy: 57.8735%, Training Loss: 0.8612%\n",
      "Epoch [10/300], Step [129/225], Training Accuracy: 57.8246%, Training Loss: 0.8619%\n",
      "Epoch [10/300], Step [130/225], Training Accuracy: 57.7524%, Training Loss: 0.8627%\n",
      "Epoch [10/300], Step [131/225], Training Accuracy: 57.6813%, Training Loss: 0.8633%\n",
      "Epoch [10/300], Step [132/225], Training Accuracy: 57.5758%, Training Loss: 0.8648%\n",
      "Epoch [10/300], Step [133/225], Training Accuracy: 57.5540%, Training Loss: 0.8654%\n",
      "Epoch [10/300], Step [134/225], Training Accuracy: 57.4394%, Training Loss: 0.8667%\n",
      "Epoch [10/300], Step [135/225], Training Accuracy: 57.4769%, Training Loss: 0.8666%\n",
      "Epoch [10/300], Step [136/225], Training Accuracy: 57.4908%, Training Loss: 0.8661%\n",
      "Epoch [10/300], Step [137/225], Training Accuracy: 57.5274%, Training Loss: 0.8655%\n",
      "Epoch [10/300], Step [138/225], Training Accuracy: 57.5408%, Training Loss: 0.8650%\n",
      "Epoch [10/300], Step [139/225], Training Accuracy: 57.5090%, Training Loss: 0.8648%\n",
      "Epoch [10/300], Step [140/225], Training Accuracy: 57.5781%, Training Loss: 0.8645%\n",
      "Epoch [10/300], Step [141/225], Training Accuracy: 57.5355%, Training Loss: 0.8647%\n",
      "Epoch [10/300], Step [142/225], Training Accuracy: 57.5594%, Training Loss: 0.8639%\n",
      "Epoch [10/300], Step [143/225], Training Accuracy: 57.5393%, Training Loss: 0.8638%\n",
      "Epoch [10/300], Step [144/225], Training Accuracy: 57.6063%, Training Loss: 0.8635%\n",
      "Epoch [10/300], Step [145/225], Training Accuracy: 57.6293%, Training Loss: 0.8631%\n",
      "Epoch [10/300], Step [146/225], Training Accuracy: 57.5878%, Training Loss: 0.8632%\n",
      "Epoch [10/300], Step [147/225], Training Accuracy: 57.5255%, Training Loss: 0.8635%\n",
      "Epoch [10/300], Step [148/225], Training Accuracy: 57.5908%, Training Loss: 0.8631%\n",
      "Epoch [10/300], Step [149/225], Training Accuracy: 57.5084%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [150/225], Training Accuracy: 57.5625%, Training Loss: 0.8628%\n",
      "Epoch [10/300], Step [151/225], Training Accuracy: 57.5642%, Training Loss: 0.8629%\n",
      "Epoch [10/300], Step [152/225], Training Accuracy: 57.5555%, Training Loss: 0.8639%\n",
      "Epoch [10/300], Step [153/225], Training Accuracy: 57.5470%, Training Loss: 0.8640%\n",
      "Epoch [10/300], Step [154/225], Training Accuracy: 57.5386%, Training Loss: 0.8639%\n",
      "Epoch [10/300], Step [155/225], Training Accuracy: 57.4698%, Training Loss: 0.8640%\n",
      "Epoch [10/300], Step [156/225], Training Accuracy: 57.4619%, Training Loss: 0.8644%\n",
      "Epoch [10/300], Step [157/225], Training Accuracy: 57.4940%, Training Loss: 0.8639%\n",
      "Epoch [10/300], Step [158/225], Training Accuracy: 57.4169%, Training Loss: 0.8653%\n",
      "Epoch [10/300], Step [159/225], Training Accuracy: 57.3801%, Training Loss: 0.8657%\n",
      "Epoch [10/300], Step [160/225], Training Accuracy: 57.3438%, Training Loss: 0.8656%\n",
      "Epoch [10/300], Step [161/225], Training Accuracy: 57.3661%, Training Loss: 0.8652%\n",
      "Epoch [10/300], Step [162/225], Training Accuracy: 57.4363%, Training Loss: 0.8646%\n",
      "Epoch [10/300], Step [163/225], Training Accuracy: 57.4482%, Training Loss: 0.8644%\n",
      "Epoch [10/300], Step [164/225], Training Accuracy: 57.5076%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [165/225], Training Accuracy: 57.5379%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [166/225], Training Accuracy: 57.5772%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [167/225], Training Accuracy: 57.5225%, Training Loss: 0.8635%\n",
      "Epoch [10/300], Step [168/225], Training Accuracy: 57.4963%, Training Loss: 0.8634%\n",
      "Epoch [10/300], Step [169/225], Training Accuracy: 57.4612%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [170/225], Training Accuracy: 57.4540%, Training Loss: 0.8635%\n",
      "Epoch [10/300], Step [171/225], Training Accuracy: 57.4561%, Training Loss: 0.8634%\n",
      "Epoch [10/300], Step [172/225], Training Accuracy: 57.4582%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [173/225], Training Accuracy: 57.4332%, Training Loss: 0.8636%\n",
      "Epoch [10/300], Step [174/225], Training Accuracy: 57.4264%, Training Loss: 0.8639%\n",
      "Epoch [10/300], Step [175/225], Training Accuracy: 57.4554%, Training Loss: 0.8637%\n",
      "Epoch [10/300], Step [176/225], Training Accuracy: 57.4663%, Training Loss: 0.8634%\n",
      "Epoch [10/300], Step [177/225], Training Accuracy: 57.4770%, Training Loss: 0.8634%\n",
      "Epoch [10/300], Step [178/225], Training Accuracy: 57.4789%, Training Loss: 0.8633%\n",
      "Epoch [10/300], Step [179/225], Training Accuracy: 57.5506%, Training Loss: 0.8627%\n",
      "Epoch [10/300], Step [180/225], Training Accuracy: 57.6042%, Training Loss: 0.8620%\n",
      "Epoch [10/300], Step [181/225], Training Accuracy: 57.5535%, Training Loss: 0.8634%\n",
      "Epoch [10/300], Step [182/225], Training Accuracy: 57.5549%, Training Loss: 0.8633%\n",
      "Epoch [10/300], Step [183/225], Training Accuracy: 57.5564%, Training Loss: 0.8632%\n",
      "Epoch [10/300], Step [184/225], Training Accuracy: 57.5917%, Training Loss: 0.8629%\n",
      "Epoch [10/300], Step [185/225], Training Accuracy: 57.6436%, Training Loss: 0.8627%\n",
      "Epoch [10/300], Step [186/225], Training Accuracy: 57.6949%, Training Loss: 0.8619%\n",
      "Epoch [10/300], Step [187/225], Training Accuracy: 57.6537%, Training Loss: 0.8617%\n",
      "Epoch [10/300], Step [188/225], Training Accuracy: 57.6795%, Training Loss: 0.8616%\n",
      "Epoch [10/300], Step [189/225], Training Accuracy: 57.6802%, Training Loss: 0.8612%\n",
      "Epoch [10/300], Step [190/225], Training Accuracy: 57.6809%, Training Loss: 0.8612%\n",
      "Epoch [10/300], Step [191/225], Training Accuracy: 57.6898%, Training Loss: 0.8608%\n",
      "Epoch [10/300], Step [192/225], Training Accuracy: 57.7230%, Training Loss: 0.8607%\n",
      "Epoch [10/300], Step [193/225], Training Accuracy: 57.7234%, Training Loss: 0.8608%\n",
      "Epoch [10/300], Step [194/225], Training Accuracy: 57.7481%, Training Loss: 0.8604%\n",
      "Epoch [10/300], Step [195/225], Training Accuracy: 57.7484%, Training Loss: 0.8599%\n",
      "Epoch [10/300], Step [196/225], Training Accuracy: 57.7168%, Training Loss: 0.8608%\n",
      "Epoch [10/300], Step [197/225], Training Accuracy: 57.6777%, Training Loss: 0.8612%\n",
      "Epoch [10/300], Step [198/225], Training Accuracy: 57.7415%, Training Loss: 0.8603%\n",
      "Epoch [10/300], Step [199/225], Training Accuracy: 57.7654%, Training Loss: 0.8596%\n",
      "Epoch [10/300], Step [200/225], Training Accuracy: 57.7422%, Training Loss: 0.8602%\n",
      "Epoch [10/300], Step [201/225], Training Accuracy: 57.6881%, Training Loss: 0.8611%\n",
      "Epoch [10/300], Step [202/225], Training Accuracy: 57.6810%, Training Loss: 0.8610%\n",
      "Epoch [10/300], Step [203/225], Training Accuracy: 57.7432%, Training Loss: 0.8606%\n",
      "Epoch [10/300], Step [204/225], Training Accuracy: 57.7819%, Training Loss: 0.8605%\n",
      "Epoch [10/300], Step [205/225], Training Accuracy: 57.8430%, Training Loss: 0.8603%\n",
      "Epoch [10/300], Step [206/225], Training Accuracy: 57.8428%, Training Loss: 0.8605%\n",
      "Epoch [10/300], Step [207/225], Training Accuracy: 57.8125%, Training Loss: 0.8606%\n",
      "Epoch [10/300], Step [208/225], Training Accuracy: 57.8125%, Training Loss: 0.8604%\n",
      "Epoch [10/300], Step [209/225], Training Accuracy: 57.8424%, Training Loss: 0.8607%\n",
      "Epoch [10/300], Step [210/225], Training Accuracy: 57.8051%, Training Loss: 0.8610%\n",
      "Epoch [10/300], Step [211/225], Training Accuracy: 57.8347%, Training Loss: 0.8611%\n",
      "Epoch [10/300], Step [212/225], Training Accuracy: 57.8420%, Training Loss: 0.8613%\n",
      "Epoch [10/300], Step [213/225], Training Accuracy: 57.8272%, Training Loss: 0.8620%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [214/225], Training Accuracy: 57.8344%, Training Loss: 0.8621%\n",
      "Epoch [10/300], Step [215/225], Training Accuracy: 57.7980%, Training Loss: 0.8621%\n",
      "Epoch [10/300], Step [216/225], Training Accuracy: 57.7546%, Training Loss: 0.8628%\n",
      "Epoch [10/300], Step [217/225], Training Accuracy: 57.7261%, Training Loss: 0.8631%\n",
      "Epoch [10/300], Step [218/225], Training Accuracy: 57.7122%, Training Loss: 0.8638%\n",
      "Epoch [10/300], Step [219/225], Training Accuracy: 57.7197%, Training Loss: 0.8635%\n",
      "Epoch [10/300], Step [220/225], Training Accuracy: 57.7060%, Training Loss: 0.8634%\n",
      "Epoch [10/300], Step [221/225], Training Accuracy: 57.6499%, Training Loss: 0.8641%\n",
      "Epoch [10/300], Step [222/225], Training Accuracy: 57.6295%, Training Loss: 0.8638%\n",
      "Epoch [10/300], Step [223/225], Training Accuracy: 57.6233%, Training Loss: 0.8638%\n",
      "Epoch [10/300], Step [224/225], Training Accuracy: 57.6172%, Training Loss: 0.8638%\n",
      "Epoch [10/300], Step [225/225], Training Accuracy: 57.5875%, Training Loss: 0.8645%\n",
      "Epoch [11/300], Step [1/225], Training Accuracy: 65.6250%, Training Loss: 0.7364%\n",
      "Epoch [11/300], Step [2/225], Training Accuracy: 67.1875%, Training Loss: 0.7851%\n",
      "Epoch [11/300], Step [3/225], Training Accuracy: 63.5417%, Training Loss: 0.8172%\n",
      "Epoch [11/300], Step [4/225], Training Accuracy: 61.3281%, Training Loss: 0.8295%\n",
      "Epoch [11/300], Step [5/225], Training Accuracy: 62.8125%, Training Loss: 0.8135%\n",
      "Epoch [11/300], Step [6/225], Training Accuracy: 61.9792%, Training Loss: 0.8328%\n",
      "Epoch [11/300], Step [7/225], Training Accuracy: 62.7232%, Training Loss: 0.8208%\n",
      "Epoch [11/300], Step [8/225], Training Accuracy: 61.5234%, Training Loss: 0.8274%\n",
      "Epoch [11/300], Step [9/225], Training Accuracy: 60.7639%, Training Loss: 0.8346%\n",
      "Epoch [11/300], Step [10/225], Training Accuracy: 59.6875%, Training Loss: 0.8508%\n",
      "Epoch [11/300], Step [11/225], Training Accuracy: 60.3693%, Training Loss: 0.8489%\n",
      "Epoch [11/300], Step [12/225], Training Accuracy: 60.8073%, Training Loss: 0.8504%\n",
      "Epoch [11/300], Step [13/225], Training Accuracy: 61.1779%, Training Loss: 0.8432%\n",
      "Epoch [11/300], Step [14/225], Training Accuracy: 61.4955%, Training Loss: 0.8400%\n",
      "Epoch [11/300], Step [15/225], Training Accuracy: 61.4583%, Training Loss: 0.8422%\n",
      "Epoch [11/300], Step [16/225], Training Accuracy: 61.3281%, Training Loss: 0.8396%\n",
      "Epoch [11/300], Step [17/225], Training Accuracy: 61.5809%, Training Loss: 0.8365%\n",
      "Epoch [11/300], Step [18/225], Training Accuracy: 61.2847%, Training Loss: 0.8399%\n",
      "Epoch [11/300], Step [19/225], Training Accuracy: 61.1020%, Training Loss: 0.8407%\n",
      "Epoch [11/300], Step [20/225], Training Accuracy: 61.1719%, Training Loss: 0.8386%\n",
      "Epoch [11/300], Step [21/225], Training Accuracy: 61.4583%, Training Loss: 0.8333%\n",
      "Epoch [11/300], Step [22/225], Training Accuracy: 61.0085%, Training Loss: 0.8416%\n",
      "Epoch [11/300], Step [23/225], Training Accuracy: 61.4810%, Training Loss: 0.8359%\n",
      "Epoch [11/300], Step [24/225], Training Accuracy: 60.8724%, Training Loss: 0.8395%\n",
      "Epoch [11/300], Step [25/225], Training Accuracy: 60.8750%, Training Loss: 0.8369%\n",
      "Epoch [11/300], Step [26/225], Training Accuracy: 60.5168%, Training Loss: 0.8372%\n",
      "Epoch [11/300], Step [27/225], Training Accuracy: 60.4745%, Training Loss: 0.8382%\n",
      "Epoch [11/300], Step [28/225], Training Accuracy: 60.8259%, Training Loss: 0.8355%\n",
      "Epoch [11/300], Step [29/225], Training Accuracy: 60.8297%, Training Loss: 0.8368%\n",
      "Epoch [11/300], Step [30/225], Training Accuracy: 60.8854%, Training Loss: 0.8335%\n",
      "Epoch [11/300], Step [31/225], Training Accuracy: 60.8367%, Training Loss: 0.8353%\n",
      "Epoch [11/300], Step [32/225], Training Accuracy: 60.6934%, Training Loss: 0.8338%\n",
      "Epoch [11/300], Step [33/225], Training Accuracy: 60.8902%, Training Loss: 0.8292%\n",
      "Epoch [11/300], Step [34/225], Training Accuracy: 60.5699%, Training Loss: 0.8333%\n",
      "Epoch [11/300], Step [35/225], Training Accuracy: 60.4464%, Training Loss: 0.8353%\n",
      "Epoch [11/300], Step [36/225], Training Accuracy: 60.5035%, Training Loss: 0.8335%\n",
      "Epoch [11/300], Step [37/225], Training Accuracy: 60.4307%, Training Loss: 0.8321%\n",
      "Epoch [11/300], Step [38/225], Training Accuracy: 60.5263%, Training Loss: 0.8301%\n",
      "Epoch [11/300], Step [39/225], Training Accuracy: 60.3766%, Training Loss: 0.8305%\n",
      "Epoch [11/300], Step [40/225], Training Accuracy: 60.1562%, Training Loss: 0.8327%\n",
      "Epoch [11/300], Step [41/225], Training Accuracy: 60.0610%, Training Loss: 0.8341%\n",
      "Epoch [11/300], Step [42/225], Training Accuracy: 59.9330%, Training Loss: 0.8342%\n",
      "Epoch [11/300], Step [43/225], Training Accuracy: 59.8474%, Training Loss: 0.8343%\n",
      "Epoch [11/300], Step [44/225], Training Accuracy: 60.2628%, Training Loss: 0.8303%\n",
      "Epoch [11/300], Step [45/225], Training Accuracy: 60.3125%, Training Loss: 0.8295%\n",
      "Epoch [11/300], Step [46/225], Training Accuracy: 60.3601%, Training Loss: 0.8287%\n",
      "Epoch [11/300], Step [47/225], Training Accuracy: 60.2394%, Training Loss: 0.8279%\n",
      "Epoch [11/300], Step [48/225], Training Accuracy: 60.2214%, Training Loss: 0.8283%\n",
      "Epoch [11/300], Step [49/225], Training Accuracy: 60.2679%, Training Loss: 0.8275%\n",
      "Epoch [11/300], Step [50/225], Training Accuracy: 60.2812%, Training Loss: 0.8279%\n",
      "Epoch [11/300], Step [51/225], Training Accuracy: 60.2941%, Training Loss: 0.8274%\n",
      "Epoch [11/300], Step [52/225], Training Accuracy: 60.3666%, Training Loss: 0.8257%\n",
      "Epoch [11/300], Step [53/225], Training Accuracy: 60.4068%, Training Loss: 0.8254%\n",
      "Epoch [11/300], Step [54/225], Training Accuracy: 60.2431%, Training Loss: 0.8255%\n",
      "Epoch [11/300], Step [55/225], Training Accuracy: 60.0852%, Training Loss: 0.8275%\n",
      "Epoch [11/300], Step [56/225], Training Accuracy: 60.0167%, Training Loss: 0.8277%\n",
      "Epoch [11/300], Step [57/225], Training Accuracy: 60.0877%, Training Loss: 0.8264%\n",
      "Epoch [11/300], Step [58/225], Training Accuracy: 60.0485%, Training Loss: 0.8283%\n",
      "Epoch [11/300], Step [59/225], Training Accuracy: 60.1695%, Training Loss: 0.8270%\n",
      "Epoch [11/300], Step [60/225], Training Accuracy: 60.2604%, Training Loss: 0.8259%\n",
      "Epoch [11/300], Step [61/225], Training Accuracy: 60.2715%, Training Loss: 0.8263%\n",
      "Epoch [11/300], Step [62/225], Training Accuracy: 60.2319%, Training Loss: 0.8262%\n",
      "Epoch [11/300], Step [63/225], Training Accuracy: 60.0446%, Training Loss: 0.8276%\n",
      "Epoch [11/300], Step [64/225], Training Accuracy: 60.2539%, Training Loss: 0.8268%\n",
      "Epoch [11/300], Step [65/225], Training Accuracy: 60.1683%, Training Loss: 0.8274%\n",
      "Epoch [11/300], Step [66/225], Training Accuracy: 60.2983%, Training Loss: 0.8266%\n",
      "Epoch [11/300], Step [67/225], Training Accuracy: 60.3312%, Training Loss: 0.8274%\n",
      "Epoch [11/300], Step [68/225], Training Accuracy: 60.3860%, Training Loss: 0.8285%\n",
      "Epoch [11/300], Step [69/225], Training Accuracy: 60.2808%, Training Loss: 0.8283%\n",
      "Epoch [11/300], Step [70/225], Training Accuracy: 60.1786%, Training Loss: 0.8284%\n",
      "Epoch [11/300], Step [71/225], Training Accuracy: 60.1673%, Training Loss: 0.8276%\n",
      "Epoch [11/300], Step [72/225], Training Accuracy: 60.1780%, Training Loss: 0.8284%\n",
      "Epoch [11/300], Step [73/225], Training Accuracy: 60.0813%, Training Loss: 0.8295%\n",
      "Epoch [11/300], Step [74/225], Training Accuracy: 60.1562%, Training Loss: 0.8277%\n",
      "Epoch [11/300], Step [75/225], Training Accuracy: 60.2708%, Training Loss: 0.8270%\n",
      "Epoch [11/300], Step [76/225], Training Accuracy: 60.1151%, Training Loss: 0.8290%\n",
      "Epoch [11/300], Step [77/225], Training Accuracy: 60.1055%, Training Loss: 0.8293%\n",
      "Epoch [11/300], Step [78/225], Training Accuracy: 59.9559%, Training Loss: 0.8301%\n",
      "Epoch [11/300], Step [79/225], Training Accuracy: 59.8497%, Training Loss: 0.8307%\n",
      "Epoch [11/300], Step [80/225], Training Accuracy: 59.8242%, Training Loss: 0.8313%\n",
      "Epoch [11/300], Step [81/225], Training Accuracy: 59.6836%, Training Loss: 0.8320%\n",
      "Epoch [11/300], Step [82/225], Training Accuracy: 59.6799%, Training Loss: 0.8321%\n",
      "Epoch [11/300], Step [83/225], Training Accuracy: 59.6197%, Training Loss: 0.8319%\n",
      "Epoch [11/300], Step [84/225], Training Accuracy: 59.5796%, Training Loss: 0.8316%\n",
      "Epoch [11/300], Step [85/225], Training Accuracy: 59.5404%, Training Loss: 0.8301%\n",
      "Epoch [11/300], Step [86/225], Training Accuracy: 59.5022%, Training Loss: 0.8321%\n",
      "Epoch [11/300], Step [87/225], Training Accuracy: 59.5546%, Training Loss: 0.8325%\n",
      "Epoch [11/300], Step [88/225], Training Accuracy: 59.6058%, Training Loss: 0.8324%\n",
      "Epoch [11/300], Step [89/225], Training Accuracy: 59.5330%, Training Loss: 0.8344%\n",
      "Epoch [11/300], Step [90/225], Training Accuracy: 59.4271%, Training Loss: 0.8369%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Step [91/225], Training Accuracy: 59.3063%, Training Loss: 0.8372%\n",
      "Epoch [11/300], Step [92/225], Training Accuracy: 59.2561%, Training Loss: 0.8366%\n",
      "Epoch [11/300], Step [93/225], Training Accuracy: 59.3414%, Training Loss: 0.8356%\n",
      "Epoch [11/300], Step [94/225], Training Accuracy: 59.3085%, Training Loss: 0.8348%\n",
      "Epoch [11/300], Step [95/225], Training Accuracy: 59.2763%, Training Loss: 0.8351%\n",
      "Epoch [11/300], Step [96/225], Training Accuracy: 59.3424%, Training Loss: 0.8338%\n",
      "Epoch [11/300], Step [97/225], Training Accuracy: 59.3589%, Training Loss: 0.8339%\n",
      "Epoch [11/300], Step [98/225], Training Accuracy: 59.2474%, Training Loss: 0.8348%\n",
      "Epoch [11/300], Step [99/225], Training Accuracy: 59.2487%, Training Loss: 0.8358%\n",
      "Epoch [11/300], Step [100/225], Training Accuracy: 59.1875%, Training Loss: 0.8375%\n",
      "Epoch [11/300], Step [101/225], Training Accuracy: 59.1429%, Training Loss: 0.8382%\n",
      "Epoch [11/300], Step [102/225], Training Accuracy: 59.0993%, Training Loss: 0.8377%\n",
      "Epoch [11/300], Step [103/225], Training Accuracy: 59.1171%, Training Loss: 0.8375%\n",
      "Epoch [11/300], Step [104/225], Training Accuracy: 59.0144%, Training Loss: 0.8385%\n",
      "Epoch [11/300], Step [105/225], Training Accuracy: 59.0327%, Training Loss: 0.8378%\n",
      "Epoch [11/300], Step [106/225], Training Accuracy: 59.0507%, Training Loss: 0.8374%\n",
      "Epoch [11/300], Step [107/225], Training Accuracy: 59.0391%, Training Loss: 0.8381%\n",
      "Epoch [11/300], Step [108/225], Training Accuracy: 59.0133%, Training Loss: 0.8382%\n",
      "Epoch [11/300], Step [109/225], Training Accuracy: 58.9019%, Training Loss: 0.8382%\n",
      "Epoch [11/300], Step [110/225], Training Accuracy: 58.9205%, Training Loss: 0.8378%\n",
      "Epoch [11/300], Step [111/225], Training Accuracy: 58.8682%, Training Loss: 0.8379%\n",
      "Epoch [11/300], Step [112/225], Training Accuracy: 58.9286%, Training Loss: 0.8372%\n",
      "Epoch [11/300], Step [113/225], Training Accuracy: 58.9187%, Training Loss: 0.8371%\n",
      "Epoch [11/300], Step [114/225], Training Accuracy: 58.9090%, Training Loss: 0.8367%\n",
      "Epoch [11/300], Step [115/225], Training Accuracy: 58.9674%, Training Loss: 0.8363%\n",
      "Epoch [11/300], Step [116/225], Training Accuracy: 58.9574%, Training Loss: 0.8360%\n",
      "Epoch [11/300], Step [117/225], Training Accuracy: 58.8809%, Training Loss: 0.8371%\n",
      "Epoch [11/300], Step [118/225], Training Accuracy: 58.8718%, Training Loss: 0.8374%\n",
      "Epoch [11/300], Step [119/225], Training Accuracy: 58.8892%, Training Loss: 0.8375%\n",
      "Epoch [11/300], Step [120/225], Training Accuracy: 58.9193%, Training Loss: 0.8379%\n",
      "Epoch [11/300], Step [121/225], Training Accuracy: 58.8972%, Training Loss: 0.8379%\n",
      "Epoch [11/300], Step [122/225], Training Accuracy: 58.9652%, Training Loss: 0.8378%\n",
      "Epoch [11/300], Step [123/225], Training Accuracy: 58.9431%, Training Loss: 0.8378%\n",
      "Epoch [11/300], Step [124/225], Training Accuracy: 58.9214%, Training Loss: 0.8374%\n",
      "Epoch [11/300], Step [125/225], Training Accuracy: 58.8750%, Training Loss: 0.8384%\n",
      "Epoch [11/300], Step [126/225], Training Accuracy: 58.8418%, Training Loss: 0.8397%\n",
      "Epoch [11/300], Step [127/225], Training Accuracy: 58.7844%, Training Loss: 0.8405%\n",
      "Epoch [11/300], Step [128/225], Training Accuracy: 58.7646%, Training Loss: 0.8416%\n",
      "Epoch [11/300], Step [129/225], Training Accuracy: 58.8178%, Training Loss: 0.8423%\n",
      "Epoch [11/300], Step [130/225], Training Accuracy: 58.7380%, Training Loss: 0.8434%\n",
      "Epoch [11/300], Step [131/225], Training Accuracy: 58.6594%, Training Loss: 0.8445%\n",
      "Epoch [11/300], Step [132/225], Training Accuracy: 58.5582%, Training Loss: 0.8455%\n",
      "Epoch [11/300], Step [133/225], Training Accuracy: 58.5291%, Training Loss: 0.8460%\n",
      "Epoch [11/300], Step [134/225], Training Accuracy: 58.4888%, Training Loss: 0.8474%\n",
      "Epoch [11/300], Step [135/225], Training Accuracy: 58.5417%, Training Loss: 0.8473%\n",
      "Epoch [11/300], Step [136/225], Training Accuracy: 58.5593%, Training Loss: 0.8473%\n",
      "Epoch [11/300], Step [137/225], Training Accuracy: 58.5880%, Training Loss: 0.8466%\n",
      "Epoch [11/300], Step [138/225], Training Accuracy: 58.6164%, Training Loss: 0.8464%\n",
      "Epoch [11/300], Step [139/225], Training Accuracy: 58.5881%, Training Loss: 0.8463%\n",
      "Epoch [11/300], Step [140/225], Training Accuracy: 58.6496%, Training Loss: 0.8455%\n",
      "Epoch [11/300], Step [141/225], Training Accuracy: 58.6436%, Training Loss: 0.8454%\n",
      "Epoch [11/300], Step [142/225], Training Accuracy: 58.7148%, Training Loss: 0.8447%\n",
      "Epoch [11/300], Step [143/225], Training Accuracy: 58.7303%, Training Loss: 0.8443%\n",
      "Epoch [11/300], Step [144/225], Training Accuracy: 58.7457%, Training Loss: 0.8442%\n",
      "Epoch [11/300], Step [145/225], Training Accuracy: 58.7392%, Training Loss: 0.8440%\n",
      "Epoch [11/300], Step [146/225], Training Accuracy: 58.6794%, Training Loss: 0.8443%\n",
      "Epoch [11/300], Step [147/225], Training Accuracy: 58.6310%, Training Loss: 0.8449%\n",
      "Epoch [11/300], Step [148/225], Training Accuracy: 58.6571%, Training Loss: 0.8448%\n",
      "Epoch [11/300], Step [149/225], Training Accuracy: 58.5885%, Training Loss: 0.8453%\n",
      "Epoch [11/300], Step [150/225], Training Accuracy: 58.6562%, Training Loss: 0.8444%\n",
      "Epoch [11/300], Step [151/225], Training Accuracy: 58.7024%, Training Loss: 0.8439%\n",
      "Epoch [11/300], Step [152/225], Training Accuracy: 58.6554%, Training Loss: 0.8451%\n",
      "Epoch [11/300], Step [153/225], Training Accuracy: 58.6601%, Training Loss: 0.8455%\n",
      "Epoch [11/300], Step [154/225], Training Accuracy: 58.7054%, Training Loss: 0.8453%\n",
      "Epoch [11/300], Step [155/225], Training Accuracy: 58.6593%, Training Loss: 0.8454%\n",
      "Epoch [11/300], Step [156/225], Training Accuracy: 58.6438%, Training Loss: 0.8462%\n",
      "Epoch [11/300], Step [157/225], Training Accuracy: 58.6584%, Training Loss: 0.8460%\n",
      "Epoch [11/300], Step [158/225], Training Accuracy: 58.5443%, Training Loss: 0.8474%\n",
      "Epoch [11/300], Step [159/225], Training Accuracy: 58.4611%, Training Loss: 0.8482%\n",
      "Epoch [11/300], Step [160/225], Training Accuracy: 58.4766%, Training Loss: 0.8480%\n",
      "Epoch [11/300], Step [161/225], Training Accuracy: 58.5016%, Training Loss: 0.8478%\n",
      "Epoch [11/300], Step [162/225], Training Accuracy: 58.5359%, Training Loss: 0.8472%\n",
      "Epoch [11/300], Step [163/225], Training Accuracy: 58.5314%, Training Loss: 0.8470%\n",
      "Epoch [11/300], Step [164/225], Training Accuracy: 58.5842%, Training Loss: 0.8463%\n",
      "Epoch [11/300], Step [165/225], Training Accuracy: 58.5795%, Training Loss: 0.8461%\n",
      "Epoch [11/300], Step [166/225], Training Accuracy: 58.6126%, Training Loss: 0.8458%\n",
      "Epoch [11/300], Step [167/225], Training Accuracy: 58.5891%, Training Loss: 0.8460%\n",
      "Epoch [11/300], Step [168/225], Training Accuracy: 58.5938%, Training Loss: 0.8458%\n",
      "Epoch [11/300], Step [169/225], Training Accuracy: 58.5614%, Training Loss: 0.8460%\n",
      "Epoch [11/300], Step [170/225], Training Accuracy: 58.5754%, Training Loss: 0.8461%\n",
      "Epoch [11/300], Step [171/225], Training Accuracy: 58.5709%, Training Loss: 0.8462%\n",
      "Epoch [11/300], Step [172/225], Training Accuracy: 58.5483%, Training Loss: 0.8466%\n",
      "Epoch [11/300], Step [173/225], Training Accuracy: 58.5621%, Training Loss: 0.8464%\n",
      "Epoch [11/300], Step [174/225], Training Accuracy: 58.5489%, Training Loss: 0.8467%\n",
      "Epoch [11/300], Step [175/225], Training Accuracy: 58.5804%, Training Loss: 0.8461%\n",
      "Epoch [11/300], Step [176/225], Training Accuracy: 58.6204%, Training Loss: 0.8458%\n",
      "Epoch [11/300], Step [177/225], Training Accuracy: 58.6776%, Training Loss: 0.8456%\n",
      "Epoch [11/300], Step [178/225], Training Accuracy: 58.6640%, Training Loss: 0.8455%\n",
      "Epoch [11/300], Step [179/225], Training Accuracy: 58.7116%, Training Loss: 0.8448%\n",
      "Epoch [11/300], Step [180/225], Training Accuracy: 58.7153%, Training Loss: 0.8449%\n",
      "Epoch [11/300], Step [181/225], Training Accuracy: 58.7103%, Training Loss: 0.8458%\n",
      "Epoch [11/300], Step [182/225], Training Accuracy: 58.7740%, Training Loss: 0.8454%\n",
      "Epoch [11/300], Step [183/225], Training Accuracy: 58.7688%, Training Loss: 0.8452%\n",
      "Epoch [11/300], Step [184/225], Training Accuracy: 58.7891%, Training Loss: 0.8448%\n",
      "Epoch [11/300], Step [185/225], Training Accuracy: 58.8598%, Training Loss: 0.8443%\n",
      "Epoch [11/300], Step [186/225], Training Accuracy: 58.9214%, Training Loss: 0.8434%\n",
      "Epoch [11/300], Step [187/225], Training Accuracy: 58.9071%, Training Loss: 0.8433%\n",
      "Epoch [11/300], Step [188/225], Training Accuracy: 58.9345%, Training Loss: 0.8429%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Step [189/225], Training Accuracy: 58.9534%, Training Loss: 0.8423%\n",
      "Epoch [11/300], Step [190/225], Training Accuracy: 58.9885%, Training Loss: 0.8422%\n",
      "Epoch [11/300], Step [191/225], Training Accuracy: 58.9578%, Training Loss: 0.8419%\n",
      "Epoch [11/300], Step [192/225], Training Accuracy: 59.0088%, Training Loss: 0.8414%\n",
      "Epoch [11/300], Step [193/225], Training Accuracy: 59.0107%, Training Loss: 0.8414%\n",
      "Epoch [11/300], Step [194/225], Training Accuracy: 58.9803%, Training Loss: 0.8410%\n",
      "Epoch [11/300], Step [195/225], Training Accuracy: 58.9984%, Training Loss: 0.8407%\n",
      "Epoch [11/300], Step [196/225], Training Accuracy: 58.9923%, Training Loss: 0.8412%\n",
      "Epoch [11/300], Step [197/225], Training Accuracy: 58.9864%, Training Loss: 0.8415%\n",
      "Epoch [11/300], Step [198/225], Training Accuracy: 59.0751%, Training Loss: 0.8405%\n",
      "Epoch [11/300], Step [199/225], Training Accuracy: 59.0845%, Training Loss: 0.8400%\n",
      "Epoch [11/300], Step [200/225], Training Accuracy: 59.1250%, Training Loss: 0.8399%\n",
      "Epoch [11/300], Step [201/225], Training Accuracy: 59.0796%, Training Loss: 0.8404%\n",
      "Epoch [11/300], Step [202/225], Training Accuracy: 59.0579%, Training Loss: 0.8403%\n",
      "Epoch [11/300], Step [203/225], Training Accuracy: 59.1056%, Training Loss: 0.8400%\n",
      "Epoch [11/300], Step [204/225], Training Accuracy: 59.1069%, Training Loss: 0.8402%\n",
      "Epoch [11/300], Step [205/225], Training Accuracy: 59.1616%, Training Loss: 0.8401%\n",
      "Epoch [11/300], Step [206/225], Training Accuracy: 59.1399%, Training Loss: 0.8401%\n",
      "Epoch [11/300], Step [207/225], Training Accuracy: 59.1184%, Training Loss: 0.8403%\n",
      "Epoch [11/300], Step [208/225], Training Accuracy: 59.1496%, Training Loss: 0.8397%\n",
      "Epoch [11/300], Step [209/225], Training Accuracy: 59.1507%, Training Loss: 0.8398%\n",
      "Epoch [11/300], Step [210/225], Training Accuracy: 59.1443%, Training Loss: 0.8400%\n",
      "Epoch [11/300], Step [211/225], Training Accuracy: 59.1602%, Training Loss: 0.8398%\n",
      "Epoch [11/300], Step [212/225], Training Accuracy: 59.1613%, Training Loss: 0.8398%\n",
      "Epoch [11/300], Step [213/225], Training Accuracy: 59.1403%, Training Loss: 0.8405%\n",
      "Epoch [11/300], Step [214/225], Training Accuracy: 59.1706%, Training Loss: 0.8403%\n",
      "Epoch [11/300], Step [215/225], Training Accuracy: 59.1206%, Training Loss: 0.8405%\n",
      "Epoch [11/300], Step [216/225], Training Accuracy: 59.0856%, Training Loss: 0.8409%\n",
      "Epoch [11/300], Step [217/225], Training Accuracy: 59.0726%, Training Loss: 0.8413%\n",
      "Epoch [11/300], Step [218/225], Training Accuracy: 59.0668%, Training Loss: 0.8414%\n",
      "Epoch [11/300], Step [219/225], Training Accuracy: 59.0611%, Training Loss: 0.8412%\n",
      "Epoch [11/300], Step [220/225], Training Accuracy: 59.0483%, Training Loss: 0.8414%\n",
      "Epoch [11/300], Step [221/225], Training Accuracy: 59.0286%, Training Loss: 0.8414%\n",
      "Epoch [11/300], Step [222/225], Training Accuracy: 59.0160%, Training Loss: 0.8413%\n",
      "Epoch [11/300], Step [223/225], Training Accuracy: 58.9826%, Training Loss: 0.8417%\n",
      "Epoch [11/300], Step [224/225], Training Accuracy: 58.9286%, Training Loss: 0.8418%\n",
      "Epoch [11/300], Step [225/225], Training Accuracy: 58.8938%, Training Loss: 0.8423%\n",
      "Epoch [12/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.7934%\n",
      "Epoch [12/300], Step [2/225], Training Accuracy: 64.0625%, Training Loss: 0.7883%\n",
      "Epoch [12/300], Step [3/225], Training Accuracy: 64.0625%, Training Loss: 0.8239%\n",
      "Epoch [12/300], Step [4/225], Training Accuracy: 61.7188%, Training Loss: 0.8201%\n",
      "Epoch [12/300], Step [5/225], Training Accuracy: 61.8750%, Training Loss: 0.8044%\n",
      "Epoch [12/300], Step [6/225], Training Accuracy: 60.1562%, Training Loss: 0.8345%\n",
      "Epoch [12/300], Step [7/225], Training Accuracy: 60.9375%, Training Loss: 0.8351%\n",
      "Epoch [12/300], Step [8/225], Training Accuracy: 60.5469%, Training Loss: 0.8369%\n",
      "Epoch [12/300], Step [9/225], Training Accuracy: 59.8958%, Training Loss: 0.8423%\n",
      "Epoch [12/300], Step [10/225], Training Accuracy: 59.0625%, Training Loss: 0.8542%\n",
      "Epoch [12/300], Step [11/225], Training Accuracy: 60.0852%, Training Loss: 0.8521%\n",
      "Epoch [12/300], Step [12/225], Training Accuracy: 60.0260%, Training Loss: 0.8501%\n",
      "Epoch [12/300], Step [13/225], Training Accuracy: 60.4567%, Training Loss: 0.8408%\n",
      "Epoch [12/300], Step [14/225], Training Accuracy: 60.3795%, Training Loss: 0.8381%\n",
      "Epoch [12/300], Step [15/225], Training Accuracy: 60.3125%, Training Loss: 0.8406%\n",
      "Epoch [12/300], Step [16/225], Training Accuracy: 59.9609%, Training Loss: 0.8365%\n",
      "Epoch [12/300], Step [17/225], Training Accuracy: 60.3860%, Training Loss: 0.8288%\n",
      "Epoch [12/300], Step [18/225], Training Accuracy: 60.4167%, Training Loss: 0.8272%\n",
      "Epoch [12/300], Step [19/225], Training Accuracy: 60.6908%, Training Loss: 0.8252%\n",
      "Epoch [12/300], Step [20/225], Training Accuracy: 60.9375%, Training Loss: 0.8215%\n",
      "Epoch [12/300], Step [21/225], Training Accuracy: 61.3839%, Training Loss: 0.8151%\n",
      "Epoch [12/300], Step [22/225], Training Accuracy: 60.8665%, Training Loss: 0.8192%\n",
      "Epoch [12/300], Step [23/225], Training Accuracy: 61.3451%, Training Loss: 0.8133%\n",
      "Epoch [12/300], Step [24/225], Training Accuracy: 61.1979%, Training Loss: 0.8178%\n",
      "Epoch [12/300], Step [25/225], Training Accuracy: 61.0000%, Training Loss: 0.8160%\n",
      "Epoch [12/300], Step [26/225], Training Accuracy: 61.1178%, Training Loss: 0.8135%\n",
      "Epoch [12/300], Step [27/225], Training Accuracy: 60.8218%, Training Loss: 0.8133%\n",
      "Epoch [12/300], Step [28/225], Training Accuracy: 60.8817%, Training Loss: 0.8115%\n",
      "Epoch [12/300], Step [29/225], Training Accuracy: 61.1530%, Training Loss: 0.8084%\n",
      "Epoch [12/300], Step [30/225], Training Accuracy: 61.3021%, Training Loss: 0.8056%\n",
      "Epoch [12/300], Step [31/225], Training Accuracy: 61.0887%, Training Loss: 0.8120%\n",
      "Epoch [12/300], Step [32/225], Training Accuracy: 61.0352%, Training Loss: 0.8112%\n",
      "Epoch [12/300], Step [33/225], Training Accuracy: 61.2216%, Training Loss: 0.8075%\n",
      "Epoch [12/300], Step [34/225], Training Accuracy: 61.1213%, Training Loss: 0.8086%\n",
      "Epoch [12/300], Step [35/225], Training Accuracy: 61.0268%, Training Loss: 0.8108%\n",
      "Epoch [12/300], Step [36/225], Training Accuracy: 61.0677%, Training Loss: 0.8089%\n",
      "Epoch [12/300], Step [37/225], Training Accuracy: 61.0642%, Training Loss: 0.8080%\n",
      "Epoch [12/300], Step [38/225], Training Accuracy: 60.8964%, Training Loss: 0.8088%\n",
      "Epoch [12/300], Step [39/225], Training Accuracy: 61.0978%, Training Loss: 0.8075%\n",
      "Epoch [12/300], Step [40/225], Training Accuracy: 60.9766%, Training Loss: 0.8076%\n",
      "Epoch [12/300], Step [41/225], Training Accuracy: 60.9756%, Training Loss: 0.8091%\n",
      "Epoch [12/300], Step [42/225], Training Accuracy: 60.9747%, Training Loss: 0.8087%\n",
      "Epoch [12/300], Step [43/225], Training Accuracy: 60.9738%, Training Loss: 0.8085%\n",
      "Epoch [12/300], Step [44/225], Training Accuracy: 61.2571%, Training Loss: 0.8047%\n",
      "Epoch [12/300], Step [45/225], Training Accuracy: 61.2847%, Training Loss: 0.8025%\n",
      "Epoch [12/300], Step [46/225], Training Accuracy: 61.4130%, Training Loss: 0.8018%\n",
      "Epoch [12/300], Step [47/225], Training Accuracy: 61.3697%, Training Loss: 0.8018%\n",
      "Epoch [12/300], Step [48/225], Training Accuracy: 61.3932%, Training Loss: 0.8024%\n",
      "Epoch [12/300], Step [49/225], Training Accuracy: 61.4158%, Training Loss: 0.8020%\n",
      "Epoch [12/300], Step [50/225], Training Accuracy: 61.4688%, Training Loss: 0.8023%\n",
      "Epoch [12/300], Step [51/225], Training Accuracy: 61.5502%, Training Loss: 0.8006%\n",
      "Epoch [12/300], Step [52/225], Training Accuracy: 61.4784%, Training Loss: 0.8010%\n",
      "Epoch [12/300], Step [53/225], Training Accuracy: 61.4976%, Training Loss: 0.8016%\n",
      "Epoch [12/300], Step [54/225], Training Accuracy: 61.3715%, Training Loss: 0.8025%\n",
      "Epoch [12/300], Step [55/225], Training Accuracy: 61.2784%, Training Loss: 0.8056%\n",
      "Epoch [12/300], Step [56/225], Training Accuracy: 61.2723%, Training Loss: 0.8060%\n",
      "Epoch [12/300], Step [57/225], Training Accuracy: 61.3213%, Training Loss: 0.8044%\n",
      "Epoch [12/300], Step [58/225], Training Accuracy: 61.3147%, Training Loss: 0.8050%\n",
      "Epoch [12/300], Step [59/225], Training Accuracy: 61.3612%, Training Loss: 0.8040%\n",
      "Epoch [12/300], Step [60/225], Training Accuracy: 61.4583%, Training Loss: 0.8022%\n",
      "Epoch [12/300], Step [61/225], Training Accuracy: 61.4242%, Training Loss: 0.8016%\n",
      "Epoch [12/300], Step [62/225], Training Accuracy: 61.4163%, Training Loss: 0.8014%\n",
      "Epoch [12/300], Step [63/225], Training Accuracy: 61.2847%, Training Loss: 0.8025%\n",
      "Epoch [12/300], Step [64/225], Training Accuracy: 61.3525%, Training Loss: 0.8014%\n",
      "Epoch [12/300], Step [65/225], Training Accuracy: 61.3462%, Training Loss: 0.8023%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Step [66/225], Training Accuracy: 61.4820%, Training Loss: 0.8016%\n",
      "Epoch [12/300], Step [67/225], Training Accuracy: 61.4039%, Training Loss: 0.8027%\n",
      "Epoch [12/300], Step [68/225], Training Accuracy: 61.2822%, Training Loss: 0.8039%\n",
      "Epoch [12/300], Step [69/225], Training Accuracy: 61.2545%, Training Loss: 0.8040%\n",
      "Epoch [12/300], Step [70/225], Training Accuracy: 61.2500%, Training Loss: 0.8032%\n",
      "Epoch [12/300], Step [71/225], Training Accuracy: 61.2456%, Training Loss: 0.8025%\n",
      "Epoch [12/300], Step [72/225], Training Accuracy: 61.1979%, Training Loss: 0.8033%\n",
      "Epoch [12/300], Step [73/225], Training Accuracy: 61.1087%, Training Loss: 0.8042%\n",
      "Epoch [12/300], Step [74/225], Training Accuracy: 61.0431%, Training Loss: 0.8039%\n",
      "Epoch [12/300], Step [75/225], Training Accuracy: 61.2292%, Training Loss: 0.8024%\n",
      "Epoch [12/300], Step [76/225], Training Accuracy: 61.0814%, Training Loss: 0.8042%\n",
      "Epoch [12/300], Step [77/225], Training Accuracy: 61.1201%, Training Loss: 0.8026%\n",
      "Epoch [12/300], Step [78/225], Training Accuracy: 61.0777%, Training Loss: 0.8027%\n",
      "Epoch [12/300], Step [79/225], Training Accuracy: 61.0166%, Training Loss: 0.8035%\n",
      "Epoch [12/300], Step [80/225], Training Accuracy: 60.9766%, Training Loss: 0.8042%\n",
      "Epoch [12/300], Step [81/225], Training Accuracy: 60.9375%, Training Loss: 0.8042%\n",
      "Epoch [12/300], Step [82/225], Training Accuracy: 61.0137%, Training Loss: 0.8031%\n",
      "Epoch [12/300], Step [83/225], Training Accuracy: 60.9940%, Training Loss: 0.8031%\n",
      "Epoch [12/300], Step [84/225], Training Accuracy: 60.9933%, Training Loss: 0.8024%\n",
      "Epoch [12/300], Step [85/225], Training Accuracy: 60.9743%, Training Loss: 0.8012%\n",
      "Epoch [12/300], Step [86/225], Training Accuracy: 60.9557%, Training Loss: 0.8014%\n",
      "Epoch [12/300], Step [87/225], Training Accuracy: 60.9375%, Training Loss: 0.8018%\n",
      "Epoch [12/300], Step [88/225], Training Accuracy: 61.0085%, Training Loss: 0.8012%\n",
      "Epoch [12/300], Step [89/225], Training Accuracy: 61.0077%, Training Loss: 0.8028%\n",
      "Epoch [12/300], Step [90/225], Training Accuracy: 60.9201%, Training Loss: 0.8044%\n",
      "Epoch [12/300], Step [91/225], Training Accuracy: 60.7658%, Training Loss: 0.8055%\n",
      "Epoch [12/300], Step [92/225], Training Accuracy: 60.6827%, Training Loss: 0.8065%\n",
      "Epoch [12/300], Step [93/225], Training Accuracy: 60.6687%, Training Loss: 0.8058%\n",
      "Epoch [12/300], Step [94/225], Training Accuracy: 60.7547%, Training Loss: 0.8045%\n",
      "Epoch [12/300], Step [95/225], Training Accuracy: 60.7072%, Training Loss: 0.8048%\n",
      "Epoch [12/300], Step [96/225], Training Accuracy: 60.7259%, Training Loss: 0.8037%\n",
      "Epoch [12/300], Step [97/225], Training Accuracy: 60.7120%, Training Loss: 0.8027%\n",
      "Epoch [12/300], Step [98/225], Training Accuracy: 60.6027%, Training Loss: 0.8038%\n",
      "Epoch [12/300], Step [99/225], Training Accuracy: 60.5903%, Training Loss: 0.8056%\n",
      "Epoch [12/300], Step [100/225], Training Accuracy: 60.5781%, Training Loss: 0.8066%\n",
      "Epoch [12/300], Step [101/225], Training Accuracy: 60.5662%, Training Loss: 0.8070%\n",
      "Epoch [12/300], Step [102/225], Training Accuracy: 60.5239%, Training Loss: 0.8075%\n",
      "Epoch [12/300], Step [103/225], Training Accuracy: 60.5886%, Training Loss: 0.8066%\n",
      "Epoch [12/300], Step [104/225], Training Accuracy: 60.4417%, Training Loss: 0.8076%\n",
      "Epoch [12/300], Step [105/225], Training Accuracy: 60.4613%, Training Loss: 0.8067%\n",
      "Epoch [12/300], Step [106/225], Training Accuracy: 60.4511%, Training Loss: 0.8066%\n",
      "Epoch [12/300], Step [107/225], Training Accuracy: 60.4410%, Training Loss: 0.8077%\n",
      "Epoch [12/300], Step [108/225], Training Accuracy: 60.4311%, Training Loss: 0.8081%\n",
      "Epoch [12/300], Step [109/225], Training Accuracy: 60.3354%, Training Loss: 0.8081%\n",
      "Epoch [12/300], Step [110/225], Training Accuracy: 60.3409%, Training Loss: 0.8076%\n",
      "Epoch [12/300], Step [111/225], Training Accuracy: 60.2900%, Training Loss: 0.8075%\n",
      "Epoch [12/300], Step [112/225], Training Accuracy: 60.3376%, Training Loss: 0.8076%\n",
      "Epoch [12/300], Step [113/225], Training Accuracy: 60.3706%, Training Loss: 0.8075%\n",
      "Epoch [12/300], Step [114/225], Training Accuracy: 60.4030%, Training Loss: 0.8071%\n",
      "Epoch [12/300], Step [115/225], Training Accuracy: 60.4484%, Training Loss: 0.8065%\n",
      "Epoch [12/300], Step [116/225], Training Accuracy: 60.4930%, Training Loss: 0.8064%\n",
      "Epoch [12/300], Step [117/225], Training Accuracy: 60.4033%, Training Loss: 0.8082%\n",
      "Epoch [12/300], Step [118/225], Training Accuracy: 60.3946%, Training Loss: 0.8095%\n",
      "Epoch [12/300], Step [119/225], Training Accuracy: 60.3598%, Training Loss: 0.8097%\n",
      "Epoch [12/300], Step [120/225], Training Accuracy: 60.3906%, Training Loss: 0.8104%\n",
      "Epoch [12/300], Step [121/225], Training Accuracy: 60.3048%, Training Loss: 0.8106%\n",
      "Epoch [12/300], Step [122/225], Training Accuracy: 60.3356%, Training Loss: 0.8105%\n",
      "Epoch [12/300], Step [123/225], Training Accuracy: 60.3532%, Training Loss: 0.8100%\n",
      "Epoch [12/300], Step [124/225], Training Accuracy: 60.3705%, Training Loss: 0.8090%\n",
      "Epoch [12/300], Step [125/225], Training Accuracy: 60.2750%, Training Loss: 0.8096%\n",
      "Epoch [12/300], Step [126/225], Training Accuracy: 60.3051%, Training Loss: 0.8100%\n",
      "Epoch [12/300], Step [127/225], Training Accuracy: 60.2608%, Training Loss: 0.8109%\n",
      "Epoch [12/300], Step [128/225], Training Accuracy: 60.2173%, Training Loss: 0.8121%\n",
      "Epoch [12/300], Step [129/225], Training Accuracy: 60.2350%, Training Loss: 0.8128%\n",
      "Epoch [12/300], Step [130/225], Training Accuracy: 60.2043%, Training Loss: 0.8138%\n",
      "Epoch [12/300], Step [131/225], Training Accuracy: 60.2099%, Training Loss: 0.8146%\n",
      "Epoch [12/300], Step [132/225], Training Accuracy: 60.1562%, Training Loss: 0.8152%\n",
      "Epoch [12/300], Step [133/225], Training Accuracy: 60.1269%, Training Loss: 0.8157%\n",
      "Epoch [12/300], Step [134/225], Training Accuracy: 60.0746%, Training Loss: 0.8165%\n",
      "Epoch [12/300], Step [135/225], Training Accuracy: 60.1042%, Training Loss: 0.8166%\n",
      "Epoch [12/300], Step [136/225], Training Accuracy: 60.1448%, Training Loss: 0.8165%\n",
      "Epoch [12/300], Step [137/225], Training Accuracy: 60.1505%, Training Loss: 0.8162%\n",
      "Epoch [12/300], Step [138/225], Training Accuracy: 60.1676%, Training Loss: 0.8160%\n",
      "Epoch [12/300], Step [139/225], Training Accuracy: 60.1394%, Training Loss: 0.8159%\n",
      "Epoch [12/300], Step [140/225], Training Accuracy: 60.1786%, Training Loss: 0.8151%\n",
      "Epoch [12/300], Step [141/225], Training Accuracy: 60.1729%, Training Loss: 0.8146%\n",
      "Epoch [12/300], Step [142/225], Training Accuracy: 60.2663%, Training Loss: 0.8135%\n",
      "Epoch [12/300], Step [143/225], Training Accuracy: 60.2928%, Training Loss: 0.8134%\n",
      "Epoch [12/300], Step [144/225], Training Accuracy: 60.2756%, Training Loss: 0.8138%\n",
      "Epoch [12/300], Step [145/225], Training Accuracy: 60.2586%, Training Loss: 0.8136%\n",
      "Epoch [12/300], Step [146/225], Training Accuracy: 60.2526%, Training Loss: 0.8139%\n",
      "Epoch [12/300], Step [147/225], Training Accuracy: 60.1935%, Training Loss: 0.8146%\n",
      "Epoch [12/300], Step [148/225], Training Accuracy: 60.2618%, Training Loss: 0.8141%\n",
      "Epoch [12/300], Step [149/225], Training Accuracy: 60.1825%, Training Loss: 0.8148%\n",
      "Epoch [12/300], Step [150/225], Training Accuracy: 60.1979%, Training Loss: 0.8141%\n",
      "Epoch [12/300], Step [151/225], Training Accuracy: 60.3063%, Training Loss: 0.8135%\n",
      "Epoch [12/300], Step [152/225], Training Accuracy: 60.2282%, Training Loss: 0.8145%\n",
      "Epoch [12/300], Step [153/225], Training Accuracy: 60.2328%, Training Loss: 0.8144%\n",
      "Epoch [12/300], Step [154/225], Training Accuracy: 60.3084%, Training Loss: 0.8141%\n",
      "Epoch [12/300], Step [155/225], Training Accuracy: 60.2621%, Training Loss: 0.8145%\n",
      "Epoch [12/300], Step [156/225], Training Accuracy: 60.1963%, Training Loss: 0.8154%\n",
      "Epoch [12/300], Step [157/225], Training Accuracy: 60.2110%, Training Loss: 0.8156%\n",
      "Epoch [12/300], Step [158/225], Training Accuracy: 60.1760%, Training Loss: 0.8163%\n",
      "Epoch [12/300], Step [159/225], Training Accuracy: 60.1219%, Training Loss: 0.8169%\n",
      "Epoch [12/300], Step [160/225], Training Accuracy: 60.0684%, Training Loss: 0.8174%\n",
      "Epoch [12/300], Step [161/225], Training Accuracy: 60.0543%, Training Loss: 0.8174%\n",
      "Epoch [12/300], Step [162/225], Training Accuracy: 60.1080%, Training Loss: 0.8170%\n",
      "Epoch [12/300], Step [163/225], Training Accuracy: 60.0844%, Training Loss: 0.8169%\n",
      "Epoch [12/300], Step [164/225], Training Accuracy: 60.1467%, Training Loss: 0.8162%\n",
      "Epoch [12/300], Step [165/225], Training Accuracy: 60.1326%, Training Loss: 0.8164%\n",
      "Epoch [12/300], Step [166/225], Training Accuracy: 60.1751%, Training Loss: 0.8162%\n",
      "Epoch [12/300], Step [167/225], Training Accuracy: 60.1984%, Training Loss: 0.8157%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Step [168/225], Training Accuracy: 60.1469%, Training Loss: 0.8161%\n",
      "Epoch [12/300], Step [169/225], Training Accuracy: 60.0777%, Training Loss: 0.8166%\n",
      "Epoch [12/300], Step [170/225], Training Accuracy: 60.0919%, Training Loss: 0.8166%\n",
      "Epoch [12/300], Step [171/225], Training Accuracy: 60.1060%, Training Loss: 0.8170%\n",
      "Epoch [12/300], Step [172/225], Training Accuracy: 60.1199%, Training Loss: 0.8175%\n",
      "Epoch [12/300], Step [173/225], Training Accuracy: 60.1246%, Training Loss: 0.8177%\n",
      "Epoch [12/300], Step [174/225], Training Accuracy: 60.1114%, Training Loss: 0.8179%\n",
      "Epoch [12/300], Step [175/225], Training Accuracy: 60.1607%, Training Loss: 0.8174%\n",
      "Epoch [12/300], Step [176/225], Training Accuracy: 60.1740%, Training Loss: 0.8173%\n",
      "Epoch [12/300], Step [177/225], Training Accuracy: 60.2136%, Training Loss: 0.8168%\n",
      "Epoch [12/300], Step [178/225], Training Accuracy: 60.2177%, Training Loss: 0.8168%\n",
      "Epoch [12/300], Step [179/225], Training Accuracy: 60.2479%, Training Loss: 0.8165%\n",
      "Epoch [12/300], Step [180/225], Training Accuracy: 60.2344%, Training Loss: 0.8161%\n",
      "Epoch [12/300], Step [181/225], Training Accuracy: 60.1692%, Training Loss: 0.8168%\n",
      "Epoch [12/300], Step [182/225], Training Accuracy: 60.1562%, Training Loss: 0.8167%\n",
      "Epoch [12/300], Step [183/225], Training Accuracy: 60.1178%, Training Loss: 0.8169%\n",
      "Epoch [12/300], Step [184/225], Training Accuracy: 60.1308%, Training Loss: 0.8169%\n",
      "Epoch [12/300], Step [185/225], Training Accuracy: 60.2027%, Training Loss: 0.8165%\n",
      "Epoch [12/300], Step [186/225], Training Accuracy: 60.2319%, Training Loss: 0.8160%\n",
      "Epoch [12/300], Step [187/225], Training Accuracy: 60.1855%, Training Loss: 0.8160%\n",
      "Epoch [12/300], Step [188/225], Training Accuracy: 60.1978%, Training Loss: 0.8157%\n",
      "Epoch [12/300], Step [189/225], Training Accuracy: 60.2100%, Training Loss: 0.8152%\n",
      "Epoch [12/300], Step [190/225], Training Accuracy: 60.2220%, Training Loss: 0.8150%\n",
      "Epoch [12/300], Step [191/225], Training Accuracy: 60.2012%, Training Loss: 0.8150%\n",
      "Epoch [12/300], Step [192/225], Training Accuracy: 60.2458%, Training Loss: 0.8145%\n",
      "Epoch [12/300], Step [193/225], Training Accuracy: 60.2170%, Training Loss: 0.8147%\n",
      "Epoch [12/300], Step [194/225], Training Accuracy: 60.2207%, Training Loss: 0.8143%\n",
      "Epoch [12/300], Step [195/225], Training Accuracy: 60.2244%, Training Loss: 0.8140%\n",
      "Epoch [12/300], Step [196/225], Training Accuracy: 60.1961%, Training Loss: 0.8144%\n",
      "Epoch [12/300], Step [197/225], Training Accuracy: 60.2078%, Training Loss: 0.8143%\n",
      "Epoch [12/300], Step [198/225], Training Accuracy: 60.2667%, Training Loss: 0.8134%\n",
      "Epoch [12/300], Step [199/225], Training Accuracy: 60.2780%, Training Loss: 0.8131%\n",
      "Epoch [12/300], Step [200/225], Training Accuracy: 60.2500%, Training Loss: 0.8133%\n",
      "Epoch [12/300], Step [201/225], Training Accuracy: 60.2068%, Training Loss: 0.8134%\n",
      "Epoch [12/300], Step [202/225], Training Accuracy: 60.2104%, Training Loss: 0.8132%\n",
      "Epoch [12/300], Step [203/225], Training Accuracy: 60.2909%, Training Loss: 0.8127%\n",
      "Epoch [12/300], Step [204/225], Training Accuracy: 60.3018%, Training Loss: 0.8131%\n",
      "Epoch [12/300], Step [205/225], Training Accuracy: 60.3811%, Training Loss: 0.8126%\n",
      "Epoch [12/300], Step [206/225], Training Accuracy: 60.4066%, Training Loss: 0.8126%\n",
      "Epoch [12/300], Step [207/225], Training Accuracy: 60.3714%, Training Loss: 0.8127%\n",
      "Epoch [12/300], Step [208/225], Training Accuracy: 60.3816%, Training Loss: 0.8123%\n",
      "Epoch [12/300], Step [209/225], Training Accuracy: 60.4067%, Training Loss: 0.8118%\n",
      "Epoch [12/300], Step [210/225], Training Accuracy: 60.3571%, Training Loss: 0.8122%\n",
      "Epoch [12/300], Step [211/225], Training Accuracy: 60.3821%, Training Loss: 0.8122%\n",
      "Epoch [12/300], Step [212/225], Training Accuracy: 60.3995%, Training Loss: 0.8123%\n",
      "Epoch [12/300], Step [213/225], Training Accuracy: 60.3873%, Training Loss: 0.8130%\n",
      "Epoch [12/300], Step [214/225], Training Accuracy: 60.3899%, Training Loss: 0.8129%\n",
      "Epoch [12/300], Step [215/225], Training Accuracy: 60.3343%, Training Loss: 0.8130%\n",
      "Epoch [12/300], Step [216/225], Training Accuracy: 60.3299%, Training Loss: 0.8134%\n",
      "Epoch [12/300], Step [217/225], Training Accuracy: 60.2823%, Training Loss: 0.8137%\n",
      "Epoch [12/300], Step [218/225], Training Accuracy: 60.2423%, Training Loss: 0.8140%\n",
      "Epoch [12/300], Step [219/225], Training Accuracy: 60.2597%, Training Loss: 0.8138%\n",
      "Epoch [12/300], Step [220/225], Training Accuracy: 60.2557%, Training Loss: 0.8142%\n",
      "Epoch [12/300], Step [221/225], Training Accuracy: 60.2446%, Training Loss: 0.8145%\n",
      "Epoch [12/300], Step [222/225], Training Accuracy: 60.2337%, Training Loss: 0.8143%\n",
      "Epoch [12/300], Step [223/225], Training Accuracy: 60.1808%, Training Loss: 0.8147%\n",
      "Epoch [12/300], Step [224/225], Training Accuracy: 60.1493%, Training Loss: 0.8148%\n",
      "Epoch [12/300], Step [225/225], Training Accuracy: 60.0959%, Training Loss: 0.8155%\n",
      "Epoch [13/300], Step [1/225], Training Accuracy: 62.5000%, Training Loss: 0.7604%\n",
      "Epoch [13/300], Step [2/225], Training Accuracy: 63.2812%, Training Loss: 0.7765%\n",
      "Epoch [13/300], Step [3/225], Training Accuracy: 64.0625%, Training Loss: 0.7838%\n",
      "Epoch [13/300], Step [4/225], Training Accuracy: 62.8906%, Training Loss: 0.7892%\n",
      "Epoch [13/300], Step [5/225], Training Accuracy: 62.8125%, Training Loss: 0.7844%\n",
      "Epoch [13/300], Step [6/225], Training Accuracy: 61.7188%, Training Loss: 0.8122%\n",
      "Epoch [13/300], Step [7/225], Training Accuracy: 61.8304%, Training Loss: 0.8130%\n",
      "Epoch [13/300], Step [8/225], Training Accuracy: 60.9375%, Training Loss: 0.8134%\n",
      "Epoch [13/300], Step [9/225], Training Accuracy: 59.7222%, Training Loss: 0.8282%\n",
      "Epoch [13/300], Step [10/225], Training Accuracy: 59.0625%, Training Loss: 0.8430%\n",
      "Epoch [13/300], Step [11/225], Training Accuracy: 59.6591%, Training Loss: 0.8395%\n",
      "Epoch [13/300], Step [12/225], Training Accuracy: 59.8958%, Training Loss: 0.8420%\n",
      "Epoch [13/300], Step [13/225], Training Accuracy: 60.8173%, Training Loss: 0.8310%\n",
      "Epoch [13/300], Step [14/225], Training Accuracy: 60.7143%, Training Loss: 0.8344%\n",
      "Epoch [13/300], Step [15/225], Training Accuracy: 60.6250%, Training Loss: 0.8328%\n",
      "Epoch [13/300], Step [16/225], Training Accuracy: 60.8398%, Training Loss: 0.8276%\n",
      "Epoch [13/300], Step [17/225], Training Accuracy: 60.9375%, Training Loss: 0.8239%\n",
      "Epoch [13/300], Step [18/225], Training Accuracy: 60.6771%, Training Loss: 0.8261%\n",
      "Epoch [13/300], Step [19/225], Training Accuracy: 60.8553%, Training Loss: 0.8252%\n",
      "Epoch [13/300], Step [20/225], Training Accuracy: 60.7812%, Training Loss: 0.8232%\n",
      "Epoch [13/300], Step [21/225], Training Accuracy: 61.0863%, Training Loss: 0.8160%\n",
      "Epoch [13/300], Step [22/225], Training Accuracy: 60.4403%, Training Loss: 0.8247%\n",
      "Epoch [13/300], Step [23/225], Training Accuracy: 60.8016%, Training Loss: 0.8199%\n",
      "Epoch [13/300], Step [24/225], Training Accuracy: 60.4167%, Training Loss: 0.8246%\n",
      "Epoch [13/300], Step [25/225], Training Accuracy: 60.5625%, Training Loss: 0.8213%\n",
      "Epoch [13/300], Step [26/225], Training Accuracy: 60.4567%, Training Loss: 0.8184%\n",
      "Epoch [13/300], Step [27/225], Training Accuracy: 60.5324%, Training Loss: 0.8142%\n",
      "Epoch [13/300], Step [28/225], Training Accuracy: 60.8259%, Training Loss: 0.8105%\n",
      "Epoch [13/300], Step [29/225], Training Accuracy: 60.7759%, Training Loss: 0.8100%\n",
      "Epoch [13/300], Step [30/225], Training Accuracy: 61.0938%, Training Loss: 0.8061%\n",
      "Epoch [13/300], Step [31/225], Training Accuracy: 60.8367%, Training Loss: 0.8119%\n",
      "Epoch [13/300], Step [32/225], Training Accuracy: 60.6934%, Training Loss: 0.8125%\n",
      "Epoch [13/300], Step [33/225], Training Accuracy: 60.7955%, Training Loss: 0.8076%\n",
      "Epoch [13/300], Step [34/225], Training Accuracy: 60.6158%, Training Loss: 0.8076%\n",
      "Epoch [13/300], Step [35/225], Training Accuracy: 60.5804%, Training Loss: 0.8081%\n",
      "Epoch [13/300], Step [36/225], Training Accuracy: 60.5903%, Training Loss: 0.8062%\n",
      "Epoch [13/300], Step [37/225], Training Accuracy: 60.6419%, Training Loss: 0.8035%\n",
      "Epoch [13/300], Step [38/225], Training Accuracy: 60.5674%, Training Loss: 0.8030%\n",
      "Epoch [13/300], Step [39/225], Training Accuracy: 60.4567%, Training Loss: 0.8035%\n",
      "Epoch [13/300], Step [40/225], Training Accuracy: 60.2344%, Training Loss: 0.8056%\n",
      "Epoch [13/300], Step [41/225], Training Accuracy: 60.0991%, Training Loss: 0.8078%\n",
      "Epoch [13/300], Step [42/225], Training Accuracy: 60.0074%, Training Loss: 0.8066%\n",
      "Epoch [13/300], Step [43/225], Training Accuracy: 59.9927%, Training Loss: 0.8067%\n",
      "Epoch [13/300], Step [44/225], Training Accuracy: 60.3693%, Training Loss: 0.8037%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Step [45/225], Training Accuracy: 60.4167%, Training Loss: 0.8009%\n",
      "Epoch [13/300], Step [46/225], Training Accuracy: 60.3940%, Training Loss: 0.7995%\n",
      "Epoch [13/300], Step [47/225], Training Accuracy: 60.2394%, Training Loss: 0.7998%\n",
      "Epoch [13/300], Step [48/225], Training Accuracy: 60.1237%, Training Loss: 0.8012%\n",
      "Epoch [13/300], Step [49/225], Training Accuracy: 60.2997%, Training Loss: 0.7998%\n",
      "Epoch [13/300], Step [50/225], Training Accuracy: 60.4375%, Training Loss: 0.7986%\n",
      "Epoch [13/300], Step [51/225], Training Accuracy: 60.6618%, Training Loss: 0.7964%\n",
      "Epoch [13/300], Step [52/225], Training Accuracy: 60.6370%, Training Loss: 0.7949%\n",
      "Epoch [13/300], Step [53/225], Training Accuracy: 60.5542%, Training Loss: 0.7947%\n",
      "Epoch [13/300], Step [54/225], Training Accuracy: 60.4745%, Training Loss: 0.7946%\n",
      "Epoch [13/300], Step [55/225], Training Accuracy: 60.4261%, Training Loss: 0.7968%\n",
      "Epoch [13/300], Step [56/225], Training Accuracy: 60.4353%, Training Loss: 0.7967%\n",
      "Epoch [13/300], Step [57/225], Training Accuracy: 60.5537%, Training Loss: 0.7957%\n",
      "Epoch [13/300], Step [58/225], Training Accuracy: 60.5603%, Training Loss: 0.7964%\n",
      "Epoch [13/300], Step [59/225], Training Accuracy: 60.6462%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [60/225], Training Accuracy: 60.7812%, Training Loss: 0.7939%\n",
      "Epoch [13/300], Step [61/225], Training Accuracy: 60.8094%, Training Loss: 0.7937%\n",
      "Epoch [13/300], Step [62/225], Training Accuracy: 60.7611%, Training Loss: 0.7941%\n",
      "Epoch [13/300], Step [63/225], Training Accuracy: 60.6151%, Training Loss: 0.7957%\n",
      "Epoch [13/300], Step [64/225], Training Accuracy: 60.8154%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [65/225], Training Accuracy: 60.8654%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [66/225], Training Accuracy: 61.0322%, Training Loss: 0.7935%\n",
      "Epoch [13/300], Step [67/225], Training Accuracy: 61.0541%, Training Loss: 0.7956%\n",
      "Epoch [13/300], Step [68/225], Training Accuracy: 60.9835%, Training Loss: 0.7958%\n",
      "Epoch [13/300], Step [69/225], Training Accuracy: 60.8696%, Training Loss: 0.7963%\n",
      "Epoch [13/300], Step [70/225], Training Accuracy: 60.8036%, Training Loss: 0.7957%\n",
      "Epoch [13/300], Step [71/225], Training Accuracy: 60.8495%, Training Loss: 0.7947%\n",
      "Epoch [13/300], Step [72/225], Training Accuracy: 60.8290%, Training Loss: 0.7951%\n",
      "Epoch [13/300], Step [73/225], Training Accuracy: 60.9161%, Training Loss: 0.7941%\n",
      "Epoch [13/300], Step [74/225], Training Accuracy: 60.9164%, Training Loss: 0.7932%\n",
      "Epoch [13/300], Step [75/225], Training Accuracy: 61.0625%, Training Loss: 0.7914%\n",
      "Epoch [13/300], Step [76/225], Training Accuracy: 60.8964%, Training Loss: 0.7934%\n",
      "Epoch [13/300], Step [77/225], Training Accuracy: 60.9781%, Training Loss: 0.7921%\n",
      "Epoch [13/300], Step [78/225], Training Accuracy: 60.8974%, Training Loss: 0.7933%\n",
      "Epoch [13/300], Step [79/225], Training Accuracy: 60.8386%, Training Loss: 0.7938%\n",
      "Epoch [13/300], Step [80/225], Training Accuracy: 60.8984%, Training Loss: 0.7942%\n",
      "Epoch [13/300], Step [81/225], Training Accuracy: 60.9182%, Training Loss: 0.7937%\n",
      "Epoch [13/300], Step [82/225], Training Accuracy: 61.0518%, Training Loss: 0.7921%\n",
      "Epoch [13/300], Step [83/225], Training Accuracy: 60.9940%, Training Loss: 0.7922%\n",
      "Epoch [13/300], Step [84/225], Training Accuracy: 61.0119%, Training Loss: 0.7917%\n",
      "Epoch [13/300], Step [85/225], Training Accuracy: 60.9926%, Training Loss: 0.7904%\n",
      "Epoch [13/300], Step [86/225], Training Accuracy: 60.9557%, Training Loss: 0.7907%\n",
      "Epoch [13/300], Step [87/225], Training Accuracy: 60.9555%, Training Loss: 0.7920%\n",
      "Epoch [13/300], Step [88/225], Training Accuracy: 60.8842%, Training Loss: 0.7918%\n",
      "Epoch [13/300], Step [89/225], Training Accuracy: 60.8146%, Training Loss: 0.7932%\n",
      "Epoch [13/300], Step [90/225], Training Accuracy: 60.8160%, Training Loss: 0.7940%\n",
      "Epoch [13/300], Step [91/225], Training Accuracy: 60.7486%, Training Loss: 0.7941%\n",
      "Epoch [13/300], Step [92/225], Training Accuracy: 60.7167%, Training Loss: 0.7941%\n",
      "Epoch [13/300], Step [93/225], Training Accuracy: 60.8535%, Training Loss: 0.7933%\n",
      "Epoch [13/300], Step [94/225], Training Accuracy: 60.9375%, Training Loss: 0.7919%\n",
      "Epoch [13/300], Step [95/225], Training Accuracy: 60.8388%, Training Loss: 0.7924%\n",
      "Epoch [13/300], Step [96/225], Training Accuracy: 60.9538%, Training Loss: 0.7906%\n",
      "Epoch [13/300], Step [97/225], Training Accuracy: 60.9697%, Training Loss: 0.7910%\n",
      "Epoch [13/300], Step [98/225], Training Accuracy: 60.9056%, Training Loss: 0.7920%\n",
      "Epoch [13/300], Step [99/225], Training Accuracy: 60.8112%, Training Loss: 0.7935%\n",
      "Epoch [13/300], Step [100/225], Training Accuracy: 60.8125%, Training Loss: 0.7939%\n",
      "Epoch [13/300], Step [101/225], Training Accuracy: 60.9220%, Training Loss: 0.7943%\n",
      "Epoch [13/300], Step [102/225], Training Accuracy: 60.8915%, Training Loss: 0.7941%\n",
      "Epoch [13/300], Step [103/225], Training Accuracy: 60.9223%, Training Loss: 0.7932%\n",
      "Epoch [13/300], Step [104/225], Training Accuracy: 60.8323%, Training Loss: 0.7942%\n",
      "Epoch [13/300], Step [105/225], Training Accuracy: 60.8333%, Training Loss: 0.7937%\n",
      "Epoch [13/300], Step [106/225], Training Accuracy: 60.8491%, Training Loss: 0.7936%\n",
      "Epoch [13/300], Step [107/225], Training Accuracy: 60.8791%, Training Loss: 0.7946%\n",
      "Epoch [13/300], Step [108/225], Training Accuracy: 60.8796%, Training Loss: 0.7949%\n",
      "Epoch [13/300], Step [109/225], Training Accuracy: 60.8372%, Training Loss: 0.7949%\n",
      "Epoch [13/300], Step [110/225], Training Accuracy: 60.8097%, Training Loss: 0.7944%\n",
      "Epoch [13/300], Step [111/225], Training Accuracy: 60.7827%, Training Loss: 0.7940%\n",
      "Epoch [13/300], Step [112/225], Training Accuracy: 60.8119%, Training Loss: 0.7933%\n",
      "Epoch [13/300], Step [113/225], Training Accuracy: 60.8545%, Training Loss: 0.7930%\n",
      "Epoch [13/300], Step [114/225], Training Accuracy: 60.8690%, Training Loss: 0.7924%\n",
      "Epoch [13/300], Step [115/225], Training Accuracy: 60.9647%, Training Loss: 0.7925%\n",
      "Epoch [13/300], Step [116/225], Training Accuracy: 60.9914%, Training Loss: 0.7922%\n",
      "Epoch [13/300], Step [117/225], Training Accuracy: 60.9642%, Training Loss: 0.7933%\n",
      "Epoch [13/300], Step [118/225], Training Accuracy: 60.9640%, Training Loss: 0.7932%\n",
      "Epoch [13/300], Step [119/225], Training Accuracy: 60.9375%, Training Loss: 0.7939%\n",
      "Epoch [13/300], Step [120/225], Training Accuracy: 60.9766%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [121/225], Training Accuracy: 60.9246%, Training Loss: 0.7952%\n",
      "Epoch [13/300], Step [122/225], Training Accuracy: 60.9503%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [123/225], Training Accuracy: 60.9375%, Training Loss: 0.7947%\n",
      "Epoch [13/300], Step [124/225], Training Accuracy: 60.9375%, Training Loss: 0.7941%\n",
      "Epoch [13/300], Step [125/225], Training Accuracy: 60.9000%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [126/225], Training Accuracy: 60.9251%, Training Loss: 0.7955%\n",
      "Epoch [13/300], Step [127/225], Training Accuracy: 60.9006%, Training Loss: 0.7959%\n",
      "Epoch [13/300], Step [128/225], Training Accuracy: 60.8643%, Training Loss: 0.7971%\n",
      "Epoch [13/300], Step [129/225], Training Accuracy: 60.8527%, Training Loss: 0.7978%\n",
      "Epoch [13/300], Step [130/225], Training Accuracy: 60.7692%, Training Loss: 0.7984%\n",
      "Epoch [13/300], Step [131/225], Training Accuracy: 60.7586%, Training Loss: 0.7991%\n",
      "Epoch [13/300], Step [132/225], Training Accuracy: 60.7599%, Training Loss: 0.7995%\n",
      "Epoch [13/300], Step [133/225], Training Accuracy: 60.7378%, Training Loss: 0.7996%\n",
      "Epoch [13/300], Step [134/225], Training Accuracy: 60.6576%, Training Loss: 0.8015%\n",
      "Epoch [13/300], Step [135/225], Training Accuracy: 60.6829%, Training Loss: 0.8014%\n",
      "Epoch [13/300], Step [136/225], Training Accuracy: 60.7077%, Training Loss: 0.8011%\n",
      "Epoch [13/300], Step [137/225], Training Accuracy: 60.7892%, Training Loss: 0.8005%\n",
      "Epoch [13/300], Step [138/225], Training Accuracy: 60.8243%, Training Loss: 0.7999%\n",
      "Epoch [13/300], Step [139/225], Training Accuracy: 60.7801%, Training Loss: 0.8002%\n",
      "Epoch [13/300], Step [140/225], Training Accuracy: 60.8594%, Training Loss: 0.7996%\n",
      "Epoch [13/300], Step [141/225], Training Accuracy: 60.8599%, Training Loss: 0.7992%\n",
      "Epoch [13/300], Step [142/225], Training Accuracy: 60.8385%, Training Loss: 0.7985%\n",
      "Epoch [13/300], Step [143/225], Training Accuracy: 60.9047%, Training Loss: 0.7983%\n",
      "Epoch [13/300], Step [144/225], Training Accuracy: 60.8724%, Training Loss: 0.7983%\n",
      "Epoch [13/300], Step [145/225], Training Accuracy: 60.8944%, Training Loss: 0.7978%\n",
      "Epoch [13/300], Step [146/225], Training Accuracy: 60.8626%, Training Loss: 0.7985%\n",
      "Epoch [13/300], Step [147/225], Training Accuracy: 60.7993%, Training Loss: 0.7994%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Step [148/225], Training Accuracy: 60.8108%, Training Loss: 0.7994%\n",
      "Epoch [13/300], Step [149/225], Training Accuracy: 60.7802%, Training Loss: 0.7998%\n",
      "Epoch [13/300], Step [150/225], Training Accuracy: 60.8021%, Training Loss: 0.7994%\n",
      "Epoch [13/300], Step [151/225], Training Accuracy: 60.8547%, Training Loss: 0.7989%\n",
      "Epoch [13/300], Step [152/225], Training Accuracy: 60.7936%, Training Loss: 0.7996%\n",
      "Epoch [13/300], Step [153/225], Training Accuracy: 60.7843%, Training Loss: 0.7999%\n",
      "Epoch [13/300], Step [154/225], Training Accuracy: 60.8360%, Training Loss: 0.7999%\n",
      "Epoch [13/300], Step [155/225], Training Accuracy: 60.7964%, Training Loss: 0.8000%\n",
      "Epoch [13/300], Step [156/225], Training Accuracy: 60.7572%, Training Loss: 0.8006%\n",
      "Epoch [13/300], Step [157/225], Training Accuracy: 60.7484%, Training Loss: 0.8005%\n",
      "Epoch [13/300], Step [158/225], Training Accuracy: 60.6903%, Training Loss: 0.8014%\n",
      "Epoch [13/300], Step [159/225], Training Accuracy: 60.6230%, Training Loss: 0.8022%\n",
      "Epoch [13/300], Step [160/225], Training Accuracy: 60.6152%, Training Loss: 0.8019%\n",
      "Epoch [13/300], Step [161/225], Training Accuracy: 60.6464%, Training Loss: 0.8017%\n",
      "Epoch [13/300], Step [162/225], Training Accuracy: 60.7542%, Training Loss: 0.8007%\n",
      "Epoch [13/300], Step [163/225], Training Accuracy: 60.7458%, Training Loss: 0.8006%\n",
      "Epoch [13/300], Step [164/225], Training Accuracy: 60.8136%, Training Loss: 0.7999%\n",
      "Epoch [13/300], Step [165/225], Training Accuracy: 60.8428%, Training Loss: 0.7997%\n",
      "Epoch [13/300], Step [166/225], Training Accuracy: 60.8810%, Training Loss: 0.7997%\n",
      "Epoch [13/300], Step [167/225], Training Accuracy: 60.8814%, Training Loss: 0.7995%\n",
      "Epoch [13/300], Step [168/225], Training Accuracy: 60.8445%, Training Loss: 0.8000%\n",
      "Epoch [13/300], Step [169/225], Training Accuracy: 60.7803%, Training Loss: 0.8003%\n",
      "Epoch [13/300], Step [170/225], Training Accuracy: 60.7996%, Training Loss: 0.8004%\n",
      "Epoch [13/300], Step [171/225], Training Accuracy: 60.7730%, Training Loss: 0.8004%\n",
      "Epoch [13/300], Step [172/225], Training Accuracy: 60.7649%, Training Loss: 0.8004%\n",
      "Epoch [13/300], Step [173/225], Training Accuracy: 60.6936%, Training Loss: 0.8009%\n",
      "Epoch [13/300], Step [174/225], Training Accuracy: 60.6681%, Training Loss: 0.8010%\n",
      "Epoch [13/300], Step [175/225], Training Accuracy: 60.7321%, Training Loss: 0.8005%\n",
      "Epoch [13/300], Step [176/225], Training Accuracy: 60.7777%, Training Loss: 0.8001%\n",
      "Epoch [13/300], Step [177/225], Training Accuracy: 60.7874%, Training Loss: 0.7996%\n",
      "Epoch [13/300], Step [178/225], Training Accuracy: 60.8146%, Training Loss: 0.7997%\n",
      "Epoch [13/300], Step [179/225], Training Accuracy: 60.8502%, Training Loss: 0.7993%\n",
      "Epoch [13/300], Step [180/225], Training Accuracy: 60.8420%, Training Loss: 0.7990%\n",
      "Epoch [13/300], Step [181/225], Training Accuracy: 60.8253%, Training Loss: 0.7993%\n",
      "Epoch [13/300], Step [182/225], Training Accuracy: 60.9032%, Training Loss: 0.7992%\n",
      "Epoch [13/300], Step [183/225], Training Accuracy: 60.9290%, Training Loss: 0.7992%\n",
      "Epoch [13/300], Step [184/225], Training Accuracy: 60.9630%, Training Loss: 0.7989%\n",
      "Epoch [13/300], Step [185/225], Training Accuracy: 61.0304%, Training Loss: 0.7984%\n",
      "Epoch [13/300], Step [186/225], Training Accuracy: 61.0719%, Training Loss: 0.7977%\n",
      "Epoch [13/300], Step [187/225], Training Accuracy: 61.0628%, Training Loss: 0.7975%\n",
      "Epoch [13/300], Step [188/225], Training Accuracy: 61.0871%, Training Loss: 0.7971%\n",
      "Epoch [13/300], Step [189/225], Training Accuracy: 61.1194%, Training Loss: 0.7966%\n",
      "Epoch [13/300], Step [190/225], Training Accuracy: 61.1184%, Training Loss: 0.7967%\n",
      "Epoch [13/300], Step [191/225], Training Accuracy: 61.1257%, Training Loss: 0.7962%\n",
      "Epoch [13/300], Step [192/225], Training Accuracy: 61.1735%, Training Loss: 0.7957%\n",
      "Epoch [13/300], Step [193/225], Training Accuracy: 61.1723%, Training Loss: 0.7958%\n",
      "Epoch [13/300], Step [194/225], Training Accuracy: 61.2113%, Training Loss: 0.7955%\n",
      "Epoch [13/300], Step [195/225], Training Accuracy: 61.2260%, Training Loss: 0.7951%\n",
      "Epoch [13/300], Step [196/225], Training Accuracy: 61.1607%, Training Loss: 0.7955%\n",
      "Epoch [13/300], Step [197/225], Training Accuracy: 61.1675%, Training Loss: 0.7958%\n",
      "Epoch [13/300], Step [198/225], Training Accuracy: 61.2532%, Training Loss: 0.7950%\n",
      "Epoch [13/300], Step [199/225], Training Accuracy: 61.2673%, Training Loss: 0.7945%\n",
      "Epoch [13/300], Step [200/225], Training Accuracy: 61.2422%, Training Loss: 0.7947%\n",
      "Epoch [13/300], Step [201/225], Training Accuracy: 61.2251%, Training Loss: 0.7955%\n",
      "Epoch [13/300], Step [202/225], Training Accuracy: 61.2005%, Training Loss: 0.7954%\n",
      "Epoch [13/300], Step [203/225], Training Accuracy: 61.2531%, Training Loss: 0.7952%\n",
      "Epoch [13/300], Step [204/225], Training Accuracy: 61.2669%, Training Loss: 0.7953%\n",
      "Epoch [13/300], Step [205/225], Training Accuracy: 61.3262%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [206/225], Training Accuracy: 61.3167%, Training Loss: 0.7948%\n",
      "Epoch [13/300], Step [207/225], Training Accuracy: 61.3149%, Training Loss: 0.7949%\n",
      "Epoch [13/300], Step [208/225], Training Accuracy: 61.3431%, Training Loss: 0.7945%\n",
      "Epoch [13/300], Step [209/225], Training Accuracy: 61.3636%, Training Loss: 0.7944%\n",
      "Epoch [13/300], Step [210/225], Training Accuracy: 61.3318%, Training Loss: 0.7947%\n",
      "Epoch [13/300], Step [211/225], Training Accuracy: 61.3522%, Training Loss: 0.7947%\n",
      "Epoch [13/300], Step [212/225], Training Accuracy: 61.3502%, Training Loss: 0.7950%\n",
      "Epoch [13/300], Step [213/225], Training Accuracy: 61.3410%, Training Loss: 0.7957%\n",
      "Epoch [13/300], Step [214/225], Training Accuracy: 61.3610%, Training Loss: 0.7954%\n",
      "Epoch [13/300], Step [215/225], Training Accuracy: 61.3227%, Training Loss: 0.7955%\n",
      "Epoch [13/300], Step [216/225], Training Accuracy: 61.2703%, Training Loss: 0.7959%\n",
      "Epoch [13/300], Step [217/225], Training Accuracy: 61.2327%, Training Loss: 0.7963%\n",
      "Epoch [13/300], Step [218/225], Training Accuracy: 61.2385%, Training Loss: 0.7963%\n",
      "Epoch [13/300], Step [219/225], Training Accuracy: 61.2372%, Training Loss: 0.7963%\n",
      "Epoch [13/300], Step [220/225], Training Accuracy: 61.2358%, Training Loss: 0.7967%\n",
      "Epoch [13/300], Step [221/225], Training Accuracy: 61.1779%, Training Loss: 0.7971%\n",
      "Epoch [13/300], Step [222/225], Training Accuracy: 61.2120%, Training Loss: 0.7967%\n",
      "Epoch [13/300], Step [223/225], Training Accuracy: 61.1897%, Training Loss: 0.7972%\n",
      "Epoch [13/300], Step [224/225], Training Accuracy: 61.1816%, Training Loss: 0.7973%\n",
      "Epoch [13/300], Step [225/225], Training Accuracy: 61.1729%, Training Loss: 0.7975%\n",
      "Epoch [14/300], Step [1/225], Training Accuracy: 71.8750%, Training Loss: 0.7105%\n",
      "Epoch [14/300], Step [2/225], Training Accuracy: 67.9688%, Training Loss: 0.7543%\n",
      "Epoch [14/300], Step [3/225], Training Accuracy: 66.6667%, Training Loss: 0.7636%\n",
      "Epoch [14/300], Step [4/225], Training Accuracy: 64.0625%, Training Loss: 0.7926%\n",
      "Epoch [14/300], Step [5/225], Training Accuracy: 65.0000%, Training Loss: 0.7722%\n",
      "Epoch [14/300], Step [6/225], Training Accuracy: 63.0208%, Training Loss: 0.8141%\n",
      "Epoch [14/300], Step [7/225], Training Accuracy: 63.1696%, Training Loss: 0.8099%\n",
      "Epoch [14/300], Step [8/225], Training Accuracy: 62.1094%, Training Loss: 0.8031%\n",
      "Epoch [14/300], Step [9/225], Training Accuracy: 61.1111%, Training Loss: 0.8089%\n",
      "Epoch [14/300], Step [10/225], Training Accuracy: 60.7812%, Training Loss: 0.8141%\n",
      "Epoch [14/300], Step [11/225], Training Accuracy: 61.3636%, Training Loss: 0.8066%\n",
      "Epoch [14/300], Step [12/225], Training Accuracy: 61.9792%, Training Loss: 0.8067%\n",
      "Epoch [14/300], Step [13/225], Training Accuracy: 63.1010%, Training Loss: 0.7918%\n",
      "Epoch [14/300], Step [14/225], Training Accuracy: 63.0580%, Training Loss: 0.7903%\n",
      "Epoch [14/300], Step [15/225], Training Accuracy: 63.0208%, Training Loss: 0.7960%\n",
      "Epoch [14/300], Step [16/225], Training Accuracy: 62.6953%, Training Loss: 0.7978%\n",
      "Epoch [14/300], Step [17/225], Training Accuracy: 62.5919%, Training Loss: 0.7954%\n",
      "Epoch [14/300], Step [18/225], Training Accuracy: 62.1528%, Training Loss: 0.7988%\n",
      "Epoch [14/300], Step [19/225], Training Accuracy: 62.0066%, Training Loss: 0.7996%\n",
      "Epoch [14/300], Step [20/225], Training Accuracy: 61.8750%, Training Loss: 0.7979%\n",
      "Epoch [14/300], Step [21/225], Training Accuracy: 62.3512%, Training Loss: 0.7900%\n",
      "Epoch [14/300], Step [22/225], Training Accuracy: 62.0739%, Training Loss: 0.7980%\n",
      "Epoch [14/300], Step [23/225], Training Accuracy: 62.5679%, Training Loss: 0.7926%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300], Step [24/225], Training Accuracy: 61.8490%, Training Loss: 0.7996%\n",
      "Epoch [14/300], Step [25/225], Training Accuracy: 62.1250%, Training Loss: 0.7957%\n",
      "Epoch [14/300], Step [26/225], Training Accuracy: 62.0793%, Training Loss: 0.7932%\n",
      "Epoch [14/300], Step [27/225], Training Accuracy: 61.7477%, Training Loss: 0.7960%\n",
      "Epoch [14/300], Step [28/225], Training Accuracy: 62.2210%, Training Loss: 0.7942%\n",
      "Epoch [14/300], Step [29/225], Training Accuracy: 62.2845%, Training Loss: 0.7955%\n",
      "Epoch [14/300], Step [30/225], Training Accuracy: 62.4479%, Training Loss: 0.7937%\n",
      "Epoch [14/300], Step [31/225], Training Accuracy: 62.1472%, Training Loss: 0.7990%\n",
      "Epoch [14/300], Step [32/225], Training Accuracy: 62.0117%, Training Loss: 0.7993%\n",
      "Epoch [14/300], Step [33/225], Training Accuracy: 62.3106%, Training Loss: 0.7950%\n",
      "Epoch [14/300], Step [34/225], Training Accuracy: 62.1324%, Training Loss: 0.7960%\n",
      "Epoch [14/300], Step [35/225], Training Accuracy: 62.0089%, Training Loss: 0.7959%\n",
      "Epoch [14/300], Step [36/225], Training Accuracy: 62.4132%, Training Loss: 0.7926%\n",
      "Epoch [14/300], Step [37/225], Training Accuracy: 62.5000%, Training Loss: 0.7898%\n",
      "Epoch [14/300], Step [38/225], Training Accuracy: 62.4589%, Training Loss: 0.7893%\n",
      "Epoch [14/300], Step [39/225], Training Accuracy: 62.1795%, Training Loss: 0.7904%\n",
      "Epoch [14/300], Step [40/225], Training Accuracy: 62.2266%, Training Loss: 0.7905%\n",
      "Epoch [14/300], Step [41/225], Training Accuracy: 62.3095%, Training Loss: 0.7901%\n",
      "Epoch [14/300], Step [42/225], Training Accuracy: 62.2768%, Training Loss: 0.7896%\n",
      "Epoch [14/300], Step [43/225], Training Accuracy: 62.2456%, Training Loss: 0.7898%\n",
      "Epoch [14/300], Step [44/225], Training Accuracy: 62.5355%, Training Loss: 0.7853%\n",
      "Epoch [14/300], Step [45/225], Training Accuracy: 62.4306%, Training Loss: 0.7827%\n",
      "Epoch [14/300], Step [46/225], Training Accuracy: 62.3641%, Training Loss: 0.7820%\n",
      "Epoch [14/300], Step [47/225], Training Accuracy: 62.3005%, Training Loss: 0.7820%\n",
      "Epoch [14/300], Step [48/225], Training Accuracy: 62.1419%, Training Loss: 0.7842%\n",
      "Epoch [14/300], Step [49/225], Training Accuracy: 62.3087%, Training Loss: 0.7829%\n",
      "Epoch [14/300], Step [50/225], Training Accuracy: 62.4375%, Training Loss: 0.7822%\n",
      "Epoch [14/300], Step [51/225], Training Accuracy: 62.5000%, Training Loss: 0.7809%\n",
      "Epoch [14/300], Step [52/225], Training Accuracy: 62.4399%, Training Loss: 0.7813%\n",
      "Epoch [14/300], Step [53/225], Training Accuracy: 62.3231%, Training Loss: 0.7816%\n",
      "Epoch [14/300], Step [54/225], Training Accuracy: 62.2975%, Training Loss: 0.7814%\n",
      "Epoch [14/300], Step [55/225], Training Accuracy: 62.1023%, Training Loss: 0.7835%\n",
      "Epoch [14/300], Step [56/225], Training Accuracy: 62.0815%, Training Loss: 0.7836%\n",
      "Epoch [14/300], Step [57/225], Training Accuracy: 62.1436%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [58/225], Training Accuracy: 62.1498%, Training Loss: 0.7838%\n",
      "Epoch [14/300], Step [59/225], Training Accuracy: 62.3411%, Training Loss: 0.7818%\n",
      "Epoch [14/300], Step [60/225], Training Accuracy: 62.3958%, Training Loss: 0.7813%\n",
      "Epoch [14/300], Step [61/225], Training Accuracy: 62.3463%, Training Loss: 0.7819%\n",
      "Epoch [14/300], Step [62/225], Training Accuracy: 62.3488%, Training Loss: 0.7811%\n",
      "Epoch [14/300], Step [63/225], Training Accuracy: 62.2024%, Training Loss: 0.7826%\n",
      "Epoch [14/300], Step [64/225], Training Accuracy: 62.1826%, Training Loss: 0.7822%\n",
      "Epoch [14/300], Step [65/225], Training Accuracy: 62.0913%, Training Loss: 0.7828%\n",
      "Epoch [14/300], Step [66/225], Training Accuracy: 62.2633%, Training Loss: 0.7811%\n",
      "Epoch [14/300], Step [67/225], Training Accuracy: 62.3601%, Training Loss: 0.7811%\n",
      "Epoch [14/300], Step [68/225], Training Accuracy: 62.2013%, Training Loss: 0.7816%\n",
      "Epoch [14/300], Step [69/225], Training Accuracy: 62.2736%, Training Loss: 0.7807%\n",
      "Epoch [14/300], Step [70/225], Training Accuracy: 62.2768%, Training Loss: 0.7802%\n",
      "Epoch [14/300], Step [71/225], Training Accuracy: 62.3680%, Training Loss: 0.7791%\n",
      "Epoch [14/300], Step [72/225], Training Accuracy: 62.3047%, Training Loss: 0.7796%\n",
      "Epoch [14/300], Step [73/225], Training Accuracy: 62.3074%, Training Loss: 0.7791%\n",
      "Epoch [14/300], Step [74/225], Training Accuracy: 62.2889%, Training Loss: 0.7777%\n",
      "Epoch [14/300], Step [75/225], Training Accuracy: 62.3750%, Training Loss: 0.7769%\n",
      "Epoch [14/300], Step [76/225], Training Accuracy: 62.1711%, Training Loss: 0.7792%\n",
      "Epoch [14/300], Step [77/225], Training Accuracy: 62.2768%, Training Loss: 0.7776%\n",
      "Epoch [14/300], Step [78/225], Training Accuracy: 62.2796%, Training Loss: 0.7775%\n",
      "Epoch [14/300], Step [79/225], Training Accuracy: 62.2824%, Training Loss: 0.7772%\n",
      "Epoch [14/300], Step [80/225], Training Accuracy: 62.2656%, Training Loss: 0.7775%\n",
      "Epoch [14/300], Step [81/225], Training Accuracy: 62.1914%, Training Loss: 0.7776%\n",
      "Epoch [14/300], Step [82/225], Training Accuracy: 62.1570%, Training Loss: 0.7772%\n",
      "Epoch [14/300], Step [83/225], Training Accuracy: 62.1235%, Training Loss: 0.7765%\n",
      "Epoch [14/300], Step [84/225], Training Accuracy: 62.1466%, Training Loss: 0.7764%\n",
      "Epoch [14/300], Step [85/225], Training Accuracy: 62.1140%, Training Loss: 0.7757%\n",
      "Epoch [14/300], Step [86/225], Training Accuracy: 62.1185%, Training Loss: 0.7766%\n",
      "Epoch [14/300], Step [87/225], Training Accuracy: 62.1228%, Training Loss: 0.7763%\n",
      "Epoch [14/300], Step [88/225], Training Accuracy: 62.0739%, Training Loss: 0.7767%\n",
      "Epoch [14/300], Step [89/225], Training Accuracy: 62.0787%, Training Loss: 0.7774%\n",
      "Epoch [14/300], Step [90/225], Training Accuracy: 62.0312%, Training Loss: 0.7797%\n",
      "Epoch [14/300], Step [91/225], Training Accuracy: 62.0021%, Training Loss: 0.7803%\n",
      "Epoch [14/300], Step [92/225], Training Accuracy: 62.0075%, Training Loss: 0.7799%\n",
      "Epoch [14/300], Step [93/225], Training Accuracy: 62.0632%, Training Loss: 0.7795%\n",
      "Epoch [14/300], Step [94/225], Training Accuracy: 62.1177%, Training Loss: 0.7782%\n",
      "Epoch [14/300], Step [95/225], Training Accuracy: 62.2204%, Training Loss: 0.7784%\n",
      "Epoch [14/300], Step [96/225], Training Accuracy: 62.2396%, Training Loss: 0.7769%\n",
      "Epoch [14/300], Step [97/225], Training Accuracy: 62.2745%, Training Loss: 0.7770%\n",
      "Epoch [14/300], Step [98/225], Training Accuracy: 62.1971%, Training Loss: 0.7778%\n",
      "Epoch [14/300], Step [99/225], Training Accuracy: 62.1686%, Training Loss: 0.7790%\n",
      "Epoch [14/300], Step [100/225], Training Accuracy: 62.1250%, Training Loss: 0.7813%\n",
      "Epoch [14/300], Step [101/225], Training Accuracy: 62.1132%, Training Loss: 0.7822%\n",
      "Epoch [14/300], Step [102/225], Training Accuracy: 62.1017%, Training Loss: 0.7818%\n",
      "Epoch [14/300], Step [103/225], Training Accuracy: 62.1359%, Training Loss: 0.7816%\n",
      "Epoch [14/300], Step [104/225], Training Accuracy: 61.9892%, Training Loss: 0.7825%\n",
      "Epoch [14/300], Step [105/225], Training Accuracy: 62.0685%, Training Loss: 0.7820%\n",
      "Epoch [14/300], Step [106/225], Training Accuracy: 62.0430%, Training Loss: 0.7820%\n",
      "Epoch [14/300], Step [107/225], Training Accuracy: 62.0473%, Training Loss: 0.7826%\n",
      "Epoch [14/300], Step [108/225], Training Accuracy: 62.0226%, Training Loss: 0.7828%\n",
      "Epoch [14/300], Step [109/225], Training Accuracy: 61.9266%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [110/225], Training Accuracy: 61.9034%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [111/225], Training Accuracy: 61.9088%, Training Loss: 0.7822%\n",
      "Epoch [14/300], Step [112/225], Training Accuracy: 61.9141%, Training Loss: 0.7820%\n",
      "Epoch [14/300], Step [113/225], Training Accuracy: 62.0022%, Training Loss: 0.7817%\n",
      "Epoch [14/300], Step [114/225], Training Accuracy: 62.0614%, Training Loss: 0.7810%\n",
      "Epoch [14/300], Step [115/225], Training Accuracy: 62.1332%, Training Loss: 0.7806%\n",
      "Epoch [14/300], Step [116/225], Training Accuracy: 62.1767%, Training Loss: 0.7800%\n",
      "Epoch [14/300], Step [117/225], Training Accuracy: 62.1528%, Training Loss: 0.7812%\n",
      "Epoch [14/300], Step [118/225], Training Accuracy: 62.1292%, Training Loss: 0.7819%\n",
      "Epoch [14/300], Step [119/225], Training Accuracy: 62.1586%, Training Loss: 0.7823%\n",
      "Epoch [14/300], Step [120/225], Training Accuracy: 62.2135%, Training Loss: 0.7828%\n",
      "Epoch [14/300], Step [121/225], Training Accuracy: 62.0868%, Training Loss: 0.7832%\n",
      "Epoch [14/300], Step [122/225], Training Accuracy: 62.1286%, Training Loss: 0.7829%\n",
      "Epoch [14/300], Step [123/225], Training Accuracy: 62.0935%, Training Loss: 0.7831%\n",
      "Epoch [14/300], Step [124/225], Training Accuracy: 62.1220%, Training Loss: 0.7822%\n",
      "Epoch [14/300], Step [125/225], Training Accuracy: 62.0875%, Training Loss: 0.7825%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300], Step [126/225], Training Accuracy: 62.0908%, Training Loss: 0.7829%\n",
      "Epoch [14/300], Step [127/225], Training Accuracy: 62.0571%, Training Loss: 0.7837%\n",
      "Epoch [14/300], Step [128/225], Training Accuracy: 62.0361%, Training Loss: 0.7849%\n",
      "Epoch [14/300], Step [129/225], Training Accuracy: 62.0397%, Training Loss: 0.7857%\n",
      "Epoch [14/300], Step [130/225], Training Accuracy: 61.9832%, Training Loss: 0.7859%\n",
      "Epoch [14/300], Step [131/225], Training Accuracy: 61.9036%, Training Loss: 0.7868%\n",
      "Epoch [14/300], Step [132/225], Training Accuracy: 61.8490%, Training Loss: 0.7874%\n",
      "Epoch [14/300], Step [133/225], Training Accuracy: 61.8304%, Training Loss: 0.7870%\n",
      "Epoch [14/300], Step [134/225], Training Accuracy: 61.7654%, Training Loss: 0.7882%\n",
      "Epoch [14/300], Step [135/225], Training Accuracy: 61.7593%, Training Loss: 0.7879%\n",
      "Epoch [14/300], Step [136/225], Training Accuracy: 61.7877%, Training Loss: 0.7879%\n",
      "Epoch [14/300], Step [137/225], Training Accuracy: 61.7929%, Training Loss: 0.7873%\n",
      "Epoch [14/300], Step [138/225], Training Accuracy: 61.8433%, Training Loss: 0.7865%\n",
      "Epoch [14/300], Step [139/225], Training Accuracy: 61.7918%, Training Loss: 0.7863%\n",
      "Epoch [14/300], Step [140/225], Training Accuracy: 61.8304%, Training Loss: 0.7859%\n",
      "Epoch [14/300], Step [141/225], Training Accuracy: 61.8351%, Training Loss: 0.7858%\n",
      "Epoch [14/300], Step [142/225], Training Accuracy: 61.8618%, Training Loss: 0.7848%\n",
      "Epoch [14/300], Step [143/225], Training Accuracy: 61.8881%, Training Loss: 0.7846%\n",
      "Epoch [14/300], Step [144/225], Training Accuracy: 61.8707%, Training Loss: 0.7846%\n",
      "Epoch [14/300], Step [145/225], Training Accuracy: 61.8858%, Training Loss: 0.7840%\n",
      "Epoch [14/300], Step [146/225], Training Accuracy: 61.8793%, Training Loss: 0.7840%\n",
      "Epoch [14/300], Step [147/225], Training Accuracy: 61.8197%, Training Loss: 0.7846%\n",
      "Epoch [14/300], Step [148/225], Training Accuracy: 61.8243%, Training Loss: 0.7841%\n",
      "Epoch [14/300], Step [149/225], Training Accuracy: 61.8079%, Training Loss: 0.7848%\n",
      "Epoch [14/300], Step [150/225], Training Accuracy: 61.8125%, Training Loss: 0.7844%\n",
      "Epoch [14/300], Step [151/225], Training Accuracy: 61.8377%, Training Loss: 0.7838%\n",
      "Epoch [14/300], Step [152/225], Training Accuracy: 61.7907%, Training Loss: 0.7845%\n",
      "Epoch [14/300], Step [153/225], Training Accuracy: 61.8260%, Training Loss: 0.7846%\n",
      "Epoch [14/300], Step [154/225], Training Accuracy: 61.8608%, Training Loss: 0.7841%\n",
      "Epoch [14/300], Step [155/225], Training Accuracy: 61.8649%, Training Loss: 0.7838%\n",
      "Epoch [14/300], Step [156/225], Training Accuracy: 61.8690%, Training Loss: 0.7844%\n",
      "Epoch [14/300], Step [157/225], Training Accuracy: 61.8830%, Training Loss: 0.7841%\n",
      "Epoch [14/300], Step [158/225], Training Accuracy: 61.8473%, Training Loss: 0.7849%\n",
      "Epoch [14/300], Step [159/225], Training Accuracy: 61.8318%, Training Loss: 0.7853%\n",
      "Epoch [14/300], Step [160/225], Training Accuracy: 61.8262%, Training Loss: 0.7851%\n",
      "Epoch [14/300], Step [161/225], Training Accuracy: 61.8789%, Training Loss: 0.7847%\n",
      "Epoch [14/300], Step [162/225], Training Accuracy: 61.8634%, Training Loss: 0.7841%\n",
      "Epoch [14/300], Step [163/225], Training Accuracy: 61.8673%, Training Loss: 0.7841%\n",
      "Epoch [14/300], Step [164/225], Training Accuracy: 61.9569%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [165/225], Training Accuracy: 61.9602%, Training Loss: 0.7829%\n",
      "Epoch [14/300], Step [166/225], Training Accuracy: 61.9823%, Training Loss: 0.7828%\n",
      "Epoch [14/300], Step [167/225], Training Accuracy: 61.9667%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [168/225], Training Accuracy: 61.9792%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [169/225], Training Accuracy: 61.9545%, Training Loss: 0.7832%\n",
      "Epoch [14/300], Step [170/225], Training Accuracy: 61.9577%, Training Loss: 0.7833%\n",
      "Epoch [14/300], Step [171/225], Training Accuracy: 61.8969%, Training Loss: 0.7838%\n",
      "Epoch [14/300], Step [172/225], Training Accuracy: 61.9277%, Training Loss: 0.7835%\n",
      "Epoch [14/300], Step [173/225], Training Accuracy: 61.9220%, Training Loss: 0.7835%\n",
      "Epoch [14/300], Step [174/225], Training Accuracy: 61.9522%, Training Loss: 0.7830%\n",
      "Epoch [14/300], Step [175/225], Training Accuracy: 61.9643%, Training Loss: 0.7827%\n",
      "Epoch [14/300], Step [176/225], Training Accuracy: 61.9851%, Training Loss: 0.7821%\n",
      "Epoch [14/300], Step [177/225], Training Accuracy: 62.0145%, Training Loss: 0.7817%\n",
      "Epoch [14/300], Step [178/225], Training Accuracy: 62.0172%, Training Loss: 0.7813%\n",
      "Epoch [14/300], Step [179/225], Training Accuracy: 62.0723%, Training Loss: 0.7807%\n",
      "Epoch [14/300], Step [180/225], Training Accuracy: 62.1007%, Training Loss: 0.7802%\n",
      "Epoch [14/300], Step [181/225], Training Accuracy: 62.0425%, Training Loss: 0.7807%\n",
      "Epoch [14/300], Step [182/225], Training Accuracy: 62.0793%, Training Loss: 0.7805%\n",
      "Epoch [14/300], Step [183/225], Training Accuracy: 62.1158%, Training Loss: 0.7802%\n",
      "Epoch [14/300], Step [184/225], Training Accuracy: 62.1264%, Training Loss: 0.7799%\n",
      "Epoch [14/300], Step [185/225], Training Accuracy: 62.1706%, Training Loss: 0.7791%\n",
      "Epoch [14/300], Step [186/225], Training Accuracy: 62.2144%, Training Loss: 0.7786%\n",
      "Epoch [14/300], Step [187/225], Training Accuracy: 62.2326%, Training Loss: 0.7783%\n",
      "Epoch [14/300], Step [188/225], Training Accuracy: 62.2922%, Training Loss: 0.7776%\n",
      "Epoch [14/300], Step [189/225], Training Accuracy: 62.3512%, Training Loss: 0.7767%\n",
      "Epoch [14/300], Step [190/225], Training Accuracy: 62.3684%, Training Loss: 0.7764%\n",
      "Epoch [14/300], Step [191/225], Training Accuracy: 62.3609%, Training Loss: 0.7763%\n",
      "Epoch [14/300], Step [192/225], Training Accuracy: 62.4105%, Training Loss: 0.7755%\n",
      "Epoch [14/300], Step [193/225], Training Accuracy: 62.3948%, Training Loss: 0.7753%\n",
      "Epoch [14/300], Step [194/225], Training Accuracy: 62.3872%, Training Loss: 0.7752%\n",
      "Epoch [14/300], Step [195/225], Training Accuracy: 62.4038%, Training Loss: 0.7748%\n",
      "Epoch [14/300], Step [196/225], Training Accuracy: 62.3166%, Training Loss: 0.7759%\n",
      "Epoch [14/300], Step [197/225], Training Accuracy: 62.3255%, Training Loss: 0.7761%\n",
      "Epoch [14/300], Step [198/225], Training Accuracy: 62.3737%, Training Loss: 0.7753%\n",
      "Epoch [14/300], Step [199/225], Training Accuracy: 62.3901%, Training Loss: 0.7750%\n",
      "Epoch [14/300], Step [200/225], Training Accuracy: 62.3984%, Training Loss: 0.7749%\n",
      "Epoch [14/300], Step [201/225], Training Accuracy: 62.3678%, Training Loss: 0.7754%\n",
      "Epoch [14/300], Step [202/225], Training Accuracy: 62.3453%, Training Loss: 0.7753%\n",
      "Epoch [14/300], Step [203/225], Training Accuracy: 62.3768%, Training Loss: 0.7753%\n",
      "Epoch [14/300], Step [204/225], Training Accuracy: 62.3698%, Training Loss: 0.7761%\n",
      "Epoch [14/300], Step [205/225], Training Accuracy: 62.4695%, Training Loss: 0.7756%\n",
      "Epoch [14/300], Step [206/225], Training Accuracy: 62.4697%, Training Loss: 0.7763%\n",
      "Epoch [14/300], Step [207/225], Training Accuracy: 62.4396%, Training Loss: 0.7764%\n",
      "Epoch [14/300], Step [208/225], Training Accuracy: 62.4249%, Training Loss: 0.7763%\n",
      "Epoch [14/300], Step [209/225], Training Accuracy: 62.4252%, Training Loss: 0.7763%\n",
      "Epoch [14/300], Step [210/225], Training Accuracy: 62.3884%, Training Loss: 0.7766%\n",
      "Epoch [14/300], Step [211/225], Training Accuracy: 62.4037%, Training Loss: 0.7764%\n",
      "Epoch [14/300], Step [212/225], Training Accuracy: 62.4263%, Training Loss: 0.7763%\n",
      "Epoch [14/300], Step [213/225], Training Accuracy: 62.3900%, Training Loss: 0.7771%\n",
      "Epoch [14/300], Step [214/225], Training Accuracy: 62.4416%, Training Loss: 0.7767%\n",
      "Epoch [14/300], Step [215/225], Training Accuracy: 62.4055%, Training Loss: 0.7769%\n",
      "Epoch [14/300], Step [216/225], Training Accuracy: 62.3770%, Training Loss: 0.7774%\n",
      "Epoch [14/300], Step [217/225], Training Accuracy: 62.3416%, Training Loss: 0.7779%\n",
      "Epoch [14/300], Step [218/225], Training Accuracy: 62.3136%, Training Loss: 0.7785%\n",
      "Epoch [14/300], Step [219/225], Training Accuracy: 62.3430%, Training Loss: 0.7784%\n",
      "Epoch [14/300], Step [220/225], Training Accuracy: 62.3153%, Training Loss: 0.7786%\n",
      "Epoch [14/300], Step [221/225], Training Accuracy: 62.2738%, Training Loss: 0.7786%\n",
      "Epoch [14/300], Step [222/225], Training Accuracy: 62.2818%, Training Loss: 0.7784%\n",
      "Epoch [14/300], Step [223/225], Training Accuracy: 62.2337%, Training Loss: 0.7788%\n",
      "Epoch [14/300], Step [224/225], Training Accuracy: 62.2001%, Training Loss: 0.7789%\n",
      "Epoch [14/300], Step [225/225], Training Accuracy: 62.1248%, Training Loss: 0.7794%\n",
      "Epoch [15/300], Step [1/225], Training Accuracy: 60.9375%, Training Loss: 0.7571%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Step [2/225], Training Accuracy: 61.7188%, Training Loss: 0.8023%\n",
      "Epoch [15/300], Step [3/225], Training Accuracy: 62.5000%, Training Loss: 0.8069%\n",
      "Epoch [15/300], Step [4/225], Training Accuracy: 62.1094%, Training Loss: 0.7971%\n",
      "Epoch [15/300], Step [5/225], Training Accuracy: 62.1875%, Training Loss: 0.7896%\n",
      "Epoch [15/300], Step [6/225], Training Accuracy: 60.4167%, Training Loss: 0.8133%\n",
      "Epoch [15/300], Step [7/225], Training Accuracy: 61.3839%, Training Loss: 0.8084%\n",
      "Epoch [15/300], Step [8/225], Training Accuracy: 60.7422%, Training Loss: 0.8165%\n",
      "Epoch [15/300], Step [9/225], Training Accuracy: 60.9375%, Training Loss: 0.8193%\n",
      "Epoch [15/300], Step [10/225], Training Accuracy: 60.0000%, Training Loss: 0.8339%\n",
      "Epoch [15/300], Step [11/225], Training Accuracy: 60.6534%, Training Loss: 0.8253%\n",
      "Epoch [15/300], Step [12/225], Training Accuracy: 61.1979%, Training Loss: 0.8270%\n",
      "Epoch [15/300], Step [13/225], Training Accuracy: 62.0192%, Training Loss: 0.8142%\n",
      "Epoch [15/300], Step [14/225], Training Accuracy: 62.0536%, Training Loss: 0.8123%\n",
      "Epoch [15/300], Step [15/225], Training Accuracy: 62.0833%, Training Loss: 0.8122%\n",
      "Epoch [15/300], Step [16/225], Training Accuracy: 62.2070%, Training Loss: 0.8058%\n",
      "Epoch [15/300], Step [17/225], Training Accuracy: 62.2243%, Training Loss: 0.8037%\n",
      "Epoch [15/300], Step [18/225], Training Accuracy: 62.0660%, Training Loss: 0.8028%\n",
      "Epoch [15/300], Step [19/225], Training Accuracy: 61.8421%, Training Loss: 0.8021%\n",
      "Epoch [15/300], Step [20/225], Training Accuracy: 61.7969%, Training Loss: 0.8008%\n",
      "Epoch [15/300], Step [21/225], Training Accuracy: 61.9048%, Training Loss: 0.7945%\n",
      "Epoch [15/300], Step [22/225], Training Accuracy: 61.2926%, Training Loss: 0.8015%\n",
      "Epoch [15/300], Step [23/225], Training Accuracy: 62.0245%, Training Loss: 0.7929%\n",
      "Epoch [15/300], Step [24/225], Training Accuracy: 61.6536%, Training Loss: 0.7968%\n",
      "Epoch [15/300], Step [25/225], Training Accuracy: 61.8750%, Training Loss: 0.7952%\n",
      "Epoch [15/300], Step [26/225], Training Accuracy: 61.7788%, Training Loss: 0.7948%\n",
      "Epoch [15/300], Step [27/225], Training Accuracy: 61.9792%, Training Loss: 0.7923%\n",
      "Epoch [15/300], Step [28/225], Training Accuracy: 62.0536%, Training Loss: 0.7895%\n",
      "Epoch [15/300], Step [29/225], Training Accuracy: 62.1228%, Training Loss: 0.7874%\n",
      "Epoch [15/300], Step [30/225], Training Accuracy: 62.4479%, Training Loss: 0.7849%\n",
      "Epoch [15/300], Step [31/225], Training Accuracy: 62.2984%, Training Loss: 0.7879%\n",
      "Epoch [15/300], Step [32/225], Training Accuracy: 62.2070%, Training Loss: 0.7869%\n",
      "Epoch [15/300], Step [33/225], Training Accuracy: 62.4053%, Training Loss: 0.7837%\n",
      "Epoch [15/300], Step [34/225], Training Accuracy: 62.1324%, Training Loss: 0.7839%\n",
      "Epoch [15/300], Step [35/225], Training Accuracy: 62.1429%, Training Loss: 0.7826%\n",
      "Epoch [15/300], Step [36/225], Training Accuracy: 62.4132%, Training Loss: 0.7795%\n",
      "Epoch [15/300], Step [37/225], Training Accuracy: 62.5422%, Training Loss: 0.7776%\n",
      "Epoch [15/300], Step [38/225], Training Accuracy: 62.5822%, Training Loss: 0.7780%\n",
      "Epoch [15/300], Step [39/225], Training Accuracy: 62.5000%, Training Loss: 0.7790%\n",
      "Epoch [15/300], Step [40/225], Training Accuracy: 62.4609%, Training Loss: 0.7795%\n",
      "Epoch [15/300], Step [41/225], Training Accuracy: 62.1951%, Training Loss: 0.7819%\n",
      "Epoch [15/300], Step [42/225], Training Accuracy: 61.9420%, Training Loss: 0.7811%\n",
      "Epoch [15/300], Step [43/225], Training Accuracy: 61.8096%, Training Loss: 0.7818%\n",
      "Epoch [15/300], Step [44/225], Training Accuracy: 62.1094%, Training Loss: 0.7787%\n",
      "Epoch [15/300], Step [45/225], Training Accuracy: 62.1528%, Training Loss: 0.7767%\n",
      "Epoch [15/300], Step [46/225], Training Accuracy: 62.0245%, Training Loss: 0.7755%\n",
      "Epoch [15/300], Step [47/225], Training Accuracy: 61.8351%, Training Loss: 0.7752%\n",
      "Epoch [15/300], Step [48/225], Training Accuracy: 61.7513%, Training Loss: 0.7771%\n",
      "Epoch [15/300], Step [49/225], Training Accuracy: 61.8304%, Training Loss: 0.7762%\n",
      "Epoch [15/300], Step [50/225], Training Accuracy: 61.9375%, Training Loss: 0.7762%\n",
      "Epoch [15/300], Step [51/225], Training Accuracy: 62.1017%, Training Loss: 0.7743%\n",
      "Epoch [15/300], Step [52/225], Training Accuracy: 62.0793%, Training Loss: 0.7735%\n",
      "Epoch [15/300], Step [53/225], Training Accuracy: 62.1757%, Training Loss: 0.7730%\n",
      "Epoch [15/300], Step [54/225], Training Accuracy: 62.1238%, Training Loss: 0.7722%\n",
      "Epoch [15/300], Step [55/225], Training Accuracy: 61.9034%, Training Loss: 0.7740%\n",
      "Epoch [15/300], Step [56/225], Training Accuracy: 61.8583%, Training Loss: 0.7741%\n",
      "Epoch [15/300], Step [57/225], Training Accuracy: 61.9792%, Training Loss: 0.7720%\n",
      "Epoch [15/300], Step [58/225], Training Accuracy: 62.0151%, Training Loss: 0.7741%\n",
      "Epoch [15/300], Step [59/225], Training Accuracy: 61.9968%, Training Loss: 0.7729%\n",
      "Epoch [15/300], Step [60/225], Training Accuracy: 62.0052%, Training Loss: 0.7720%\n",
      "Epoch [15/300], Step [61/225], Training Accuracy: 62.0133%, Training Loss: 0.7726%\n",
      "Epoch [15/300], Step [62/225], Training Accuracy: 62.0212%, Training Loss: 0.7729%\n",
      "Epoch [15/300], Step [63/225], Training Accuracy: 62.0536%, Training Loss: 0.7743%\n",
      "Epoch [15/300], Step [64/225], Training Accuracy: 62.1582%, Training Loss: 0.7730%\n",
      "Epoch [15/300], Step [65/225], Training Accuracy: 62.3558%, Training Loss: 0.7729%\n",
      "Epoch [15/300], Step [66/225], Training Accuracy: 62.4763%, Training Loss: 0.7712%\n",
      "Epoch [15/300], Step [67/225], Training Accuracy: 62.5233%, Training Loss: 0.7715%\n",
      "Epoch [15/300], Step [68/225], Training Accuracy: 62.3162%, Training Loss: 0.7734%\n",
      "Epoch [15/300], Step [69/225], Training Accuracy: 62.2962%, Training Loss: 0.7729%\n",
      "Epoch [15/300], Step [70/225], Training Accuracy: 62.2321%, Training Loss: 0.7723%\n",
      "Epoch [15/300], Step [71/225], Training Accuracy: 62.3460%, Training Loss: 0.7701%\n",
      "Epoch [15/300], Step [72/225], Training Accuracy: 62.2179%, Training Loss: 0.7723%\n",
      "Epoch [15/300], Step [73/225], Training Accuracy: 62.2432%, Training Loss: 0.7712%\n",
      "Epoch [15/300], Step [74/225], Training Accuracy: 62.2889%, Training Loss: 0.7698%\n",
      "Epoch [15/300], Step [75/225], Training Accuracy: 62.3958%, Training Loss: 0.7686%\n",
      "Epoch [15/300], Step [76/225], Training Accuracy: 62.1711%, Training Loss: 0.7705%\n",
      "Epoch [15/300], Step [77/225], Training Accuracy: 62.2768%, Training Loss: 0.7691%\n",
      "Epoch [15/300], Step [78/225], Training Accuracy: 62.2396%, Training Loss: 0.7695%\n",
      "Epoch [15/300], Step [79/225], Training Accuracy: 62.2824%, Training Loss: 0.7687%\n",
      "Epoch [15/300], Step [80/225], Training Accuracy: 62.3633%, Training Loss: 0.7693%\n",
      "Epoch [15/300], Step [81/225], Training Accuracy: 62.3650%, Training Loss: 0.7696%\n",
      "Epoch [15/300], Step [82/225], Training Accuracy: 62.4428%, Training Loss: 0.7680%\n",
      "Epoch [15/300], Step [83/225], Training Accuracy: 62.5000%, Training Loss: 0.7666%\n",
      "Epoch [15/300], Step [84/225], Training Accuracy: 62.5558%, Training Loss: 0.7666%\n",
      "Epoch [15/300], Step [85/225], Training Accuracy: 62.5551%, Training Loss: 0.7654%\n",
      "Epoch [15/300], Step [86/225], Training Accuracy: 62.4637%, Training Loss: 0.7663%\n",
      "Epoch [15/300], Step [87/225], Training Accuracy: 62.4641%, Training Loss: 0.7663%\n",
      "Epoch [15/300], Step [88/225], Training Accuracy: 62.3935%, Training Loss: 0.7663%\n",
      "Epoch [15/300], Step [89/225], Training Accuracy: 62.3420%, Training Loss: 0.7678%\n",
      "Epoch [15/300], Step [90/225], Training Accuracy: 62.2569%, Training Loss: 0.7696%\n",
      "Epoch [15/300], Step [91/225], Training Accuracy: 62.2253%, Training Loss: 0.7697%\n",
      "Epoch [15/300], Step [92/225], Training Accuracy: 62.1773%, Training Loss: 0.7698%\n",
      "Epoch [15/300], Step [93/225], Training Accuracy: 62.2648%, Training Loss: 0.7685%\n",
      "Epoch [15/300], Step [94/225], Training Accuracy: 62.3338%, Training Loss: 0.7673%\n",
      "Epoch [15/300], Step [95/225], Training Accuracy: 62.3520%, Training Loss: 0.7671%\n",
      "Epoch [15/300], Step [96/225], Training Accuracy: 62.5000%, Training Loss: 0.7654%\n",
      "Epoch [15/300], Step [97/225], Training Accuracy: 62.5322%, Training Loss: 0.7649%\n",
      "Epoch [15/300], Step [98/225], Training Accuracy: 62.4681%, Training Loss: 0.7667%\n",
      "Epoch [15/300], Step [99/225], Training Accuracy: 62.4842%, Training Loss: 0.7667%\n",
      "Epoch [15/300], Step [100/225], Training Accuracy: 62.4062%, Training Loss: 0.7679%\n",
      "Epoch [15/300], Step [101/225], Training Accuracy: 62.4226%, Training Loss: 0.7682%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Step [102/225], Training Accuracy: 62.4387%, Training Loss: 0.7678%\n",
      "Epoch [15/300], Step [103/225], Training Accuracy: 62.5000%, Training Loss: 0.7674%\n",
      "Epoch [15/300], Step [104/225], Training Accuracy: 62.3798%, Training Loss: 0.7687%\n",
      "Epoch [15/300], Step [105/225], Training Accuracy: 62.3810%, Training Loss: 0.7679%\n",
      "Epoch [15/300], Step [106/225], Training Accuracy: 62.4116%, Training Loss: 0.7680%\n",
      "Epoch [15/300], Step [107/225], Training Accuracy: 62.3978%, Training Loss: 0.7689%\n",
      "Epoch [15/300], Step [108/225], Training Accuracy: 62.3698%, Training Loss: 0.7690%\n",
      "Epoch [15/300], Step [109/225], Training Accuracy: 62.2706%, Training Loss: 0.7685%\n",
      "Epoch [15/300], Step [110/225], Training Accuracy: 62.2443%, Training Loss: 0.7677%\n",
      "Epoch [15/300], Step [111/225], Training Accuracy: 62.1903%, Training Loss: 0.7672%\n",
      "Epoch [15/300], Step [112/225], Training Accuracy: 62.1512%, Training Loss: 0.7676%\n",
      "Epoch [15/300], Step [113/225], Training Accuracy: 62.1543%, Training Loss: 0.7677%\n",
      "Epoch [15/300], Step [114/225], Training Accuracy: 62.1848%, Training Loss: 0.7677%\n",
      "Epoch [15/300], Step [115/225], Training Accuracy: 62.2011%, Training Loss: 0.7676%\n",
      "Epoch [15/300], Step [116/225], Training Accuracy: 62.2171%, Training Loss: 0.7671%\n",
      "Epoch [15/300], Step [117/225], Training Accuracy: 62.1928%, Training Loss: 0.7682%\n",
      "Epoch [15/300], Step [118/225], Training Accuracy: 62.1425%, Training Loss: 0.7687%\n",
      "Epoch [15/300], Step [119/225], Training Accuracy: 62.1980%, Training Loss: 0.7683%\n",
      "Epoch [15/300], Step [120/225], Training Accuracy: 62.1094%, Training Loss: 0.7699%\n",
      "Epoch [15/300], Step [121/225], Training Accuracy: 62.0222%, Training Loss: 0.7700%\n",
      "Epoch [15/300], Step [122/225], Training Accuracy: 62.0902%, Training Loss: 0.7698%\n",
      "Epoch [15/300], Step [123/225], Training Accuracy: 62.0681%, Training Loss: 0.7695%\n",
      "Epoch [15/300], Step [124/225], Training Accuracy: 62.0842%, Training Loss: 0.7691%\n",
      "Epoch [15/300], Step [125/225], Training Accuracy: 62.0875%, Training Loss: 0.7701%\n",
      "Epoch [15/300], Step [126/225], Training Accuracy: 62.0660%, Training Loss: 0.7704%\n",
      "Epoch [15/300], Step [127/225], Training Accuracy: 61.9464%, Training Loss: 0.7716%\n",
      "Epoch [15/300], Step [128/225], Training Accuracy: 61.9019%, Training Loss: 0.7731%\n",
      "Epoch [15/300], Step [129/225], Training Accuracy: 61.8823%, Training Loss: 0.7743%\n",
      "Epoch [15/300], Step [130/225], Training Accuracy: 61.7788%, Training Loss: 0.7753%\n",
      "Epoch [15/300], Step [131/225], Training Accuracy: 61.8082%, Training Loss: 0.7755%\n",
      "Epoch [15/300], Step [132/225], Training Accuracy: 61.7898%, Training Loss: 0.7757%\n",
      "Epoch [15/300], Step [133/225], Training Accuracy: 61.8069%, Training Loss: 0.7753%\n",
      "Epoch [15/300], Step [134/225], Training Accuracy: 61.6604%, Training Loss: 0.7768%\n",
      "Epoch [15/300], Step [135/225], Training Accuracy: 61.6319%, Training Loss: 0.7768%\n",
      "Epoch [15/300], Step [136/225], Training Accuracy: 61.6039%, Training Loss: 0.7768%\n",
      "Epoch [15/300], Step [137/225], Training Accuracy: 61.5990%, Training Loss: 0.7766%\n",
      "Epoch [15/300], Step [138/225], Training Accuracy: 61.6508%, Training Loss: 0.7763%\n",
      "Epoch [15/300], Step [139/225], Training Accuracy: 61.6007%, Training Loss: 0.7767%\n",
      "Epoch [15/300], Step [140/225], Training Accuracy: 61.6629%, Training Loss: 0.7762%\n",
      "Epoch [15/300], Step [141/225], Training Accuracy: 61.6467%, Training Loss: 0.7764%\n",
      "Epoch [15/300], Step [142/225], Training Accuracy: 61.6967%, Training Loss: 0.7758%\n",
      "Epoch [15/300], Step [143/225], Training Accuracy: 61.7133%, Training Loss: 0.7757%\n",
      "Epoch [15/300], Step [144/225], Training Accuracy: 61.7188%, Training Loss: 0.7756%\n",
      "Epoch [15/300], Step [145/225], Training Accuracy: 61.7026%, Training Loss: 0.7758%\n",
      "Epoch [15/300], Step [146/225], Training Accuracy: 61.6759%, Training Loss: 0.7765%\n",
      "Epoch [15/300], Step [147/225], Training Accuracy: 61.6497%, Training Loss: 0.7767%\n",
      "Epoch [15/300], Step [148/225], Training Accuracy: 61.6765%, Training Loss: 0.7765%\n",
      "Epoch [15/300], Step [149/225], Training Accuracy: 61.6716%, Training Loss: 0.7772%\n",
      "Epoch [15/300], Step [150/225], Training Accuracy: 61.6146%, Training Loss: 0.7769%\n",
      "Epoch [15/300], Step [151/225], Training Accuracy: 61.6825%, Training Loss: 0.7765%\n",
      "Epoch [15/300], Step [152/225], Training Accuracy: 61.6674%, Training Loss: 0.7770%\n",
      "Epoch [15/300], Step [153/225], Training Accuracy: 61.6728%, Training Loss: 0.7770%\n",
      "Epoch [15/300], Step [154/225], Training Accuracy: 61.6477%, Training Loss: 0.7771%\n",
      "Epoch [15/300], Step [155/225], Training Accuracy: 61.5927%, Training Loss: 0.7774%\n",
      "Epoch [15/300], Step [156/225], Training Accuracy: 61.6286%, Training Loss: 0.7777%\n",
      "Epoch [15/300], Step [157/225], Training Accuracy: 61.6143%, Training Loss: 0.7778%\n",
      "Epoch [15/300], Step [158/225], Training Accuracy: 61.5407%, Training Loss: 0.7785%\n",
      "Epoch [15/300], Step [159/225], Training Accuracy: 61.4780%, Training Loss: 0.7793%\n",
      "Epoch [15/300], Step [160/225], Training Accuracy: 61.4746%, Training Loss: 0.7792%\n",
      "Epoch [15/300], Step [161/225], Training Accuracy: 61.4616%, Training Loss: 0.7791%\n",
      "Epoch [15/300], Step [162/225], Training Accuracy: 61.5355%, Training Loss: 0.7785%\n",
      "Epoch [15/300], Step [163/225], Training Accuracy: 61.5414%, Training Loss: 0.7784%\n",
      "Epoch [15/300], Step [164/225], Training Accuracy: 61.6806%, Training Loss: 0.7772%\n",
      "Epoch [15/300], Step [165/225], Training Accuracy: 61.7235%, Training Loss: 0.7769%\n",
      "Epoch [15/300], Step [166/225], Training Accuracy: 61.7658%, Training Loss: 0.7763%\n",
      "Epoch [15/300], Step [167/225], Training Accuracy: 61.7702%, Training Loss: 0.7762%\n",
      "Epoch [15/300], Step [168/225], Training Accuracy: 61.7374%, Training Loss: 0.7765%\n",
      "Epoch [15/300], Step [169/225], Training Accuracy: 61.7326%, Training Loss: 0.7766%\n",
      "Epoch [15/300], Step [170/225], Training Accuracy: 61.7647%, Training Loss: 0.7764%\n",
      "Epoch [15/300], Step [171/225], Training Accuracy: 61.7690%, Training Loss: 0.7767%\n",
      "Epoch [15/300], Step [172/225], Training Accuracy: 61.7460%, Training Loss: 0.7769%\n",
      "Epoch [15/300], Step [173/225], Training Accuracy: 61.7504%, Training Loss: 0.7769%\n",
      "Epoch [15/300], Step [174/225], Training Accuracy: 61.7906%, Training Loss: 0.7766%\n",
      "Epoch [15/300], Step [175/225], Training Accuracy: 61.8304%, Training Loss: 0.7760%\n",
      "Epoch [15/300], Step [176/225], Training Accuracy: 61.8253%, Training Loss: 0.7758%\n",
      "Epoch [15/300], Step [177/225], Training Accuracy: 61.8291%, Training Loss: 0.7757%\n",
      "Epoch [15/300], Step [178/225], Training Accuracy: 61.8768%, Training Loss: 0.7752%\n",
      "Epoch [15/300], Step [179/225], Training Accuracy: 61.9501%, Training Loss: 0.7742%\n",
      "Epoch [15/300], Step [180/225], Training Accuracy: 62.0052%, Training Loss: 0.7737%\n",
      "Epoch [15/300], Step [181/225], Training Accuracy: 61.9993%, Training Loss: 0.7740%\n",
      "Epoch [15/300], Step [182/225], Training Accuracy: 62.0278%, Training Loss: 0.7739%\n",
      "Epoch [15/300], Step [183/225], Training Accuracy: 62.0304%, Training Loss: 0.7738%\n",
      "Epoch [15/300], Step [184/225], Training Accuracy: 62.0245%, Training Loss: 0.7736%\n",
      "Epoch [15/300], Step [185/225], Training Accuracy: 62.0946%, Training Loss: 0.7729%\n",
      "Epoch [15/300], Step [186/225], Training Accuracy: 62.1388%, Training Loss: 0.7726%\n",
      "Epoch [15/300], Step [187/225], Training Accuracy: 62.1407%, Training Loss: 0.7725%\n",
      "Epoch [15/300], Step [188/225], Training Accuracy: 62.1925%, Training Loss: 0.7720%\n",
      "Epoch [15/300], Step [189/225], Training Accuracy: 62.2189%, Training Loss: 0.7716%\n",
      "Epoch [15/300], Step [190/225], Training Accuracy: 62.2368%, Training Loss: 0.7716%\n",
      "Epoch [15/300], Step [191/225], Training Accuracy: 62.2382%, Training Loss: 0.7712%\n",
      "Epoch [15/300], Step [192/225], Training Accuracy: 62.2965%, Training Loss: 0.7705%\n",
      "Epoch [15/300], Step [193/225], Training Accuracy: 62.2733%, Training Loss: 0.7707%\n",
      "Epoch [15/300], Step [194/225], Training Accuracy: 62.2664%, Training Loss: 0.7705%\n",
      "Epoch [15/300], Step [195/225], Training Accuracy: 62.2596%, Training Loss: 0.7702%\n",
      "Epoch [15/300], Step [196/225], Training Accuracy: 62.2449%, Training Loss: 0.7702%\n",
      "Epoch [15/300], Step [197/225], Training Accuracy: 62.2779%, Training Loss: 0.7704%\n",
      "Epoch [15/300], Step [198/225], Training Accuracy: 62.3343%, Training Loss: 0.7697%\n",
      "Epoch [15/300], Step [199/225], Training Accuracy: 62.3351%, Training Loss: 0.7693%\n",
      "Epoch [15/300], Step [200/225], Training Accuracy: 62.3359%, Training Loss: 0.7691%\n",
      "Epoch [15/300], Step [201/225], Training Accuracy: 62.3057%, Training Loss: 0.7694%\n",
      "Epoch [15/300], Step [202/225], Training Accuracy: 62.2989%, Training Loss: 0.7690%\n",
      "Epoch [15/300], Step [203/225], Training Accuracy: 62.3615%, Training Loss: 0.7687%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Step [204/225], Training Accuracy: 62.3698%, Training Loss: 0.7691%\n",
      "Epoch [15/300], Step [205/225], Training Accuracy: 62.4238%, Training Loss: 0.7688%\n",
      "Epoch [15/300], Step [206/225], Training Accuracy: 62.4469%, Training Loss: 0.7686%\n",
      "Epoch [15/300], Step [207/225], Training Accuracy: 62.4396%, Training Loss: 0.7687%\n",
      "Epoch [15/300], Step [208/225], Training Accuracy: 62.4549%, Training Loss: 0.7682%\n",
      "Epoch [15/300], Step [209/225], Training Accuracy: 62.4178%, Training Loss: 0.7683%\n",
      "Epoch [15/300], Step [210/225], Training Accuracy: 62.3810%, Training Loss: 0.7687%\n",
      "Epoch [15/300], Step [211/225], Training Accuracy: 62.4185%, Training Loss: 0.7684%\n",
      "Epoch [15/300], Step [212/225], Training Accuracy: 62.4042%, Training Loss: 0.7685%\n",
      "Epoch [15/300], Step [213/225], Training Accuracy: 62.4120%, Training Loss: 0.7688%\n",
      "Epoch [15/300], Step [214/225], Training Accuracy: 62.4343%, Training Loss: 0.7686%\n",
      "Epoch [15/300], Step [215/225], Training Accuracy: 62.4128%, Training Loss: 0.7686%\n",
      "Epoch [15/300], Step [216/225], Training Accuracy: 62.4421%, Training Loss: 0.7686%\n",
      "Epoch [15/300], Step [217/225], Training Accuracy: 62.4352%, Training Loss: 0.7691%\n",
      "Epoch [15/300], Step [218/225], Training Accuracy: 62.4068%, Training Loss: 0.7691%\n",
      "Epoch [15/300], Step [219/225], Training Accuracy: 62.4215%, Training Loss: 0.7690%\n",
      "Epoch [15/300], Step [220/225], Training Accuracy: 62.4077%, Training Loss: 0.7695%\n",
      "Epoch [15/300], Step [221/225], Training Accuracy: 62.3657%, Training Loss: 0.7700%\n",
      "Epoch [15/300], Step [222/225], Training Accuracy: 62.3803%, Training Loss: 0.7698%\n",
      "Epoch [15/300], Step [223/225], Training Accuracy: 62.3739%, Training Loss: 0.7702%\n",
      "Epoch [15/300], Step [224/225], Training Accuracy: 62.3465%, Training Loss: 0.7703%\n",
      "Epoch [15/300], Step [225/225], Training Accuracy: 62.3402%, Training Loss: 0.7708%\n",
      "Epoch [16/300], Step [1/225], Training Accuracy: 67.1875%, Training Loss: 0.6693%\n",
      "Epoch [16/300], Step [2/225], Training Accuracy: 67.1875%, Training Loss: 0.7162%\n",
      "Epoch [16/300], Step [3/225], Training Accuracy: 64.5833%, Training Loss: 0.7544%\n",
      "Epoch [16/300], Step [4/225], Training Accuracy: 63.2812%, Training Loss: 0.7590%\n",
      "Epoch [16/300], Step [5/225], Training Accuracy: 64.3750%, Training Loss: 0.7351%\n",
      "Epoch [16/300], Step [6/225], Training Accuracy: 63.5417%, Training Loss: 0.7717%\n",
      "Epoch [16/300], Step [7/225], Training Accuracy: 64.0625%, Training Loss: 0.7605%\n",
      "Epoch [16/300], Step [8/225], Training Accuracy: 63.0859%, Training Loss: 0.7630%\n",
      "Epoch [16/300], Step [9/225], Training Accuracy: 62.5000%, Training Loss: 0.7736%\n",
      "Epoch [16/300], Step [10/225], Training Accuracy: 61.4062%, Training Loss: 0.7883%\n",
      "Epoch [16/300], Step [11/225], Training Accuracy: 61.7898%, Training Loss: 0.7878%\n",
      "Epoch [16/300], Step [12/225], Training Accuracy: 62.5000%, Training Loss: 0.7931%\n",
      "Epoch [16/300], Step [13/225], Training Accuracy: 63.4615%, Training Loss: 0.7805%\n",
      "Epoch [16/300], Step [14/225], Training Accuracy: 63.5045%, Training Loss: 0.7784%\n",
      "Epoch [16/300], Step [15/225], Training Accuracy: 63.6458%, Training Loss: 0.7748%\n",
      "Epoch [16/300], Step [16/225], Training Accuracy: 63.8672%, Training Loss: 0.7695%\n",
      "Epoch [16/300], Step [17/225], Training Accuracy: 64.2463%, Training Loss: 0.7618%\n",
      "Epoch [16/300], Step [18/225], Training Accuracy: 64.3229%, Training Loss: 0.7609%\n",
      "Epoch [16/300], Step [19/225], Training Accuracy: 64.7204%, Training Loss: 0.7571%\n",
      "Epoch [16/300], Step [20/225], Training Accuracy: 64.6094%, Training Loss: 0.7541%\n",
      "Epoch [16/300], Step [21/225], Training Accuracy: 65.1042%, Training Loss: 0.7465%\n",
      "Epoch [16/300], Step [22/225], Training Accuracy: 64.4886%, Training Loss: 0.7544%\n",
      "Epoch [16/300], Step [23/225], Training Accuracy: 64.8777%, Training Loss: 0.7467%\n",
      "Epoch [16/300], Step [24/225], Training Accuracy: 64.3880%, Training Loss: 0.7505%\n",
      "Epoch [16/300], Step [25/225], Training Accuracy: 64.5625%, Training Loss: 0.7487%\n",
      "Epoch [16/300], Step [26/225], Training Accuracy: 64.5433%, Training Loss: 0.7480%\n",
      "Epoch [16/300], Step [27/225], Training Accuracy: 64.2361%, Training Loss: 0.7486%\n",
      "Epoch [16/300], Step [28/225], Training Accuracy: 64.4531%, Training Loss: 0.7450%\n",
      "Epoch [16/300], Step [29/225], Training Accuracy: 64.4397%, Training Loss: 0.7457%\n",
      "Epoch [16/300], Step [30/225], Training Accuracy: 64.4792%, Training Loss: 0.7447%\n",
      "Epoch [16/300], Step [31/225], Training Accuracy: 64.2641%, Training Loss: 0.7492%\n",
      "Epoch [16/300], Step [32/225], Training Accuracy: 63.9648%, Training Loss: 0.7494%\n",
      "Epoch [16/300], Step [33/225], Training Accuracy: 64.2045%, Training Loss: 0.7454%\n",
      "Epoch [16/300], Step [34/225], Training Accuracy: 63.8787%, Training Loss: 0.7474%\n",
      "Epoch [16/300], Step [35/225], Training Accuracy: 64.0625%, Training Loss: 0.7443%\n",
      "Epoch [16/300], Step [36/225], Training Accuracy: 64.3229%, Training Loss: 0.7408%\n",
      "Epoch [16/300], Step [37/225], Training Accuracy: 64.4848%, Training Loss: 0.7385%\n",
      "Epoch [16/300], Step [38/225], Training Accuracy: 64.4737%, Training Loss: 0.7373%\n",
      "Epoch [16/300], Step [39/225], Training Accuracy: 64.4631%, Training Loss: 0.7379%\n",
      "Epoch [16/300], Step [40/225], Training Accuracy: 64.4531%, Training Loss: 0.7384%\n",
      "Epoch [16/300], Step [41/225], Training Accuracy: 64.5579%, Training Loss: 0.7382%\n",
      "Epoch [16/300], Step [42/225], Training Accuracy: 64.2857%, Training Loss: 0.7388%\n",
      "Epoch [16/300], Step [43/225], Training Accuracy: 64.2442%, Training Loss: 0.7396%\n",
      "Epoch [16/300], Step [44/225], Training Accuracy: 64.5952%, Training Loss: 0.7356%\n",
      "Epoch [16/300], Step [45/225], Training Accuracy: 64.6875%, Training Loss: 0.7333%\n",
      "Epoch [16/300], Step [46/225], Training Accuracy: 64.5720%, Training Loss: 0.7328%\n",
      "Epoch [16/300], Step [47/225], Training Accuracy: 64.3949%, Training Loss: 0.7333%\n",
      "Epoch [16/300], Step [48/225], Training Accuracy: 64.4206%, Training Loss: 0.7343%\n",
      "Epoch [16/300], Step [49/225], Training Accuracy: 64.5089%, Training Loss: 0.7327%\n",
      "Epoch [16/300], Step [50/225], Training Accuracy: 64.5312%, Training Loss: 0.7328%\n",
      "Epoch [16/300], Step [51/225], Training Accuracy: 64.7059%, Training Loss: 0.7310%\n",
      "Epoch [16/300], Step [52/225], Training Accuracy: 64.6334%, Training Loss: 0.7303%\n",
      "Epoch [16/300], Step [53/225], Training Accuracy: 64.5637%, Training Loss: 0.7304%\n",
      "Epoch [16/300], Step [54/225], Training Accuracy: 64.6123%, Training Loss: 0.7305%\n",
      "Epoch [16/300], Step [55/225], Training Accuracy: 64.4602%, Training Loss: 0.7331%\n",
      "Epoch [16/300], Step [56/225], Training Accuracy: 64.3973%, Training Loss: 0.7337%\n",
      "Epoch [16/300], Step [57/225], Training Accuracy: 64.3914%, Training Loss: 0.7327%\n",
      "Epoch [16/300], Step [58/225], Training Accuracy: 64.3588%, Training Loss: 0.7347%\n",
      "Epoch [16/300], Step [59/225], Training Accuracy: 64.5127%, Training Loss: 0.7335%\n",
      "Epoch [16/300], Step [60/225], Training Accuracy: 64.5573%, Training Loss: 0.7316%\n",
      "Epoch [16/300], Step [61/225], Training Accuracy: 64.5236%, Training Loss: 0.7329%\n",
      "Epoch [16/300], Step [62/225], Training Accuracy: 64.5665%, Training Loss: 0.7324%\n",
      "Epoch [16/300], Step [63/225], Training Accuracy: 64.3353%, Training Loss: 0.7356%\n",
      "Epoch [16/300], Step [64/225], Training Accuracy: 64.4043%, Training Loss: 0.7352%\n",
      "Epoch [16/300], Step [65/225], Training Accuracy: 64.3750%, Training Loss: 0.7361%\n",
      "Epoch [16/300], Step [66/225], Training Accuracy: 64.6070%, Training Loss: 0.7344%\n",
      "Epoch [16/300], Step [67/225], Training Accuracy: 64.5289%, Training Loss: 0.7350%\n",
      "Epoch [16/300], Step [68/225], Training Accuracy: 64.4072%, Training Loss: 0.7362%\n",
      "Epoch [16/300], Step [69/225], Training Accuracy: 64.2889%, Training Loss: 0.7369%\n",
      "Epoch [16/300], Step [70/225], Training Accuracy: 64.1295%, Training Loss: 0.7372%\n",
      "Epoch [16/300], Step [71/225], Training Accuracy: 64.1505%, Training Loss: 0.7369%\n",
      "Epoch [16/300], Step [72/225], Training Accuracy: 64.2144%, Training Loss: 0.7370%\n",
      "Epoch [16/300], Step [73/225], Training Accuracy: 64.2765%, Training Loss: 0.7362%\n",
      "Epoch [16/300], Step [74/225], Training Accuracy: 64.2525%, Training Loss: 0.7360%\n",
      "Epoch [16/300], Step [75/225], Training Accuracy: 64.3333%, Training Loss: 0.7354%\n",
      "Epoch [16/300], Step [76/225], Training Accuracy: 64.2475%, Training Loss: 0.7372%\n",
      "Epoch [16/300], Step [77/225], Training Accuracy: 64.4278%, Training Loss: 0.7364%\n",
      "Epoch [16/300], Step [78/225], Training Accuracy: 64.2428%, Training Loss: 0.7368%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Step [79/225], Training Accuracy: 64.1812%, Training Loss: 0.7374%\n",
      "Epoch [16/300], Step [80/225], Training Accuracy: 64.2383%, Training Loss: 0.7376%\n",
      "Epoch [16/300], Step [81/225], Training Accuracy: 64.3326%, Training Loss: 0.7367%\n",
      "Epoch [16/300], Step [82/225], Training Accuracy: 64.3293%, Training Loss: 0.7363%\n",
      "Epoch [16/300], Step [83/225], Training Accuracy: 64.3072%, Training Loss: 0.7358%\n",
      "Epoch [16/300], Step [84/225], Training Accuracy: 64.3043%, Training Loss: 0.7352%\n",
      "Epoch [16/300], Step [85/225], Training Accuracy: 64.3566%, Training Loss: 0.7338%\n",
      "Epoch [16/300], Step [86/225], Training Accuracy: 64.2987%, Training Loss: 0.7340%\n",
      "Epoch [16/300], Step [87/225], Training Accuracy: 64.2780%, Training Loss: 0.7347%\n",
      "Epoch [16/300], Step [88/225], Training Accuracy: 64.1868%, Training Loss: 0.7355%\n",
      "Epoch [16/300], Step [89/225], Training Accuracy: 64.1152%, Training Loss: 0.7368%\n",
      "Epoch [16/300], Step [90/225], Training Accuracy: 64.1146%, Training Loss: 0.7381%\n",
      "Epoch [16/300], Step [91/225], Training Accuracy: 64.0453%, Training Loss: 0.7385%\n",
      "Epoch [16/300], Step [92/225], Training Accuracy: 64.0625%, Training Loss: 0.7391%\n",
      "Epoch [16/300], Step [93/225], Training Accuracy: 64.0625%, Training Loss: 0.7384%\n",
      "Epoch [16/300], Step [94/225], Training Accuracy: 64.1124%, Training Loss: 0.7380%\n",
      "Epoch [16/300], Step [95/225], Training Accuracy: 64.1118%, Training Loss: 0.7387%\n",
      "Epoch [16/300], Step [96/225], Training Accuracy: 64.1764%, Training Loss: 0.7373%\n",
      "Epoch [16/300], Step [97/225], Training Accuracy: 64.2397%, Training Loss: 0.7367%\n",
      "Epoch [16/300], Step [98/225], Training Accuracy: 64.1422%, Training Loss: 0.7389%\n",
      "Epoch [16/300], Step [99/225], Training Accuracy: 64.1098%, Training Loss: 0.7409%\n",
      "Epoch [16/300], Step [100/225], Training Accuracy: 64.0469%, Training Loss: 0.7428%\n",
      "Epoch [16/300], Step [101/225], Training Accuracy: 64.0780%, Training Loss: 0.7430%\n",
      "Epoch [16/300], Step [102/225], Training Accuracy: 63.9706%, Training Loss: 0.7444%\n",
      "Epoch [16/300], Step [103/225], Training Accuracy: 64.0322%, Training Loss: 0.7437%\n",
      "Epoch [16/300], Step [104/225], Training Accuracy: 63.9423%, Training Loss: 0.7447%\n",
      "Epoch [16/300], Step [105/225], Training Accuracy: 63.8839%, Training Loss: 0.7448%\n",
      "Epoch [16/300], Step [106/225], Training Accuracy: 63.9004%, Training Loss: 0.7450%\n",
      "Epoch [16/300], Step [107/225], Training Accuracy: 63.8435%, Training Loss: 0.7457%\n",
      "Epoch [16/300], Step [108/225], Training Accuracy: 63.8600%, Training Loss: 0.7456%\n",
      "Epoch [16/300], Step [109/225], Training Accuracy: 63.8188%, Training Loss: 0.7453%\n",
      "Epoch [16/300], Step [110/225], Training Accuracy: 63.8068%, Training Loss: 0.7447%\n",
      "Epoch [16/300], Step [111/225], Training Accuracy: 63.7810%, Training Loss: 0.7443%\n",
      "Epoch [16/300], Step [112/225], Training Accuracy: 63.7277%, Training Loss: 0.7442%\n",
      "Epoch [16/300], Step [113/225], Training Accuracy: 63.7583%, Training Loss: 0.7442%\n",
      "Epoch [16/300], Step [114/225], Training Accuracy: 63.8021%, Training Loss: 0.7445%\n",
      "Epoch [16/300], Step [115/225], Training Accuracy: 63.8315%, Training Loss: 0.7444%\n",
      "Epoch [16/300], Step [116/225], Training Accuracy: 63.7931%, Training Loss: 0.7447%\n",
      "Epoch [16/300], Step [117/225], Training Accuracy: 63.7019%, Training Loss: 0.7463%\n",
      "Epoch [16/300], Step [118/225], Training Accuracy: 63.6520%, Training Loss: 0.7465%\n",
      "Epoch [16/300], Step [119/225], Training Accuracy: 63.6949%, Training Loss: 0.7463%\n",
      "Epoch [16/300], Step [120/225], Training Accuracy: 63.6719%, Training Loss: 0.7469%\n",
      "Epoch [16/300], Step [121/225], Training Accuracy: 63.5976%, Training Loss: 0.7476%\n",
      "Epoch [16/300], Step [122/225], Training Accuracy: 63.5886%, Training Loss: 0.7478%\n",
      "Epoch [16/300], Step [123/225], Training Accuracy: 63.5036%, Training Loss: 0.7484%\n",
      "Epoch [16/300], Step [124/225], Training Accuracy: 63.5081%, Training Loss: 0.7482%\n",
      "Epoch [16/300], Step [125/225], Training Accuracy: 63.4750%, Training Loss: 0.7487%\n",
      "Epoch [16/300], Step [126/225], Training Accuracy: 63.3929%, Training Loss: 0.7496%\n",
      "Epoch [16/300], Step [127/225], Training Accuracy: 63.2505%, Training Loss: 0.7504%\n",
      "Epoch [16/300], Step [128/225], Training Accuracy: 63.1470%, Training Loss: 0.7526%\n",
      "Epoch [16/300], Step [129/225], Training Accuracy: 63.1541%, Training Loss: 0.7532%\n",
      "Epoch [16/300], Step [130/225], Training Accuracy: 63.0529%, Training Loss: 0.7541%\n",
      "Epoch [16/300], Step [131/225], Training Accuracy: 63.0606%, Training Loss: 0.7545%\n",
      "Epoch [16/300], Step [132/225], Training Accuracy: 62.9972%, Training Loss: 0.7555%\n",
      "Epoch [16/300], Step [133/225], Training Accuracy: 62.9464%, Training Loss: 0.7556%\n",
      "Epoch [16/300], Step [134/225], Training Accuracy: 62.8382%, Training Loss: 0.7573%\n",
      "Epoch [16/300], Step [135/225], Training Accuracy: 62.8704%, Training Loss: 0.7570%\n",
      "Epoch [16/300], Step [136/225], Training Accuracy: 62.9021%, Training Loss: 0.7566%\n",
      "Epoch [16/300], Step [137/225], Training Accuracy: 62.9106%, Training Loss: 0.7563%\n",
      "Epoch [16/300], Step [138/225], Training Accuracy: 62.9982%, Training Loss: 0.7552%\n",
      "Epoch [16/300], Step [139/225], Training Accuracy: 62.9609%, Training Loss: 0.7553%\n",
      "Epoch [16/300], Step [140/225], Training Accuracy: 62.9911%, Training Loss: 0.7552%\n",
      "Epoch [16/300], Step [141/225], Training Accuracy: 62.9876%, Training Loss: 0.7553%\n",
      "Epoch [16/300], Step [142/225], Training Accuracy: 63.0392%, Training Loss: 0.7548%\n",
      "Epoch [16/300], Step [143/225], Training Accuracy: 63.0463%, Training Loss: 0.7546%\n",
      "Epoch [16/300], Step [144/225], Training Accuracy: 62.9774%, Training Loss: 0.7550%\n",
      "Epoch [16/300], Step [145/225], Training Accuracy: 63.0065%, Training Loss: 0.7548%\n",
      "Epoch [16/300], Step [146/225], Training Accuracy: 63.0030%, Training Loss: 0.7551%\n",
      "Epoch [16/300], Step [147/225], Training Accuracy: 62.9677%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [148/225], Training Accuracy: 62.9645%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [149/225], Training Accuracy: 62.9614%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [150/225], Training Accuracy: 63.0104%, Training Loss: 0.7550%\n",
      "Epoch [16/300], Step [151/225], Training Accuracy: 63.0691%, Training Loss: 0.7549%\n",
      "Epoch [16/300], Step [152/225], Training Accuracy: 63.0345%, Training Loss: 0.7554%\n",
      "Epoch [16/300], Step [153/225], Training Accuracy: 63.0208%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [154/225], Training Accuracy: 63.0276%, Training Loss: 0.7554%\n",
      "Epoch [16/300], Step [155/225], Training Accuracy: 63.0141%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [156/225], Training Accuracy: 63.0208%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [157/225], Training Accuracy: 63.0275%, Training Loss: 0.7561%\n",
      "Epoch [16/300], Step [158/225], Training Accuracy: 62.9252%, Training Loss: 0.7569%\n",
      "Epoch [16/300], Step [159/225], Training Accuracy: 62.8734%, Training Loss: 0.7570%\n",
      "Epoch [16/300], Step [160/225], Training Accuracy: 62.8516%, Training Loss: 0.7565%\n",
      "Epoch [16/300], Step [161/225], Training Accuracy: 62.8494%, Training Loss: 0.7563%\n",
      "Epoch [16/300], Step [162/225], Training Accuracy: 62.9437%, Training Loss: 0.7556%\n",
      "Epoch [16/300], Step [163/225], Training Accuracy: 62.9410%, Training Loss: 0.7555%\n",
      "Epoch [16/300], Step [164/225], Training Accuracy: 63.0240%, Training Loss: 0.7543%\n",
      "Epoch [16/300], Step [165/225], Training Accuracy: 63.0114%, Training Loss: 0.7540%\n",
      "Epoch [16/300], Step [166/225], Training Accuracy: 63.0930%, Training Loss: 0.7535%\n",
      "Epoch [16/300], Step [167/225], Training Accuracy: 63.0614%, Training Loss: 0.7539%\n",
      "Epoch [16/300], Step [168/225], Training Accuracy: 63.0487%, Training Loss: 0.7543%\n",
      "Epoch [16/300], Step [169/225], Training Accuracy: 63.0640%, Training Loss: 0.7543%\n",
      "Epoch [16/300], Step [170/225], Training Accuracy: 63.1158%, Training Loss: 0.7543%\n",
      "Epoch [16/300], Step [171/225], Training Accuracy: 63.1122%, Training Loss: 0.7551%\n",
      "Epoch [16/300], Step [172/225], Training Accuracy: 63.1177%, Training Loss: 0.7556%\n",
      "Epoch [16/300], Step [173/225], Training Accuracy: 63.0690%, Training Loss: 0.7558%\n",
      "Epoch [16/300], Step [174/225], Training Accuracy: 63.0747%, Training Loss: 0.7561%\n",
      "Epoch [16/300], Step [175/225], Training Accuracy: 63.1071%, Training Loss: 0.7558%\n",
      "Epoch [16/300], Step [176/225], Training Accuracy: 63.1037%, Training Loss: 0.7553%\n",
      "Epoch [16/300], Step [177/225], Training Accuracy: 63.1179%, Training Loss: 0.7556%\n",
      "Epoch [16/300], Step [178/225], Training Accuracy: 63.1320%, Training Loss: 0.7558%\n",
      "Epoch [16/300], Step [179/225], Training Accuracy: 63.1896%, Training Loss: 0.7552%\n",
      "Epoch [16/300], Step [180/225], Training Accuracy: 63.2205%, Training Loss: 0.7549%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Step [181/225], Training Accuracy: 63.1992%, Training Loss: 0.7556%\n",
      "Epoch [16/300], Step [182/225], Training Accuracy: 63.2297%, Training Loss: 0.7559%\n",
      "Epoch [16/300], Step [183/225], Training Accuracy: 63.2343%, Training Loss: 0.7557%\n",
      "Epoch [16/300], Step [184/225], Training Accuracy: 63.2388%, Training Loss: 0.7550%\n",
      "Epoch [16/300], Step [185/225], Training Accuracy: 63.2770%, Training Loss: 0.7544%\n",
      "Epoch [16/300], Step [186/225], Training Accuracy: 63.3149%, Training Loss: 0.7541%\n",
      "Epoch [16/300], Step [187/225], Training Accuracy: 63.3021%, Training Loss: 0.7540%\n",
      "Epoch [16/300], Step [188/225], Training Accuracy: 63.2979%, Training Loss: 0.7537%\n",
      "Epoch [16/300], Step [189/225], Training Accuracy: 63.3185%, Training Loss: 0.7532%\n",
      "Epoch [16/300], Step [190/225], Training Accuracy: 63.2977%, Training Loss: 0.7532%\n",
      "Epoch [16/300], Step [191/225], Training Accuracy: 63.2772%, Training Loss: 0.7532%\n",
      "Epoch [16/300], Step [192/225], Training Accuracy: 63.3464%, Training Loss: 0.7524%\n",
      "Epoch [16/300], Step [193/225], Training Accuracy: 63.3986%, Training Loss: 0.7527%\n",
      "Epoch [16/300], Step [194/225], Training Accuracy: 63.4021%, Training Loss: 0.7524%\n",
      "Epoch [16/300], Step [195/225], Training Accuracy: 63.4455%, Training Loss: 0.7518%\n",
      "Epoch [16/300], Step [196/225], Training Accuracy: 63.4088%, Training Loss: 0.7521%\n",
      "Epoch [16/300], Step [197/225], Training Accuracy: 63.4121%, Training Loss: 0.7520%\n",
      "Epoch [16/300], Step [198/225], Training Accuracy: 63.4864%, Training Loss: 0.7513%\n",
      "Epoch [16/300], Step [199/225], Training Accuracy: 63.4736%, Training Loss: 0.7510%\n",
      "Epoch [16/300], Step [200/225], Training Accuracy: 63.4609%, Training Loss: 0.7513%\n",
      "Epoch [16/300], Step [201/225], Training Accuracy: 63.4717%, Training Loss: 0.7514%\n",
      "Epoch [16/300], Step [202/225], Training Accuracy: 63.4824%, Training Loss: 0.7511%\n",
      "Epoch [16/300], Step [203/225], Training Accuracy: 63.5468%, Training Loss: 0.7507%\n",
      "Epoch [16/300], Step [204/225], Training Accuracy: 63.5417%, Training Loss: 0.7509%\n",
      "Epoch [16/300], Step [205/225], Training Accuracy: 63.5899%, Training Loss: 0.7508%\n",
      "Epoch [16/300], Step [206/225], Training Accuracy: 63.5543%, Training Loss: 0.7514%\n",
      "Epoch [16/300], Step [207/225], Training Accuracy: 63.5492%, Training Loss: 0.7518%\n",
      "Epoch [16/300], Step [208/225], Training Accuracy: 63.5592%, Training Loss: 0.7514%\n",
      "Epoch [16/300], Step [209/225], Training Accuracy: 63.5541%, Training Loss: 0.7514%\n",
      "Epoch [16/300], Step [210/225], Training Accuracy: 63.4970%, Training Loss: 0.7519%\n",
      "Epoch [16/300], Step [211/225], Training Accuracy: 63.5515%, Training Loss: 0.7515%\n",
      "Epoch [16/300], Step [212/225], Training Accuracy: 63.5613%, Training Loss: 0.7516%\n",
      "Epoch [16/300], Step [213/225], Training Accuracy: 63.5490%, Training Loss: 0.7519%\n",
      "Epoch [16/300], Step [214/225], Training Accuracy: 63.5733%, Training Loss: 0.7516%\n",
      "Epoch [16/300], Step [215/225], Training Accuracy: 63.5174%, Training Loss: 0.7518%\n",
      "Epoch [16/300], Step [216/225], Training Accuracy: 63.4693%, Training Loss: 0.7522%\n",
      "Epoch [16/300], Step [217/225], Training Accuracy: 63.4289%, Training Loss: 0.7527%\n",
      "Epoch [16/300], Step [218/225], Training Accuracy: 63.3816%, Training Loss: 0.7528%\n",
      "Epoch [16/300], Step [219/225], Training Accuracy: 63.3847%, Training Loss: 0.7527%\n",
      "Epoch [16/300], Step [220/225], Training Accuracy: 63.3736%, Training Loss: 0.7530%\n",
      "Epoch [16/300], Step [221/225], Training Accuracy: 63.3555%, Training Loss: 0.7530%\n",
      "Epoch [16/300], Step [222/225], Training Accuracy: 63.3446%, Training Loss: 0.7529%\n",
      "Epoch [16/300], Step [223/225], Training Accuracy: 63.3128%, Training Loss: 0.7532%\n",
      "Epoch [16/300], Step [224/225], Training Accuracy: 63.3022%, Training Loss: 0.7529%\n",
      "Epoch [16/300], Step [225/225], Training Accuracy: 63.2782%, Training Loss: 0.7534%\n",
      "Epoch [17/300], Step [1/225], Training Accuracy: 71.8750%, Training Loss: 0.6992%\n",
      "Epoch [17/300], Step [2/225], Training Accuracy: 67.9688%, Training Loss: 0.7299%\n",
      "Epoch [17/300], Step [3/225], Training Accuracy: 65.1042%, Training Loss: 0.7587%\n",
      "Epoch [17/300], Step [4/225], Training Accuracy: 64.8438%, Training Loss: 0.7582%\n",
      "Epoch [17/300], Step [5/225], Training Accuracy: 65.6250%, Training Loss: 0.7360%\n",
      "Epoch [17/300], Step [6/225], Training Accuracy: 64.3229%, Training Loss: 0.7641%\n",
      "Epoch [17/300], Step [7/225], Training Accuracy: 65.4018%, Training Loss: 0.7560%\n",
      "Epoch [17/300], Step [8/225], Training Accuracy: 65.4297%, Training Loss: 0.7526%\n",
      "Epoch [17/300], Step [9/225], Training Accuracy: 64.9306%, Training Loss: 0.7579%\n",
      "Epoch [17/300], Step [10/225], Training Accuracy: 63.7500%, Training Loss: 0.7700%\n",
      "Epoch [17/300], Step [11/225], Training Accuracy: 63.9205%, Training Loss: 0.7673%\n",
      "Epoch [17/300], Step [12/225], Training Accuracy: 63.4115%, Training Loss: 0.7753%\n",
      "Epoch [17/300], Step [13/225], Training Accuracy: 64.5433%, Training Loss: 0.7591%\n",
      "Epoch [17/300], Step [14/225], Training Accuracy: 64.7321%, Training Loss: 0.7551%\n",
      "Epoch [17/300], Step [15/225], Training Accuracy: 64.4792%, Training Loss: 0.7591%\n",
      "Epoch [17/300], Step [16/225], Training Accuracy: 64.3555%, Training Loss: 0.7547%\n",
      "Epoch [17/300], Step [17/225], Training Accuracy: 64.0625%, Training Loss: 0.7525%\n",
      "Epoch [17/300], Step [18/225], Training Accuracy: 64.1493%, Training Loss: 0.7536%\n",
      "Epoch [17/300], Step [19/225], Training Accuracy: 64.2270%, Training Loss: 0.7530%\n",
      "Epoch [17/300], Step [20/225], Training Accuracy: 64.5312%, Training Loss: 0.7489%\n",
      "Epoch [17/300], Step [21/225], Training Accuracy: 64.8065%, Training Loss: 0.7439%\n",
      "Epoch [17/300], Step [22/225], Training Accuracy: 64.2045%, Training Loss: 0.7524%\n",
      "Epoch [17/300], Step [23/225], Training Accuracy: 64.5380%, Training Loss: 0.7467%\n",
      "Epoch [17/300], Step [24/225], Training Accuracy: 63.9323%, Training Loss: 0.7527%\n",
      "Epoch [17/300], Step [25/225], Training Accuracy: 64.1875%, Training Loss: 0.7494%\n",
      "Epoch [17/300], Step [26/225], Training Accuracy: 64.2428%, Training Loss: 0.7483%\n",
      "Epoch [17/300], Step [27/225], Training Accuracy: 64.1782%, Training Loss: 0.7451%\n",
      "Epoch [17/300], Step [28/225], Training Accuracy: 64.6205%, Training Loss: 0.7385%\n",
      "Epoch [17/300], Step [29/225], Training Accuracy: 64.6552%, Training Loss: 0.7398%\n",
      "Epoch [17/300], Step [30/225], Training Accuracy: 64.5833%, Training Loss: 0.7385%\n",
      "Epoch [17/300], Step [31/225], Training Accuracy: 64.2641%, Training Loss: 0.7439%\n",
      "Epoch [17/300], Step [32/225], Training Accuracy: 64.0625%, Training Loss: 0.7439%\n",
      "Epoch [17/300], Step [33/225], Training Accuracy: 64.2519%, Training Loss: 0.7404%\n",
      "Epoch [17/300], Step [34/225], Training Accuracy: 64.2463%, Training Loss: 0.7409%\n",
      "Epoch [17/300], Step [35/225], Training Accuracy: 64.4196%, Training Loss: 0.7399%\n",
      "Epoch [17/300], Step [36/225], Training Accuracy: 64.5833%, Training Loss: 0.7368%\n",
      "Epoch [17/300], Step [37/225], Training Accuracy: 64.5270%, Training Loss: 0.7350%\n",
      "Epoch [17/300], Step [38/225], Training Accuracy: 64.5559%, Training Loss: 0.7347%\n",
      "Epoch [17/300], Step [39/225], Training Accuracy: 64.5032%, Training Loss: 0.7348%\n",
      "Epoch [17/300], Step [40/225], Training Accuracy: 64.3750%, Training Loss: 0.7346%\n",
      "Epoch [17/300], Step [41/225], Training Accuracy: 64.4817%, Training Loss: 0.7347%\n",
      "Epoch [17/300], Step [42/225], Training Accuracy: 64.4345%, Training Loss: 0.7335%\n",
      "Epoch [17/300], Step [43/225], Training Accuracy: 64.4985%, Training Loss: 0.7336%\n",
      "Epoch [17/300], Step [44/225], Training Accuracy: 64.7017%, Training Loss: 0.7311%\n",
      "Epoch [17/300], Step [45/225], Training Accuracy: 64.8611%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [46/225], Training Accuracy: 64.8438%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [47/225], Training Accuracy: 64.5944%, Training Loss: 0.7299%\n",
      "Epoch [17/300], Step [48/225], Training Accuracy: 64.3555%, Training Loss: 0.7308%\n",
      "Epoch [17/300], Step [49/225], Training Accuracy: 64.4770%, Training Loss: 0.7287%\n",
      "Epoch [17/300], Step [50/225], Training Accuracy: 64.4375%, Training Loss: 0.7289%\n",
      "Epoch [17/300], Step [51/225], Training Accuracy: 64.5527%, Training Loss: 0.7280%\n",
      "Epoch [17/300], Step [52/225], Training Accuracy: 64.5733%, Training Loss: 0.7270%\n",
      "Epoch [17/300], Step [53/225], Training Accuracy: 64.4752%, Training Loss: 0.7268%\n",
      "Epoch [17/300], Step [54/225], Training Accuracy: 64.4676%, Training Loss: 0.7277%\n",
      "Epoch [17/300], Step [55/225], Training Accuracy: 64.2898%, Training Loss: 0.7302%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Step [56/225], Training Accuracy: 64.1741%, Training Loss: 0.7319%\n",
      "Epoch [17/300], Step [57/225], Training Accuracy: 64.3640%, Training Loss: 0.7298%\n",
      "Epoch [17/300], Step [58/225], Training Accuracy: 64.4666%, Training Loss: 0.7304%\n",
      "Epoch [17/300], Step [59/225], Training Accuracy: 64.5922%, Training Loss: 0.7287%\n",
      "Epoch [17/300], Step [60/225], Training Accuracy: 64.6615%, Training Loss: 0.7275%\n",
      "Epoch [17/300], Step [61/225], Training Accuracy: 64.6516%, Training Loss: 0.7279%\n",
      "Epoch [17/300], Step [62/225], Training Accuracy: 64.6169%, Training Loss: 0.7280%\n",
      "Epoch [17/300], Step [63/225], Training Accuracy: 64.4841%, Training Loss: 0.7301%\n",
      "Epoch [17/300], Step [64/225], Training Accuracy: 64.5752%, Training Loss: 0.7296%\n",
      "Epoch [17/300], Step [65/225], Training Accuracy: 64.6635%, Training Loss: 0.7300%\n",
      "Epoch [17/300], Step [66/225], Training Accuracy: 64.7727%, Training Loss: 0.7286%\n",
      "Epoch [17/300], Step [67/225], Training Accuracy: 64.7155%, Training Loss: 0.7295%\n",
      "Epoch [17/300], Step [68/225], Training Accuracy: 64.6829%, Training Loss: 0.7299%\n",
      "Epoch [17/300], Step [69/225], Training Accuracy: 64.6060%, Training Loss: 0.7311%\n",
      "Epoch [17/300], Step [70/225], Training Accuracy: 64.6429%, Training Loss: 0.7306%\n",
      "Epoch [17/300], Step [71/225], Training Accuracy: 64.7447%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [72/225], Training Accuracy: 64.6701%, Training Loss: 0.7286%\n",
      "Epoch [17/300], Step [73/225], Training Accuracy: 64.6190%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [74/225], Training Accuracy: 64.6748%, Training Loss: 0.7279%\n",
      "Epoch [17/300], Step [75/225], Training Accuracy: 64.7500%, Training Loss: 0.7274%\n",
      "Epoch [17/300], Step [76/225], Training Accuracy: 64.5354%, Training Loss: 0.7304%\n",
      "Epoch [17/300], Step [77/225], Training Accuracy: 64.6510%, Training Loss: 0.7289%\n",
      "Epoch [17/300], Step [78/225], Training Accuracy: 64.6034%, Training Loss: 0.7292%\n",
      "Epoch [17/300], Step [79/225], Training Accuracy: 64.7152%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [80/225], Training Accuracy: 64.7266%, Training Loss: 0.7296%\n",
      "Epoch [17/300], Step [81/225], Training Accuracy: 64.8148%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [82/225], Training Accuracy: 64.8628%, Training Loss: 0.7284%\n",
      "Epoch [17/300], Step [83/225], Training Accuracy: 64.9285%, Training Loss: 0.7270%\n",
      "Epoch [17/300], Step [84/225], Training Accuracy: 64.9554%, Training Loss: 0.7269%\n",
      "Epoch [17/300], Step [85/225], Training Accuracy: 64.9449%, Training Loss: 0.7266%\n",
      "Epoch [17/300], Step [86/225], Training Accuracy: 64.9528%, Training Loss: 0.7271%\n",
      "Epoch [17/300], Step [87/225], Training Accuracy: 65.0144%, Training Loss: 0.7265%\n",
      "Epoch [17/300], Step [88/225], Training Accuracy: 64.9503%, Training Loss: 0.7274%\n",
      "Epoch [17/300], Step [89/225], Training Accuracy: 64.9579%, Training Loss: 0.7275%\n",
      "Epoch [17/300], Step [90/225], Training Accuracy: 64.9132%, Training Loss: 0.7290%\n",
      "Epoch [17/300], Step [91/225], Training Accuracy: 64.8523%, Training Loss: 0.7303%\n",
      "Epoch [17/300], Step [92/225], Training Accuracy: 64.8438%, Training Loss: 0.7306%\n",
      "Epoch [17/300], Step [93/225], Training Accuracy: 64.8690%, Training Loss: 0.7302%\n",
      "Epoch [17/300], Step [94/225], Training Accuracy: 64.9102%, Training Loss: 0.7294%\n",
      "Epoch [17/300], Step [95/225], Training Accuracy: 64.8849%, Training Loss: 0.7299%\n",
      "Epoch [17/300], Step [96/225], Training Accuracy: 64.9414%, Training Loss: 0.7283%\n",
      "Epoch [17/300], Step [97/225], Training Accuracy: 64.9323%, Training Loss: 0.7281%\n",
      "Epoch [17/300], Step [98/225], Training Accuracy: 64.8278%, Training Loss: 0.7298%\n",
      "Epoch [17/300], Step [99/225], Training Accuracy: 64.7254%, Training Loss: 0.7311%\n",
      "Epoch [17/300], Step [100/225], Training Accuracy: 64.6719%, Training Loss: 0.7324%\n",
      "Epoch [17/300], Step [101/225], Training Accuracy: 64.7123%, Training Loss: 0.7327%\n",
      "Epoch [17/300], Step [102/225], Training Accuracy: 64.6599%, Training Loss: 0.7327%\n",
      "Epoch [17/300], Step [103/225], Training Accuracy: 64.6541%, Training Loss: 0.7322%\n",
      "Epoch [17/300], Step [104/225], Training Accuracy: 64.5282%, Training Loss: 0.7331%\n",
      "Epoch [17/300], Step [105/225], Training Accuracy: 64.5536%, Training Loss: 0.7323%\n",
      "Epoch [17/300], Step [106/225], Training Accuracy: 64.5489%, Training Loss: 0.7322%\n",
      "Epoch [17/300], Step [107/225], Training Accuracy: 64.4860%, Training Loss: 0.7336%\n",
      "Epoch [17/300], Step [108/225], Training Accuracy: 64.5399%, Training Loss: 0.7334%\n",
      "Epoch [17/300], Step [109/225], Training Accuracy: 64.5356%, Training Loss: 0.7333%\n",
      "Epoch [17/300], Step [110/225], Training Accuracy: 64.4886%, Training Loss: 0.7332%\n",
      "Epoch [17/300], Step [111/225], Training Accuracy: 64.5130%, Training Loss: 0.7325%\n",
      "Epoch [17/300], Step [112/225], Training Accuracy: 64.5089%, Training Loss: 0.7323%\n",
      "Epoch [17/300], Step [113/225], Training Accuracy: 64.5465%, Training Loss: 0.7322%\n",
      "Epoch [17/300], Step [114/225], Training Accuracy: 64.5970%, Training Loss: 0.7317%\n",
      "Epoch [17/300], Step [115/225], Training Accuracy: 64.6332%, Training Loss: 0.7318%\n",
      "Epoch [17/300], Step [116/225], Training Accuracy: 64.6417%, Training Loss: 0.7318%\n",
      "Epoch [17/300], Step [117/225], Training Accuracy: 64.5967%, Training Loss: 0.7327%\n",
      "Epoch [17/300], Step [118/225], Training Accuracy: 64.4862%, Training Loss: 0.7340%\n",
      "Epoch [17/300], Step [119/225], Training Accuracy: 64.4564%, Training Loss: 0.7345%\n",
      "Epoch [17/300], Step [120/225], Training Accuracy: 64.4271%, Training Loss: 0.7359%\n",
      "Epoch [17/300], Step [121/225], Training Accuracy: 64.3466%, Training Loss: 0.7365%\n",
      "Epoch [17/300], Step [122/225], Training Accuracy: 64.4083%, Training Loss: 0.7360%\n",
      "Epoch [17/300], Step [123/225], Training Accuracy: 64.3801%, Training Loss: 0.7362%\n",
      "Epoch [17/300], Step [124/225], Training Accuracy: 64.4153%, Training Loss: 0.7357%\n",
      "Epoch [17/300], Step [125/225], Training Accuracy: 64.3750%, Training Loss: 0.7363%\n",
      "Epoch [17/300], Step [126/225], Training Accuracy: 64.3229%, Training Loss: 0.7369%\n",
      "Epoch [17/300], Step [127/225], Training Accuracy: 64.2470%, Training Loss: 0.7378%\n",
      "Epoch [17/300], Step [128/225], Training Accuracy: 64.1602%, Training Loss: 0.7390%\n",
      "Epoch [17/300], Step [129/225], Training Accuracy: 64.0988%, Training Loss: 0.7400%\n",
      "Epoch [17/300], Step [130/225], Training Accuracy: 64.0865%, Training Loss: 0.7403%\n",
      "Epoch [17/300], Step [131/225], Training Accuracy: 64.0864%, Training Loss: 0.7409%\n",
      "Epoch [17/300], Step [132/225], Training Accuracy: 64.0743%, Training Loss: 0.7415%\n",
      "Epoch [17/300], Step [133/225], Training Accuracy: 63.9685%, Training Loss: 0.7420%\n",
      "Epoch [17/300], Step [134/225], Training Accuracy: 63.8643%, Training Loss: 0.7438%\n",
      "Epoch [17/300], Step [135/225], Training Accuracy: 63.8310%, Training Loss: 0.7439%\n",
      "Epoch [17/300], Step [136/225], Training Accuracy: 63.8557%, Training Loss: 0.7435%\n",
      "Epoch [17/300], Step [137/225], Training Accuracy: 63.9028%, Training Loss: 0.7427%\n",
      "Epoch [17/300], Step [138/225], Training Accuracy: 63.9493%, Training Loss: 0.7425%\n",
      "Epoch [17/300], Step [139/225], Training Accuracy: 63.9051%, Training Loss: 0.7430%\n",
      "Epoch [17/300], Step [140/225], Training Accuracy: 63.9174%, Training Loss: 0.7426%\n",
      "Epoch [17/300], Step [141/225], Training Accuracy: 63.9295%, Training Loss: 0.7427%\n",
      "Epoch [17/300], Step [142/225], Training Accuracy: 63.9085%, Training Loss: 0.7424%\n",
      "Epoch [17/300], Step [143/225], Training Accuracy: 63.9532%, Training Loss: 0.7423%\n",
      "Epoch [17/300], Step [144/225], Training Accuracy: 63.9540%, Training Loss: 0.7429%\n",
      "Epoch [17/300], Step [145/225], Training Accuracy: 63.9978%, Training Loss: 0.7423%\n",
      "Epoch [17/300], Step [146/225], Training Accuracy: 63.9341%, Training Loss: 0.7428%\n",
      "Epoch [17/300], Step [147/225], Training Accuracy: 63.9137%, Training Loss: 0.7427%\n",
      "Epoch [17/300], Step [148/225], Training Accuracy: 63.9253%, Training Loss: 0.7427%\n",
      "Epoch [17/300], Step [149/225], Training Accuracy: 63.9052%, Training Loss: 0.7428%\n",
      "Epoch [17/300], Step [150/225], Training Accuracy: 63.8438%, Training Loss: 0.7429%\n",
      "Epoch [17/300], Step [151/225], Training Accuracy: 63.8969%, Training Loss: 0.7422%\n",
      "Epoch [17/300], Step [152/225], Training Accuracy: 63.8569%, Training Loss: 0.7425%\n",
      "Epoch [17/300], Step [153/225], Training Accuracy: 63.8276%, Training Loss: 0.7429%\n",
      "Epoch [17/300], Step [154/225], Training Accuracy: 63.8697%, Training Loss: 0.7429%\n",
      "Epoch [17/300], Step [155/225], Training Accuracy: 63.8810%, Training Loss: 0.7432%\n",
      "Epoch [17/300], Step [156/225], Training Accuracy: 63.8622%, Training Loss: 0.7433%\n",
      "Epoch [17/300], Step [157/225], Training Accuracy: 63.8635%, Training Loss: 0.7437%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Step [158/225], Training Accuracy: 63.7559%, Training Loss: 0.7446%\n",
      "Epoch [17/300], Step [159/225], Training Accuracy: 63.7186%, Training Loss: 0.7450%\n",
      "Epoch [17/300], Step [160/225], Training Accuracy: 63.6914%, Training Loss: 0.7447%\n",
      "Epoch [17/300], Step [161/225], Training Accuracy: 63.6840%, Training Loss: 0.7444%\n",
      "Epoch [17/300], Step [162/225], Training Accuracy: 63.7731%, Training Loss: 0.7435%\n",
      "Epoch [17/300], Step [163/225], Training Accuracy: 63.8037%, Training Loss: 0.7432%\n",
      "Epoch [17/300], Step [164/225], Training Accuracy: 63.8338%, Training Loss: 0.7421%\n",
      "Epoch [17/300], Step [165/225], Training Accuracy: 63.8163%, Training Loss: 0.7425%\n",
      "Epoch [17/300], Step [166/225], Training Accuracy: 63.8366%, Training Loss: 0.7423%\n",
      "Epoch [17/300], Step [167/225], Training Accuracy: 63.8286%, Training Loss: 0.7424%\n",
      "Epoch [17/300], Step [168/225], Training Accuracy: 63.8486%, Training Loss: 0.7423%\n",
      "Epoch [17/300], Step [169/225], Training Accuracy: 63.8591%, Training Loss: 0.7425%\n",
      "Epoch [17/300], Step [170/225], Training Accuracy: 63.8787%, Training Loss: 0.7428%\n",
      "Epoch [17/300], Step [171/225], Training Accuracy: 63.8706%, Training Loss: 0.7430%\n",
      "Epoch [17/300], Step [172/225], Training Accuracy: 63.8445%, Training Loss: 0.7430%\n",
      "Epoch [17/300], Step [173/225], Training Accuracy: 63.8548%, Training Loss: 0.7429%\n",
      "Epoch [17/300], Step [174/225], Training Accuracy: 63.8560%, Training Loss: 0.7428%\n",
      "Epoch [17/300], Step [175/225], Training Accuracy: 63.8839%, Training Loss: 0.7424%\n",
      "Epoch [17/300], Step [176/225], Training Accuracy: 63.8849%, Training Loss: 0.7422%\n",
      "Epoch [17/300], Step [177/225], Training Accuracy: 63.9213%, Training Loss: 0.7420%\n",
      "Epoch [17/300], Step [178/225], Training Accuracy: 63.9045%, Training Loss: 0.7419%\n",
      "Epoch [17/300], Step [179/225], Training Accuracy: 63.9316%, Training Loss: 0.7416%\n",
      "Epoch [17/300], Step [180/225], Training Accuracy: 63.9410%, Training Loss: 0.7410%\n",
      "Epoch [17/300], Step [181/225], Training Accuracy: 63.9330%, Training Loss: 0.7412%\n",
      "Epoch [17/300], Step [182/225], Training Accuracy: 63.9251%, Training Loss: 0.7413%\n",
      "Epoch [17/300], Step [183/225], Training Accuracy: 63.9515%, Training Loss: 0.7410%\n",
      "Epoch [17/300], Step [184/225], Training Accuracy: 63.9436%, Training Loss: 0.7410%\n",
      "Epoch [17/300], Step [185/225], Training Accuracy: 63.9865%, Training Loss: 0.7404%\n",
      "Epoch [17/300], Step [186/225], Training Accuracy: 64.0037%, Training Loss: 0.7400%\n",
      "Epoch [17/300], Step [187/225], Training Accuracy: 64.0040%, Training Loss: 0.7399%\n",
      "Epoch [17/300], Step [188/225], Training Accuracy: 64.0043%, Training Loss: 0.7396%\n",
      "Epoch [17/300], Step [189/225], Training Accuracy: 64.0460%, Training Loss: 0.7392%\n",
      "Epoch [17/300], Step [190/225], Training Accuracy: 64.0461%, Training Loss: 0.7391%\n",
      "Epoch [17/300], Step [191/225], Training Accuracy: 64.0298%, Training Loss: 0.7390%\n",
      "Epoch [17/300], Step [192/225], Training Accuracy: 64.0462%, Training Loss: 0.7386%\n",
      "Epoch [17/300], Step [193/225], Training Accuracy: 64.0382%, Training Loss: 0.7388%\n",
      "Epoch [17/300], Step [194/225], Training Accuracy: 64.0786%, Training Loss: 0.7386%\n",
      "Epoch [17/300], Step [195/225], Training Accuracy: 64.0625%, Training Loss: 0.7386%\n",
      "Epoch [17/300], Step [196/225], Training Accuracy: 64.0147%, Training Loss: 0.7390%\n",
      "Epoch [17/300], Step [197/225], Training Accuracy: 64.0308%, Training Loss: 0.7392%\n",
      "Epoch [17/300], Step [198/225], Training Accuracy: 64.0704%, Training Loss: 0.7381%\n",
      "Epoch [17/300], Step [199/225], Training Accuracy: 64.0704%, Training Loss: 0.7378%\n",
      "Epoch [17/300], Step [200/225], Training Accuracy: 64.0703%, Training Loss: 0.7376%\n",
      "Epoch [17/300], Step [201/225], Training Accuracy: 64.0780%, Training Loss: 0.7377%\n",
      "Epoch [17/300], Step [202/225], Training Accuracy: 64.1399%, Training Loss: 0.7372%\n",
      "Epoch [17/300], Step [203/225], Training Accuracy: 64.2010%, Training Loss: 0.7367%\n",
      "Epoch [17/300], Step [204/225], Training Accuracy: 64.2080%, Training Loss: 0.7367%\n",
      "Epoch [17/300], Step [205/225], Training Accuracy: 64.2759%, Training Loss: 0.7359%\n",
      "Epoch [17/300], Step [206/225], Training Accuracy: 64.2445%, Training Loss: 0.7363%\n",
      "Epoch [17/300], Step [207/225], Training Accuracy: 64.2210%, Training Loss: 0.7366%\n",
      "Epoch [17/300], Step [208/225], Training Accuracy: 64.2578%, Training Loss: 0.7360%\n",
      "Epoch [17/300], Step [209/225], Training Accuracy: 64.2344%, Training Loss: 0.7358%\n",
      "Epoch [17/300], Step [210/225], Training Accuracy: 64.2039%, Training Loss: 0.7362%\n",
      "Epoch [17/300], Step [211/225], Training Accuracy: 64.2550%, Training Loss: 0.7356%\n",
      "Epoch [17/300], Step [212/225], Training Accuracy: 64.2320%, Training Loss: 0.7358%\n",
      "Epoch [17/300], Step [213/225], Training Accuracy: 64.2312%, Training Loss: 0.7364%\n",
      "Epoch [17/300], Step [214/225], Training Accuracy: 64.2961%, Training Loss: 0.7357%\n",
      "Epoch [17/300], Step [215/225], Training Accuracy: 64.2660%, Training Loss: 0.7357%\n",
      "Epoch [17/300], Step [216/225], Training Accuracy: 64.2506%, Training Loss: 0.7359%\n",
      "Epoch [17/300], Step [217/225], Training Accuracy: 64.2209%, Training Loss: 0.7367%\n",
      "Epoch [17/300], Step [218/225], Training Accuracy: 64.1843%, Training Loss: 0.7368%\n",
      "Epoch [17/300], Step [219/225], Training Accuracy: 64.1909%, Training Loss: 0.7368%\n",
      "Epoch [17/300], Step [220/225], Training Accuracy: 64.1761%, Training Loss: 0.7369%\n",
      "Epoch [17/300], Step [221/225], Training Accuracy: 64.1473%, Training Loss: 0.7371%\n",
      "Epoch [17/300], Step [222/225], Training Accuracy: 64.1681%, Training Loss: 0.7368%\n",
      "Epoch [17/300], Step [223/225], Training Accuracy: 64.1186%, Training Loss: 0.7372%\n",
      "Epoch [17/300], Step [224/225], Training Accuracy: 64.0834%, Training Loss: 0.7371%\n",
      "Epoch [17/300], Step [225/225], Training Accuracy: 64.0495%, Training Loss: 0.7378%\n",
      "Epoch [18/300], Step [1/225], Training Accuracy: 60.9375%, Training Loss: 0.6763%\n",
      "Epoch [18/300], Step [2/225], Training Accuracy: 67.9688%, Training Loss: 0.6925%\n",
      "Epoch [18/300], Step [3/225], Training Accuracy: 65.1042%, Training Loss: 0.7401%\n",
      "Epoch [18/300], Step [4/225], Training Accuracy: 64.4531%, Training Loss: 0.7409%\n",
      "Epoch [18/300], Step [5/225], Training Accuracy: 65.9375%, Training Loss: 0.7215%\n",
      "Epoch [18/300], Step [6/225], Training Accuracy: 65.3646%, Training Loss: 0.7362%\n",
      "Epoch [18/300], Step [7/225], Training Accuracy: 66.0714%, Training Loss: 0.7375%\n",
      "Epoch [18/300], Step [8/225], Training Accuracy: 65.6250%, Training Loss: 0.7467%\n",
      "Epoch [18/300], Step [9/225], Training Accuracy: 64.5833%, Training Loss: 0.7580%\n",
      "Epoch [18/300], Step [10/225], Training Accuracy: 63.1250%, Training Loss: 0.7756%\n",
      "Epoch [18/300], Step [11/225], Training Accuracy: 63.2102%, Training Loss: 0.7768%\n",
      "Epoch [18/300], Step [12/225], Training Accuracy: 63.4115%, Training Loss: 0.7791%\n",
      "Epoch [18/300], Step [13/225], Training Accuracy: 64.0625%, Training Loss: 0.7667%\n",
      "Epoch [18/300], Step [14/225], Training Accuracy: 64.3973%, Training Loss: 0.7595%\n",
      "Epoch [18/300], Step [15/225], Training Accuracy: 64.2708%, Training Loss: 0.7582%\n",
      "Epoch [18/300], Step [16/225], Training Accuracy: 64.4531%, Training Loss: 0.7561%\n",
      "Epoch [18/300], Step [17/225], Training Accuracy: 64.7059%, Training Loss: 0.7532%\n",
      "Epoch [18/300], Step [18/225], Training Accuracy: 64.6701%, Training Loss: 0.7524%\n",
      "Epoch [18/300], Step [19/225], Training Accuracy: 65.1316%, Training Loss: 0.7509%\n",
      "Epoch [18/300], Step [20/225], Training Accuracy: 64.8438%, Training Loss: 0.7498%\n",
      "Epoch [18/300], Step [21/225], Training Accuracy: 65.2530%, Training Loss: 0.7423%\n",
      "Epoch [18/300], Step [22/225], Training Accuracy: 64.8438%, Training Loss: 0.7509%\n",
      "Epoch [18/300], Step [23/225], Training Accuracy: 65.2174%, Training Loss: 0.7441%\n",
      "Epoch [18/300], Step [24/225], Training Accuracy: 64.8438%, Training Loss: 0.7471%\n",
      "Epoch [18/300], Step [25/225], Training Accuracy: 64.9375%, Training Loss: 0.7456%\n",
      "Epoch [18/300], Step [26/225], Training Accuracy: 65.1442%, Training Loss: 0.7436%\n",
      "Epoch [18/300], Step [27/225], Training Accuracy: 65.2778%, Training Loss: 0.7435%\n",
      "Epoch [18/300], Step [28/225], Training Accuracy: 65.7366%, Training Loss: 0.7368%\n",
      "Epoch [18/300], Step [29/225], Training Accuracy: 65.6250%, Training Loss: 0.7423%\n",
      "Epoch [18/300], Step [30/225], Training Accuracy: 65.5208%, Training Loss: 0.7416%\n",
      "Epoch [18/300], Step [31/225], Training Accuracy: 65.2218%, Training Loss: 0.7483%\n",
      "Epoch [18/300], Step [32/225], Training Accuracy: 65.0391%, Training Loss: 0.7473%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Step [33/225], Training Accuracy: 65.1515%, Training Loss: 0.7436%\n",
      "Epoch [18/300], Step [34/225], Training Accuracy: 65.0276%, Training Loss: 0.7424%\n",
      "Epoch [18/300], Step [35/225], Training Accuracy: 65.2232%, Training Loss: 0.7405%\n",
      "Epoch [18/300], Step [36/225], Training Accuracy: 65.1910%, Training Loss: 0.7388%\n",
      "Epoch [18/300], Step [37/225], Training Accuracy: 65.1605%, Training Loss: 0.7374%\n",
      "Epoch [18/300], Step [38/225], Training Accuracy: 65.1316%, Training Loss: 0.7365%\n",
      "Epoch [18/300], Step [39/225], Training Accuracy: 65.1843%, Training Loss: 0.7383%\n",
      "Epoch [18/300], Step [40/225], Training Accuracy: 65.0781%, Training Loss: 0.7390%\n",
      "Epoch [18/300], Step [41/225], Training Accuracy: 65.0534%, Training Loss: 0.7384%\n",
      "Epoch [18/300], Step [42/225], Training Accuracy: 64.9926%, Training Loss: 0.7379%\n",
      "Epoch [18/300], Step [43/225], Training Accuracy: 65.0436%, Training Loss: 0.7375%\n",
      "Epoch [18/300], Step [44/225], Training Accuracy: 65.3054%, Training Loss: 0.7339%\n",
      "Epoch [18/300], Step [45/225], Training Accuracy: 65.3125%, Training Loss: 0.7323%\n",
      "Epoch [18/300], Step [46/225], Training Accuracy: 65.2174%, Training Loss: 0.7316%\n",
      "Epoch [18/300], Step [47/225], Training Accuracy: 64.9934%, Training Loss: 0.7326%\n",
      "Epoch [18/300], Step [48/225], Training Accuracy: 64.9089%, Training Loss: 0.7338%\n",
      "Epoch [18/300], Step [49/225], Training Accuracy: 64.9872%, Training Loss: 0.7335%\n",
      "Epoch [18/300], Step [50/225], Training Accuracy: 64.8125%, Training Loss: 0.7341%\n",
      "Epoch [18/300], Step [51/225], Training Accuracy: 65.0123%, Training Loss: 0.7336%\n",
      "Epoch [18/300], Step [52/225], Training Accuracy: 64.9639%, Training Loss: 0.7337%\n",
      "Epoch [18/300], Step [53/225], Training Accuracy: 64.7995%, Training Loss: 0.7343%\n",
      "Epoch [18/300], Step [54/225], Training Accuracy: 64.8148%, Training Loss: 0.7350%\n",
      "Epoch [18/300], Step [55/225], Training Accuracy: 64.6023%, Training Loss: 0.7376%\n",
      "Epoch [18/300], Step [56/225], Training Accuracy: 64.5647%, Training Loss: 0.7375%\n",
      "Epoch [18/300], Step [57/225], Training Accuracy: 64.6382%, Training Loss: 0.7355%\n",
      "Epoch [18/300], Step [58/225], Training Accuracy: 64.4935%, Training Loss: 0.7395%\n",
      "Epoch [18/300], Step [59/225], Training Accuracy: 64.5922%, Training Loss: 0.7374%\n",
      "Epoch [18/300], Step [60/225], Training Accuracy: 64.6875%, Training Loss: 0.7352%\n",
      "Epoch [18/300], Step [61/225], Training Accuracy: 64.7029%, Training Loss: 0.7350%\n",
      "Epoch [18/300], Step [62/225], Training Accuracy: 64.6421%, Training Loss: 0.7357%\n",
      "Epoch [18/300], Step [63/225], Training Accuracy: 64.5337%, Training Loss: 0.7374%\n",
      "Epoch [18/300], Step [64/225], Training Accuracy: 64.5996%, Training Loss: 0.7358%\n",
      "Epoch [18/300], Step [65/225], Training Accuracy: 64.6154%, Training Loss: 0.7370%\n",
      "Epoch [18/300], Step [66/225], Training Accuracy: 64.7017%, Training Loss: 0.7357%\n",
      "Epoch [18/300], Step [67/225], Training Accuracy: 64.6222%, Training Loss: 0.7368%\n",
      "Epoch [18/300], Step [68/225], Training Accuracy: 64.5221%, Training Loss: 0.7369%\n",
      "Epoch [18/300], Step [69/225], Training Accuracy: 64.4928%, Training Loss: 0.7369%\n",
      "Epoch [18/300], Step [70/225], Training Accuracy: 64.5312%, Training Loss: 0.7371%\n",
      "Epoch [18/300], Step [71/225], Training Accuracy: 64.5026%, Training Loss: 0.7365%\n",
      "Epoch [18/300], Step [72/225], Training Accuracy: 64.5182%, Training Loss: 0.7358%\n",
      "Epoch [18/300], Step [73/225], Training Accuracy: 64.4264%, Training Loss: 0.7352%\n",
      "Epoch [18/300], Step [74/225], Training Accuracy: 64.4637%, Training Loss: 0.7343%\n",
      "Epoch [18/300], Step [75/225], Training Accuracy: 64.5833%, Training Loss: 0.7331%\n",
      "Epoch [18/300], Step [76/225], Training Accuracy: 64.3709%, Training Loss: 0.7352%\n",
      "Epoch [18/300], Step [77/225], Training Accuracy: 64.4683%, Training Loss: 0.7342%\n",
      "Epoch [18/300], Step [78/225], Training Accuracy: 64.4231%, Training Loss: 0.7337%\n",
      "Epoch [18/300], Step [79/225], Training Accuracy: 64.4383%, Training Loss: 0.7332%\n",
      "Epoch [18/300], Step [80/225], Training Accuracy: 64.4336%, Training Loss: 0.7332%\n",
      "Epoch [18/300], Step [81/225], Training Accuracy: 64.5062%, Training Loss: 0.7329%\n",
      "Epoch [18/300], Step [82/225], Training Accuracy: 64.5770%, Training Loss: 0.7318%\n",
      "Epoch [18/300], Step [83/225], Training Accuracy: 64.6084%, Training Loss: 0.7307%\n",
      "Epoch [18/300], Step [84/225], Training Accuracy: 64.6205%, Training Loss: 0.7299%\n",
      "Epoch [18/300], Step [85/225], Training Accuracy: 64.7978%, Training Loss: 0.7282%\n",
      "Epoch [18/300], Step [86/225], Training Accuracy: 64.7892%, Training Loss: 0.7289%\n",
      "Epoch [18/300], Step [87/225], Training Accuracy: 64.6911%, Training Loss: 0.7302%\n",
      "Epoch [18/300], Step [88/225], Training Accuracy: 64.7727%, Training Loss: 0.7296%\n",
      "Epoch [18/300], Step [89/225], Training Accuracy: 64.6770%, Training Loss: 0.7305%\n",
      "Epoch [18/300], Step [90/225], Training Accuracy: 64.6701%, Training Loss: 0.7313%\n",
      "Epoch [18/300], Step [91/225], Training Accuracy: 64.4918%, Training Loss: 0.7327%\n",
      "Epoch [18/300], Step [92/225], Training Accuracy: 64.4361%, Training Loss: 0.7332%\n",
      "Epoch [18/300], Step [93/225], Training Accuracy: 64.4825%, Training Loss: 0.7325%\n",
      "Epoch [18/300], Step [94/225], Training Accuracy: 64.5778%, Training Loss: 0.7311%\n",
      "Epoch [18/300], Step [95/225], Training Accuracy: 64.5559%, Training Loss: 0.7310%\n",
      "Epoch [18/300], Step [96/225], Training Accuracy: 64.5833%, Training Loss: 0.7296%\n",
      "Epoch [18/300], Step [97/225], Training Accuracy: 64.6424%, Training Loss: 0.7289%\n",
      "Epoch [18/300], Step [98/225], Training Accuracy: 64.5886%, Training Loss: 0.7298%\n",
      "Epoch [18/300], Step [99/225], Training Accuracy: 64.5676%, Training Loss: 0.7306%\n",
      "Epoch [18/300], Step [100/225], Training Accuracy: 64.5469%, Training Loss: 0.7314%\n",
      "Epoch [18/300], Step [101/225], Training Accuracy: 64.6194%, Training Loss: 0.7319%\n",
      "Epoch [18/300], Step [102/225], Training Accuracy: 64.6293%, Training Loss: 0.7318%\n",
      "Epoch [18/300], Step [103/225], Training Accuracy: 64.6845%, Training Loss: 0.7307%\n",
      "Epoch [18/300], Step [104/225], Training Accuracy: 64.5733%, Training Loss: 0.7319%\n",
      "Epoch [18/300], Step [105/225], Training Accuracy: 64.5387%, Training Loss: 0.7312%\n",
      "Epoch [18/300], Step [106/225], Training Accuracy: 64.5489%, Training Loss: 0.7309%\n",
      "Epoch [18/300], Step [107/225], Training Accuracy: 64.5152%, Training Loss: 0.7316%\n",
      "Epoch [18/300], Step [108/225], Training Accuracy: 64.4965%, Training Loss: 0.7314%\n",
      "Epoch [18/300], Step [109/225], Training Accuracy: 64.4639%, Training Loss: 0.7308%\n",
      "Epoch [18/300], Step [110/225], Training Accuracy: 64.4886%, Training Loss: 0.7305%\n",
      "Epoch [18/300], Step [111/225], Training Accuracy: 64.5411%, Training Loss: 0.7291%\n",
      "Epoch [18/300], Step [112/225], Training Accuracy: 64.4671%, Training Loss: 0.7291%\n",
      "Epoch [18/300], Step [113/225], Training Accuracy: 64.4912%, Training Loss: 0.7286%\n",
      "Epoch [18/300], Step [114/225], Training Accuracy: 64.4737%, Training Loss: 0.7285%\n",
      "Epoch [18/300], Step [115/225], Training Accuracy: 64.4837%, Training Loss: 0.7288%\n",
      "Epoch [18/300], Step [116/225], Training Accuracy: 64.5070%, Training Loss: 0.7282%\n",
      "Epoch [18/300], Step [117/225], Training Accuracy: 64.4097%, Training Loss: 0.7295%\n",
      "Epoch [18/300], Step [118/225], Training Accuracy: 64.3935%, Training Loss: 0.7296%\n",
      "Epoch [18/300], Step [119/225], Training Accuracy: 64.4695%, Training Loss: 0.7290%\n",
      "Epoch [18/300], Step [120/225], Training Accuracy: 64.4531%, Training Loss: 0.7302%\n",
      "Epoch [18/300], Step [121/225], Training Accuracy: 64.4241%, Training Loss: 0.7307%\n",
      "Epoch [18/300], Step [122/225], Training Accuracy: 64.4467%, Training Loss: 0.7306%\n",
      "Epoch [18/300], Step [123/225], Training Accuracy: 64.4309%, Training Loss: 0.7306%\n",
      "Epoch [18/300], Step [124/225], Training Accuracy: 64.4531%, Training Loss: 0.7298%\n",
      "Epoch [18/300], Step [125/225], Training Accuracy: 64.4375%, Training Loss: 0.7303%\n",
      "Epoch [18/300], Step [126/225], Training Accuracy: 64.4345%, Training Loss: 0.7307%\n",
      "Epoch [18/300], Step [127/225], Training Accuracy: 64.4070%, Training Loss: 0.7318%\n",
      "Epoch [18/300], Step [128/225], Training Accuracy: 64.3433%, Training Loss: 0.7333%\n",
      "Epoch [18/300], Step [129/225], Training Accuracy: 64.3532%, Training Loss: 0.7337%\n",
      "Epoch [18/300], Step [130/225], Training Accuracy: 64.3269%, Training Loss: 0.7343%\n",
      "Epoch [18/300], Step [131/225], Training Accuracy: 64.3488%, Training Loss: 0.7346%\n",
      "Epoch [18/300], Step [132/225], Training Accuracy: 64.3939%, Training Loss: 0.7349%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Step [133/225], Training Accuracy: 64.3210%, Training Loss: 0.7354%\n",
      "Epoch [18/300], Step [134/225], Training Accuracy: 64.2257%, Training Loss: 0.7375%\n",
      "Epoch [18/300], Step [135/225], Training Accuracy: 64.1782%, Training Loss: 0.7377%\n",
      "Epoch [18/300], Step [136/225], Training Accuracy: 64.2233%, Training Loss: 0.7369%\n",
      "Epoch [18/300], Step [137/225], Training Accuracy: 64.2792%, Training Loss: 0.7367%\n",
      "Epoch [18/300], Step [138/225], Training Accuracy: 64.3682%, Training Loss: 0.7354%\n",
      "Epoch [18/300], Step [139/225], Training Accuracy: 64.3548%, Training Loss: 0.7355%\n",
      "Epoch [18/300], Step [140/225], Training Accuracy: 64.3638%, Training Loss: 0.7352%\n",
      "Epoch [18/300], Step [141/225], Training Accuracy: 64.3506%, Training Loss: 0.7353%\n",
      "Epoch [18/300], Step [142/225], Training Accuracy: 64.3816%, Training Loss: 0.7348%\n",
      "Epoch [18/300], Step [143/225], Training Accuracy: 64.4668%, Training Loss: 0.7343%\n",
      "Epoch [18/300], Step [144/225], Training Accuracy: 64.4206%, Training Loss: 0.7342%\n",
      "Epoch [18/300], Step [145/225], Training Accuracy: 64.4612%, Training Loss: 0.7337%\n",
      "Epoch [18/300], Step [146/225], Training Accuracy: 64.4371%, Training Loss: 0.7338%\n",
      "Epoch [18/300], Step [147/225], Training Accuracy: 64.3814%, Training Loss: 0.7346%\n",
      "Epoch [18/300], Step [148/225], Training Accuracy: 64.4426%, Training Loss: 0.7341%\n",
      "Epoch [18/300], Step [149/225], Training Accuracy: 64.4190%, Training Loss: 0.7344%\n",
      "Epoch [18/300], Step [150/225], Training Accuracy: 64.4375%, Training Loss: 0.7338%\n",
      "Epoch [18/300], Step [151/225], Training Accuracy: 64.4557%, Training Loss: 0.7337%\n",
      "Epoch [18/300], Step [152/225], Training Accuracy: 64.4634%, Training Loss: 0.7340%\n",
      "Epoch [18/300], Step [153/225], Training Accuracy: 64.4608%, Training Loss: 0.7344%\n",
      "Epoch [18/300], Step [154/225], Training Accuracy: 64.4886%, Training Loss: 0.7342%\n",
      "Epoch [18/300], Step [155/225], Training Accuracy: 64.4153%, Training Loss: 0.7346%\n",
      "Epoch [18/300], Step [156/225], Training Accuracy: 64.4030%, Training Loss: 0.7348%\n",
      "Epoch [18/300], Step [157/225], Training Accuracy: 64.3810%, Training Loss: 0.7357%\n",
      "Epoch [18/300], Step [158/225], Training Accuracy: 64.2998%, Training Loss: 0.7366%\n",
      "Epoch [18/300], Step [159/225], Training Accuracy: 64.2394%, Training Loss: 0.7375%\n",
      "Epoch [18/300], Step [160/225], Training Accuracy: 64.2480%, Training Loss: 0.7375%\n",
      "Epoch [18/300], Step [161/225], Training Accuracy: 64.2469%, Training Loss: 0.7377%\n",
      "Epoch [18/300], Step [162/225], Training Accuracy: 64.3133%, Training Loss: 0.7369%\n",
      "Epoch [18/300], Step [163/225], Training Accuracy: 64.2830%, Training Loss: 0.7368%\n",
      "Epoch [18/300], Step [164/225], Training Accuracy: 64.3293%, Training Loss: 0.7364%\n",
      "Epoch [18/300], Step [165/225], Training Accuracy: 64.3655%, Training Loss: 0.7360%\n",
      "Epoch [18/300], Step [166/225], Training Accuracy: 64.4202%, Training Loss: 0.7357%\n",
      "Epoch [18/300], Step [167/225], Training Accuracy: 64.4087%, Training Loss: 0.7357%\n",
      "Epoch [18/300], Step [168/225], Training Accuracy: 64.3880%, Training Loss: 0.7362%\n",
      "Epoch [18/300], Step [169/225], Training Accuracy: 64.3953%, Training Loss: 0.7361%\n",
      "Epoch [18/300], Step [170/225], Training Accuracy: 64.3566%, Training Loss: 0.7365%\n",
      "Epoch [18/300], Step [171/225], Training Accuracy: 64.3275%, Training Loss: 0.7369%\n",
      "Epoch [18/300], Step [172/225], Training Accuracy: 64.2805%, Training Loss: 0.7373%\n",
      "Epoch [18/300], Step [173/225], Training Accuracy: 64.3154%, Training Loss: 0.7368%\n",
      "Epoch [18/300], Step [174/225], Training Accuracy: 64.3139%, Training Loss: 0.7369%\n",
      "Epoch [18/300], Step [175/225], Training Accuracy: 64.3661%, Training Loss: 0.7362%\n",
      "Epoch [18/300], Step [176/225], Training Accuracy: 64.3910%, Training Loss: 0.7359%\n",
      "Epoch [18/300], Step [177/225], Training Accuracy: 64.4244%, Training Loss: 0.7358%\n",
      "Epoch [18/300], Step [178/225], Training Accuracy: 64.4312%, Training Loss: 0.7355%\n",
      "Epoch [18/300], Step [179/225], Training Accuracy: 64.4728%, Training Loss: 0.7351%\n",
      "Epoch [18/300], Step [180/225], Training Accuracy: 64.4792%, Training Loss: 0.7351%\n",
      "Epoch [18/300], Step [181/225], Training Accuracy: 64.4510%, Training Loss: 0.7356%\n",
      "Epoch [18/300], Step [182/225], Training Accuracy: 64.4660%, Training Loss: 0.7359%\n",
      "Epoch [18/300], Step [183/225], Training Accuracy: 64.4638%, Training Loss: 0.7359%\n",
      "Epoch [18/300], Step [184/225], Training Accuracy: 64.4701%, Training Loss: 0.7357%\n",
      "Epoch [18/300], Step [185/225], Training Accuracy: 64.5186%, Training Loss: 0.7351%\n",
      "Epoch [18/300], Step [186/225], Training Accuracy: 64.5665%, Training Loss: 0.7344%\n",
      "Epoch [18/300], Step [187/225], Training Accuracy: 64.5137%, Training Loss: 0.7348%\n",
      "Epoch [18/300], Step [188/225], Training Accuracy: 64.5362%, Training Loss: 0.7342%\n",
      "Epoch [18/300], Step [189/225], Training Accuracy: 64.5999%, Training Loss: 0.7335%\n",
      "Epoch [18/300], Step [190/225], Training Accuracy: 64.6135%, Training Loss: 0.7334%\n",
      "Epoch [18/300], Step [191/225], Training Accuracy: 64.6106%, Training Loss: 0.7333%\n",
      "Epoch [18/300], Step [192/225], Training Accuracy: 64.6403%, Training Loss: 0.7329%\n",
      "Epoch [18/300], Step [193/225], Training Accuracy: 64.6454%, Training Loss: 0.7332%\n",
      "Epoch [18/300], Step [194/225], Training Accuracy: 64.6263%, Training Loss: 0.7332%\n",
      "Epoch [18/300], Step [195/225], Training Accuracy: 64.6314%, Training Loss: 0.7330%\n",
      "Epoch [18/300], Step [196/225], Training Accuracy: 64.5807%, Training Loss: 0.7333%\n",
      "Epoch [18/300], Step [197/225], Training Accuracy: 64.5939%, Training Loss: 0.7330%\n",
      "Epoch [18/300], Step [198/225], Training Accuracy: 64.6386%, Training Loss: 0.7323%\n",
      "Epoch [18/300], Step [199/225], Training Accuracy: 64.6749%, Training Loss: 0.7318%\n",
      "Epoch [18/300], Step [200/225], Training Accuracy: 64.6328%, Training Loss: 0.7321%\n",
      "Epoch [18/300], Step [201/225], Training Accuracy: 64.5678%, Training Loss: 0.7327%\n",
      "Epoch [18/300], Step [202/225], Training Accuracy: 64.6040%, Training Loss: 0.7325%\n",
      "Epoch [18/300], Step [203/225], Training Accuracy: 64.6475%, Training Loss: 0.7325%\n",
      "Epoch [18/300], Step [204/225], Training Accuracy: 64.6446%, Training Loss: 0.7324%\n",
      "Epoch [18/300], Step [205/225], Training Accuracy: 64.6951%, Training Loss: 0.7320%\n",
      "Epoch [18/300], Step [206/225], Training Accuracy: 64.6769%, Training Loss: 0.7322%\n",
      "Epoch [18/300], Step [207/225], Training Accuracy: 64.6664%, Training Loss: 0.7323%\n",
      "Epoch [18/300], Step [208/225], Training Accuracy: 64.7085%, Training Loss: 0.7315%\n",
      "Epoch [18/300], Step [209/225], Training Accuracy: 64.7054%, Training Loss: 0.7316%\n",
      "Epoch [18/300], Step [210/225], Training Accuracy: 64.6801%, Training Loss: 0.7318%\n",
      "Epoch [18/300], Step [211/225], Training Accuracy: 64.6919%, Training Loss: 0.7316%\n",
      "Epoch [18/300], Step [212/225], Training Accuracy: 64.7111%, Training Loss: 0.7318%\n",
      "Epoch [18/300], Step [213/225], Training Accuracy: 64.7374%, Training Loss: 0.7320%\n",
      "Epoch [18/300], Step [214/225], Training Accuracy: 64.7561%, Training Loss: 0.7321%\n",
      "Epoch [18/300], Step [215/225], Training Accuracy: 64.7602%, Training Loss: 0.7322%\n",
      "Epoch [18/300], Step [216/225], Training Accuracy: 64.7569%, Training Loss: 0.7325%\n",
      "Epoch [18/300], Step [217/225], Training Accuracy: 64.7393%, Training Loss: 0.7329%\n",
      "Epoch [18/300], Step [218/225], Training Accuracy: 64.7291%, Training Loss: 0.7329%\n",
      "Epoch [18/300], Step [219/225], Training Accuracy: 64.7189%, Training Loss: 0.7327%\n",
      "Epoch [18/300], Step [220/225], Training Accuracy: 64.6946%, Training Loss: 0.7333%\n",
      "Epoch [18/300], Step [221/225], Training Accuracy: 64.6635%, Training Loss: 0.7335%\n",
      "Epoch [18/300], Step [222/225], Training Accuracy: 64.6326%, Training Loss: 0.7335%\n",
      "Epoch [18/300], Step [223/225], Training Accuracy: 64.5670%, Training Loss: 0.7348%\n",
      "Epoch [18/300], Step [224/225], Training Accuracy: 64.5578%, Training Loss: 0.7349%\n",
      "Epoch [18/300], Step [225/225], Training Accuracy: 64.5636%, Training Loss: 0.7351%\n",
      "Epoch [19/300], Step [1/225], Training Accuracy: 68.7500%, Training Loss: 0.6517%\n",
      "Epoch [19/300], Step [2/225], Training Accuracy: 68.7500%, Training Loss: 0.6828%\n",
      "Epoch [19/300], Step [3/225], Training Accuracy: 69.7917%, Training Loss: 0.6901%\n",
      "Epoch [19/300], Step [4/225], Training Accuracy: 67.1875%, Training Loss: 0.7089%\n",
      "Epoch [19/300], Step [5/225], Training Accuracy: 67.8125%, Training Loss: 0.7000%\n",
      "Epoch [19/300], Step [6/225], Training Accuracy: 66.4062%, Training Loss: 0.7258%\n",
      "Epoch [19/300], Step [7/225], Training Accuracy: 68.3036%, Training Loss: 0.7137%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [8/225], Training Accuracy: 67.5781%, Training Loss: 0.7228%\n",
      "Epoch [19/300], Step [9/225], Training Accuracy: 66.1458%, Training Loss: 0.7403%\n",
      "Epoch [19/300], Step [10/225], Training Accuracy: 65.7812%, Training Loss: 0.7487%\n",
      "Epoch [19/300], Step [11/225], Training Accuracy: 66.0511%, Training Loss: 0.7430%\n",
      "Epoch [19/300], Step [12/225], Training Accuracy: 65.7552%, Training Loss: 0.7477%\n",
      "Epoch [19/300], Step [13/225], Training Accuracy: 66.2260%, Training Loss: 0.7365%\n",
      "Epoch [19/300], Step [14/225], Training Accuracy: 66.1830%, Training Loss: 0.7318%\n",
      "Epoch [19/300], Step [15/225], Training Accuracy: 66.5625%, Training Loss: 0.7299%\n",
      "Epoch [19/300], Step [16/225], Training Accuracy: 66.3086%, Training Loss: 0.7296%\n",
      "Epoch [19/300], Step [17/225], Training Accuracy: 66.5441%, Training Loss: 0.7245%\n",
      "Epoch [19/300], Step [18/225], Training Accuracy: 66.4931%, Training Loss: 0.7253%\n",
      "Epoch [19/300], Step [19/225], Training Accuracy: 66.6941%, Training Loss: 0.7246%\n",
      "Epoch [19/300], Step [20/225], Training Accuracy: 66.6406%, Training Loss: 0.7195%\n",
      "Epoch [19/300], Step [21/225], Training Accuracy: 67.1131%, Training Loss: 0.7108%\n",
      "Epoch [19/300], Step [22/225], Training Accuracy: 66.1932%, Training Loss: 0.7194%\n",
      "Epoch [19/300], Step [23/225], Training Accuracy: 66.7799%, Training Loss: 0.7134%\n",
      "Epoch [19/300], Step [24/225], Training Accuracy: 66.1458%, Training Loss: 0.7194%\n",
      "Epoch [19/300], Step [25/225], Training Accuracy: 66.1875%, Training Loss: 0.7177%\n",
      "Epoch [19/300], Step [26/225], Training Accuracy: 66.0457%, Training Loss: 0.7176%\n",
      "Epoch [19/300], Step [27/225], Training Accuracy: 65.7407%, Training Loss: 0.7183%\n",
      "Epoch [19/300], Step [28/225], Training Accuracy: 66.1830%, Training Loss: 0.7108%\n",
      "Epoch [19/300], Step [29/225], Training Accuracy: 66.1638%, Training Loss: 0.7100%\n",
      "Epoch [19/300], Step [30/225], Training Accuracy: 66.3542%, Training Loss: 0.7071%\n",
      "Epoch [19/300], Step [31/225], Training Accuracy: 66.0282%, Training Loss: 0.7159%\n",
      "Epoch [19/300], Step [32/225], Training Accuracy: 65.8203%, Training Loss: 0.7178%\n",
      "Epoch [19/300], Step [33/225], Training Accuracy: 65.9091%, Training Loss: 0.7133%\n",
      "Epoch [19/300], Step [34/225], Training Accuracy: 65.8548%, Training Loss: 0.7123%\n",
      "Epoch [19/300], Step [35/225], Training Accuracy: 65.7143%, Training Loss: 0.7121%\n",
      "Epoch [19/300], Step [36/225], Training Accuracy: 65.8854%, Training Loss: 0.7104%\n",
      "Epoch [19/300], Step [37/225], Training Accuracy: 65.9628%, Training Loss: 0.7109%\n",
      "Epoch [19/300], Step [38/225], Training Accuracy: 65.8717%, Training Loss: 0.7137%\n",
      "Epoch [19/300], Step [39/225], Training Accuracy: 65.7853%, Training Loss: 0.7137%\n",
      "Epoch [19/300], Step [40/225], Training Accuracy: 65.7422%, Training Loss: 0.7139%\n",
      "Epoch [19/300], Step [41/225], Training Accuracy: 65.8537%, Training Loss: 0.7165%\n",
      "Epoch [19/300], Step [42/225], Training Accuracy: 65.8854%, Training Loss: 0.7159%\n",
      "Epoch [19/300], Step [43/225], Training Accuracy: 65.9520%, Training Loss: 0.7157%\n",
      "Epoch [19/300], Step [44/225], Training Accuracy: 66.1222%, Training Loss: 0.7128%\n",
      "Epoch [19/300], Step [45/225], Training Accuracy: 66.1458%, Training Loss: 0.7104%\n",
      "Epoch [19/300], Step [46/225], Training Accuracy: 66.1345%, Training Loss: 0.7090%\n",
      "Epoch [19/300], Step [47/225], Training Accuracy: 65.9242%, Training Loss: 0.7091%\n",
      "Epoch [19/300], Step [48/225], Training Accuracy: 65.7878%, Training Loss: 0.7101%\n",
      "Epoch [19/300], Step [49/225], Training Accuracy: 65.8801%, Training Loss: 0.7084%\n",
      "Epoch [19/300], Step [50/225], Training Accuracy: 66.0625%, Training Loss: 0.7085%\n",
      "Epoch [19/300], Step [51/225], Training Accuracy: 66.2377%, Training Loss: 0.7073%\n",
      "Epoch [19/300], Step [52/225], Training Accuracy: 66.2260%, Training Loss: 0.7073%\n",
      "Epoch [19/300], Step [53/225], Training Accuracy: 66.3031%, Training Loss: 0.7070%\n",
      "Epoch [19/300], Step [54/225], Training Accuracy: 66.2905%, Training Loss: 0.7077%\n",
      "Epoch [19/300], Step [55/225], Training Accuracy: 66.1080%, Training Loss: 0.7100%\n",
      "Epoch [19/300], Step [56/225], Training Accuracy: 65.9040%, Training Loss: 0.7107%\n",
      "Epoch [19/300], Step [57/225], Training Accuracy: 65.8717%, Training Loss: 0.7092%\n",
      "Epoch [19/300], Step [58/225], Training Accuracy: 65.8136%, Training Loss: 0.7109%\n",
      "Epoch [19/300], Step [59/225], Training Accuracy: 66.0222%, Training Loss: 0.7087%\n",
      "Epoch [19/300], Step [60/225], Training Accuracy: 66.1719%, Training Loss: 0.7071%\n",
      "Epoch [19/300], Step [61/225], Training Accuracy: 66.0348%, Training Loss: 0.7086%\n",
      "Epoch [19/300], Step [62/225], Training Accuracy: 66.0786%, Training Loss: 0.7087%\n",
      "Epoch [19/300], Step [63/225], Training Accuracy: 65.9970%, Training Loss: 0.7108%\n",
      "Epoch [19/300], Step [64/225], Training Accuracy: 66.1133%, Training Loss: 0.7093%\n",
      "Epoch [19/300], Step [65/225], Training Accuracy: 66.0577%, Training Loss: 0.7103%\n",
      "Epoch [19/300], Step [66/225], Training Accuracy: 66.2169%, Training Loss: 0.7087%\n",
      "Epoch [19/300], Step [67/225], Training Accuracy: 66.1614%, Training Loss: 0.7091%\n",
      "Epoch [19/300], Step [68/225], Training Accuracy: 66.0616%, Training Loss: 0.7095%\n",
      "Epoch [19/300], Step [69/225], Training Accuracy: 66.0100%, Training Loss: 0.7100%\n",
      "Epoch [19/300], Step [70/225], Training Accuracy: 65.9598%, Training Loss: 0.7102%\n",
      "Epoch [19/300], Step [71/225], Training Accuracy: 65.9771%, Training Loss: 0.7093%\n",
      "Epoch [19/300], Step [72/225], Training Accuracy: 65.9288%, Training Loss: 0.7090%\n",
      "Epoch [19/300], Step [73/225], Training Accuracy: 65.8604%, Training Loss: 0.7085%\n",
      "Epoch [19/300], Step [74/225], Training Accuracy: 65.7939%, Training Loss: 0.7080%\n",
      "Epoch [19/300], Step [75/225], Training Accuracy: 65.9167%, Training Loss: 0.7070%\n",
      "Epoch [19/300], Step [76/225], Training Accuracy: 65.7895%, Training Loss: 0.7090%\n",
      "Epoch [19/300], Step [77/225], Training Accuracy: 65.8685%, Training Loss: 0.7079%\n",
      "Epoch [19/300], Step [78/225], Training Accuracy: 65.7853%, Training Loss: 0.7071%\n",
      "Epoch [19/300], Step [79/225], Training Accuracy: 65.8030%, Training Loss: 0.7061%\n",
      "Epoch [19/300], Step [80/225], Training Accuracy: 65.8398%, Training Loss: 0.7060%\n",
      "Epoch [19/300], Step [81/225], Training Accuracy: 65.8758%, Training Loss: 0.7063%\n",
      "Epoch [19/300], Step [82/225], Training Accuracy: 65.8537%, Training Loss: 0.7059%\n",
      "Epoch [19/300], Step [83/225], Training Accuracy: 65.9262%, Training Loss: 0.7050%\n",
      "Epoch [19/300], Step [84/225], Training Accuracy: 65.9412%, Training Loss: 0.7044%\n",
      "Epoch [19/300], Step [85/225], Training Accuracy: 65.9926%, Training Loss: 0.7029%\n",
      "Epoch [19/300], Step [86/225], Training Accuracy: 66.0247%, Training Loss: 0.7031%\n",
      "Epoch [19/300], Step [87/225], Training Accuracy: 66.0920%, Training Loss: 0.7030%\n",
      "Epoch [19/300], Step [88/225], Training Accuracy: 66.1222%, Training Loss: 0.7031%\n",
      "Epoch [19/300], Step [89/225], Training Accuracy: 66.1166%, Training Loss: 0.7038%\n",
      "Epoch [19/300], Step [90/225], Training Accuracy: 66.1111%, Training Loss: 0.7046%\n",
      "Epoch [19/300], Step [91/225], Training Accuracy: 66.0027%, Training Loss: 0.7059%\n",
      "Epoch [19/300], Step [92/225], Training Accuracy: 65.9986%, Training Loss: 0.7065%\n",
      "Epoch [19/300], Step [93/225], Training Accuracy: 66.0954%, Training Loss: 0.7056%\n",
      "Epoch [19/300], Step [94/225], Training Accuracy: 66.1237%, Training Loss: 0.7049%\n",
      "Epoch [19/300], Step [95/225], Training Accuracy: 66.2007%, Training Loss: 0.7054%\n",
      "Epoch [19/300], Step [96/225], Training Accuracy: 66.2272%, Training Loss: 0.7043%\n",
      "Epoch [19/300], Step [97/225], Training Accuracy: 66.2693%, Training Loss: 0.7039%\n",
      "Epoch [19/300], Step [98/225], Training Accuracy: 66.1990%, Training Loss: 0.7051%\n",
      "Epoch [19/300], Step [99/225], Training Accuracy: 66.1616%, Training Loss: 0.7064%\n",
      "Epoch [19/300], Step [100/225], Training Accuracy: 66.1719%, Training Loss: 0.7080%\n",
      "Epoch [19/300], Step [101/225], Training Accuracy: 66.2283%, Training Loss: 0.7087%\n",
      "Epoch [19/300], Step [102/225], Training Accuracy: 66.1152%, Training Loss: 0.7090%\n",
      "Epoch [19/300], Step [103/225], Training Accuracy: 66.1711%, Training Loss: 0.7084%\n",
      "Epoch [19/300], Step [104/225], Training Accuracy: 66.0757%, Training Loss: 0.7095%\n",
      "Epoch [19/300], Step [105/225], Training Accuracy: 66.0863%, Training Loss: 0.7087%\n",
      "Epoch [19/300], Step [106/225], Training Accuracy: 66.0377%, Training Loss: 0.7088%\n",
      "Epoch [19/300], Step [107/225], Training Accuracy: 66.0485%, Training Loss: 0.7094%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [108/225], Training Accuracy: 66.0735%, Training Loss: 0.7093%\n",
      "Epoch [19/300], Step [109/225], Training Accuracy: 66.0407%, Training Loss: 0.7088%\n",
      "Epoch [19/300], Step [110/225], Training Accuracy: 66.0369%, Training Loss: 0.7083%\n",
      "Epoch [19/300], Step [111/225], Training Accuracy: 66.0473%, Training Loss: 0.7073%\n",
      "Epoch [19/300], Step [112/225], Training Accuracy: 66.0714%, Training Loss: 0.7074%\n",
      "Epoch [19/300], Step [113/225], Training Accuracy: 66.0675%, Training Loss: 0.7072%\n",
      "Epoch [19/300], Step [114/225], Training Accuracy: 66.1047%, Training Loss: 0.7065%\n",
      "Epoch [19/300], Step [115/225], Training Accuracy: 66.1005%, Training Loss: 0.7066%\n",
      "Epoch [19/300], Step [116/225], Training Accuracy: 66.1234%, Training Loss: 0.7064%\n",
      "Epoch [19/300], Step [117/225], Training Accuracy: 66.0791%, Training Loss: 0.7075%\n",
      "Epoch [19/300], Step [118/225], Training Accuracy: 66.0222%, Training Loss: 0.7080%\n",
      "Epoch [19/300], Step [119/225], Training Accuracy: 66.0452%, Training Loss: 0.7075%\n",
      "Epoch [19/300], Step [120/225], Training Accuracy: 66.0156%, Training Loss: 0.7083%\n",
      "Epoch [19/300], Step [121/225], Training Accuracy: 65.9349%, Training Loss: 0.7089%\n",
      "Epoch [19/300], Step [122/225], Training Accuracy: 65.9580%, Training Loss: 0.7090%\n",
      "Epoch [19/300], Step [123/225], Training Accuracy: 65.9807%, Training Loss: 0.7087%\n",
      "Epoch [19/300], Step [124/225], Training Accuracy: 65.9400%, Training Loss: 0.7084%\n",
      "Epoch [19/300], Step [125/225], Training Accuracy: 65.9000%, Training Loss: 0.7091%\n",
      "Epoch [19/300], Step [126/225], Training Accuracy: 65.8606%, Training Loss: 0.7097%\n",
      "Epoch [19/300], Step [127/225], Training Accuracy: 65.7480%, Training Loss: 0.7115%\n",
      "Epoch [19/300], Step [128/225], Training Accuracy: 65.6494%, Training Loss: 0.7128%\n",
      "Epoch [19/300], Step [129/225], Training Accuracy: 65.6250%, Training Loss: 0.7138%\n",
      "Epoch [19/300], Step [130/225], Training Accuracy: 65.5649%, Training Loss: 0.7147%\n",
      "Epoch [19/300], Step [131/225], Training Accuracy: 65.5654%, Training Loss: 0.7148%\n",
      "Epoch [19/300], Step [132/225], Training Accuracy: 65.5421%, Training Loss: 0.7149%\n",
      "Epoch [19/300], Step [133/225], Training Accuracy: 65.4840%, Training Loss: 0.7151%\n",
      "Epoch [19/300], Step [134/225], Training Accuracy: 65.4035%, Training Loss: 0.7173%\n",
      "Epoch [19/300], Step [135/225], Training Accuracy: 65.3588%, Training Loss: 0.7176%\n",
      "Epoch [19/300], Step [136/225], Training Accuracy: 65.3608%, Training Loss: 0.7178%\n",
      "Epoch [19/300], Step [137/225], Training Accuracy: 65.3513%, Training Loss: 0.7177%\n",
      "Epoch [19/300], Step [138/225], Training Accuracy: 65.4325%, Training Loss: 0.7169%\n",
      "Epoch [19/300], Step [139/225], Training Accuracy: 65.3440%, Training Loss: 0.7174%\n",
      "Epoch [19/300], Step [140/225], Training Accuracy: 65.3906%, Training Loss: 0.7170%\n",
      "Epoch [19/300], Step [141/225], Training Accuracy: 65.3701%, Training Loss: 0.7171%\n",
      "Epoch [19/300], Step [142/225], Training Accuracy: 65.3609%, Training Loss: 0.7168%\n",
      "Epoch [19/300], Step [143/225], Training Accuracy: 65.3955%, Training Loss: 0.7166%\n",
      "Epoch [19/300], Step [144/225], Training Accuracy: 65.3537%, Training Loss: 0.7167%\n",
      "Epoch [19/300], Step [145/225], Training Accuracy: 65.4418%, Training Loss: 0.7170%\n",
      "Epoch [19/300], Step [146/225], Training Accuracy: 65.4752%, Training Loss: 0.7171%\n",
      "Epoch [19/300], Step [147/225], Training Accuracy: 65.4337%, Training Loss: 0.7175%\n",
      "Epoch [19/300], Step [148/225], Training Accuracy: 65.4772%, Training Loss: 0.7173%\n",
      "Epoch [19/300], Step [149/225], Training Accuracy: 65.4677%, Training Loss: 0.7172%\n",
      "Epoch [19/300], Step [150/225], Training Accuracy: 65.4896%, Training Loss: 0.7167%\n",
      "Epoch [19/300], Step [151/225], Training Accuracy: 65.5526%, Training Loss: 0.7162%\n",
      "Epoch [19/300], Step [152/225], Training Accuracy: 65.5119%, Training Loss: 0.7175%\n",
      "Epoch [19/300], Step [153/225], Training Accuracy: 65.4718%, Training Loss: 0.7184%\n",
      "Epoch [19/300], Step [154/225], Training Accuracy: 65.4728%, Training Loss: 0.7183%\n",
      "Epoch [19/300], Step [155/225], Training Accuracy: 65.3931%, Training Loss: 0.7184%\n",
      "Epoch [19/300], Step [156/225], Training Accuracy: 65.3846%, Training Loss: 0.7187%\n",
      "Epoch [19/300], Step [157/225], Training Accuracy: 65.3861%, Training Loss: 0.7189%\n",
      "Epoch [19/300], Step [158/225], Training Accuracy: 65.2987%, Training Loss: 0.7199%\n",
      "Epoch [19/300], Step [159/225], Training Accuracy: 65.2614%, Training Loss: 0.7206%\n",
      "Epoch [19/300], Step [160/225], Training Accuracy: 65.3125%, Training Loss: 0.7199%\n",
      "Epoch [19/300], Step [161/225], Training Accuracy: 65.2465%, Training Loss: 0.7200%\n",
      "Epoch [19/300], Step [162/225], Training Accuracy: 65.2778%, Training Loss: 0.7195%\n",
      "Epoch [19/300], Step [163/225], Training Accuracy: 65.2703%, Training Loss: 0.7193%\n",
      "Epoch [19/300], Step [164/225], Training Accuracy: 65.3392%, Training Loss: 0.7184%\n",
      "Epoch [19/300], Step [165/225], Training Accuracy: 65.3314%, Training Loss: 0.7184%\n",
      "Epoch [19/300], Step [166/225], Training Accuracy: 65.3709%, Training Loss: 0.7180%\n",
      "Epoch [19/300], Step [167/225], Training Accuracy: 65.3537%, Training Loss: 0.7183%\n",
      "Epoch [19/300], Step [168/225], Training Accuracy: 65.3553%, Training Loss: 0.7184%\n",
      "Epoch [19/300], Step [169/225], Training Accuracy: 65.3476%, Training Loss: 0.7185%\n",
      "Epoch [19/300], Step [170/225], Training Accuracy: 65.2941%, Training Loss: 0.7187%\n",
      "Epoch [19/300], Step [171/225], Training Accuracy: 65.2961%, Training Loss: 0.7187%\n",
      "Epoch [19/300], Step [172/225], Training Accuracy: 65.2616%, Training Loss: 0.7192%\n",
      "Epoch [19/300], Step [173/225], Training Accuracy: 65.2818%, Training Loss: 0.7189%\n",
      "Epoch [19/300], Step [174/225], Training Accuracy: 65.3107%, Training Loss: 0.7189%\n",
      "Epoch [19/300], Step [175/225], Training Accuracy: 65.3482%, Training Loss: 0.7185%\n",
      "Epoch [19/300], Step [176/225], Training Accuracy: 65.3764%, Training Loss: 0.7178%\n",
      "Epoch [19/300], Step [177/225], Training Accuracy: 65.4220%, Training Loss: 0.7177%\n",
      "Epoch [19/300], Step [178/225], Training Accuracy: 65.4319%, Training Loss: 0.7172%\n",
      "Epoch [19/300], Step [179/225], Training Accuracy: 65.4679%, Training Loss: 0.7170%\n",
      "Epoch [19/300], Step [180/225], Training Accuracy: 65.4688%, Training Loss: 0.7169%\n",
      "Epoch [19/300], Step [181/225], Training Accuracy: 65.4092%, Training Loss: 0.7174%\n",
      "Epoch [19/300], Step [182/225], Training Accuracy: 65.4619%, Training Loss: 0.7175%\n",
      "Epoch [19/300], Step [183/225], Training Accuracy: 65.4457%, Training Loss: 0.7176%\n",
      "Epoch [19/300], Step [184/225], Training Accuracy: 65.4891%, Training Loss: 0.7171%\n",
      "Epoch [19/300], Step [185/225], Training Accuracy: 65.5152%, Training Loss: 0.7162%\n",
      "Epoch [19/300], Step [186/225], Training Accuracy: 65.5914%, Training Loss: 0.7156%\n",
      "Epoch [19/300], Step [187/225], Training Accuracy: 65.5999%, Training Loss: 0.7158%\n",
      "Epoch [19/300], Step [188/225], Training Accuracy: 65.6250%, Training Loss: 0.7153%\n",
      "Epoch [19/300], Step [189/225], Training Accuracy: 65.6498%, Training Loss: 0.7147%\n",
      "Epoch [19/300], Step [190/225], Training Accuracy: 65.6579%, Training Loss: 0.7149%\n",
      "Epoch [19/300], Step [191/225], Training Accuracy: 65.6823%, Training Loss: 0.7146%\n",
      "Epoch [19/300], Step [192/225], Training Accuracy: 65.7552%, Training Loss: 0.7141%\n",
      "Epoch [19/300], Step [193/225], Training Accuracy: 65.7464%, Training Loss: 0.7143%\n",
      "Epoch [19/300], Step [194/225], Training Accuracy: 65.7539%, Training Loss: 0.7142%\n",
      "Epoch [19/300], Step [195/225], Training Accuracy: 65.7532%, Training Loss: 0.7141%\n",
      "Epoch [19/300], Step [196/225], Training Accuracy: 65.7366%, Training Loss: 0.7143%\n",
      "Epoch [19/300], Step [197/225], Training Accuracy: 65.7678%, Training Loss: 0.7139%\n",
      "Epoch [19/300], Step [198/225], Training Accuracy: 65.8144%, Training Loss: 0.7135%\n",
      "Epoch [19/300], Step [199/225], Training Accuracy: 65.8527%, Training Loss: 0.7131%\n",
      "Epoch [19/300], Step [200/225], Training Accuracy: 65.8438%, Training Loss: 0.7132%\n",
      "Epoch [19/300], Step [201/225], Training Accuracy: 65.8116%, Training Loss: 0.7136%\n",
      "Epoch [19/300], Step [202/225], Training Accuracy: 65.8493%, Training Loss: 0.7129%\n",
      "Epoch [19/300], Step [203/225], Training Accuracy: 65.9175%, Training Loss: 0.7125%\n",
      "Epoch [19/300], Step [204/225], Training Accuracy: 65.9237%, Training Loss: 0.7124%\n",
      "Epoch [19/300], Step [205/225], Training Accuracy: 65.9527%, Training Loss: 0.7120%\n",
      "Epoch [19/300], Step [206/225], Training Accuracy: 65.9587%, Training Loss: 0.7122%\n",
      "Epoch [19/300], Step [207/225], Training Accuracy: 65.9571%, Training Loss: 0.7121%\n",
      "Epoch [19/300], Step [208/225], Training Accuracy: 66.0006%, Training Loss: 0.7118%\n",
      "Epoch [19/300], Step [209/225], Training Accuracy: 66.0138%, Training Loss: 0.7116%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [210/225], Training Accuracy: 66.0045%, Training Loss: 0.7117%\n",
      "Epoch [19/300], Step [211/225], Training Accuracy: 66.0471%, Training Loss: 0.7116%\n",
      "Epoch [19/300], Step [212/225], Training Accuracy: 66.0377%, Training Loss: 0.7115%\n",
      "Epoch [19/300], Step [213/225], Training Accuracy: 66.0578%, Training Loss: 0.7121%\n",
      "Epoch [19/300], Step [214/225], Training Accuracy: 66.0923%, Training Loss: 0.7120%\n",
      "Epoch [19/300], Step [215/225], Training Accuracy: 66.0756%, Training Loss: 0.7118%\n",
      "Epoch [19/300], Step [216/225], Training Accuracy: 66.0373%, Training Loss: 0.7121%\n",
      "Epoch [19/300], Step [217/225], Training Accuracy: 66.0354%, Training Loss: 0.7124%\n",
      "Epoch [19/300], Step [218/225], Training Accuracy: 66.0264%, Training Loss: 0.7122%\n",
      "Epoch [19/300], Step [219/225], Training Accuracy: 66.0245%, Training Loss: 0.7121%\n",
      "Epoch [19/300], Step [220/225], Training Accuracy: 66.0156%, Training Loss: 0.7123%\n",
      "Epoch [19/300], Step [221/225], Training Accuracy: 65.9926%, Training Loss: 0.7124%\n",
      "Epoch [19/300], Step [222/225], Training Accuracy: 66.0051%, Training Loss: 0.7121%\n",
      "Epoch [19/300], Step [223/225], Training Accuracy: 65.9753%, Training Loss: 0.7124%\n",
      "Epoch [19/300], Step [224/225], Training Accuracy: 65.9807%, Training Loss: 0.7122%\n",
      "Epoch [19/300], Step [225/225], Training Accuracy: 65.9742%, Training Loss: 0.7126%\n",
      "Epoch [20/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.6282%\n",
      "Epoch [20/300], Step [2/225], Training Accuracy: 67.9688%, Training Loss: 0.6712%\n",
      "Epoch [20/300], Step [3/225], Training Accuracy: 66.1458%, Training Loss: 0.7092%\n",
      "Epoch [20/300], Step [4/225], Training Accuracy: 66.4062%, Training Loss: 0.7053%\n",
      "Epoch [20/300], Step [5/225], Training Accuracy: 67.1875%, Training Loss: 0.7029%\n",
      "Epoch [20/300], Step [6/225], Training Accuracy: 65.6250%, Training Loss: 0.7388%\n",
      "Epoch [20/300], Step [7/225], Training Accuracy: 65.1786%, Training Loss: 0.7414%\n",
      "Epoch [20/300], Step [8/225], Training Accuracy: 65.2344%, Training Loss: 0.7367%\n",
      "Epoch [20/300], Step [9/225], Training Accuracy: 64.5833%, Training Loss: 0.7453%\n",
      "Epoch [20/300], Step [10/225], Training Accuracy: 64.6875%, Training Loss: 0.7471%\n",
      "Epoch [20/300], Step [11/225], Training Accuracy: 64.9148%, Training Loss: 0.7388%\n",
      "Epoch [20/300], Step [12/225], Training Accuracy: 64.7135%, Training Loss: 0.7369%\n",
      "Epoch [20/300], Step [13/225], Training Accuracy: 65.6250%, Training Loss: 0.7211%\n",
      "Epoch [20/300], Step [14/225], Training Accuracy: 66.0714%, Training Loss: 0.7183%\n",
      "Epoch [20/300], Step [15/225], Training Accuracy: 66.4583%, Training Loss: 0.7185%\n",
      "Epoch [20/300], Step [16/225], Training Accuracy: 66.3086%, Training Loss: 0.7166%\n",
      "Epoch [20/300], Step [17/225], Training Accuracy: 66.6360%, Training Loss: 0.7101%\n",
      "Epoch [20/300], Step [18/225], Training Accuracy: 66.5799%, Training Loss: 0.7127%\n",
      "Epoch [20/300], Step [19/225], Training Accuracy: 66.7763%, Training Loss: 0.7140%\n",
      "Epoch [20/300], Step [20/225], Training Accuracy: 66.8750%, Training Loss: 0.7101%\n",
      "Epoch [20/300], Step [21/225], Training Accuracy: 67.3363%, Training Loss: 0.7041%\n",
      "Epoch [20/300], Step [22/225], Training Accuracy: 67.1165%, Training Loss: 0.7094%\n",
      "Epoch [20/300], Step [23/225], Training Accuracy: 67.3234%, Training Loss: 0.7038%\n",
      "Epoch [20/300], Step [24/225], Training Accuracy: 66.8620%, Training Loss: 0.7084%\n",
      "Epoch [20/300], Step [25/225], Training Accuracy: 66.6875%, Training Loss: 0.7091%\n",
      "Epoch [20/300], Step [26/225], Training Accuracy: 66.7668%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [27/225], Training Accuracy: 66.6088%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [28/225], Training Accuracy: 66.9643%, Training Loss: 0.7062%\n",
      "Epoch [20/300], Step [29/225], Training Accuracy: 66.9720%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [30/225], Training Accuracy: 67.2396%, Training Loss: 0.7059%\n",
      "Epoch [20/300], Step [31/225], Training Accuracy: 66.8851%, Training Loss: 0.7134%\n",
      "Epoch [20/300], Step [32/225], Training Accuracy: 66.7969%, Training Loss: 0.7140%\n",
      "Epoch [20/300], Step [33/225], Training Accuracy: 66.8561%, Training Loss: 0.7115%\n",
      "Epoch [20/300], Step [34/225], Training Accuracy: 66.7279%, Training Loss: 0.7104%\n",
      "Epoch [20/300], Step [35/225], Training Accuracy: 66.8750%, Training Loss: 0.7092%\n",
      "Epoch [20/300], Step [36/225], Training Accuracy: 67.0573%, Training Loss: 0.7073%\n",
      "Epoch [20/300], Step [37/225], Training Accuracy: 67.1875%, Training Loss: 0.7060%\n",
      "Epoch [20/300], Step [38/225], Training Accuracy: 67.0230%, Training Loss: 0.7059%\n",
      "Epoch [20/300], Step [39/225], Training Accuracy: 67.2276%, Training Loss: 0.7051%\n",
      "Epoch [20/300], Step [40/225], Training Accuracy: 67.4219%, Training Loss: 0.7033%\n",
      "Epoch [20/300], Step [41/225], Training Accuracy: 67.6067%, Training Loss: 0.7022%\n",
      "Epoch [20/300], Step [42/225], Training Accuracy: 67.5223%, Training Loss: 0.7005%\n",
      "Epoch [20/300], Step [43/225], Training Accuracy: 67.3692%, Training Loss: 0.7026%\n",
      "Epoch [20/300], Step [44/225], Training Accuracy: 67.6136%, Training Loss: 0.6995%\n",
      "Epoch [20/300], Step [45/225], Training Accuracy: 67.6736%, Training Loss: 0.6972%\n",
      "Epoch [20/300], Step [46/225], Training Accuracy: 67.6970%, Training Loss: 0.6953%\n",
      "Epoch [20/300], Step [47/225], Training Accuracy: 67.4535%, Training Loss: 0.6970%\n",
      "Epoch [20/300], Step [48/225], Training Accuracy: 67.3828%, Training Loss: 0.6989%\n",
      "Epoch [20/300], Step [49/225], Training Accuracy: 67.5064%, Training Loss: 0.6976%\n",
      "Epoch [20/300], Step [50/225], Training Accuracy: 67.5938%, Training Loss: 0.6976%\n",
      "Epoch [20/300], Step [51/225], Training Accuracy: 67.6777%, Training Loss: 0.6970%\n",
      "Epoch [20/300], Step [52/225], Training Accuracy: 67.7885%, Training Loss: 0.6952%\n",
      "Epoch [20/300], Step [53/225], Training Accuracy: 67.8066%, Training Loss: 0.6952%\n",
      "Epoch [20/300], Step [54/225], Training Accuracy: 67.7373%, Training Loss: 0.6957%\n",
      "Epoch [20/300], Step [55/225], Training Accuracy: 67.6705%, Training Loss: 0.6980%\n",
      "Epoch [20/300], Step [56/225], Training Accuracy: 67.5502%, Training Loss: 0.6994%\n",
      "Epoch [20/300], Step [57/225], Training Accuracy: 67.5987%, Training Loss: 0.6983%\n",
      "Epoch [20/300], Step [58/225], Training Accuracy: 67.5108%, Training Loss: 0.6997%\n",
      "Epoch [20/300], Step [59/225], Training Accuracy: 67.5583%, Training Loss: 0.6981%\n",
      "Epoch [20/300], Step [60/225], Training Accuracy: 67.6042%, Training Loss: 0.6968%\n",
      "Epoch [20/300], Step [61/225], Training Accuracy: 67.5205%, Training Loss: 0.6972%\n",
      "Epoch [20/300], Step [62/225], Training Accuracy: 67.4647%, Training Loss: 0.6978%\n",
      "Epoch [20/300], Step [63/225], Training Accuracy: 67.2867%, Training Loss: 0.6997%\n",
      "Epoch [20/300], Step [64/225], Training Accuracy: 67.4561%, Training Loss: 0.6972%\n",
      "Epoch [20/300], Step [65/225], Training Accuracy: 67.4038%, Training Loss: 0.6974%\n",
      "Epoch [20/300], Step [66/225], Training Accuracy: 67.5663%, Training Loss: 0.6956%\n",
      "Epoch [20/300], Step [67/225], Training Accuracy: 67.4907%, Training Loss: 0.6952%\n",
      "Epoch [20/300], Step [68/225], Training Accuracy: 67.3713%, Training Loss: 0.6972%\n",
      "Epoch [20/300], Step [69/225], Training Accuracy: 67.3234%, Training Loss: 0.6975%\n",
      "Epoch [20/300], Step [70/225], Training Accuracy: 67.2321%, Training Loss: 0.6976%\n",
      "Epoch [20/300], Step [71/225], Training Accuracy: 67.2975%, Training Loss: 0.6967%\n",
      "Epoch [20/300], Step [72/225], Training Accuracy: 67.2743%, Training Loss: 0.6972%\n",
      "Epoch [20/300], Step [73/225], Training Accuracy: 67.1661%, Training Loss: 0.6977%\n",
      "Epoch [20/300], Step [74/225], Training Accuracy: 67.1030%, Training Loss: 0.6981%\n",
      "Epoch [20/300], Step [75/225], Training Accuracy: 67.2292%, Training Loss: 0.6970%\n",
      "Epoch [20/300], Step [76/225], Training Accuracy: 67.1464%, Training Loss: 0.6991%\n",
      "Epoch [20/300], Step [77/225], Training Accuracy: 67.2078%, Training Loss: 0.6986%\n",
      "Epoch [20/300], Step [78/225], Training Accuracy: 67.1675%, Training Loss: 0.6988%\n",
      "Epoch [20/300], Step [79/225], Training Accuracy: 67.0491%, Training Loss: 0.6993%\n",
      "Epoch [20/300], Step [80/225], Training Accuracy: 67.0508%, Training Loss: 0.6997%\n",
      "Epoch [20/300], Step [81/225], Training Accuracy: 66.9946%, Training Loss: 0.7000%\n",
      "Epoch [20/300], Step [82/225], Training Accuracy: 66.9970%, Training Loss: 0.6994%\n",
      "Epoch [20/300], Step [83/225], Training Accuracy: 66.9992%, Training Loss: 0.6987%\n",
      "Epoch [20/300], Step [84/225], Training Accuracy: 67.0387%, Training Loss: 0.6980%\n",
      "Epoch [20/300], Step [85/225], Training Accuracy: 67.0588%, Training Loss: 0.6968%\n",
      "Epoch [20/300], Step [86/225], Training Accuracy: 67.0603%, Training Loss: 0.6967%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/300], Step [87/225], Training Accuracy: 67.0618%, Training Loss: 0.6970%\n",
      "Epoch [20/300], Step [88/225], Training Accuracy: 67.0632%, Training Loss: 0.6978%\n",
      "Epoch [20/300], Step [89/225], Training Accuracy: 66.9768%, Training Loss: 0.6996%\n",
      "Epoch [20/300], Step [90/225], Training Accuracy: 66.9097%, Training Loss: 0.7012%\n",
      "Epoch [20/300], Step [91/225], Training Accuracy: 66.8613%, Training Loss: 0.7017%\n",
      "Epoch [20/300], Step [92/225], Training Accuracy: 66.7629%, Training Loss: 0.7023%\n",
      "Epoch [20/300], Step [93/225], Training Accuracy: 66.7843%, Training Loss: 0.7017%\n",
      "Epoch [20/300], Step [94/225], Training Accuracy: 66.8384%, Training Loss: 0.7007%\n",
      "Epoch [20/300], Step [95/225], Training Accuracy: 66.8421%, Training Loss: 0.7014%\n",
      "Epoch [20/300], Step [96/225], Training Accuracy: 66.8457%, Training Loss: 0.7006%\n",
      "Epoch [20/300], Step [97/225], Training Accuracy: 66.8976%, Training Loss: 0.7002%\n",
      "Epoch [20/300], Step [98/225], Training Accuracy: 66.7889%, Training Loss: 0.7023%\n",
      "Epoch [20/300], Step [99/225], Training Accuracy: 66.7929%, Training Loss: 0.7028%\n",
      "Epoch [20/300], Step [100/225], Training Accuracy: 66.7812%, Training Loss: 0.7031%\n",
      "Epoch [20/300], Step [101/225], Training Accuracy: 66.8626%, Training Loss: 0.7030%\n",
      "Epoch [20/300], Step [102/225], Training Accuracy: 66.8811%, Training Loss: 0.7031%\n",
      "Epoch [20/300], Step [103/225], Training Accuracy: 66.9144%, Training Loss: 0.7025%\n",
      "Epoch [20/300], Step [104/225], Training Accuracy: 66.8119%, Training Loss: 0.7035%\n",
      "Epoch [20/300], Step [105/225], Training Accuracy: 66.8006%, Training Loss: 0.7030%\n",
      "Epoch [20/300], Step [106/225], Training Accuracy: 66.7895%, Training Loss: 0.7031%\n",
      "Epoch [20/300], Step [107/225], Training Accuracy: 66.7494%, Training Loss: 0.7041%\n",
      "Epoch [20/300], Step [108/225], Training Accuracy: 66.7390%, Training Loss: 0.7040%\n",
      "Epoch [20/300], Step [109/225], Training Accuracy: 66.7431%, Training Loss: 0.7036%\n",
      "Epoch [20/300], Step [110/225], Training Accuracy: 66.6903%, Training Loss: 0.7033%\n",
      "Epoch [20/300], Step [111/225], Training Accuracy: 66.7370%, Training Loss: 0.7022%\n",
      "Epoch [20/300], Step [112/225], Training Accuracy: 66.7132%, Training Loss: 0.7027%\n",
      "Epoch [20/300], Step [113/225], Training Accuracy: 66.7035%, Training Loss: 0.7025%\n",
      "Epoch [20/300], Step [114/225], Training Accuracy: 66.7078%, Training Loss: 0.7020%\n",
      "Epoch [20/300], Step [115/225], Training Accuracy: 66.7799%, Training Loss: 0.7010%\n",
      "Epoch [20/300], Step [116/225], Training Accuracy: 66.8373%, Training Loss: 0.7008%\n",
      "Epoch [20/300], Step [117/225], Training Accuracy: 66.7735%, Training Loss: 0.7020%\n",
      "Epoch [20/300], Step [118/225], Training Accuracy: 66.7770%, Training Loss: 0.7016%\n",
      "Epoch [20/300], Step [119/225], Training Accuracy: 66.7805%, Training Loss: 0.7019%\n",
      "Epoch [20/300], Step [120/225], Training Accuracy: 66.7578%, Training Loss: 0.7022%\n",
      "Epoch [20/300], Step [121/225], Training Accuracy: 66.6710%, Training Loss: 0.7023%\n",
      "Epoch [20/300], Step [122/225], Training Accuracy: 66.7136%, Training Loss: 0.7024%\n",
      "Epoch [20/300], Step [123/225], Training Accuracy: 66.6540%, Training Loss: 0.7023%\n",
      "Epoch [20/300], Step [124/225], Training Accuracy: 66.6961%, Training Loss: 0.7016%\n",
      "Epoch [20/300], Step [125/225], Training Accuracy: 66.6750%, Training Loss: 0.7018%\n",
      "Epoch [20/300], Step [126/225], Training Accuracy: 66.6667%, Training Loss: 0.7022%\n",
      "Epoch [20/300], Step [127/225], Training Accuracy: 66.6093%, Training Loss: 0.7032%\n",
      "Epoch [20/300], Step [128/225], Training Accuracy: 66.5405%, Training Loss: 0.7043%\n",
      "Epoch [20/300], Step [129/225], Training Accuracy: 66.5213%, Training Loss: 0.7057%\n",
      "Epoch [20/300], Step [130/225], Training Accuracy: 66.4543%, Training Loss: 0.7062%\n",
      "Epoch [20/300], Step [131/225], Training Accuracy: 66.4480%, Training Loss: 0.7064%\n",
      "Epoch [20/300], Step [132/225], Training Accuracy: 66.4181%, Training Loss: 0.7065%\n",
      "Epoch [20/300], Step [133/225], Training Accuracy: 66.3769%, Training Loss: 0.7067%\n",
      "Epoch [20/300], Step [134/225], Training Accuracy: 66.2780%, Training Loss: 0.7082%\n",
      "Epoch [20/300], Step [135/225], Training Accuracy: 66.2731%, Training Loss: 0.7088%\n",
      "Epoch [20/300], Step [136/225], Training Accuracy: 66.3488%, Training Loss: 0.7077%\n",
      "Epoch [20/300], Step [137/225], Training Accuracy: 66.2865%, Training Loss: 0.7080%\n",
      "Epoch [20/300], Step [138/225], Training Accuracy: 66.4062%, Training Loss: 0.7064%\n",
      "Epoch [20/300], Step [139/225], Training Accuracy: 66.4119%, Training Loss: 0.7065%\n",
      "Epoch [20/300], Step [140/225], Training Accuracy: 66.4509%, Training Loss: 0.7062%\n",
      "Epoch [20/300], Step [141/225], Training Accuracy: 66.4118%, Training Loss: 0.7066%\n",
      "Epoch [20/300], Step [142/225], Training Accuracy: 66.3842%, Training Loss: 0.7066%\n",
      "Epoch [20/300], Step [143/225], Training Accuracy: 66.4117%, Training Loss: 0.7066%\n",
      "Epoch [20/300], Step [144/225], Training Accuracy: 66.4062%, Training Loss: 0.7067%\n",
      "Epoch [20/300], Step [145/225], Training Accuracy: 66.4547%, Training Loss: 0.7062%\n",
      "Epoch [20/300], Step [146/225], Training Accuracy: 66.4705%, Training Loss: 0.7066%\n",
      "Epoch [20/300], Step [147/225], Training Accuracy: 66.4116%, Training Loss: 0.7072%\n",
      "Epoch [20/300], Step [148/225], Training Accuracy: 66.4274%, Training Loss: 0.7071%\n",
      "Epoch [20/300], Step [149/225], Training Accuracy: 66.4115%, Training Loss: 0.7078%\n",
      "Epoch [20/300], Step [150/225], Training Accuracy: 66.3646%, Training Loss: 0.7082%\n",
      "Epoch [20/300], Step [151/225], Training Accuracy: 66.4114%, Training Loss: 0.7079%\n",
      "Epoch [20/300], Step [152/225], Training Accuracy: 66.3549%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [153/225], Training Accuracy: 66.3092%, Training Loss: 0.7085%\n",
      "Epoch [20/300], Step [154/225], Training Accuracy: 66.3251%, Training Loss: 0.7087%\n",
      "Epoch [20/300], Step [155/225], Training Accuracy: 66.2601%, Training Loss: 0.7091%\n",
      "Epoch [20/300], Step [156/225], Training Accuracy: 66.2660%, Training Loss: 0.7092%\n",
      "Epoch [20/300], Step [157/225], Training Accuracy: 66.2520%, Training Loss: 0.7094%\n",
      "Epoch [20/300], Step [158/225], Training Accuracy: 66.2085%, Training Loss: 0.7102%\n",
      "Epoch [20/300], Step [159/225], Training Accuracy: 66.1262%, Training Loss: 0.7106%\n",
      "Epoch [20/300], Step [160/225], Training Accuracy: 66.1328%, Training Loss: 0.7107%\n",
      "Epoch [20/300], Step [161/225], Training Accuracy: 66.1102%, Training Loss: 0.7110%\n",
      "Epoch [20/300], Step [162/225], Training Accuracy: 66.1362%, Training Loss: 0.7109%\n",
      "Epoch [20/300], Step [163/225], Training Accuracy: 66.0947%, Training Loss: 0.7110%\n",
      "Epoch [20/300], Step [164/225], Training Accuracy: 66.1109%, Training Loss: 0.7102%\n",
      "Epoch [20/300], Step [165/225], Training Accuracy: 66.1269%, Training Loss: 0.7101%\n",
      "Epoch [20/300], Step [166/225], Training Accuracy: 66.1145%, Training Loss: 0.7101%\n",
      "Epoch [20/300], Step [167/225], Training Accuracy: 66.0928%, Training Loss: 0.7107%\n",
      "Epoch [20/300], Step [168/225], Training Accuracy: 66.0528%, Training Loss: 0.7109%\n",
      "Epoch [20/300], Step [169/225], Training Accuracy: 66.0503%, Training Loss: 0.7112%\n",
      "Epoch [20/300], Step [170/225], Training Accuracy: 66.0478%, Training Loss: 0.7113%\n",
      "Epoch [20/300], Step [171/225], Training Accuracy: 66.0545%, Training Loss: 0.7114%\n",
      "Epoch [20/300], Step [172/225], Training Accuracy: 65.9793%, Training Loss: 0.7119%\n",
      "Epoch [20/300], Step [173/225], Training Accuracy: 65.9592%, Training Loss: 0.7121%\n",
      "Epoch [20/300], Step [174/225], Training Accuracy: 65.9393%, Training Loss: 0.7122%\n",
      "Epoch [20/300], Step [175/225], Training Accuracy: 65.9911%, Training Loss: 0.7117%\n",
      "Epoch [20/300], Step [176/225], Training Accuracy: 65.9712%, Training Loss: 0.7116%\n",
      "Epoch [20/300], Step [177/225], Training Accuracy: 65.9958%, Training Loss: 0.7117%\n",
      "Epoch [20/300], Step [178/225], Training Accuracy: 65.9761%, Training Loss: 0.7120%\n",
      "Epoch [20/300], Step [179/225], Training Accuracy: 66.0353%, Training Loss: 0.7114%\n",
      "Epoch [20/300], Step [180/225], Training Accuracy: 65.9896%, Training Loss: 0.7118%\n",
      "Epoch [20/300], Step [181/225], Training Accuracy: 65.9876%, Training Loss: 0.7117%\n",
      "Epoch [20/300], Step [182/225], Training Accuracy: 65.9942%, Training Loss: 0.7118%\n",
      "Epoch [20/300], Step [183/225], Training Accuracy: 66.0092%, Training Loss: 0.7118%\n",
      "Epoch [20/300], Step [184/225], Training Accuracy: 65.9986%, Training Loss: 0.7118%\n",
      "Epoch [20/300], Step [185/225], Training Accuracy: 66.0304%, Training Loss: 0.7115%\n",
      "Epoch [20/300], Step [186/225], Training Accuracy: 66.0366%, Training Loss: 0.7113%\n",
      "Epoch [20/300], Step [187/225], Training Accuracy: 66.0595%, Training Loss: 0.7111%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/300], Step [188/225], Training Accuracy: 66.0655%, Training Loss: 0.7108%\n",
      "Epoch [20/300], Step [189/225], Training Accuracy: 66.1293%, Training Loss: 0.7102%\n",
      "Epoch [20/300], Step [190/225], Training Accuracy: 66.1020%, Training Loss: 0.7102%\n",
      "Epoch [20/300], Step [191/225], Training Accuracy: 66.0995%, Training Loss: 0.7102%\n",
      "Epoch [20/300], Step [192/225], Training Accuracy: 66.1621%, Training Loss: 0.7097%\n",
      "Epoch [20/300], Step [193/225], Training Accuracy: 66.1674%, Training Loss: 0.7097%\n",
      "Epoch [20/300], Step [194/225], Training Accuracy: 66.1485%, Training Loss: 0.7094%\n",
      "Epoch [20/300], Step [195/225], Training Accuracy: 66.1619%, Training Loss: 0.7091%\n",
      "Epoch [20/300], Step [196/225], Training Accuracy: 66.1113%, Training Loss: 0.7095%\n",
      "Epoch [20/300], Step [197/225], Training Accuracy: 66.1247%, Training Loss: 0.7094%\n",
      "Epoch [20/300], Step [198/225], Training Accuracy: 66.1616%, Training Loss: 0.7087%\n",
      "Epoch [20/300], Step [199/225], Training Accuracy: 66.1354%, Training Loss: 0.7085%\n",
      "Epoch [20/300], Step [200/225], Training Accuracy: 66.1094%, Training Loss: 0.7087%\n",
      "Epoch [20/300], Step [201/225], Training Accuracy: 66.1070%, Training Loss: 0.7090%\n",
      "Epoch [20/300], Step [202/225], Training Accuracy: 66.1278%, Training Loss: 0.7086%\n",
      "Epoch [20/300], Step [203/225], Training Accuracy: 66.1561%, Training Loss: 0.7083%\n",
      "Epoch [20/300], Step [204/225], Training Accuracy: 66.1688%, Training Loss: 0.7080%\n",
      "Epoch [20/300], Step [205/225], Training Accuracy: 66.2271%, Training Loss: 0.7076%\n",
      "Epoch [20/300], Step [206/225], Training Accuracy: 66.2242%, Training Loss: 0.7080%\n",
      "Epoch [20/300], Step [207/225], Training Accuracy: 66.2515%, Training Loss: 0.7078%\n",
      "Epoch [20/300], Step [208/225], Training Accuracy: 66.2936%, Training Loss: 0.7072%\n",
      "Epoch [20/300], Step [209/225], Training Accuracy: 66.2754%, Training Loss: 0.7073%\n",
      "Epoch [20/300], Step [210/225], Training Accuracy: 66.2649%, Training Loss: 0.7074%\n",
      "Epoch [20/300], Step [211/225], Training Accuracy: 66.3137%, Training Loss: 0.7067%\n",
      "Epoch [20/300], Step [212/225], Training Accuracy: 66.3104%, Training Loss: 0.7069%\n",
      "Epoch [20/300], Step [213/225], Training Accuracy: 66.3072%, Training Loss: 0.7073%\n",
      "Epoch [20/300], Step [214/225], Training Accuracy: 66.3259%, Training Loss: 0.7070%\n",
      "Epoch [20/300], Step [215/225], Training Accuracy: 66.3081%, Training Loss: 0.7069%\n",
      "Epoch [20/300], Step [216/225], Training Accuracy: 66.2977%, Training Loss: 0.7073%\n",
      "Epoch [20/300], Step [217/225], Training Accuracy: 66.2730%, Training Loss: 0.7074%\n",
      "Epoch [20/300], Step [218/225], Training Accuracy: 66.2486%, Training Loss: 0.7076%\n",
      "Epoch [20/300], Step [219/225], Training Accuracy: 66.2743%, Training Loss: 0.7075%\n",
      "Epoch [20/300], Step [220/225], Training Accuracy: 66.2713%, Training Loss: 0.7078%\n",
      "Epoch [20/300], Step [221/225], Training Accuracy: 66.2542%, Training Loss: 0.7083%\n",
      "Epoch [20/300], Step [222/225], Training Accuracy: 66.2584%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [223/225], Training Accuracy: 66.2486%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [224/225], Training Accuracy: 66.2319%, Training Loss: 0.7081%\n",
      "Epoch [20/300], Step [225/225], Training Accuracy: 66.2104%, Training Loss: 0.7081%\n",
      "Epoch [21/300], Step [1/225], Training Accuracy: 68.7500%, Training Loss: 0.5827%\n",
      "Epoch [21/300], Step [2/225], Training Accuracy: 69.5312%, Training Loss: 0.6359%\n",
      "Epoch [21/300], Step [3/225], Training Accuracy: 68.2292%, Training Loss: 0.6532%\n",
      "Epoch [21/300], Step [4/225], Training Accuracy: 67.9688%, Training Loss: 0.6598%\n",
      "Epoch [21/300], Step [5/225], Training Accuracy: 66.8750%, Training Loss: 0.6573%\n",
      "Epoch [21/300], Step [6/225], Training Accuracy: 66.9271%, Training Loss: 0.6790%\n",
      "Epoch [21/300], Step [7/225], Training Accuracy: 67.8571%, Training Loss: 0.6774%\n",
      "Epoch [21/300], Step [8/225], Training Accuracy: 67.9688%, Training Loss: 0.6764%\n",
      "Epoch [21/300], Step [9/225], Training Accuracy: 67.3611%, Training Loss: 0.6886%\n",
      "Epoch [21/300], Step [10/225], Training Accuracy: 66.5625%, Training Loss: 0.7070%\n",
      "Epoch [21/300], Step [11/225], Training Accuracy: 66.7614%, Training Loss: 0.7069%\n",
      "Epoch [21/300], Step [12/225], Training Accuracy: 66.6667%, Training Loss: 0.7118%\n",
      "Epoch [21/300], Step [13/225], Training Accuracy: 67.5481%, Training Loss: 0.7031%\n",
      "Epoch [21/300], Step [14/225], Training Accuracy: 67.5223%, Training Loss: 0.7009%\n",
      "Epoch [21/300], Step [15/225], Training Accuracy: 67.7083%, Training Loss: 0.7047%\n",
      "Epoch [21/300], Step [16/225], Training Accuracy: 67.4805%, Training Loss: 0.7027%\n",
      "Epoch [21/300], Step [17/225], Training Accuracy: 67.7390%, Training Loss: 0.6979%\n",
      "Epoch [21/300], Step [18/225], Training Accuracy: 67.6215%, Training Loss: 0.6974%\n",
      "Epoch [21/300], Step [19/225], Training Accuracy: 68.0099%, Training Loss: 0.6950%\n",
      "Epoch [21/300], Step [20/225], Training Accuracy: 68.1250%, Training Loss: 0.6891%\n",
      "Epoch [21/300], Step [21/225], Training Accuracy: 68.6756%, Training Loss: 0.6822%\n",
      "Epoch [21/300], Step [22/225], Training Accuracy: 68.0398%, Training Loss: 0.6896%\n",
      "Epoch [21/300], Step [23/225], Training Accuracy: 68.2745%, Training Loss: 0.6859%\n",
      "Epoch [21/300], Step [24/225], Training Accuracy: 67.7083%, Training Loss: 0.6921%\n",
      "Epoch [21/300], Step [25/225], Training Accuracy: 67.9375%, Training Loss: 0.6916%\n",
      "Epoch [21/300], Step [26/225], Training Accuracy: 67.9688%, Training Loss: 0.6921%\n",
      "Epoch [21/300], Step [27/225], Training Accuracy: 68.0556%, Training Loss: 0.6885%\n",
      "Epoch [21/300], Step [28/225], Training Accuracy: 68.1920%, Training Loss: 0.6847%\n",
      "Epoch [21/300], Step [29/225], Training Accuracy: 68.2112%, Training Loss: 0.6850%\n",
      "Epoch [21/300], Step [30/225], Training Accuracy: 68.2812%, Training Loss: 0.6824%\n",
      "Epoch [21/300], Step [31/225], Training Accuracy: 67.9435%, Training Loss: 0.6878%\n",
      "Epoch [21/300], Step [32/225], Training Accuracy: 67.7246%, Training Loss: 0.6888%\n",
      "Epoch [21/300], Step [33/225], Training Accuracy: 67.6610%, Training Loss: 0.6861%\n",
      "Epoch [21/300], Step [34/225], Training Accuracy: 67.4632%, Training Loss: 0.6868%\n",
      "Epoch [21/300], Step [35/225], Training Accuracy: 67.7232%, Training Loss: 0.6843%\n",
      "Epoch [21/300], Step [36/225], Training Accuracy: 67.8385%, Training Loss: 0.6827%\n",
      "Epoch [21/300], Step [37/225], Training Accuracy: 67.9054%, Training Loss: 0.6840%\n",
      "Epoch [21/300], Step [38/225], Training Accuracy: 67.8454%, Training Loss: 0.6848%\n",
      "Epoch [21/300], Step [39/225], Training Accuracy: 67.7885%, Training Loss: 0.6853%\n",
      "Epoch [21/300], Step [40/225], Training Accuracy: 67.6953%, Training Loss: 0.6859%\n",
      "Epoch [21/300], Step [41/225], Training Accuracy: 67.5686%, Training Loss: 0.6853%\n",
      "Epoch [21/300], Step [42/225], Training Accuracy: 67.4107%, Training Loss: 0.6852%\n",
      "Epoch [21/300], Step [43/225], Training Accuracy: 67.4782%, Training Loss: 0.6860%\n",
      "Epoch [21/300], Step [44/225], Training Accuracy: 67.7912%, Training Loss: 0.6821%\n",
      "Epoch [21/300], Step [45/225], Training Accuracy: 67.9167%, Training Loss: 0.6798%\n",
      "Epoch [21/300], Step [46/225], Training Accuracy: 67.9688%, Training Loss: 0.6782%\n",
      "Epoch [21/300], Step [47/225], Training Accuracy: 67.8524%, Training Loss: 0.6793%\n",
      "Epoch [21/300], Step [48/225], Training Accuracy: 67.7409%, Training Loss: 0.6810%\n",
      "Epoch [21/300], Step [49/225], Training Accuracy: 67.8890%, Training Loss: 0.6791%\n",
      "Epoch [21/300], Step [50/225], Training Accuracy: 67.8750%, Training Loss: 0.6816%\n",
      "Epoch [21/300], Step [51/225], Training Accuracy: 68.0147%, Training Loss: 0.6812%\n",
      "Epoch [21/300], Step [52/225], Training Accuracy: 67.9688%, Training Loss: 0.6819%\n",
      "Epoch [21/300], Step [53/225], Training Accuracy: 68.1014%, Training Loss: 0.6815%\n",
      "Epoch [21/300], Step [54/225], Training Accuracy: 68.0845%, Training Loss: 0.6819%\n",
      "Epoch [21/300], Step [55/225], Training Accuracy: 67.8693%, Training Loss: 0.6845%\n",
      "Epoch [21/300], Step [56/225], Training Accuracy: 67.7176%, Training Loss: 0.6860%\n",
      "Epoch [21/300], Step [57/225], Training Accuracy: 67.7906%, Training Loss: 0.6842%\n",
      "Epoch [21/300], Step [58/225], Training Accuracy: 67.9418%, Training Loss: 0.6837%\n",
      "Epoch [21/300], Step [59/225], Training Accuracy: 68.0350%, Training Loss: 0.6827%\n",
      "Epoch [21/300], Step [60/225], Training Accuracy: 68.0729%, Training Loss: 0.6817%\n",
      "Epoch [21/300], Step [61/225], Training Accuracy: 68.0072%, Training Loss: 0.6825%\n",
      "Epoch [21/300], Step [62/225], Training Accuracy: 67.9183%, Training Loss: 0.6826%\n",
      "Epoch [21/300], Step [63/225], Training Accuracy: 67.8075%, Training Loss: 0.6860%\n",
      "Epoch [21/300], Step [64/225], Training Accuracy: 68.0176%, Training Loss: 0.6835%\n",
      "Epoch [21/300], Step [65/225], Training Accuracy: 68.0769%, Training Loss: 0.6832%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Step [66/225], Training Accuracy: 68.2055%, Training Loss: 0.6812%\n",
      "Epoch [21/300], Step [67/225], Training Accuracy: 68.1670%, Training Loss: 0.6815%\n",
      "Epoch [21/300], Step [68/225], Training Accuracy: 68.0377%, Training Loss: 0.6828%\n",
      "Epoch [21/300], Step [69/225], Training Accuracy: 67.9801%, Training Loss: 0.6838%\n",
      "Epoch [21/300], Step [70/225], Training Accuracy: 67.9688%, Training Loss: 0.6830%\n",
      "Epoch [21/300], Step [71/225], Training Accuracy: 68.0458%, Training Loss: 0.6817%\n",
      "Epoch [21/300], Step [72/225], Training Accuracy: 68.0339%, Training Loss: 0.6819%\n",
      "Epoch [21/300], Step [73/225], Training Accuracy: 67.9580%, Training Loss: 0.6817%\n",
      "Epoch [21/300], Step [74/225], Training Accuracy: 67.8843%, Training Loss: 0.6817%\n",
      "Epoch [21/300], Step [75/225], Training Accuracy: 67.9792%, Training Loss: 0.6806%\n",
      "Epoch [21/300], Step [76/225], Training Accuracy: 67.7837%, Training Loss: 0.6825%\n",
      "Epoch [21/300], Step [77/225], Training Accuracy: 67.8571%, Training Loss: 0.6825%\n",
      "Epoch [21/300], Step [78/225], Training Accuracy: 67.7284%, Training Loss: 0.6823%\n",
      "Epoch [21/300], Step [79/225], Training Accuracy: 67.6820%, Training Loss: 0.6821%\n",
      "Epoch [21/300], Step [80/225], Training Accuracy: 67.6758%, Training Loss: 0.6821%\n",
      "Epoch [21/300], Step [81/225], Training Accuracy: 67.7276%, Training Loss: 0.6818%\n",
      "Epoch [21/300], Step [82/225], Training Accuracy: 67.6639%, Training Loss: 0.6814%\n",
      "Epoch [21/300], Step [83/225], Training Accuracy: 67.6958%, Training Loss: 0.6808%\n",
      "Epoch [21/300], Step [84/225], Training Accuracy: 67.6897%, Training Loss: 0.6808%\n",
      "Epoch [21/300], Step [85/225], Training Accuracy: 67.7206%, Training Loss: 0.6802%\n",
      "Epoch [21/300], Step [86/225], Training Accuracy: 67.6781%, Training Loss: 0.6804%\n",
      "Epoch [21/300], Step [87/225], Training Accuracy: 67.7083%, Training Loss: 0.6805%\n",
      "Epoch [21/300], Step [88/225], Training Accuracy: 67.6491%, Training Loss: 0.6817%\n",
      "Epoch [21/300], Step [89/225], Training Accuracy: 67.6440%, Training Loss: 0.6822%\n",
      "Epoch [21/300], Step [90/225], Training Accuracy: 67.6910%, Training Loss: 0.6833%\n",
      "Epoch [21/300], Step [91/225], Training Accuracy: 67.5652%, Training Loss: 0.6833%\n",
      "Epoch [21/300], Step [92/225], Training Accuracy: 67.5442%, Training Loss: 0.6838%\n",
      "Epoch [21/300], Step [93/225], Training Accuracy: 67.5739%, Training Loss: 0.6829%\n",
      "Epoch [21/300], Step [94/225], Training Accuracy: 67.6695%, Training Loss: 0.6819%\n",
      "Epoch [21/300], Step [95/225], Training Accuracy: 67.7632%, Training Loss: 0.6814%\n",
      "Epoch [21/300], Step [96/225], Training Accuracy: 67.7409%, Training Loss: 0.6804%\n",
      "Epoch [21/300], Step [97/225], Training Accuracy: 67.7513%, Training Loss: 0.6798%\n",
      "Epoch [21/300], Step [98/225], Training Accuracy: 67.6499%, Training Loss: 0.6816%\n",
      "Epoch [21/300], Step [99/225], Training Accuracy: 67.5979%, Training Loss: 0.6829%\n",
      "Epoch [21/300], Step [100/225], Training Accuracy: 67.5156%, Training Loss: 0.6846%\n",
      "Epoch [21/300], Step [101/225], Training Accuracy: 67.4814%, Training Loss: 0.6848%\n",
      "Epoch [21/300], Step [102/225], Training Accuracy: 67.4020%, Training Loss: 0.6847%\n",
      "Epoch [21/300], Step [103/225], Training Accuracy: 67.4150%, Training Loss: 0.6848%\n",
      "Epoch [21/300], Step [104/225], Training Accuracy: 67.3377%, Training Loss: 0.6861%\n",
      "Epoch [21/300], Step [105/225], Training Accuracy: 67.3363%, Training Loss: 0.6854%\n",
      "Epoch [21/300], Step [106/225], Training Accuracy: 67.3202%, Training Loss: 0.6857%\n",
      "Epoch [21/300], Step [107/225], Training Accuracy: 67.2605%, Training Loss: 0.6877%\n",
      "Epoch [21/300], Step [108/225], Training Accuracy: 67.2888%, Training Loss: 0.6869%\n",
      "Epoch [21/300], Step [109/225], Training Accuracy: 67.2448%, Training Loss: 0.6868%\n",
      "Epoch [21/300], Step [110/225], Training Accuracy: 67.2017%, Training Loss: 0.6863%\n",
      "Epoch [21/300], Step [111/225], Training Accuracy: 67.2016%, Training Loss: 0.6862%\n",
      "Epoch [21/300], Step [112/225], Training Accuracy: 67.1735%, Training Loss: 0.6871%\n",
      "Epoch [21/300], Step [113/225], Training Accuracy: 67.2013%, Training Loss: 0.6869%\n",
      "Epoch [21/300], Step [114/225], Training Accuracy: 67.2697%, Training Loss: 0.6866%\n",
      "Epoch [21/300], Step [115/225], Training Accuracy: 67.3777%, Training Loss: 0.6864%\n",
      "Epoch [21/300], Step [116/225], Training Accuracy: 67.3895%, Training Loss: 0.6867%\n",
      "Epoch [21/300], Step [117/225], Training Accuracy: 67.3077%, Training Loss: 0.6885%\n",
      "Epoch [21/300], Step [118/225], Training Accuracy: 67.2405%, Training Loss: 0.6897%\n",
      "Epoch [21/300], Step [119/225], Training Accuracy: 67.2400%, Training Loss: 0.6896%\n",
      "Epoch [21/300], Step [120/225], Training Accuracy: 67.2135%, Training Loss: 0.6905%\n",
      "Epoch [21/300], Step [121/225], Training Accuracy: 67.1229%, Training Loss: 0.6911%\n",
      "Epoch [21/300], Step [122/225], Training Accuracy: 67.1491%, Training Loss: 0.6917%\n",
      "Epoch [21/300], Step [123/225], Training Accuracy: 67.1748%, Training Loss: 0.6915%\n",
      "Epoch [21/300], Step [124/225], Training Accuracy: 67.2127%, Training Loss: 0.6908%\n",
      "Epoch [21/300], Step [125/225], Training Accuracy: 67.1375%, Training Loss: 0.6918%\n",
      "Epoch [21/300], Step [126/225], Training Accuracy: 67.1255%, Training Loss: 0.6920%\n",
      "Epoch [21/300], Step [127/225], Training Accuracy: 67.0891%, Training Loss: 0.6926%\n",
      "Epoch [21/300], Step [128/225], Training Accuracy: 67.0654%, Training Loss: 0.6934%\n",
      "Epoch [21/300], Step [129/225], Training Accuracy: 67.0543%, Training Loss: 0.6938%\n",
      "Epoch [21/300], Step [130/225], Training Accuracy: 67.0433%, Training Loss: 0.6943%\n",
      "Epoch [21/300], Step [131/225], Training Accuracy: 67.0563%, Training Loss: 0.6941%\n",
      "Epoch [21/300], Step [132/225], Training Accuracy: 67.0336%, Training Loss: 0.6944%\n",
      "Epoch [21/300], Step [133/225], Training Accuracy: 67.0113%, Training Loss: 0.6949%\n",
      "Epoch [21/300], Step [134/225], Training Accuracy: 66.9893%, Training Loss: 0.6966%\n",
      "Epoch [21/300], Step [135/225], Training Accuracy: 66.9097%, Training Loss: 0.6969%\n",
      "Epoch [21/300], Step [136/225], Training Accuracy: 66.9118%, Training Loss: 0.6966%\n",
      "Epoch [21/300], Step [137/225], Training Accuracy: 66.9252%, Training Loss: 0.6961%\n",
      "Epoch [21/300], Step [138/225], Training Accuracy: 66.9837%, Training Loss: 0.6949%\n",
      "Epoch [21/300], Step [139/225], Training Accuracy: 66.9739%, Training Loss: 0.6952%\n",
      "Epoch [21/300], Step [140/225], Training Accuracy: 67.0312%, Training Loss: 0.6948%\n",
      "Epoch [21/300], Step [141/225], Training Accuracy: 67.0324%, Training Loss: 0.6947%\n",
      "Epoch [21/300], Step [142/225], Training Accuracy: 67.0555%, Training Loss: 0.6943%\n",
      "Epoch [21/300], Step [143/225], Training Accuracy: 67.0455%, Training Loss: 0.6948%\n",
      "Epoch [21/300], Step [144/225], Training Accuracy: 67.0356%, Training Loss: 0.6950%\n",
      "Epoch [21/300], Step [145/225], Training Accuracy: 67.0366%, Training Loss: 0.6950%\n",
      "Epoch [21/300], Step [146/225], Training Accuracy: 67.0377%, Training Loss: 0.6953%\n",
      "Epoch [21/300], Step [147/225], Training Accuracy: 66.9855%, Training Loss: 0.6960%\n",
      "Epoch [21/300], Step [148/225], Training Accuracy: 67.0080%, Training Loss: 0.6959%\n",
      "Epoch [21/300], Step [149/225], Training Accuracy: 66.9987%, Training Loss: 0.6961%\n",
      "Epoch [21/300], Step [150/225], Training Accuracy: 66.9375%, Training Loss: 0.6961%\n",
      "Epoch [21/300], Step [151/225], Training Accuracy: 67.0012%, Training Loss: 0.6958%\n",
      "Epoch [21/300], Step [152/225], Training Accuracy: 66.9305%, Training Loss: 0.6964%\n",
      "Epoch [21/300], Step [153/225], Training Accuracy: 66.8607%, Training Loss: 0.6969%\n",
      "Epoch [21/300], Step [154/225], Training Accuracy: 66.8730%, Training Loss: 0.6969%\n",
      "Epoch [21/300], Step [155/225], Training Accuracy: 66.8044%, Training Loss: 0.6980%\n",
      "Epoch [21/300], Step [156/225], Training Accuracy: 66.8169%, Training Loss: 0.6982%\n",
      "Epoch [21/300], Step [157/225], Training Accuracy: 66.7894%, Training Loss: 0.6984%\n",
      "Epoch [21/300], Step [158/225], Training Accuracy: 66.7524%, Training Loss: 0.6990%\n",
      "Epoch [21/300], Step [159/225], Training Accuracy: 66.6961%, Training Loss: 0.6997%\n",
      "Epoch [21/300], Step [160/225], Training Accuracy: 66.6992%, Training Loss: 0.6996%\n",
      "Epoch [21/300], Step [161/225], Training Accuracy: 66.6925%, Training Loss: 0.6995%\n",
      "Epoch [21/300], Step [162/225], Training Accuracy: 66.7824%, Training Loss: 0.6985%\n",
      "Epoch [21/300], Step [163/225], Training Accuracy: 66.7657%, Training Loss: 0.6985%\n",
      "Epoch [21/300], Step [164/225], Training Accuracy: 66.8445%, Training Loss: 0.6973%\n",
      "Epoch [21/300], Step [165/225], Training Accuracy: 66.8655%, Training Loss: 0.6969%\n",
      "Epoch [21/300], Step [166/225], Training Accuracy: 66.8581%, Training Loss: 0.6970%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Step [167/225], Training Accuracy: 66.8132%, Training Loss: 0.6973%\n",
      "Epoch [21/300], Step [168/225], Training Accuracy: 66.8062%, Training Loss: 0.6978%\n",
      "Epoch [21/300], Step [169/225], Training Accuracy: 66.7899%, Training Loss: 0.6976%\n",
      "Epoch [21/300], Step [170/225], Training Accuracy: 66.7739%, Training Loss: 0.6977%\n",
      "Epoch [21/300], Step [171/225], Training Accuracy: 66.7946%, Training Loss: 0.6976%\n",
      "Epoch [21/300], Step [172/225], Training Accuracy: 66.8241%, Training Loss: 0.6975%\n",
      "Epoch [21/300], Step [173/225], Training Accuracy: 66.8353%, Training Loss: 0.6975%\n",
      "Epoch [21/300], Step [174/225], Training Accuracy: 66.8463%, Training Loss: 0.6975%\n",
      "Epoch [21/300], Step [175/225], Training Accuracy: 66.8571%, Training Loss: 0.6970%\n",
      "Epoch [21/300], Step [176/225], Training Accuracy: 66.8324%, Training Loss: 0.6967%\n",
      "Epoch [21/300], Step [177/225], Training Accuracy: 66.8785%, Training Loss: 0.6968%\n",
      "Epoch [21/300], Step [178/225], Training Accuracy: 66.9066%, Training Loss: 0.6965%\n",
      "Epoch [21/300], Step [179/225], Training Accuracy: 66.9605%, Training Loss: 0.6960%\n",
      "Epoch [21/300], Step [180/225], Training Accuracy: 66.9618%, Training Loss: 0.6955%\n",
      "Epoch [21/300], Step [181/225], Training Accuracy: 66.9544%, Training Loss: 0.6954%\n",
      "Epoch [21/300], Step [182/225], Training Accuracy: 66.9900%, Training Loss: 0.6953%\n",
      "Epoch [21/300], Step [183/225], Training Accuracy: 66.9997%, Training Loss: 0.6951%\n",
      "Epoch [21/300], Step [184/225], Training Accuracy: 67.0262%, Training Loss: 0.6950%\n",
      "Epoch [21/300], Step [185/225], Training Accuracy: 67.0608%, Training Loss: 0.6947%\n",
      "Epoch [21/300], Step [186/225], Training Accuracy: 67.1035%, Training Loss: 0.6943%\n",
      "Epoch [21/300], Step [187/225], Training Accuracy: 67.0872%, Training Loss: 0.6947%\n",
      "Epoch [21/300], Step [188/225], Training Accuracy: 67.0878%, Training Loss: 0.6943%\n",
      "Epoch [21/300], Step [189/225], Training Accuracy: 67.0718%, Training Loss: 0.6940%\n",
      "Epoch [21/300], Step [190/225], Training Accuracy: 67.0806%, Training Loss: 0.6940%\n",
      "Epoch [21/300], Step [191/225], Training Accuracy: 67.0893%, Training Loss: 0.6939%\n",
      "Epoch [21/300], Step [192/225], Training Accuracy: 67.1549%, Training Loss: 0.6929%\n",
      "Epoch [21/300], Step [193/225], Training Accuracy: 67.1389%, Training Loss: 0.6932%\n",
      "Epoch [21/300], Step [194/225], Training Accuracy: 67.1231%, Training Loss: 0.6931%\n",
      "Epoch [21/300], Step [195/225], Training Accuracy: 67.1635%, Training Loss: 0.6927%\n",
      "Epoch [21/300], Step [196/225], Training Accuracy: 67.1078%, Training Loss: 0.6930%\n",
      "Epoch [21/300], Step [197/225], Training Accuracy: 67.1399%, Training Loss: 0.6928%\n",
      "Epoch [21/300], Step [198/225], Training Accuracy: 67.1717%, Training Loss: 0.6922%\n",
      "Epoch [21/300], Step [199/225], Training Accuracy: 67.1718%, Training Loss: 0.6923%\n",
      "Epoch [21/300], Step [200/225], Training Accuracy: 67.1484%, Training Loss: 0.6925%\n",
      "Epoch [21/300], Step [201/225], Training Accuracy: 67.1564%, Training Loss: 0.6929%\n",
      "Epoch [21/300], Step [202/225], Training Accuracy: 67.1952%, Training Loss: 0.6927%\n",
      "Epoch [21/300], Step [203/225], Training Accuracy: 67.2722%, Training Loss: 0.6923%\n",
      "Epoch [21/300], Step [204/225], Training Accuracy: 67.2794%, Training Loss: 0.6921%\n",
      "Epoch [21/300], Step [205/225], Training Accuracy: 67.3018%, Training Loss: 0.6921%\n",
      "Epoch [21/300], Step [206/225], Training Accuracy: 67.2785%, Training Loss: 0.6920%\n",
      "Epoch [21/300], Step [207/225], Training Accuracy: 67.2781%, Training Loss: 0.6926%\n",
      "Epoch [21/300], Step [208/225], Training Accuracy: 67.3152%, Training Loss: 0.6921%\n",
      "Epoch [21/300], Step [209/225], Training Accuracy: 67.3221%, Training Loss: 0.6924%\n",
      "Epoch [21/300], Step [210/225], Training Accuracy: 67.2991%, Training Loss: 0.6926%\n",
      "Epoch [21/300], Step [211/225], Training Accuracy: 67.3060%, Training Loss: 0.6923%\n",
      "Epoch [21/300], Step [212/225], Training Accuracy: 67.3128%, Training Loss: 0.6925%\n",
      "Epoch [21/300], Step [213/225], Training Accuracy: 67.2682%, Training Loss: 0.6935%\n",
      "Epoch [21/300], Step [214/225], Training Accuracy: 67.2751%, Training Loss: 0.6932%\n",
      "Epoch [21/300], Step [215/225], Training Accuracy: 67.2456%, Training Loss: 0.6934%\n",
      "Epoch [21/300], Step [216/225], Training Accuracy: 67.2164%, Training Loss: 0.6938%\n",
      "Epoch [21/300], Step [217/225], Training Accuracy: 67.2307%, Training Loss: 0.6942%\n",
      "Epoch [21/300], Step [218/225], Training Accuracy: 67.2018%, Training Loss: 0.6943%\n",
      "Epoch [21/300], Step [219/225], Training Accuracy: 67.2089%, Training Loss: 0.6942%\n",
      "Epoch [21/300], Step [220/225], Training Accuracy: 67.2017%, Training Loss: 0.6944%\n",
      "Epoch [21/300], Step [221/225], Training Accuracy: 67.1734%, Training Loss: 0.6949%\n",
      "Epoch [21/300], Step [222/225], Training Accuracy: 67.1734%, Training Loss: 0.6948%\n",
      "Epoch [21/300], Step [223/225], Training Accuracy: 67.1455%, Training Loss: 0.6948%\n",
      "Epoch [21/300], Step [224/225], Training Accuracy: 67.1456%, Training Loss: 0.6950%\n",
      "Epoch [21/300], Step [225/225], Training Accuracy: 67.1276%, Training Loss: 0.6952%\n",
      "Epoch [22/300], Step [1/225], Training Accuracy: 67.1875%, Training Loss: 0.6390%\n",
      "Epoch [22/300], Step [2/225], Training Accuracy: 65.6250%, Training Loss: 0.6622%\n",
      "Epoch [22/300], Step [3/225], Training Accuracy: 67.1875%, Training Loss: 0.6649%\n",
      "Epoch [22/300], Step [4/225], Training Accuracy: 67.9688%, Training Loss: 0.6636%\n",
      "Epoch [22/300], Step [5/225], Training Accuracy: 68.1250%, Training Loss: 0.6486%\n",
      "Epoch [22/300], Step [6/225], Training Accuracy: 67.4479%, Training Loss: 0.6768%\n",
      "Epoch [22/300], Step [7/225], Training Accuracy: 66.9643%, Training Loss: 0.6766%\n",
      "Epoch [22/300], Step [8/225], Training Accuracy: 66.4062%, Training Loss: 0.6833%\n",
      "Epoch [22/300], Step [9/225], Training Accuracy: 66.3194%, Training Loss: 0.6952%\n",
      "Epoch [22/300], Step [10/225], Training Accuracy: 65.9375%, Training Loss: 0.7039%\n",
      "Epoch [22/300], Step [11/225], Training Accuracy: 65.9091%, Training Loss: 0.7029%\n",
      "Epoch [22/300], Step [12/225], Training Accuracy: 65.4948%, Training Loss: 0.7059%\n",
      "Epoch [22/300], Step [13/225], Training Accuracy: 66.3462%, Training Loss: 0.6921%\n",
      "Epoch [22/300], Step [14/225], Training Accuracy: 66.7411%, Training Loss: 0.6874%\n",
      "Epoch [22/300], Step [15/225], Training Accuracy: 66.7708%, Training Loss: 0.6882%\n",
      "Epoch [22/300], Step [16/225], Training Accuracy: 66.9922%, Training Loss: 0.6859%\n",
      "Epoch [22/300], Step [17/225], Training Accuracy: 67.0037%, Training Loss: 0.6822%\n",
      "Epoch [22/300], Step [18/225], Training Accuracy: 67.1875%, Training Loss: 0.6831%\n",
      "Epoch [22/300], Step [19/225], Training Accuracy: 67.5164%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [20/225], Training Accuracy: 67.9688%, Training Loss: 0.6812%\n",
      "Epoch [22/300], Step [21/225], Training Accuracy: 68.0060%, Training Loss: 0.6761%\n",
      "Epoch [22/300], Step [22/225], Training Accuracy: 67.5426%, Training Loss: 0.6844%\n",
      "Epoch [22/300], Step [23/225], Training Accuracy: 67.5951%, Training Loss: 0.6821%\n",
      "Epoch [22/300], Step [24/225], Training Accuracy: 67.1224%, Training Loss: 0.6868%\n",
      "Epoch [22/300], Step [25/225], Training Accuracy: 67.4375%, Training Loss: 0.6830%\n",
      "Epoch [22/300], Step [26/225], Training Accuracy: 67.6082%, Training Loss: 0.6826%\n",
      "Epoch [22/300], Step [27/225], Training Accuracy: 67.8241%, Training Loss: 0.6774%\n",
      "Epoch [22/300], Step [28/225], Training Accuracy: 68.3036%, Training Loss: 0.6704%\n",
      "Epoch [22/300], Step [29/225], Training Accuracy: 68.2112%, Training Loss: 0.6701%\n",
      "Epoch [22/300], Step [30/225], Training Accuracy: 68.4375%, Training Loss: 0.6692%\n",
      "Epoch [22/300], Step [31/225], Training Accuracy: 68.1956%, Training Loss: 0.6763%\n",
      "Epoch [22/300], Step [32/225], Training Accuracy: 67.9199%, Training Loss: 0.6761%\n",
      "Epoch [22/300], Step [33/225], Training Accuracy: 67.9924%, Training Loss: 0.6733%\n",
      "Epoch [22/300], Step [34/225], Training Accuracy: 68.1985%, Training Loss: 0.6717%\n",
      "Epoch [22/300], Step [35/225], Training Accuracy: 68.2589%, Training Loss: 0.6702%\n",
      "Epoch [22/300], Step [36/225], Training Accuracy: 68.2726%, Training Loss: 0.6691%\n",
      "Epoch [22/300], Step [37/225], Training Accuracy: 68.4544%, Training Loss: 0.6667%\n",
      "Epoch [22/300], Step [38/225], Training Accuracy: 68.4211%, Training Loss: 0.6673%\n",
      "Epoch [22/300], Step [39/225], Training Accuracy: 68.3494%, Training Loss: 0.6685%\n",
      "Epoch [22/300], Step [40/225], Training Accuracy: 68.2031%, Training Loss: 0.6705%\n",
      "Epoch [22/300], Step [41/225], Training Accuracy: 68.2927%, Training Loss: 0.6703%\n",
      "Epoch [22/300], Step [42/225], Training Accuracy: 68.2292%, Training Loss: 0.6692%\n",
      "Epoch [22/300], Step [43/225], Training Accuracy: 68.2413%, Training Loss: 0.6707%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Step [44/225], Training Accuracy: 68.3239%, Training Loss: 0.6692%\n",
      "Epoch [22/300], Step [45/225], Training Accuracy: 68.4722%, Training Loss: 0.6669%\n",
      "Epoch [22/300], Step [46/225], Training Accuracy: 68.3764%, Training Loss: 0.6663%\n",
      "Epoch [22/300], Step [47/225], Training Accuracy: 68.2846%, Training Loss: 0.6669%\n",
      "Epoch [22/300], Step [48/225], Training Accuracy: 68.0990%, Training Loss: 0.6673%\n",
      "Epoch [22/300], Step [49/225], Training Accuracy: 68.3036%, Training Loss: 0.6639%\n",
      "Epoch [22/300], Step [50/225], Training Accuracy: 68.4062%, Training Loss: 0.6634%\n",
      "Epoch [22/300], Step [51/225], Training Accuracy: 68.5049%, Training Loss: 0.6636%\n",
      "Epoch [22/300], Step [52/225], Training Accuracy: 68.5096%, Training Loss: 0.6638%\n",
      "Epoch [22/300], Step [53/225], Training Accuracy: 68.5731%, Training Loss: 0.6636%\n",
      "Epoch [22/300], Step [54/225], Training Accuracy: 68.5764%, Training Loss: 0.6647%\n",
      "Epoch [22/300], Step [55/225], Training Accuracy: 68.5511%, Training Loss: 0.6660%\n",
      "Epoch [22/300], Step [56/225], Training Accuracy: 68.5826%, Training Loss: 0.6657%\n",
      "Epoch [22/300], Step [57/225], Training Accuracy: 68.4759%, Training Loss: 0.6660%\n",
      "Epoch [22/300], Step [58/225], Training Accuracy: 68.4806%, Training Loss: 0.6663%\n",
      "Epoch [22/300], Step [59/225], Training Accuracy: 68.4587%, Training Loss: 0.6653%\n",
      "Epoch [22/300], Step [60/225], Training Accuracy: 68.5938%, Training Loss: 0.6645%\n",
      "Epoch [22/300], Step [61/225], Training Accuracy: 68.6219%, Training Loss: 0.6650%\n",
      "Epoch [22/300], Step [62/225], Training Accuracy: 68.6744%, Training Loss: 0.6652%\n",
      "Epoch [22/300], Step [63/225], Training Accuracy: 68.5020%, Training Loss: 0.6669%\n",
      "Epoch [22/300], Step [64/225], Training Accuracy: 68.5791%, Training Loss: 0.6663%\n",
      "Epoch [22/300], Step [65/225], Training Accuracy: 68.6058%, Training Loss: 0.6671%\n",
      "Epoch [22/300], Step [66/225], Training Accuracy: 68.5606%, Training Loss: 0.6669%\n",
      "Epoch [22/300], Step [67/225], Training Accuracy: 68.4935%, Training Loss: 0.6665%\n",
      "Epoch [22/300], Step [68/225], Training Accuracy: 68.5432%, Training Loss: 0.6670%\n",
      "Epoch [22/300], Step [69/225], Training Accuracy: 68.4556%, Training Loss: 0.6674%\n",
      "Epoch [22/300], Step [70/225], Training Accuracy: 68.3705%, Training Loss: 0.6675%\n",
      "Epoch [22/300], Step [71/225], Training Accuracy: 68.3759%, Training Loss: 0.6662%\n",
      "Epoch [22/300], Step [72/225], Training Accuracy: 68.3811%, Training Loss: 0.6658%\n",
      "Epoch [22/300], Step [73/225], Training Accuracy: 68.3647%, Training Loss: 0.6652%\n",
      "Epoch [22/300], Step [74/225], Training Accuracy: 68.3488%, Training Loss: 0.6650%\n",
      "Epoch [22/300], Step [75/225], Training Accuracy: 68.4375%, Training Loss: 0.6641%\n",
      "Epoch [22/300], Step [76/225], Training Accuracy: 68.3594%, Training Loss: 0.6658%\n",
      "Epoch [22/300], Step [77/225], Training Accuracy: 68.4456%, Training Loss: 0.6647%\n",
      "Epoch [22/300], Step [78/225], Training Accuracy: 68.3694%, Training Loss: 0.6649%\n",
      "Epoch [22/300], Step [79/225], Training Accuracy: 68.3940%, Training Loss: 0.6651%\n",
      "Epoch [22/300], Step [80/225], Training Accuracy: 68.4961%, Training Loss: 0.6649%\n",
      "Epoch [22/300], Step [81/225], Training Accuracy: 68.5764%, Training Loss: 0.6649%\n",
      "Epoch [22/300], Step [82/225], Training Accuracy: 68.5976%, Training Loss: 0.6656%\n",
      "Epoch [22/300], Step [83/225], Training Accuracy: 68.5806%, Training Loss: 0.6653%\n",
      "Epoch [22/300], Step [84/225], Training Accuracy: 68.5268%, Training Loss: 0.6654%\n",
      "Epoch [22/300], Step [85/225], Training Accuracy: 68.6581%, Training Loss: 0.6637%\n",
      "Epoch [22/300], Step [86/225], Training Accuracy: 68.6410%, Training Loss: 0.6637%\n",
      "Epoch [22/300], Step [87/225], Training Accuracy: 68.6602%, Training Loss: 0.6638%\n",
      "Epoch [22/300], Step [88/225], Training Accuracy: 68.6612%, Training Loss: 0.6643%\n",
      "Epoch [22/300], Step [89/225], Training Accuracy: 68.5569%, Training Loss: 0.6659%\n",
      "Epoch [22/300], Step [90/225], Training Accuracy: 68.5069%, Training Loss: 0.6668%\n",
      "Epoch [22/300], Step [91/225], Training Accuracy: 68.4066%, Training Loss: 0.6677%\n",
      "Epoch [22/300], Step [92/225], Training Accuracy: 68.3933%, Training Loss: 0.6681%\n",
      "Epoch [22/300], Step [93/225], Training Accuracy: 68.3804%, Training Loss: 0.6674%\n",
      "Epoch [22/300], Step [94/225], Training Accuracy: 68.4009%, Training Loss: 0.6667%\n",
      "Epoch [22/300], Step [95/225], Training Accuracy: 68.3717%, Training Loss: 0.6682%\n",
      "Epoch [22/300], Step [96/225], Training Accuracy: 68.4082%, Training Loss: 0.6670%\n",
      "Epoch [22/300], Step [97/225], Training Accuracy: 68.4278%, Training Loss: 0.6667%\n",
      "Epoch [22/300], Step [98/225], Training Accuracy: 68.3514%, Training Loss: 0.6680%\n",
      "Epoch [22/300], Step [99/225], Training Accuracy: 68.2923%, Training Loss: 0.6691%\n",
      "Epoch [22/300], Step [100/225], Training Accuracy: 68.2656%, Training Loss: 0.6702%\n",
      "Epoch [22/300], Step [101/225], Training Accuracy: 68.3323%, Training Loss: 0.6699%\n",
      "Epoch [22/300], Step [102/225], Training Accuracy: 68.2598%, Training Loss: 0.6712%\n",
      "Epoch [22/300], Step [103/225], Training Accuracy: 68.2797%, Training Loss: 0.6703%\n",
      "Epoch [22/300], Step [104/225], Training Accuracy: 68.1641%, Training Loss: 0.6720%\n",
      "Epoch [22/300], Step [105/225], Training Accuracy: 68.1845%, Training Loss: 0.6711%\n",
      "Epoch [22/300], Step [106/225], Training Accuracy: 68.2046%, Training Loss: 0.6710%\n",
      "Epoch [22/300], Step [107/225], Training Accuracy: 68.1951%, Training Loss: 0.6723%\n",
      "Epoch [22/300], Step [108/225], Training Accuracy: 68.1279%, Training Loss: 0.6731%\n",
      "Epoch [22/300], Step [109/225], Training Accuracy: 68.0619%, Training Loss: 0.6732%\n",
      "Epoch [22/300], Step [110/225], Training Accuracy: 68.1250%, Training Loss: 0.6725%\n",
      "Epoch [22/300], Step [111/225], Training Accuracy: 68.1729%, Training Loss: 0.6723%\n",
      "Epoch [22/300], Step [112/225], Training Accuracy: 68.1641%, Training Loss: 0.6727%\n",
      "Epoch [22/300], Step [113/225], Training Accuracy: 68.1692%, Training Loss: 0.6724%\n",
      "Epoch [22/300], Step [114/225], Training Accuracy: 68.1332%, Training Loss: 0.6728%\n",
      "Epoch [22/300], Step [115/225], Training Accuracy: 68.2201%, Training Loss: 0.6721%\n",
      "Epoch [22/300], Step [116/225], Training Accuracy: 68.2651%, Training Loss: 0.6717%\n",
      "Epoch [22/300], Step [117/225], Training Accuracy: 68.2559%, Training Loss: 0.6731%\n",
      "Epoch [22/300], Step [118/225], Training Accuracy: 68.1806%, Training Loss: 0.6739%\n",
      "Epoch [22/300], Step [119/225], Training Accuracy: 68.1329%, Training Loss: 0.6746%\n",
      "Epoch [22/300], Step [120/225], Training Accuracy: 68.1510%, Training Loss: 0.6751%\n",
      "Epoch [22/300], Step [121/225], Training Accuracy: 68.0527%, Training Loss: 0.6763%\n",
      "Epoch [22/300], Step [122/225], Training Accuracy: 68.0968%, Training Loss: 0.6762%\n",
      "Epoch [22/300], Step [123/225], Training Accuracy: 68.0386%, Training Loss: 0.6768%\n",
      "Epoch [22/300], Step [124/225], Training Accuracy: 68.0570%, Training Loss: 0.6764%\n",
      "Epoch [22/300], Step [125/225], Training Accuracy: 67.9750%, Training Loss: 0.6774%\n",
      "Epoch [22/300], Step [126/225], Training Accuracy: 68.0060%, Training Loss: 0.6772%\n",
      "Epoch [22/300], Step [127/225], Training Accuracy: 67.9872%, Training Loss: 0.6778%\n",
      "Epoch [22/300], Step [128/225], Training Accuracy: 67.9565%, Training Loss: 0.6786%\n",
      "Epoch [22/300], Step [129/225], Training Accuracy: 67.9748%, Training Loss: 0.6789%\n",
      "Epoch [22/300], Step [130/225], Training Accuracy: 67.9688%, Training Loss: 0.6795%\n",
      "Epoch [22/300], Step [131/225], Training Accuracy: 67.9509%, Training Loss: 0.6796%\n",
      "Epoch [22/300], Step [132/225], Training Accuracy: 67.9096%, Training Loss: 0.6805%\n",
      "Epoch [22/300], Step [133/225], Training Accuracy: 67.9159%, Training Loss: 0.6805%\n",
      "Epoch [22/300], Step [134/225], Training Accuracy: 67.8288%, Training Loss: 0.6825%\n",
      "Epoch [22/300], Step [135/225], Training Accuracy: 67.7894%, Training Loss: 0.6829%\n",
      "Epoch [22/300], Step [136/225], Training Accuracy: 67.8309%, Training Loss: 0.6829%\n",
      "Epoch [22/300], Step [137/225], Training Accuracy: 67.8262%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [138/225], Training Accuracy: 67.9461%, Training Loss: 0.6812%\n",
      "Epoch [22/300], Step [139/225], Training Accuracy: 67.9182%, Training Loss: 0.6816%\n",
      "Epoch [22/300], Step [140/225], Training Accuracy: 67.9688%, Training Loss: 0.6815%\n",
      "Epoch [22/300], Step [141/225], Training Accuracy: 67.9410%, Training Loss: 0.6820%\n",
      "Epoch [22/300], Step [142/225], Training Accuracy: 67.9247%, Training Loss: 0.6823%\n",
      "Epoch [22/300], Step [143/225], Training Accuracy: 67.9305%, Training Loss: 0.6823%\n",
      "Epoch [22/300], Step [144/225], Training Accuracy: 67.9145%, Training Loss: 0.6828%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Step [145/225], Training Accuracy: 67.9634%, Training Loss: 0.6823%\n",
      "Epoch [22/300], Step [146/225], Training Accuracy: 67.9259%, Training Loss: 0.6828%\n",
      "Epoch [22/300], Step [147/225], Training Accuracy: 67.8465%, Training Loss: 0.6833%\n",
      "Epoch [22/300], Step [148/225], Training Accuracy: 67.9054%, Training Loss: 0.6824%\n",
      "Epoch [22/300], Step [149/225], Training Accuracy: 67.9111%, Training Loss: 0.6822%\n",
      "Epoch [22/300], Step [150/225], Training Accuracy: 67.8958%, Training Loss: 0.6820%\n",
      "Epoch [22/300], Step [151/225], Training Accuracy: 67.9739%, Training Loss: 0.6810%\n",
      "Epoch [22/300], Step [152/225], Training Accuracy: 67.8968%, Training Loss: 0.6822%\n",
      "Epoch [22/300], Step [153/225], Training Accuracy: 67.8922%, Training Loss: 0.6828%\n",
      "Epoch [22/300], Step [154/225], Training Accuracy: 67.8673%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [155/225], Training Accuracy: 67.8125%, Training Loss: 0.6837%\n",
      "Epoch [22/300], Step [156/225], Training Accuracy: 67.7985%, Training Loss: 0.6841%\n",
      "Epoch [22/300], Step [157/225], Training Accuracy: 67.7747%, Training Loss: 0.6846%\n",
      "Epoch [22/300], Step [158/225], Training Accuracy: 67.7611%, Training Loss: 0.6852%\n",
      "Epoch [22/300], Step [159/225], Training Accuracy: 67.6789%, Training Loss: 0.6861%\n",
      "Epoch [22/300], Step [160/225], Training Accuracy: 67.6758%, Training Loss: 0.6862%\n",
      "Epoch [22/300], Step [161/225], Training Accuracy: 67.6339%, Training Loss: 0.6868%\n",
      "Epoch [22/300], Step [162/225], Training Accuracy: 67.7083%, Training Loss: 0.6860%\n",
      "Epoch [22/300], Step [163/225], Training Accuracy: 67.6956%, Training Loss: 0.6859%\n",
      "Epoch [22/300], Step [164/225], Training Accuracy: 67.7401%, Training Loss: 0.6849%\n",
      "Epoch [22/300], Step [165/225], Training Accuracy: 67.7367%, Training Loss: 0.6848%\n",
      "Epoch [22/300], Step [166/225], Training Accuracy: 67.7805%, Training Loss: 0.6845%\n",
      "Epoch [22/300], Step [167/225], Training Accuracy: 67.7395%, Training Loss: 0.6849%\n",
      "Epoch [22/300], Step [168/225], Training Accuracy: 67.7269%, Training Loss: 0.6851%\n",
      "Epoch [22/300], Step [169/225], Training Accuracy: 67.7053%, Training Loss: 0.6857%\n",
      "Epoch [22/300], Step [170/225], Training Accuracy: 67.6838%, Training Loss: 0.6862%\n",
      "Epoch [22/300], Step [171/225], Training Accuracy: 67.7083%, Training Loss: 0.6859%\n",
      "Epoch [22/300], Step [172/225], Training Accuracy: 67.6962%, Training Loss: 0.6859%\n",
      "Epoch [22/300], Step [173/225], Training Accuracy: 67.6662%, Training Loss: 0.6862%\n",
      "Epoch [22/300], Step [174/225], Training Accuracy: 67.6545%, Training Loss: 0.6864%\n",
      "Epoch [22/300], Step [175/225], Training Accuracy: 67.6964%, Training Loss: 0.6855%\n",
      "Epoch [22/300], Step [176/225], Training Accuracy: 67.6847%, Training Loss: 0.6853%\n",
      "Epoch [22/300], Step [177/225], Training Accuracy: 67.6995%, Training Loss: 0.6854%\n",
      "Epoch [22/300], Step [178/225], Training Accuracy: 67.6966%, Training Loss: 0.6851%\n",
      "Epoch [22/300], Step [179/225], Training Accuracy: 67.7636%, Training Loss: 0.6844%\n",
      "Epoch [22/300], Step [180/225], Training Accuracy: 67.7431%, Training Loss: 0.6849%\n",
      "Epoch [22/300], Step [181/225], Training Accuracy: 67.6968%, Training Loss: 0.6853%\n",
      "Epoch [22/300], Step [182/225], Training Accuracy: 67.7370%, Training Loss: 0.6850%\n",
      "Epoch [22/300], Step [183/225], Training Accuracy: 67.7510%, Training Loss: 0.6847%\n",
      "Epoch [22/300], Step [184/225], Training Accuracy: 67.7904%, Training Loss: 0.6839%\n",
      "Epoch [22/300], Step [185/225], Training Accuracy: 67.7872%, Training Loss: 0.6839%\n",
      "Epoch [22/300], Step [186/225], Training Accuracy: 67.8427%, Training Loss: 0.6830%\n",
      "Epoch [22/300], Step [187/225], Training Accuracy: 67.8476%, Training Loss: 0.6829%\n",
      "Epoch [22/300], Step [188/225], Training Accuracy: 67.8441%, Training Loss: 0.6825%\n",
      "Epoch [22/300], Step [189/225], Training Accuracy: 67.8654%, Training Loss: 0.6820%\n",
      "Epoch [22/300], Step [190/225], Training Accuracy: 67.8289%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [191/225], Training Accuracy: 67.8501%, Training Loss: 0.6824%\n",
      "Epoch [22/300], Step [192/225], Training Accuracy: 67.8711%, Training Loss: 0.6821%\n",
      "Epoch [22/300], Step [193/225], Training Accuracy: 67.8756%, Training Loss: 0.6824%\n",
      "Epoch [22/300], Step [194/225], Training Accuracy: 67.8963%, Training Loss: 0.6823%\n",
      "Epoch [22/300], Step [195/225], Training Accuracy: 67.9006%, Training Loss: 0.6818%\n",
      "Epoch [22/300], Step [196/225], Training Accuracy: 67.8332%, Training Loss: 0.6831%\n",
      "Epoch [22/300], Step [197/225], Training Accuracy: 67.7903%, Training Loss: 0.6835%\n",
      "Epoch [22/300], Step [198/225], Training Accuracy: 67.8504%, Training Loss: 0.6825%\n",
      "Epoch [22/300], Step [199/225], Training Accuracy: 67.8549%, Training Loss: 0.6819%\n",
      "Epoch [22/300], Step [200/225], Training Accuracy: 67.8359%, Training Loss: 0.6821%\n",
      "Epoch [22/300], Step [201/225], Training Accuracy: 67.7861%, Training Loss: 0.6831%\n",
      "Epoch [22/300], Step [202/225], Training Accuracy: 67.8218%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [203/225], Training Accuracy: 67.8648%, Training Loss: 0.6824%\n",
      "Epoch [22/300], Step [204/225], Training Accuracy: 67.8462%, Training Loss: 0.6825%\n",
      "Epoch [22/300], Step [205/225], Training Accuracy: 67.8735%, Training Loss: 0.6819%\n",
      "Epoch [22/300], Step [206/225], Training Accuracy: 67.8322%, Training Loss: 0.6826%\n",
      "Epoch [22/300], Step [207/225], Training Accuracy: 67.8291%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [208/225], Training Accuracy: 67.9011%, Training Loss: 0.6821%\n",
      "Epoch [22/300], Step [209/225], Training Accuracy: 67.9426%, Training Loss: 0.6819%\n",
      "Epoch [22/300], Step [210/225], Training Accuracy: 67.8943%, Training Loss: 0.6823%\n",
      "Epoch [22/300], Step [211/225], Training Accuracy: 67.9354%, Training Loss: 0.6821%\n",
      "Epoch [22/300], Step [212/225], Training Accuracy: 67.8803%, Training Loss: 0.6827%\n",
      "Epoch [22/300], Step [213/225], Training Accuracy: 67.8477%, Training Loss: 0.6833%\n",
      "Epoch [22/300], Step [214/225], Training Accuracy: 67.8738%, Training Loss: 0.6828%\n",
      "Epoch [22/300], Step [215/225], Training Accuracy: 67.8343%, Training Loss: 0.6829%\n",
      "Epoch [22/300], Step [216/225], Training Accuracy: 67.8385%, Training Loss: 0.6828%\n",
      "Epoch [22/300], Step [217/225], Training Accuracy: 67.8283%, Training Loss: 0.6831%\n",
      "Epoch [22/300], Step [218/225], Training Accuracy: 67.7824%, Training Loss: 0.6834%\n",
      "Epoch [22/300], Step [219/225], Training Accuracy: 67.8011%, Training Loss: 0.6832%\n",
      "Epoch [22/300], Step [220/225], Training Accuracy: 67.7912%, Training Loss: 0.6835%\n",
      "Epoch [22/300], Step [221/225], Training Accuracy: 67.7885%, Training Loss: 0.6837%\n",
      "Epoch [22/300], Step [222/225], Training Accuracy: 67.8139%, Training Loss: 0.6834%\n",
      "Epoch [22/300], Step [223/225], Training Accuracy: 67.7691%, Training Loss: 0.6837%\n",
      "Epoch [22/300], Step [224/225], Training Accuracy: 67.7734%, Training Loss: 0.6834%\n",
      "Epoch [22/300], Step [225/225], Training Accuracy: 67.7668%, Training Loss: 0.6836%\n",
      "Epoch [23/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.5705%\n",
      "Epoch [23/300], Step [2/225], Training Accuracy: 67.9688%, Training Loss: 0.6296%\n",
      "Epoch [23/300], Step [3/225], Training Accuracy: 69.2708%, Training Loss: 0.6402%\n",
      "Epoch [23/300], Step [4/225], Training Accuracy: 67.9688%, Training Loss: 0.6602%\n",
      "Epoch [23/300], Step [5/225], Training Accuracy: 68.1250%, Training Loss: 0.6473%\n",
      "Epoch [23/300], Step [6/225], Training Accuracy: 67.1875%, Training Loss: 0.6895%\n",
      "Epoch [23/300], Step [7/225], Training Accuracy: 67.6339%, Training Loss: 0.6830%\n",
      "Epoch [23/300], Step [8/225], Training Accuracy: 66.9922%, Training Loss: 0.6904%\n",
      "Epoch [23/300], Step [9/225], Training Accuracy: 66.4931%, Training Loss: 0.7050%\n",
      "Epoch [23/300], Step [10/225], Training Accuracy: 65.6250%, Training Loss: 0.7195%\n",
      "Epoch [23/300], Step [11/225], Training Accuracy: 65.9091%, Training Loss: 0.7108%\n",
      "Epoch [23/300], Step [12/225], Training Accuracy: 65.7552%, Training Loss: 0.7157%\n",
      "Epoch [23/300], Step [13/225], Training Accuracy: 65.7452%, Training Loss: 0.7096%\n",
      "Epoch [23/300], Step [14/225], Training Accuracy: 65.9598%, Training Loss: 0.7036%\n",
      "Epoch [23/300], Step [15/225], Training Accuracy: 66.0417%, Training Loss: 0.7027%\n",
      "Epoch [23/300], Step [16/225], Training Accuracy: 66.2109%, Training Loss: 0.6986%\n",
      "Epoch [23/300], Step [17/225], Training Accuracy: 66.6360%, Training Loss: 0.6925%\n",
      "Epoch [23/300], Step [18/225], Training Accuracy: 66.4931%, Training Loss: 0.6964%\n",
      "Epoch [23/300], Step [19/225], Training Accuracy: 66.6118%, Training Loss: 0.6979%\n",
      "Epoch [23/300], Step [20/225], Training Accuracy: 66.7188%, Training Loss: 0.6939%\n",
      "Epoch [23/300], Step [21/225], Training Accuracy: 66.9643%, Training Loss: 0.6873%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Step [22/225], Training Accuracy: 66.4773%, Training Loss: 0.6983%\n",
      "Epoch [23/300], Step [23/225], Training Accuracy: 66.7799%, Training Loss: 0.6934%\n",
      "Epoch [23/300], Step [24/225], Training Accuracy: 66.3411%, Training Loss: 0.7027%\n",
      "Epoch [23/300], Step [25/225], Training Accuracy: 66.5000%, Training Loss: 0.7012%\n",
      "Epoch [23/300], Step [26/225], Training Accuracy: 66.4062%, Training Loss: 0.7016%\n",
      "Epoch [23/300], Step [27/225], Training Accuracy: 66.4352%, Training Loss: 0.6993%\n",
      "Epoch [23/300], Step [28/225], Training Accuracy: 66.7969%, Training Loss: 0.6940%\n",
      "Epoch [23/300], Step [29/225], Training Accuracy: 66.8103%, Training Loss: 0.6954%\n",
      "Epoch [23/300], Step [30/225], Training Accuracy: 66.7708%, Training Loss: 0.6946%\n",
      "Epoch [23/300], Step [31/225], Training Accuracy: 66.5827%, Training Loss: 0.6989%\n",
      "Epoch [23/300], Step [32/225], Training Accuracy: 66.4551%, Training Loss: 0.6999%\n",
      "Epoch [23/300], Step [33/225], Training Accuracy: 66.4299%, Training Loss: 0.6992%\n",
      "Epoch [23/300], Step [34/225], Training Accuracy: 66.4062%, Training Loss: 0.6979%\n",
      "Epoch [23/300], Step [35/225], Training Accuracy: 66.4732%, Training Loss: 0.6972%\n",
      "Epoch [23/300], Step [36/225], Training Accuracy: 66.4062%, Training Loss: 0.6958%\n",
      "Epoch [23/300], Step [37/225], Training Accuracy: 66.5118%, Training Loss: 0.6947%\n",
      "Epoch [23/300], Step [38/225], Training Accuracy: 66.4885%, Training Loss: 0.6936%\n",
      "Epoch [23/300], Step [39/225], Training Accuracy: 66.5064%, Training Loss: 0.6939%\n",
      "Epoch [23/300], Step [40/225], Training Accuracy: 66.2891%, Training Loss: 0.6949%\n",
      "Epoch [23/300], Step [41/225], Training Accuracy: 66.3491%, Training Loss: 0.6952%\n",
      "Epoch [23/300], Step [42/225], Training Accuracy: 66.2202%, Training Loss: 0.6947%\n",
      "Epoch [23/300], Step [43/225], Training Accuracy: 66.0974%, Training Loss: 0.6948%\n",
      "Epoch [23/300], Step [44/225], Training Accuracy: 66.5128%, Training Loss: 0.6916%\n",
      "Epoch [23/300], Step [45/225], Training Accuracy: 66.6319%, Training Loss: 0.6891%\n",
      "Epoch [23/300], Step [46/225], Training Accuracy: 66.6780%, Training Loss: 0.6869%\n",
      "Epoch [23/300], Step [47/225], Training Accuracy: 66.5891%, Training Loss: 0.6869%\n",
      "Epoch [23/300], Step [48/225], Training Accuracy: 66.6016%, Training Loss: 0.6875%\n",
      "Epoch [23/300], Step [49/225], Training Accuracy: 66.9324%, Training Loss: 0.6841%\n",
      "Epoch [23/300], Step [50/225], Training Accuracy: 67.2188%, Training Loss: 0.6830%\n",
      "Epoch [23/300], Step [51/225], Training Accuracy: 67.4326%, Training Loss: 0.6814%\n",
      "Epoch [23/300], Step [52/225], Training Accuracy: 67.4279%, Training Loss: 0.6805%\n",
      "Epoch [23/300], Step [53/225], Training Accuracy: 67.4528%, Training Loss: 0.6800%\n",
      "Epoch [23/300], Step [54/225], Training Accuracy: 67.3611%, Training Loss: 0.6815%\n",
      "Epoch [23/300], Step [55/225], Training Accuracy: 67.2727%, Training Loss: 0.6826%\n",
      "Epoch [23/300], Step [56/225], Training Accuracy: 67.1317%, Training Loss: 0.6830%\n",
      "Epoch [23/300], Step [57/225], Training Accuracy: 67.1327%, Training Loss: 0.6815%\n",
      "Epoch [23/300], Step [58/225], Training Accuracy: 67.1067%, Training Loss: 0.6822%\n",
      "Epoch [23/300], Step [59/225], Training Accuracy: 67.2140%, Training Loss: 0.6810%\n",
      "Epoch [23/300], Step [60/225], Training Accuracy: 67.2396%, Training Loss: 0.6803%\n",
      "Epoch [23/300], Step [61/225], Training Accuracy: 67.1875%, Training Loss: 0.6810%\n",
      "Epoch [23/300], Step [62/225], Training Accuracy: 67.2379%, Training Loss: 0.6810%\n",
      "Epoch [23/300], Step [63/225], Training Accuracy: 67.0883%, Training Loss: 0.6830%\n",
      "Epoch [23/300], Step [64/225], Training Accuracy: 67.3096%, Training Loss: 0.6803%\n",
      "Epoch [23/300], Step [65/225], Training Accuracy: 67.2837%, Training Loss: 0.6803%\n",
      "Epoch [23/300], Step [66/225], Training Accuracy: 67.4006%, Training Loss: 0.6785%\n",
      "Epoch [23/300], Step [67/225], Training Accuracy: 67.3974%, Training Loss: 0.6774%\n",
      "Epoch [23/300], Step [68/225], Training Accuracy: 67.3254%, Training Loss: 0.6780%\n",
      "Epoch [23/300], Step [69/225], Training Accuracy: 67.2328%, Training Loss: 0.6784%\n",
      "Epoch [23/300], Step [70/225], Training Accuracy: 67.2768%, Training Loss: 0.6783%\n",
      "Epoch [23/300], Step [71/225], Training Accuracy: 67.3636%, Training Loss: 0.6764%\n",
      "Epoch [23/300], Step [72/225], Training Accuracy: 67.3611%, Training Loss: 0.6759%\n",
      "Epoch [23/300], Step [73/225], Training Accuracy: 67.4229%, Training Loss: 0.6754%\n",
      "Epoch [23/300], Step [74/225], Training Accuracy: 67.3775%, Training Loss: 0.6756%\n",
      "Epoch [23/300], Step [75/225], Training Accuracy: 67.4375%, Training Loss: 0.6747%\n",
      "Epoch [23/300], Step [76/225], Training Accuracy: 67.2492%, Training Loss: 0.6766%\n",
      "Epoch [23/300], Step [77/225], Training Accuracy: 67.3295%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [78/225], Training Accuracy: 67.3277%, Training Loss: 0.6752%\n",
      "Epoch [23/300], Step [79/225], Training Accuracy: 67.3062%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [80/225], Training Accuracy: 67.2852%, Training Loss: 0.6762%\n",
      "Epoch [23/300], Step [81/225], Training Accuracy: 67.2840%, Training Loss: 0.6769%\n",
      "Epoch [23/300], Step [82/225], Training Accuracy: 67.3590%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [83/225], Training Accuracy: 67.3758%, Training Loss: 0.6754%\n",
      "Epoch [23/300], Step [84/225], Training Accuracy: 67.4479%, Training Loss: 0.6752%\n",
      "Epoch [23/300], Step [85/225], Training Accuracy: 67.4265%, Training Loss: 0.6745%\n",
      "Epoch [23/300], Step [86/225], Training Accuracy: 67.4237%, Training Loss: 0.6739%\n",
      "Epoch [23/300], Step [87/225], Training Accuracy: 67.4389%, Training Loss: 0.6737%\n",
      "Epoch [23/300], Step [88/225], Training Accuracy: 67.3295%, Training Loss: 0.6744%\n",
      "Epoch [23/300], Step [89/225], Training Accuracy: 67.3455%, Training Loss: 0.6749%\n",
      "Epoch [23/300], Step [90/225], Training Accuracy: 67.3090%, Training Loss: 0.6761%\n",
      "Epoch [23/300], Step [91/225], Training Accuracy: 67.2562%, Training Loss: 0.6764%\n",
      "Epoch [23/300], Step [92/225], Training Accuracy: 67.2724%, Training Loss: 0.6763%\n",
      "Epoch [23/300], Step [93/225], Training Accuracy: 67.3891%, Training Loss: 0.6746%\n",
      "Epoch [23/300], Step [94/225], Training Accuracy: 67.4701%, Training Loss: 0.6735%\n",
      "Epoch [23/300], Step [95/225], Training Accuracy: 67.5493%, Training Loss: 0.6740%\n",
      "Epoch [23/300], Step [96/225], Training Accuracy: 67.5618%, Training Loss: 0.6731%\n",
      "Epoch [23/300], Step [97/225], Training Accuracy: 67.5902%, Training Loss: 0.6722%\n",
      "Epoch [23/300], Step [98/225], Training Accuracy: 67.4904%, Training Loss: 0.6736%\n",
      "Epoch [23/300], Step [99/225], Training Accuracy: 67.4558%, Training Loss: 0.6741%\n",
      "Epoch [23/300], Step [100/225], Training Accuracy: 67.3594%, Training Loss: 0.6753%\n",
      "Epoch [23/300], Step [101/225], Training Accuracy: 67.4196%, Training Loss: 0.6754%\n",
      "Epoch [23/300], Step [102/225], Training Accuracy: 67.4173%, Training Loss: 0.6757%\n",
      "Epoch [23/300], Step [103/225], Training Accuracy: 67.4150%, Training Loss: 0.6754%\n",
      "Epoch [23/300], Step [104/225], Training Accuracy: 67.3377%, Training Loss: 0.6766%\n",
      "Epoch [23/300], Step [105/225], Training Accuracy: 67.3661%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [106/225], Training Accuracy: 67.3939%, Training Loss: 0.6758%\n",
      "Epoch [23/300], Step [107/225], Training Accuracy: 67.3043%, Training Loss: 0.6764%\n",
      "Epoch [23/300], Step [108/225], Training Accuracy: 67.3466%, Training Loss: 0.6764%\n",
      "Epoch [23/300], Step [109/225], Training Accuracy: 67.3308%, Training Loss: 0.6758%\n",
      "Epoch [23/300], Step [110/225], Training Accuracy: 67.3011%, Training Loss: 0.6756%\n",
      "Epoch [23/300], Step [111/225], Training Accuracy: 67.3423%, Training Loss: 0.6747%\n",
      "Epoch [23/300], Step [112/225], Training Accuracy: 67.2991%, Training Loss: 0.6751%\n",
      "Epoch [23/300], Step [113/225], Training Accuracy: 67.3534%, Training Loss: 0.6746%\n",
      "Epoch [23/300], Step [114/225], Training Accuracy: 67.3931%, Training Loss: 0.6740%\n",
      "Epoch [23/300], Step [115/225], Training Accuracy: 67.4185%, Training Loss: 0.6735%\n",
      "Epoch [23/300], Step [116/225], Training Accuracy: 67.4973%, Training Loss: 0.6732%\n",
      "Epoch [23/300], Step [117/225], Training Accuracy: 67.4412%, Training Loss: 0.6736%\n",
      "Epoch [23/300], Step [118/225], Training Accuracy: 67.4258%, Training Loss: 0.6738%\n",
      "Epoch [23/300], Step [119/225], Training Accuracy: 67.4107%, Training Loss: 0.6741%\n",
      "Epoch [23/300], Step [120/225], Training Accuracy: 67.4219%, Training Loss: 0.6741%\n",
      "Epoch [23/300], Step [121/225], Training Accuracy: 67.3812%, Training Loss: 0.6749%\n",
      "Epoch [23/300], Step [122/225], Training Accuracy: 67.3412%, Training Loss: 0.6760%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Step [123/225], Training Accuracy: 67.3399%, Training Loss: 0.6759%\n",
      "Epoch [23/300], Step [124/225], Training Accuracy: 67.3135%, Training Loss: 0.6754%\n",
      "Epoch [23/300], Step [125/225], Training Accuracy: 67.2750%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [126/225], Training Accuracy: 67.2991%, Training Loss: 0.6752%\n",
      "Epoch [23/300], Step [127/225], Training Accuracy: 67.3474%, Training Loss: 0.6752%\n",
      "Epoch [23/300], Step [128/225], Training Accuracy: 67.2607%, Training Loss: 0.6765%\n",
      "Epoch [23/300], Step [129/225], Training Accuracy: 67.2481%, Training Loss: 0.6770%\n",
      "Epoch [23/300], Step [130/225], Training Accuracy: 67.1995%, Training Loss: 0.6775%\n",
      "Epoch [23/300], Step [131/225], Training Accuracy: 67.2233%, Training Loss: 0.6769%\n",
      "Epoch [23/300], Step [132/225], Training Accuracy: 67.1757%, Training Loss: 0.6770%\n",
      "Epoch [23/300], Step [133/225], Training Accuracy: 67.1170%, Training Loss: 0.6770%\n",
      "Epoch [23/300], Step [134/225], Training Accuracy: 67.0592%, Training Loss: 0.6781%\n",
      "Epoch [23/300], Step [135/225], Training Accuracy: 67.0370%, Training Loss: 0.6777%\n",
      "Epoch [23/300], Step [136/225], Training Accuracy: 67.0381%, Training Loss: 0.6773%\n",
      "Epoch [23/300], Step [137/225], Training Accuracy: 66.9936%, Training Loss: 0.6774%\n",
      "Epoch [23/300], Step [138/225], Training Accuracy: 67.1082%, Training Loss: 0.6764%\n",
      "Epoch [23/300], Step [139/225], Training Accuracy: 67.1313%, Training Loss: 0.6766%\n",
      "Epoch [23/300], Step [140/225], Training Accuracy: 67.2098%, Training Loss: 0.6762%\n",
      "Epoch [23/300], Step [141/225], Training Accuracy: 67.2207%, Training Loss: 0.6761%\n",
      "Epoch [23/300], Step [142/225], Training Accuracy: 67.2205%, Training Loss: 0.6759%\n",
      "Epoch [23/300], Step [143/225], Training Accuracy: 67.2421%, Training Loss: 0.6761%\n",
      "Epoch [23/300], Step [144/225], Training Accuracy: 67.2635%, Training Loss: 0.6763%\n",
      "Epoch [23/300], Step [145/225], Training Accuracy: 67.3060%, Training Loss: 0.6758%\n",
      "Epoch [23/300], Step [146/225], Training Accuracy: 67.2945%, Training Loss: 0.6761%\n",
      "Epoch [23/300], Step [147/225], Training Accuracy: 67.2832%, Training Loss: 0.6761%\n",
      "Epoch [23/300], Step [148/225], Training Accuracy: 67.3142%, Training Loss: 0.6758%\n",
      "Epoch [23/300], Step [149/225], Training Accuracy: 67.3763%, Training Loss: 0.6756%\n",
      "Epoch [23/300], Step [150/225], Training Accuracy: 67.3854%, Training Loss: 0.6752%\n",
      "Epoch [23/300], Step [151/225], Training Accuracy: 67.3841%, Training Loss: 0.6749%\n",
      "Epoch [23/300], Step [152/225], Training Accuracy: 67.3314%, Training Loss: 0.6752%\n",
      "Epoch [23/300], Step [153/225], Training Accuracy: 67.3305%, Training Loss: 0.6757%\n",
      "Epoch [23/300], Step [154/225], Training Accuracy: 67.3600%, Training Loss: 0.6754%\n",
      "Epoch [23/300], Step [155/225], Training Accuracy: 67.3790%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [156/225], Training Accuracy: 67.3978%, Training Loss: 0.6755%\n",
      "Epoch [23/300], Step [157/225], Training Accuracy: 67.3666%, Training Loss: 0.6759%\n",
      "Epoch [23/300], Step [158/225], Training Accuracy: 67.3556%, Training Loss: 0.6760%\n",
      "Epoch [23/300], Step [159/225], Training Accuracy: 67.3054%, Training Loss: 0.6767%\n",
      "Epoch [23/300], Step [160/225], Training Accuracy: 67.2949%, Training Loss: 0.6766%\n",
      "Epoch [23/300], Step [161/225], Training Accuracy: 67.2554%, Training Loss: 0.6767%\n",
      "Epoch [23/300], Step [162/225], Training Accuracy: 67.3418%, Training Loss: 0.6757%\n",
      "Epoch [23/300], Step [163/225], Training Accuracy: 67.3025%, Training Loss: 0.6756%\n",
      "Epoch [23/300], Step [164/225], Training Accuracy: 67.3685%, Training Loss: 0.6744%\n",
      "Epoch [23/300], Step [165/225], Training Accuracy: 67.3864%, Training Loss: 0.6741%\n",
      "Epoch [23/300], Step [166/225], Training Accuracy: 67.3852%, Training Loss: 0.6740%\n",
      "Epoch [23/300], Step [167/225], Training Accuracy: 67.4121%, Training Loss: 0.6737%\n",
      "Epoch [23/300], Step [168/225], Training Accuracy: 67.3735%, Training Loss: 0.6739%\n",
      "Epoch [23/300], Step [169/225], Training Accuracy: 67.3632%, Training Loss: 0.6738%\n",
      "Epoch [23/300], Step [170/225], Training Accuracy: 67.3805%, Training Loss: 0.6739%\n",
      "Epoch [23/300], Step [171/225], Training Accuracy: 67.4251%, Training Loss: 0.6740%\n",
      "Epoch [23/300], Step [172/225], Training Accuracy: 67.4055%, Training Loss: 0.6741%\n",
      "Epoch [23/300], Step [173/225], Training Accuracy: 67.3681%, Training Loss: 0.6746%\n",
      "Epoch [23/300], Step [174/225], Training Accuracy: 67.3851%, Training Loss: 0.6743%\n",
      "Epoch [23/300], Step [175/225], Training Accuracy: 67.4375%, Training Loss: 0.6737%\n",
      "Epoch [23/300], Step [176/225], Training Accuracy: 67.4716%, Training Loss: 0.6733%\n",
      "Epoch [23/300], Step [177/225], Training Accuracy: 67.4788%, Training Loss: 0.6739%\n",
      "Epoch [23/300], Step [178/225], Training Accuracy: 67.4947%, Training Loss: 0.6733%\n",
      "Epoch [23/300], Step [179/225], Training Accuracy: 67.4930%, Training Loss: 0.6730%\n",
      "Epoch [23/300], Step [180/225], Training Accuracy: 67.5087%, Training Loss: 0.6733%\n",
      "Epoch [23/300], Step [181/225], Training Accuracy: 67.4810%, Training Loss: 0.6737%\n",
      "Epoch [23/300], Step [182/225], Training Accuracy: 67.4794%, Training Loss: 0.6739%\n",
      "Epoch [23/300], Step [183/225], Training Accuracy: 67.5034%, Training Loss: 0.6740%\n",
      "Epoch [23/300], Step [184/225], Training Accuracy: 67.5442%, Training Loss: 0.6737%\n",
      "Epoch [23/300], Step [185/225], Training Accuracy: 67.5760%, Training Loss: 0.6730%\n",
      "Epoch [23/300], Step [186/225], Training Accuracy: 67.5907%, Training Loss: 0.6725%\n",
      "Epoch [23/300], Step [187/225], Training Accuracy: 67.6053%, Training Loss: 0.6723%\n",
      "Epoch [23/300], Step [188/225], Training Accuracy: 67.6446%, Training Loss: 0.6718%\n",
      "Epoch [23/300], Step [189/225], Training Accuracy: 67.6587%, Training Loss: 0.6714%\n",
      "Epoch [23/300], Step [190/225], Training Accuracy: 67.6974%, Training Loss: 0.6713%\n",
      "Epoch [23/300], Step [191/225], Training Accuracy: 67.6865%, Training Loss: 0.6711%\n",
      "Epoch [23/300], Step [192/225], Training Accuracy: 67.7246%, Training Loss: 0.6706%\n",
      "Epoch [23/300], Step [193/225], Training Accuracy: 67.6813%, Training Loss: 0.6710%\n",
      "Epoch [23/300], Step [194/225], Training Accuracy: 67.6949%, Training Loss: 0.6710%\n",
      "Epoch [23/300], Step [195/225], Training Accuracy: 67.7163%, Training Loss: 0.6706%\n",
      "Epoch [23/300], Step [196/225], Training Accuracy: 67.6578%, Training Loss: 0.6711%\n",
      "Epoch [23/300], Step [197/225], Training Accuracy: 67.6555%, Training Loss: 0.6713%\n",
      "Epoch [23/300], Step [198/225], Training Accuracy: 67.7004%, Training Loss: 0.6704%\n",
      "Epoch [23/300], Step [199/225], Training Accuracy: 67.7136%, Training Loss: 0.6701%\n",
      "Epoch [23/300], Step [200/225], Training Accuracy: 67.7266%, Training Loss: 0.6699%\n",
      "Epoch [23/300], Step [201/225], Training Accuracy: 67.7472%, Training Loss: 0.6699%\n",
      "Epoch [23/300], Step [202/225], Training Accuracy: 67.7522%, Training Loss: 0.6695%\n",
      "Epoch [23/300], Step [203/225], Training Accuracy: 67.8110%, Training Loss: 0.6690%\n",
      "Epoch [23/300], Step [204/225], Training Accuracy: 67.8156%, Training Loss: 0.6691%\n",
      "Epoch [23/300], Step [205/225], Training Accuracy: 67.8354%, Training Loss: 0.6688%\n",
      "Epoch [23/300], Step [206/225], Training Accuracy: 67.8246%, Training Loss: 0.6687%\n",
      "Epoch [23/300], Step [207/225], Training Accuracy: 67.8518%, Training Loss: 0.6687%\n",
      "Epoch [23/300], Step [208/225], Training Accuracy: 67.8786%, Training Loss: 0.6683%\n",
      "Epoch [23/300], Step [209/225], Training Accuracy: 67.8753%, Training Loss: 0.6686%\n",
      "Epoch [23/300], Step [210/225], Training Accuracy: 67.8423%, Training Loss: 0.6690%\n",
      "Epoch [23/300], Step [211/225], Training Accuracy: 67.8836%, Training Loss: 0.6686%\n",
      "Epoch [23/300], Step [212/225], Training Accuracy: 67.8435%, Training Loss: 0.6692%\n",
      "Epoch [23/300], Step [213/225], Training Accuracy: 67.8330%, Training Loss: 0.6699%\n",
      "Epoch [23/300], Step [214/225], Training Accuracy: 67.8665%, Training Loss: 0.6696%\n",
      "Epoch [23/300], Step [215/225], Training Accuracy: 67.8416%, Training Loss: 0.6698%\n",
      "Epoch [23/300], Step [216/225], Training Accuracy: 67.8168%, Training Loss: 0.6702%\n",
      "Epoch [23/300], Step [217/225], Training Accuracy: 67.8067%, Training Loss: 0.6709%\n",
      "Epoch [23/300], Step [218/225], Training Accuracy: 67.7967%, Training Loss: 0.6711%\n",
      "Epoch [23/300], Step [219/225], Training Accuracy: 67.7725%, Training Loss: 0.6710%\n",
      "Epoch [23/300], Step [220/225], Training Accuracy: 67.7841%, Training Loss: 0.6709%\n",
      "Epoch [23/300], Step [221/225], Training Accuracy: 67.7673%, Training Loss: 0.6711%\n",
      "Epoch [23/300], Step [222/225], Training Accuracy: 67.7646%, Training Loss: 0.6709%\n",
      "Epoch [23/300], Step [223/225], Training Accuracy: 67.7340%, Training Loss: 0.6716%\n",
      "Epoch [23/300], Step [224/225], Training Accuracy: 67.7176%, Training Loss: 0.6714%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Step [225/225], Training Accuracy: 67.7043%, Training Loss: 0.6715%\n",
      "Epoch [24/300], Step [1/225], Training Accuracy: 76.5625%, Training Loss: 0.5369%\n",
      "Epoch [24/300], Step [2/225], Training Accuracy: 75.0000%, Training Loss: 0.5951%\n",
      "Epoch [24/300], Step [3/225], Training Accuracy: 71.8750%, Training Loss: 0.6467%\n",
      "Epoch [24/300], Step [4/225], Training Accuracy: 71.0938%, Training Loss: 0.6487%\n",
      "Epoch [24/300], Step [5/225], Training Accuracy: 72.1875%, Training Loss: 0.6294%\n",
      "Epoch [24/300], Step [6/225], Training Accuracy: 71.0938%, Training Loss: 0.6539%\n",
      "Epoch [24/300], Step [7/225], Training Accuracy: 71.4286%, Training Loss: 0.6489%\n",
      "Epoch [24/300], Step [8/225], Training Accuracy: 70.1172%, Training Loss: 0.6647%\n",
      "Epoch [24/300], Step [9/225], Training Accuracy: 68.9236%, Training Loss: 0.6795%\n",
      "Epoch [24/300], Step [10/225], Training Accuracy: 68.9062%, Training Loss: 0.6844%\n",
      "Epoch [24/300], Step [11/225], Training Accuracy: 68.4659%, Training Loss: 0.6837%\n",
      "Epoch [24/300], Step [12/225], Training Accuracy: 68.2292%, Training Loss: 0.6826%\n",
      "Epoch [24/300], Step [13/225], Training Accuracy: 69.2308%, Training Loss: 0.6669%\n",
      "Epoch [24/300], Step [14/225], Training Accuracy: 69.0848%, Training Loss: 0.6666%\n",
      "Epoch [24/300], Step [15/225], Training Accuracy: 69.0625%, Training Loss: 0.6692%\n",
      "Epoch [24/300], Step [16/225], Training Accuracy: 69.1406%, Training Loss: 0.6661%\n",
      "Epoch [24/300], Step [17/225], Training Accuracy: 69.1176%, Training Loss: 0.6640%\n",
      "Epoch [24/300], Step [18/225], Training Accuracy: 69.5312%, Training Loss: 0.6619%\n",
      "Epoch [24/300], Step [19/225], Training Accuracy: 69.3257%, Training Loss: 0.6613%\n",
      "Epoch [24/300], Step [20/225], Training Accuracy: 69.6875%, Training Loss: 0.6575%\n",
      "Epoch [24/300], Step [21/225], Training Accuracy: 69.7173%, Training Loss: 0.6534%\n",
      "Epoch [24/300], Step [22/225], Training Accuracy: 69.1051%, Training Loss: 0.6612%\n",
      "Epoch [24/300], Step [23/225], Training Accuracy: 69.3614%, Training Loss: 0.6571%\n",
      "Epoch [24/300], Step [24/225], Training Accuracy: 68.8151%, Training Loss: 0.6633%\n",
      "Epoch [24/300], Step [25/225], Training Accuracy: 69.1250%, Training Loss: 0.6633%\n",
      "Epoch [24/300], Step [26/225], Training Accuracy: 68.9303%, Training Loss: 0.6641%\n",
      "Epoch [24/300], Step [27/225], Training Accuracy: 68.9815%, Training Loss: 0.6617%\n",
      "Epoch [24/300], Step [28/225], Training Accuracy: 69.4196%, Training Loss: 0.6541%\n",
      "Epoch [24/300], Step [29/225], Training Accuracy: 69.5582%, Training Loss: 0.6542%\n",
      "Epoch [24/300], Step [30/225], Training Accuracy: 69.6875%, Training Loss: 0.6518%\n",
      "Epoch [24/300], Step [31/225], Training Accuracy: 69.3044%, Training Loss: 0.6586%\n",
      "Epoch [24/300], Step [32/225], Training Accuracy: 69.1406%, Training Loss: 0.6600%\n",
      "Epoch [24/300], Step [33/225], Training Accuracy: 69.3182%, Training Loss: 0.6561%\n",
      "Epoch [24/300], Step [34/225], Training Accuracy: 69.1636%, Training Loss: 0.6563%\n",
      "Epoch [24/300], Step [35/225], Training Accuracy: 69.3304%, Training Loss: 0.6557%\n",
      "Epoch [24/300], Step [36/225], Training Accuracy: 69.4010%, Training Loss: 0.6533%\n",
      "Epoch [24/300], Step [37/225], Training Accuracy: 69.2990%, Training Loss: 0.6525%\n",
      "Epoch [24/300], Step [38/225], Training Accuracy: 69.0378%, Training Loss: 0.6539%\n",
      "Epoch [24/300], Step [39/225], Training Accuracy: 68.9103%, Training Loss: 0.6545%\n",
      "Epoch [24/300], Step [40/225], Training Accuracy: 68.9453%, Training Loss: 0.6548%\n",
      "Epoch [24/300], Step [41/225], Training Accuracy: 69.0549%, Training Loss: 0.6545%\n",
      "Epoch [24/300], Step [42/225], Training Accuracy: 68.8988%, Training Loss: 0.6541%\n",
      "Epoch [24/300], Step [43/225], Training Accuracy: 68.7863%, Training Loss: 0.6557%\n",
      "Epoch [24/300], Step [44/225], Training Accuracy: 69.0341%, Training Loss: 0.6528%\n",
      "Epoch [24/300], Step [45/225], Training Accuracy: 69.0972%, Training Loss: 0.6510%\n",
      "Epoch [24/300], Step [46/225], Training Accuracy: 69.0897%, Training Loss: 0.6499%\n",
      "Epoch [24/300], Step [47/225], Training Accuracy: 68.8830%, Training Loss: 0.6515%\n",
      "Epoch [24/300], Step [48/225], Training Accuracy: 68.7174%, Training Loss: 0.6529%\n",
      "Epoch [24/300], Step [49/225], Training Accuracy: 68.8776%, Training Loss: 0.6513%\n",
      "Epoch [24/300], Step [50/225], Training Accuracy: 68.8438%, Training Loss: 0.6506%\n",
      "Epoch [24/300], Step [51/225], Training Accuracy: 68.9645%, Training Loss: 0.6510%\n",
      "Epoch [24/300], Step [52/225], Training Accuracy: 69.0505%, Training Loss: 0.6492%\n",
      "Epoch [24/300], Step [53/225], Training Accuracy: 69.0743%, Training Loss: 0.6489%\n",
      "Epoch [24/300], Step [54/225], Training Accuracy: 68.9815%, Training Loss: 0.6495%\n",
      "Epoch [24/300], Step [55/225], Training Accuracy: 68.9489%, Training Loss: 0.6505%\n",
      "Epoch [24/300], Step [56/225], Training Accuracy: 68.8058%, Training Loss: 0.6519%\n",
      "Epoch [24/300], Step [57/225], Training Accuracy: 68.8871%, Training Loss: 0.6507%\n",
      "Epoch [24/300], Step [58/225], Training Accuracy: 68.9116%, Training Loss: 0.6515%\n",
      "Epoch [24/300], Step [59/225], Training Accuracy: 68.8824%, Training Loss: 0.6510%\n",
      "Epoch [24/300], Step [60/225], Training Accuracy: 68.9583%, Training Loss: 0.6498%\n",
      "Epoch [24/300], Step [61/225], Training Accuracy: 68.9293%, Training Loss: 0.6512%\n",
      "Epoch [24/300], Step [62/225], Training Accuracy: 68.9516%, Training Loss: 0.6523%\n",
      "Epoch [24/300], Step [63/225], Training Accuracy: 68.8492%, Training Loss: 0.6539%\n",
      "Epoch [24/300], Step [64/225], Training Accuracy: 68.9941%, Training Loss: 0.6522%\n",
      "Epoch [24/300], Step [65/225], Training Accuracy: 68.9904%, Training Loss: 0.6517%\n",
      "Epoch [24/300], Step [66/225], Training Accuracy: 69.1525%, Training Loss: 0.6499%\n",
      "Epoch [24/300], Step [67/225], Training Accuracy: 69.0765%, Training Loss: 0.6500%\n",
      "Epoch [24/300], Step [68/225], Training Accuracy: 69.0947%, Training Loss: 0.6505%\n",
      "Epoch [24/300], Step [69/225], Training Accuracy: 69.0670%, Training Loss: 0.6511%\n",
      "Epoch [24/300], Step [70/225], Training Accuracy: 69.0402%, Training Loss: 0.6510%\n",
      "Epoch [24/300], Step [71/225], Training Accuracy: 69.1241%, Training Loss: 0.6499%\n",
      "Epoch [24/300], Step [72/225], Training Accuracy: 69.0972%, Training Loss: 0.6501%\n",
      "Epoch [24/300], Step [73/225], Training Accuracy: 69.0068%, Training Loss: 0.6505%\n",
      "Epoch [24/300], Step [74/225], Training Accuracy: 68.9400%, Training Loss: 0.6504%\n",
      "Epoch [24/300], Step [75/225], Training Accuracy: 68.9792%, Training Loss: 0.6499%\n",
      "Epoch [24/300], Step [76/225], Training Accuracy: 68.8734%, Training Loss: 0.6519%\n",
      "Epoch [24/300], Step [77/225], Training Accuracy: 68.9732%, Training Loss: 0.6514%\n",
      "Epoch [24/300], Step [78/225], Training Accuracy: 68.8502%, Training Loss: 0.6516%\n",
      "Epoch [24/300], Step [79/225], Training Accuracy: 68.8884%, Training Loss: 0.6508%\n",
      "Epoch [24/300], Step [80/225], Training Accuracy: 68.8477%, Training Loss: 0.6505%\n",
      "Epoch [24/300], Step [81/225], Training Accuracy: 68.8079%, Training Loss: 0.6504%\n",
      "Epoch [24/300], Step [82/225], Training Accuracy: 68.8262%, Training Loss: 0.6501%\n",
      "Epoch [24/300], Step [83/225], Training Accuracy: 68.7688%, Training Loss: 0.6498%\n",
      "Epoch [24/300], Step [84/225], Training Accuracy: 68.8058%, Training Loss: 0.6492%\n",
      "Epoch [24/300], Step [85/225], Training Accuracy: 68.7868%, Training Loss: 0.6486%\n",
      "Epoch [24/300], Step [86/225], Training Accuracy: 68.7500%, Training Loss: 0.6489%\n",
      "Epoch [24/300], Step [87/225], Training Accuracy: 68.7859%, Training Loss: 0.6495%\n",
      "Epoch [24/300], Step [88/225], Training Accuracy: 68.7322%, Training Loss: 0.6496%\n",
      "Epoch [24/300], Step [89/225], Training Accuracy: 68.6973%, Training Loss: 0.6501%\n",
      "Epoch [24/300], Step [90/225], Training Accuracy: 68.5938%, Training Loss: 0.6525%\n",
      "Epoch [24/300], Step [91/225], Training Accuracy: 68.4581%, Training Loss: 0.6534%\n",
      "Epoch [24/300], Step [92/225], Training Accuracy: 68.4443%, Training Loss: 0.6530%\n",
      "Epoch [24/300], Step [93/225], Training Accuracy: 68.5820%, Training Loss: 0.6519%\n",
      "Epoch [24/300], Step [94/225], Training Accuracy: 68.6835%, Training Loss: 0.6508%\n",
      "Epoch [24/300], Step [95/225], Training Accuracy: 68.7171%, Training Loss: 0.6515%\n",
      "Epoch [24/300], Step [96/225], Training Accuracy: 68.7337%, Training Loss: 0.6511%\n",
      "Epoch [24/300], Step [97/225], Training Accuracy: 68.7661%, Training Loss: 0.6502%\n",
      "Epoch [24/300], Step [98/225], Training Accuracy: 68.6862%, Training Loss: 0.6516%\n",
      "Epoch [24/300], Step [99/225], Training Accuracy: 68.6237%, Training Loss: 0.6530%\n",
      "Epoch [24/300], Step [100/225], Training Accuracy: 68.6250%, Training Loss: 0.6542%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Step [101/225], Training Accuracy: 68.6262%, Training Loss: 0.6539%\n",
      "Epoch [24/300], Step [102/225], Training Accuracy: 68.5968%, Training Loss: 0.6539%\n",
      "Epoch [24/300], Step [103/225], Training Accuracy: 68.6286%, Training Loss: 0.6535%\n",
      "Epoch [24/300], Step [104/225], Training Accuracy: 68.5397%, Training Loss: 0.6547%\n",
      "Epoch [24/300], Step [105/225], Training Accuracy: 68.5268%, Training Loss: 0.6540%\n",
      "Epoch [24/300], Step [106/225], Training Accuracy: 68.6321%, Training Loss: 0.6535%\n",
      "Epoch [24/300], Step [107/225], Training Accuracy: 68.5894%, Training Loss: 0.6549%\n",
      "Epoch [24/300], Step [108/225], Training Accuracy: 68.5619%, Training Loss: 0.6552%\n",
      "Epoch [24/300], Step [109/225], Training Accuracy: 68.5063%, Training Loss: 0.6553%\n",
      "Epoch [24/300], Step [110/225], Training Accuracy: 68.4801%, Training Loss: 0.6558%\n",
      "Epoch [24/300], Step [111/225], Training Accuracy: 68.5107%, Training Loss: 0.6550%\n",
      "Epoch [24/300], Step [112/225], Training Accuracy: 68.5407%, Training Loss: 0.6555%\n",
      "Epoch [24/300], Step [113/225], Training Accuracy: 68.5564%, Training Loss: 0.6554%\n",
      "Epoch [24/300], Step [114/225], Training Accuracy: 68.5581%, Training Loss: 0.6557%\n",
      "Epoch [24/300], Step [115/225], Training Accuracy: 68.6549%, Training Loss: 0.6546%\n",
      "Epoch [24/300], Step [116/225], Training Accuracy: 68.6557%, Training Loss: 0.6540%\n",
      "Epoch [24/300], Step [117/225], Training Accuracy: 68.6565%, Training Loss: 0.6546%\n",
      "Epoch [24/300], Step [118/225], Training Accuracy: 68.6573%, Training Loss: 0.6548%\n",
      "Epoch [24/300], Step [119/225], Training Accuracy: 68.6581%, Training Loss: 0.6549%\n",
      "Epoch [24/300], Step [120/225], Training Accuracy: 68.6589%, Training Loss: 0.6554%\n",
      "Epoch [24/300], Step [121/225], Training Accuracy: 68.5692%, Training Loss: 0.6564%\n",
      "Epoch [24/300], Step [122/225], Training Accuracy: 68.6091%, Training Loss: 0.6569%\n",
      "Epoch [24/300], Step [123/225], Training Accuracy: 68.5849%, Training Loss: 0.6575%\n",
      "Epoch [24/300], Step [124/225], Training Accuracy: 68.6114%, Training Loss: 0.6568%\n",
      "Epoch [24/300], Step [125/225], Training Accuracy: 68.6000%, Training Loss: 0.6573%\n",
      "Epoch [24/300], Step [126/225], Training Accuracy: 68.5888%, Training Loss: 0.6573%\n",
      "Epoch [24/300], Step [127/225], Training Accuracy: 68.6147%, Training Loss: 0.6582%\n",
      "Epoch [24/300], Step [128/225], Training Accuracy: 68.5303%, Training Loss: 0.6598%\n",
      "Epoch [24/300], Step [129/225], Training Accuracy: 68.5320%, Training Loss: 0.6606%\n",
      "Epoch [24/300], Step [130/225], Training Accuracy: 68.5337%, Training Loss: 0.6612%\n",
      "Epoch [24/300], Step [131/225], Training Accuracy: 68.5353%, Training Loss: 0.6610%\n",
      "Epoch [24/300], Step [132/225], Training Accuracy: 68.5251%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [133/225], Training Accuracy: 68.5503%, Training Loss: 0.6610%\n",
      "Epoch [24/300], Step [134/225], Training Accuracy: 68.4701%, Training Loss: 0.6623%\n",
      "Epoch [24/300], Step [135/225], Training Accuracy: 68.4954%, Training Loss: 0.6619%\n",
      "Epoch [24/300], Step [136/225], Training Accuracy: 68.5087%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [137/225], Training Accuracy: 68.5105%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [138/225], Training Accuracy: 68.5915%, Training Loss: 0.6603%\n",
      "Epoch [24/300], Step [139/225], Training Accuracy: 68.5814%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [140/225], Training Accuracy: 68.5603%, Training Loss: 0.6609%\n",
      "Epoch [24/300], Step [141/225], Training Accuracy: 68.5062%, Training Loss: 0.6613%\n",
      "Epoch [24/300], Step [142/225], Training Accuracy: 68.5519%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [143/225], Training Accuracy: 68.5642%, Training Loss: 0.6606%\n",
      "Epoch [24/300], Step [144/225], Training Accuracy: 68.5764%, Training Loss: 0.6604%\n",
      "Epoch [24/300], Step [145/225], Training Accuracy: 68.6099%, Training Loss: 0.6599%\n",
      "Epoch [24/300], Step [146/225], Training Accuracy: 68.5895%, Training Loss: 0.6600%\n",
      "Epoch [24/300], Step [147/225], Training Accuracy: 68.5480%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [148/225], Training Accuracy: 68.6022%, Training Loss: 0.6602%\n",
      "Epoch [24/300], Step [149/225], Training Accuracy: 68.6346%, Training Loss: 0.6601%\n",
      "Epoch [24/300], Step [150/225], Training Accuracy: 68.6354%, Training Loss: 0.6601%\n",
      "Epoch [24/300], Step [151/225], Training Accuracy: 68.7086%, Training Loss: 0.6593%\n",
      "Epoch [24/300], Step [152/225], Training Accuracy: 68.6678%, Training Loss: 0.6606%\n",
      "Epoch [24/300], Step [153/225], Training Accuracy: 68.6377%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [154/225], Training Accuracy: 68.6587%, Training Loss: 0.6606%\n",
      "Epoch [24/300], Step [155/225], Training Accuracy: 68.6593%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [156/225], Training Accuracy: 68.6398%, Training Loss: 0.6614%\n",
      "Epoch [24/300], Step [157/225], Training Accuracy: 68.6206%, Training Loss: 0.6617%\n",
      "Epoch [24/300], Step [158/225], Training Accuracy: 68.5621%, Training Loss: 0.6622%\n",
      "Epoch [24/300], Step [159/225], Training Accuracy: 68.5043%, Training Loss: 0.6631%\n",
      "Epoch [24/300], Step [160/225], Training Accuracy: 68.4961%, Training Loss: 0.6630%\n",
      "Epoch [24/300], Step [161/225], Training Accuracy: 68.5074%, Training Loss: 0.6631%\n",
      "Epoch [24/300], Step [162/225], Training Accuracy: 68.5957%, Training Loss: 0.6623%\n",
      "Epoch [24/300], Step [163/225], Training Accuracy: 68.5870%, Training Loss: 0.6621%\n",
      "Epoch [24/300], Step [164/225], Training Accuracy: 68.6261%, Training Loss: 0.6613%\n",
      "Epoch [24/300], Step [165/225], Training Accuracy: 68.6364%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [166/225], Training Accuracy: 68.6559%, Training Loss: 0.6604%\n",
      "Epoch [24/300], Step [167/225], Training Accuracy: 68.6471%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [168/225], Training Accuracy: 68.6012%, Training Loss: 0.6615%\n",
      "Epoch [24/300], Step [169/225], Training Accuracy: 68.5836%, Training Loss: 0.6615%\n",
      "Epoch [24/300], Step [170/225], Training Accuracy: 68.5478%, Training Loss: 0.6618%\n",
      "Epoch [24/300], Step [171/225], Training Accuracy: 68.5307%, Training Loss: 0.6622%\n",
      "Epoch [24/300], Step [172/225], Training Accuracy: 68.5229%, Training Loss: 0.6622%\n",
      "Epoch [24/300], Step [173/225], Training Accuracy: 68.5242%, Training Loss: 0.6621%\n",
      "Epoch [24/300], Step [174/225], Training Accuracy: 68.5704%, Training Loss: 0.6619%\n",
      "Epoch [24/300], Step [175/225], Training Accuracy: 68.6161%, Training Loss: 0.6614%\n",
      "Epoch [24/300], Step [176/225], Training Accuracy: 68.6435%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [177/225], Training Accuracy: 68.6264%, Training Loss: 0.6617%\n",
      "Epoch [24/300], Step [178/225], Training Accuracy: 68.6096%, Training Loss: 0.6614%\n",
      "Epoch [24/300], Step [179/225], Training Accuracy: 68.6365%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [180/225], Training Accuracy: 68.6285%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [181/225], Training Accuracy: 68.6032%, Training Loss: 0.6618%\n",
      "Epoch [24/300], Step [182/225], Training Accuracy: 68.6126%, Training Loss: 0.6619%\n",
      "Epoch [24/300], Step [183/225], Training Accuracy: 68.6134%, Training Loss: 0.6619%\n",
      "Epoch [24/300], Step [184/225], Training Accuracy: 68.6226%, Training Loss: 0.6616%\n",
      "Epoch [24/300], Step [185/225], Training Accuracy: 68.6571%, Training Loss: 0.6611%\n",
      "Epoch [24/300], Step [186/225], Training Accuracy: 68.6912%, Training Loss: 0.6606%\n",
      "Epoch [24/300], Step [187/225], Training Accuracy: 68.7082%, Training Loss: 0.6605%\n",
      "Epoch [24/300], Step [188/225], Training Accuracy: 68.7251%, Training Loss: 0.6601%\n",
      "Epoch [24/300], Step [189/225], Training Accuracy: 68.7583%, Training Loss: 0.6597%\n",
      "Epoch [24/300], Step [190/225], Training Accuracy: 68.7747%, Training Loss: 0.6594%\n",
      "Epoch [24/300], Step [191/225], Training Accuracy: 68.7500%, Training Loss: 0.6593%\n",
      "Epoch [24/300], Step [192/225], Training Accuracy: 68.7500%, Training Loss: 0.6590%\n",
      "Epoch [24/300], Step [193/225], Training Accuracy: 68.7419%, Training Loss: 0.6593%\n",
      "Epoch [24/300], Step [194/225], Training Accuracy: 68.7661%, Training Loss: 0.6590%\n",
      "Epoch [24/300], Step [195/225], Training Accuracy: 68.7821%, Training Loss: 0.6587%\n",
      "Epoch [24/300], Step [196/225], Training Accuracy: 68.6942%, Training Loss: 0.6597%\n",
      "Epoch [24/300], Step [197/225], Training Accuracy: 68.7103%, Training Loss: 0.6599%\n",
      "Epoch [24/300], Step [198/225], Training Accuracy: 68.7184%, Training Loss: 0.6595%\n",
      "Epoch [24/300], Step [199/225], Training Accuracy: 68.6950%, Training Loss: 0.6596%\n",
      "Epoch [24/300], Step [200/225], Training Accuracy: 68.6797%, Training Loss: 0.6595%\n",
      "Epoch [24/300], Step [201/225], Training Accuracy: 68.6489%, Training Loss: 0.6599%\n",
      "Epoch [24/300], Step [202/225], Training Accuracy: 68.6649%, Training Loss: 0.6595%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Step [203/225], Training Accuracy: 68.7115%, Training Loss: 0.6594%\n",
      "Epoch [24/300], Step [204/225], Training Accuracy: 68.7194%, Training Loss: 0.6597%\n",
      "Epoch [24/300], Step [205/225], Training Accuracy: 68.7195%, Training Loss: 0.6593%\n",
      "Epoch [24/300], Step [206/225], Training Accuracy: 68.7121%, Training Loss: 0.6593%\n",
      "Epoch [24/300], Step [207/225], Training Accuracy: 68.6972%, Training Loss: 0.6598%\n",
      "Epoch [24/300], Step [208/225], Training Accuracy: 68.7350%, Training Loss: 0.6596%\n",
      "Epoch [24/300], Step [209/225], Training Accuracy: 68.7425%, Training Loss: 0.6597%\n",
      "Epoch [24/300], Step [210/225], Training Accuracy: 68.7351%, Training Loss: 0.6599%\n",
      "Epoch [24/300], Step [211/225], Training Accuracy: 68.7648%, Training Loss: 0.6594%\n",
      "Epoch [24/300], Step [212/225], Training Accuracy: 68.7574%, Training Loss: 0.6596%\n",
      "Epoch [24/300], Step [213/225], Training Accuracy: 68.7353%, Training Loss: 0.6604%\n",
      "Epoch [24/300], Step [214/225], Training Accuracy: 68.7281%, Training Loss: 0.6604%\n",
      "Epoch [24/300], Step [215/225], Training Accuracy: 68.7064%, Training Loss: 0.6604%\n",
      "Epoch [24/300], Step [216/225], Training Accuracy: 68.6849%, Training Loss: 0.6605%\n",
      "Epoch [24/300], Step [217/225], Training Accuracy: 68.6708%, Training Loss: 0.6608%\n",
      "Epoch [24/300], Step [218/225], Training Accuracy: 68.6425%, Training Loss: 0.6612%\n",
      "Epoch [24/300], Step [219/225], Training Accuracy: 68.6715%, Training Loss: 0.6610%\n",
      "Epoch [24/300], Step [220/225], Training Accuracy: 68.6364%, Training Loss: 0.6616%\n",
      "Epoch [24/300], Step [221/225], Training Accuracy: 68.5662%, Training Loss: 0.6625%\n",
      "Epoch [24/300], Step [222/225], Training Accuracy: 68.5600%, Training Loss: 0.6624%\n",
      "Epoch [24/300], Step [223/225], Training Accuracy: 68.5258%, Training Loss: 0.6627%\n",
      "Epoch [24/300], Step [224/225], Training Accuracy: 68.5128%, Training Loss: 0.6630%\n",
      "Epoch [24/300], Step [225/225], Training Accuracy: 68.4686%, Training Loss: 0.6634%\n",
      "Epoch [25/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.6534%\n",
      "Epoch [25/300], Step [2/225], Training Accuracy: 67.9688%, Training Loss: 0.6653%\n",
      "Epoch [25/300], Step [3/225], Training Accuracy: 68.2292%, Training Loss: 0.6604%\n",
      "Epoch [25/300], Step [4/225], Training Accuracy: 67.9688%, Training Loss: 0.6531%\n",
      "Epoch [25/300], Step [5/225], Training Accuracy: 68.4375%, Training Loss: 0.6489%\n",
      "Epoch [25/300], Step [6/225], Training Accuracy: 67.7083%, Training Loss: 0.6716%\n",
      "Epoch [25/300], Step [7/225], Training Accuracy: 68.5268%, Training Loss: 0.6622%\n",
      "Epoch [25/300], Step [8/225], Training Accuracy: 67.9688%, Training Loss: 0.6579%\n",
      "Epoch [25/300], Step [9/225], Training Accuracy: 66.8403%, Training Loss: 0.6664%\n",
      "Epoch [25/300], Step [10/225], Training Accuracy: 65.7812%, Training Loss: 0.6887%\n",
      "Epoch [25/300], Step [11/225], Training Accuracy: 66.0511%, Training Loss: 0.6849%\n",
      "Epoch [25/300], Step [12/225], Training Accuracy: 65.7552%, Training Loss: 0.6908%\n",
      "Epoch [25/300], Step [13/225], Training Accuracy: 66.7067%, Training Loss: 0.6797%\n",
      "Epoch [25/300], Step [14/225], Training Accuracy: 67.7455%, Training Loss: 0.6707%\n",
      "Epoch [25/300], Step [15/225], Training Accuracy: 67.6042%, Training Loss: 0.6707%\n",
      "Epoch [25/300], Step [16/225], Training Accuracy: 67.5781%, Training Loss: 0.6699%\n",
      "Epoch [25/300], Step [17/225], Training Accuracy: 67.7390%, Training Loss: 0.6685%\n",
      "Epoch [25/300], Step [18/225], Training Accuracy: 67.7083%, Training Loss: 0.6711%\n",
      "Epoch [25/300], Step [19/225], Training Accuracy: 67.6809%, Training Loss: 0.6710%\n",
      "Epoch [25/300], Step [20/225], Training Accuracy: 67.6562%, Training Loss: 0.6675%\n",
      "Epoch [25/300], Step [21/225], Training Accuracy: 68.2292%, Training Loss: 0.6603%\n",
      "Epoch [25/300], Step [22/225], Training Accuracy: 67.9688%, Training Loss: 0.6665%\n",
      "Epoch [25/300], Step [23/225], Training Accuracy: 68.2065%, Training Loss: 0.6614%\n",
      "Epoch [25/300], Step [24/225], Training Accuracy: 67.3828%, Training Loss: 0.6690%\n",
      "Epoch [25/300], Step [25/225], Training Accuracy: 67.7500%, Training Loss: 0.6667%\n",
      "Epoch [25/300], Step [26/225], Training Accuracy: 67.4279%, Training Loss: 0.6691%\n",
      "Epoch [25/300], Step [27/225], Training Accuracy: 67.4190%, Training Loss: 0.6674%\n",
      "Epoch [25/300], Step [28/225], Training Accuracy: 67.7455%, Training Loss: 0.6642%\n",
      "Epoch [25/300], Step [29/225], Training Accuracy: 67.9418%, Training Loss: 0.6636%\n",
      "Epoch [25/300], Step [30/225], Training Accuracy: 68.0208%, Training Loss: 0.6628%\n",
      "Epoch [25/300], Step [31/225], Training Accuracy: 67.8931%, Training Loss: 0.6675%\n",
      "Epoch [25/300], Step [32/225], Training Accuracy: 67.7734%, Training Loss: 0.6665%\n",
      "Epoch [25/300], Step [33/225], Training Accuracy: 67.8977%, Training Loss: 0.6639%\n",
      "Epoch [25/300], Step [34/225], Training Accuracy: 67.7849%, Training Loss: 0.6639%\n",
      "Epoch [25/300], Step [35/225], Training Accuracy: 67.9464%, Training Loss: 0.6615%\n",
      "Epoch [25/300], Step [36/225], Training Accuracy: 68.2726%, Training Loss: 0.6585%\n",
      "Epoch [25/300], Step [37/225], Training Accuracy: 68.4966%, Training Loss: 0.6565%\n",
      "Epoch [25/300], Step [38/225], Training Accuracy: 68.4622%, Training Loss: 0.6564%\n",
      "Epoch [25/300], Step [39/225], Training Accuracy: 68.6298%, Training Loss: 0.6560%\n",
      "Epoch [25/300], Step [40/225], Training Accuracy: 68.7891%, Training Loss: 0.6555%\n",
      "Epoch [25/300], Step [41/225], Training Accuracy: 68.6357%, Training Loss: 0.6568%\n",
      "Epoch [25/300], Step [42/225], Training Accuracy: 68.7128%, Training Loss: 0.6557%\n",
      "Epoch [25/300], Step [43/225], Training Accuracy: 68.6773%, Training Loss: 0.6579%\n",
      "Epoch [25/300], Step [44/225], Training Accuracy: 68.9631%, Training Loss: 0.6543%\n",
      "Epoch [25/300], Step [45/225], Training Accuracy: 69.0278%, Training Loss: 0.6527%\n",
      "Epoch [25/300], Step [46/225], Training Accuracy: 69.0557%, Training Loss: 0.6512%\n",
      "Epoch [25/300], Step [47/225], Training Accuracy: 68.8830%, Training Loss: 0.6537%\n",
      "Epoch [25/300], Step [48/225], Training Accuracy: 68.7174%, Training Loss: 0.6557%\n",
      "Epoch [25/300], Step [49/225], Training Accuracy: 68.7819%, Training Loss: 0.6539%\n",
      "Epoch [25/300], Step [50/225], Training Accuracy: 68.8750%, Training Loss: 0.6526%\n",
      "Epoch [25/300], Step [51/225], Training Accuracy: 68.9338%, Training Loss: 0.6515%\n",
      "Epoch [25/300], Step [52/225], Training Accuracy: 69.0805%, Training Loss: 0.6493%\n",
      "Epoch [25/300], Step [53/225], Training Accuracy: 69.0448%, Training Loss: 0.6496%\n",
      "Epoch [25/300], Step [54/225], Training Accuracy: 68.8947%, Training Loss: 0.6501%\n",
      "Epoch [25/300], Step [55/225], Training Accuracy: 68.8920%, Training Loss: 0.6524%\n",
      "Epoch [25/300], Step [56/225], Training Accuracy: 68.8058%, Training Loss: 0.6530%\n",
      "Epoch [25/300], Step [57/225], Training Accuracy: 68.9145%, Training Loss: 0.6512%\n",
      "Epoch [25/300], Step [58/225], Training Accuracy: 68.9925%, Training Loss: 0.6515%\n",
      "Epoch [25/300], Step [59/225], Training Accuracy: 69.0413%, Training Loss: 0.6500%\n",
      "Epoch [25/300], Step [60/225], Training Accuracy: 69.1667%, Training Loss: 0.6486%\n",
      "Epoch [25/300], Step [61/225], Training Accuracy: 69.0318%, Training Loss: 0.6503%\n",
      "Epoch [25/300], Step [62/225], Training Accuracy: 68.9012%, Training Loss: 0.6512%\n",
      "Epoch [25/300], Step [63/225], Training Accuracy: 68.8492%, Training Loss: 0.6524%\n",
      "Epoch [25/300], Step [64/225], Training Accuracy: 68.9209%, Training Loss: 0.6510%\n",
      "Epoch [25/300], Step [65/225], Training Accuracy: 68.8462%, Training Loss: 0.6514%\n",
      "Epoch [25/300], Step [66/225], Training Accuracy: 68.9867%, Training Loss: 0.6506%\n",
      "Epoch [25/300], Step [67/225], Training Accuracy: 69.0765%, Training Loss: 0.6500%\n",
      "Epoch [25/300], Step [68/225], Training Accuracy: 69.0257%, Training Loss: 0.6506%\n",
      "Epoch [25/300], Step [69/225], Training Accuracy: 68.9312%, Training Loss: 0.6521%\n",
      "Epoch [25/300], Step [70/225], Training Accuracy: 68.9062%, Training Loss: 0.6521%\n",
      "Epoch [25/300], Step [71/225], Training Accuracy: 68.9701%, Training Loss: 0.6508%\n",
      "Epoch [25/300], Step [72/225], Training Accuracy: 68.9236%, Training Loss: 0.6513%\n",
      "Epoch [25/300], Step [73/225], Training Accuracy: 68.9212%, Training Loss: 0.6515%\n",
      "Epoch [25/300], Step [74/225], Training Accuracy: 68.8556%, Training Loss: 0.6522%\n",
      "Epoch [25/300], Step [75/225], Training Accuracy: 68.8542%, Training Loss: 0.6514%\n",
      "Epoch [25/300], Step [76/225], Training Accuracy: 68.8117%, Training Loss: 0.6528%\n",
      "Epoch [25/300], Step [77/225], Training Accuracy: 68.9123%, Training Loss: 0.6526%\n",
      "Epoch [25/300], Step [78/225], Training Accuracy: 68.8702%, Training Loss: 0.6531%\n",
      "Epoch [25/300], Step [79/225], Training Accuracy: 68.8687%, Training Loss: 0.6534%\n",
      "Epoch [25/300], Step [80/225], Training Accuracy: 68.8086%, Training Loss: 0.6544%\n",
      "Epoch [25/300], Step [81/225], Training Accuracy: 68.8272%, Training Loss: 0.6533%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Step [82/225], Training Accuracy: 68.9024%, Training Loss: 0.6527%\n",
      "Epoch [25/300], Step [83/225], Training Accuracy: 68.9194%, Training Loss: 0.6519%\n",
      "Epoch [25/300], Step [84/225], Training Accuracy: 68.9546%, Training Loss: 0.6510%\n",
      "Epoch [25/300], Step [85/225], Training Accuracy: 68.9522%, Training Loss: 0.6501%\n",
      "Epoch [25/300], Step [86/225], Training Accuracy: 68.9862%, Training Loss: 0.6502%\n",
      "Epoch [25/300], Step [87/225], Training Accuracy: 68.9655%, Training Loss: 0.6511%\n",
      "Epoch [25/300], Step [88/225], Training Accuracy: 68.9276%, Training Loss: 0.6523%\n",
      "Epoch [25/300], Step [89/225], Training Accuracy: 68.9080%, Training Loss: 0.6532%\n",
      "Epoch [25/300], Step [90/225], Training Accuracy: 68.7500%, Training Loss: 0.6550%\n",
      "Epoch [25/300], Step [91/225], Training Accuracy: 68.6470%, Training Loss: 0.6562%\n",
      "Epoch [25/300], Step [92/225], Training Accuracy: 68.5971%, Training Loss: 0.6565%\n",
      "Epoch [25/300], Step [93/225], Training Accuracy: 68.5484%, Training Loss: 0.6564%\n",
      "Epoch [25/300], Step [94/225], Training Accuracy: 68.5505%, Training Loss: 0.6557%\n",
      "Epoch [25/300], Step [95/225], Training Accuracy: 68.6513%, Training Loss: 0.6556%\n",
      "Epoch [25/300], Step [96/225], Training Accuracy: 68.6361%, Training Loss: 0.6548%\n",
      "Epoch [25/300], Step [97/225], Training Accuracy: 68.5889%, Training Loss: 0.6552%\n",
      "Epoch [25/300], Step [98/225], Training Accuracy: 68.5427%, Training Loss: 0.6569%\n",
      "Epoch [25/300], Step [99/225], Training Accuracy: 68.4975%, Training Loss: 0.6575%\n",
      "Epoch [25/300], Step [100/225], Training Accuracy: 68.4531%, Training Loss: 0.6586%\n",
      "Epoch [25/300], Step [101/225], Training Accuracy: 68.4406%, Training Loss: 0.6593%\n",
      "Epoch [25/300], Step [102/225], Training Accuracy: 68.3824%, Training Loss: 0.6592%\n",
      "Epoch [25/300], Step [103/225], Training Accuracy: 68.4011%, Training Loss: 0.6586%\n",
      "Epoch [25/300], Step [104/225], Training Accuracy: 68.2993%, Training Loss: 0.6595%\n",
      "Epoch [25/300], Step [105/225], Training Accuracy: 68.3482%, Training Loss: 0.6585%\n",
      "Epoch [25/300], Step [106/225], Training Accuracy: 68.3815%, Training Loss: 0.6584%\n",
      "Epoch [25/300], Step [107/225], Training Accuracy: 68.2681%, Training Loss: 0.6604%\n",
      "Epoch [25/300], Step [108/225], Training Accuracy: 68.2292%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [109/225], Training Accuracy: 68.1766%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [110/225], Training Accuracy: 68.1250%, Training Loss: 0.6616%\n",
      "Epoch [25/300], Step [111/225], Training Accuracy: 68.1729%, Training Loss: 0.6609%\n",
      "Epoch [25/300], Step [112/225], Training Accuracy: 68.1641%, Training Loss: 0.6614%\n",
      "Epoch [25/300], Step [113/225], Training Accuracy: 68.1831%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [114/225], Training Accuracy: 68.2429%, Training Loss: 0.6609%\n",
      "Epoch [25/300], Step [115/225], Training Accuracy: 68.2337%, Training Loss: 0.6603%\n",
      "Epoch [25/300], Step [116/225], Training Accuracy: 68.2651%, Training Loss: 0.6596%\n",
      "Epoch [25/300], Step [117/225], Training Accuracy: 68.1891%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [118/225], Training Accuracy: 68.2071%, Training Loss: 0.6614%\n",
      "Epoch [25/300], Step [119/225], Training Accuracy: 68.2117%, Training Loss: 0.6611%\n",
      "Epoch [25/300], Step [120/225], Training Accuracy: 68.2031%, Training Loss: 0.6608%\n",
      "Epoch [25/300], Step [121/225], Training Accuracy: 68.1431%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [122/225], Training Accuracy: 68.1737%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [123/225], Training Accuracy: 68.1529%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [124/225], Training Accuracy: 68.1956%, Training Loss: 0.6608%\n",
      "Epoch [25/300], Step [125/225], Training Accuracy: 68.2125%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [126/225], Training Accuracy: 68.2044%, Training Loss: 0.6613%\n",
      "Epoch [25/300], Step [127/225], Training Accuracy: 68.1841%, Training Loss: 0.6615%\n",
      "Epoch [25/300], Step [128/225], Training Accuracy: 68.0786%, Training Loss: 0.6629%\n",
      "Epoch [25/300], Step [129/225], Training Accuracy: 68.1080%, Training Loss: 0.6631%\n",
      "Epoch [25/300], Step [130/225], Training Accuracy: 68.1731%, Training Loss: 0.6628%\n",
      "Epoch [25/300], Step [131/225], Training Accuracy: 68.2371%, Training Loss: 0.6624%\n",
      "Epoch [25/300], Step [132/225], Training Accuracy: 68.2410%, Training Loss: 0.6626%\n",
      "Epoch [25/300], Step [133/225], Training Accuracy: 68.2213%, Training Loss: 0.6625%\n",
      "Epoch [25/300], Step [134/225], Training Accuracy: 68.1903%, Training Loss: 0.6636%\n",
      "Epoch [25/300], Step [135/225], Training Accuracy: 68.1597%, Training Loss: 0.6633%\n",
      "Epoch [25/300], Step [136/225], Training Accuracy: 68.1870%, Training Loss: 0.6628%\n",
      "Epoch [25/300], Step [137/225], Training Accuracy: 68.1911%, Training Loss: 0.6625%\n",
      "Epoch [25/300], Step [138/225], Training Accuracy: 68.2971%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [139/225], Training Accuracy: 68.3341%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [140/225], Training Accuracy: 68.3482%, Training Loss: 0.6611%\n",
      "Epoch [25/300], Step [141/225], Training Accuracy: 68.3621%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [142/225], Training Accuracy: 68.3759%, Training Loss: 0.6607%\n",
      "Epoch [25/300], Step [143/225], Training Accuracy: 68.3785%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [144/225], Training Accuracy: 68.3919%, Training Loss: 0.6610%\n",
      "Epoch [25/300], Step [145/225], Training Accuracy: 68.4159%, Training Loss: 0.6607%\n",
      "Epoch [25/300], Step [146/225], Training Accuracy: 68.3754%, Training Loss: 0.6608%\n",
      "Epoch [25/300], Step [147/225], Training Accuracy: 68.3142%, Training Loss: 0.6607%\n",
      "Epoch [25/300], Step [148/225], Training Accuracy: 68.3805%, Training Loss: 0.6599%\n",
      "Epoch [25/300], Step [149/225], Training Accuracy: 68.3935%, Training Loss: 0.6602%\n",
      "Epoch [25/300], Step [150/225], Training Accuracy: 68.3854%, Training Loss: 0.6602%\n",
      "Epoch [25/300], Step [151/225], Training Accuracy: 68.4706%, Training Loss: 0.6592%\n",
      "Epoch [25/300], Step [152/225], Training Accuracy: 68.4519%, Training Loss: 0.6598%\n",
      "Epoch [25/300], Step [153/225], Training Accuracy: 68.4538%, Training Loss: 0.6599%\n",
      "Epoch [25/300], Step [154/225], Training Accuracy: 68.4963%, Training Loss: 0.6591%\n",
      "Epoch [25/300], Step [155/225], Training Accuracy: 68.4375%, Training Loss: 0.6598%\n",
      "Epoch [25/300], Step [156/225], Training Accuracy: 68.4495%, Training Loss: 0.6598%\n",
      "Epoch [25/300], Step [157/225], Training Accuracy: 68.4813%, Training Loss: 0.6598%\n",
      "Epoch [25/300], Step [158/225], Training Accuracy: 68.4039%, Training Loss: 0.6608%\n",
      "Epoch [25/300], Step [159/225], Training Accuracy: 68.3667%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [160/225], Training Accuracy: 68.3301%, Training Loss: 0.6609%\n",
      "Epoch [25/300], Step [161/225], Training Accuracy: 68.2453%, Training Loss: 0.6612%\n",
      "Epoch [25/300], Step [162/225], Training Accuracy: 68.2774%, Training Loss: 0.6605%\n",
      "Epoch [25/300], Step [163/225], Training Accuracy: 68.3282%, Training Loss: 0.6599%\n",
      "Epoch [25/300], Step [164/225], Training Accuracy: 68.3689%, Training Loss: 0.6590%\n",
      "Epoch [25/300], Step [165/225], Training Accuracy: 68.4280%, Training Loss: 0.6587%\n",
      "Epoch [25/300], Step [166/225], Training Accuracy: 68.4582%, Training Loss: 0.6583%\n",
      "Epoch [25/300], Step [167/225], Training Accuracy: 68.4693%, Training Loss: 0.6585%\n",
      "Epoch [25/300], Step [168/225], Training Accuracy: 68.5082%, Training Loss: 0.6580%\n",
      "Epoch [25/300], Step [169/225], Training Accuracy: 68.5004%, Training Loss: 0.6579%\n",
      "Epoch [25/300], Step [170/225], Training Accuracy: 68.5294%, Training Loss: 0.6576%\n",
      "Epoch [25/300], Step [171/225], Training Accuracy: 68.5764%, Training Loss: 0.6576%\n",
      "Epoch [25/300], Step [172/225], Training Accuracy: 68.5865%, Training Loss: 0.6574%\n",
      "Epoch [25/300], Step [173/225], Training Accuracy: 68.5965%, Training Loss: 0.6572%\n",
      "Epoch [25/300], Step [174/225], Training Accuracy: 68.6063%, Training Loss: 0.6570%\n",
      "Epoch [25/300], Step [175/225], Training Accuracy: 68.6250%, Training Loss: 0.6566%\n",
      "Epoch [25/300], Step [176/225], Training Accuracy: 68.6346%, Training Loss: 0.6561%\n",
      "Epoch [25/300], Step [177/225], Training Accuracy: 68.6794%, Training Loss: 0.6560%\n",
      "Epoch [25/300], Step [178/225], Training Accuracy: 68.7061%, Training Loss: 0.6560%\n",
      "Epoch [25/300], Step [179/225], Training Accuracy: 68.7587%, Training Loss: 0.6554%\n",
      "Epoch [25/300], Step [180/225], Training Accuracy: 68.7847%, Training Loss: 0.6557%\n",
      "Epoch [25/300], Step [181/225], Training Accuracy: 68.7845%, Training Loss: 0.6557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Step [182/225], Training Accuracy: 68.8015%, Training Loss: 0.6555%\n",
      "Epoch [25/300], Step [183/225], Training Accuracy: 68.8354%, Training Loss: 0.6554%\n",
      "Epoch [25/300], Step [184/225], Training Accuracy: 68.8179%, Training Loss: 0.6554%\n",
      "Epoch [25/300], Step [185/225], Training Accuracy: 68.8514%, Training Loss: 0.6552%\n",
      "Epoch [25/300], Step [186/225], Training Accuracy: 68.9096%, Training Loss: 0.6546%\n",
      "Epoch [25/300], Step [187/225], Training Accuracy: 68.8753%, Training Loss: 0.6552%\n",
      "Epoch [25/300], Step [188/225], Training Accuracy: 68.8913%, Training Loss: 0.6547%\n",
      "Epoch [25/300], Step [189/225], Training Accuracy: 68.9484%, Training Loss: 0.6545%\n",
      "Epoch [25/300], Step [190/225], Training Accuracy: 68.9391%, Training Loss: 0.6550%\n",
      "Epoch [25/300], Step [191/225], Training Accuracy: 68.9054%, Training Loss: 0.6549%\n",
      "Epoch [25/300], Step [192/225], Training Accuracy: 68.9535%, Training Loss: 0.6542%\n",
      "Epoch [25/300], Step [193/225], Training Accuracy: 68.9281%, Training Loss: 0.6549%\n",
      "Epoch [25/300], Step [194/225], Training Accuracy: 68.9352%, Training Loss: 0.6546%\n",
      "Epoch [25/300], Step [195/225], Training Accuracy: 68.9263%, Training Loss: 0.6546%\n",
      "Epoch [25/300], Step [196/225], Training Accuracy: 68.8776%, Training Loss: 0.6558%\n",
      "Epoch [25/300], Step [197/225], Training Accuracy: 68.8531%, Training Loss: 0.6560%\n",
      "Epoch [25/300], Step [198/225], Training Accuracy: 68.8999%, Training Loss: 0.6551%\n",
      "Epoch [25/300], Step [199/225], Training Accuracy: 68.9541%, Training Loss: 0.6545%\n",
      "Epoch [25/300], Step [200/225], Training Accuracy: 68.9453%, Training Loss: 0.6544%\n",
      "Epoch [25/300], Step [201/225], Training Accuracy: 68.9288%, Training Loss: 0.6551%\n",
      "Epoch [25/300], Step [202/225], Training Accuracy: 68.9821%, Training Loss: 0.6547%\n",
      "Epoch [25/300], Step [203/225], Training Accuracy: 69.0502%, Training Loss: 0.6541%\n",
      "Epoch [25/300], Step [204/225], Training Accuracy: 69.0717%, Training Loss: 0.6541%\n",
      "Epoch [25/300], Step [205/225], Training Accuracy: 69.1082%, Training Loss: 0.6536%\n",
      "Epoch [25/300], Step [206/225], Training Accuracy: 69.0989%, Training Loss: 0.6538%\n",
      "Epoch [25/300], Step [207/225], Training Accuracy: 69.0972%, Training Loss: 0.6540%\n",
      "Epoch [25/300], Step [208/225], Training Accuracy: 69.0956%, Training Loss: 0.6540%\n",
      "Epoch [25/300], Step [209/225], Training Accuracy: 69.0789%, Training Loss: 0.6541%\n",
      "Epoch [25/300], Step [210/225], Training Accuracy: 69.0774%, Training Loss: 0.6545%\n",
      "Epoch [25/300], Step [211/225], Training Accuracy: 69.1055%, Training Loss: 0.6540%\n",
      "Epoch [25/300], Step [212/225], Training Accuracy: 69.1038%, Training Loss: 0.6540%\n",
      "Epoch [25/300], Step [213/225], Training Accuracy: 69.0874%, Training Loss: 0.6544%\n",
      "Epoch [25/300], Step [214/225], Training Accuracy: 69.1005%, Training Loss: 0.6541%\n",
      "Epoch [25/300], Step [215/225], Training Accuracy: 69.0916%, Training Loss: 0.6544%\n",
      "Epoch [25/300], Step [216/225], Training Accuracy: 69.1045%, Training Loss: 0.6546%\n",
      "Epoch [25/300], Step [217/225], Training Accuracy: 69.0956%, Training Loss: 0.6552%\n",
      "Epoch [25/300], Step [218/225], Training Accuracy: 69.0582%, Training Loss: 0.6554%\n",
      "Epoch [25/300], Step [219/225], Training Accuracy: 69.0782%, Training Loss: 0.6554%\n",
      "Epoch [25/300], Step [220/225], Training Accuracy: 69.0625%, Training Loss: 0.6556%\n",
      "Epoch [25/300], Step [221/225], Training Accuracy: 68.9975%, Training Loss: 0.6562%\n",
      "Epoch [25/300], Step [222/225], Training Accuracy: 69.0175%, Training Loss: 0.6558%\n",
      "Epoch [25/300], Step [223/225], Training Accuracy: 68.9882%, Training Loss: 0.6563%\n",
      "Epoch [25/300], Step [224/225], Training Accuracy: 68.9732%, Training Loss: 0.6562%\n",
      "Epoch [25/300], Step [225/225], Training Accuracy: 68.9272%, Training Loss: 0.6564%\n",
      "Epoch [26/300], Step [1/225], Training Accuracy: 68.7500%, Training Loss: 0.6498%\n",
      "Epoch [26/300], Step [2/225], Training Accuracy: 66.4062%, Training Loss: 0.6541%\n",
      "Epoch [26/300], Step [3/225], Training Accuracy: 67.1875%, Training Loss: 0.6927%\n",
      "Epoch [26/300], Step [4/225], Training Accuracy: 67.9688%, Training Loss: 0.6921%\n",
      "Epoch [26/300], Step [5/225], Training Accuracy: 69.3750%, Training Loss: 0.6835%\n",
      "Epoch [26/300], Step [6/225], Training Accuracy: 69.5312%, Training Loss: 0.6898%\n",
      "Epoch [26/300], Step [7/225], Training Accuracy: 69.8661%, Training Loss: 0.6853%\n",
      "Epoch [26/300], Step [8/225], Training Accuracy: 69.3359%, Training Loss: 0.6861%\n",
      "Epoch [26/300], Step [9/225], Training Accuracy: 69.4444%, Training Loss: 0.6894%\n",
      "Epoch [26/300], Step [10/225], Training Accuracy: 68.7500%, Training Loss: 0.7026%\n",
      "Epoch [26/300], Step [11/225], Training Accuracy: 68.8920%, Training Loss: 0.6963%\n",
      "Epoch [26/300], Step [12/225], Training Accuracy: 68.3594%, Training Loss: 0.6971%\n",
      "Epoch [26/300], Step [13/225], Training Accuracy: 68.8702%, Training Loss: 0.6828%\n",
      "Epoch [26/300], Step [14/225], Training Accuracy: 69.5312%, Training Loss: 0.6726%\n",
      "Epoch [26/300], Step [15/225], Training Accuracy: 69.4792%, Training Loss: 0.6720%\n",
      "Epoch [26/300], Step [16/225], Training Accuracy: 69.4336%, Training Loss: 0.6673%\n",
      "Epoch [26/300], Step [17/225], Training Accuracy: 69.6691%, Training Loss: 0.6629%\n",
      "Epoch [26/300], Step [18/225], Training Accuracy: 69.7049%, Training Loss: 0.6646%\n",
      "Epoch [26/300], Step [19/225], Training Accuracy: 69.8191%, Training Loss: 0.6628%\n",
      "Epoch [26/300], Step [20/225], Training Accuracy: 69.8438%, Training Loss: 0.6638%\n",
      "Epoch [26/300], Step [21/225], Training Accuracy: 70.1637%, Training Loss: 0.6583%\n",
      "Epoch [26/300], Step [22/225], Training Accuracy: 69.7443%, Training Loss: 0.6631%\n",
      "Epoch [26/300], Step [23/225], Training Accuracy: 70.0408%, Training Loss: 0.6565%\n",
      "Epoch [26/300], Step [24/225], Training Accuracy: 69.7266%, Training Loss: 0.6592%\n",
      "Epoch [26/300], Step [25/225], Training Accuracy: 69.6250%, Training Loss: 0.6604%\n",
      "Epoch [26/300], Step [26/225], Training Accuracy: 69.3510%, Training Loss: 0.6625%\n",
      "Epoch [26/300], Step [27/225], Training Accuracy: 69.2708%, Training Loss: 0.6618%\n",
      "Epoch [26/300], Step [28/225], Training Accuracy: 69.6987%, Training Loss: 0.6557%\n",
      "Epoch [26/300], Step [29/225], Training Accuracy: 69.7198%, Training Loss: 0.6566%\n",
      "Epoch [26/300], Step [30/225], Training Accuracy: 69.9479%, Training Loss: 0.6555%\n",
      "Epoch [26/300], Step [31/225], Training Accuracy: 69.5565%, Training Loss: 0.6639%\n",
      "Epoch [26/300], Step [32/225], Training Accuracy: 69.3848%, Training Loss: 0.6660%\n",
      "Epoch [26/300], Step [33/225], Training Accuracy: 69.6023%, Training Loss: 0.6622%\n",
      "Epoch [26/300], Step [34/225], Training Accuracy: 69.6691%, Training Loss: 0.6596%\n",
      "Epoch [26/300], Step [35/225], Training Accuracy: 69.5982%, Training Loss: 0.6583%\n",
      "Epoch [26/300], Step [36/225], Training Accuracy: 69.6615%, Training Loss: 0.6565%\n",
      "Epoch [26/300], Step [37/225], Training Accuracy: 69.6791%, Training Loss: 0.6574%\n",
      "Epoch [26/300], Step [38/225], Training Accuracy: 69.9424%, Training Loss: 0.6546%\n",
      "Epoch [26/300], Step [39/225], Training Accuracy: 69.9920%, Training Loss: 0.6549%\n",
      "Epoch [26/300], Step [40/225], Training Accuracy: 69.9609%, Training Loss: 0.6540%\n",
      "Epoch [26/300], Step [41/225], Training Accuracy: 69.8933%, Training Loss: 0.6546%\n",
      "Epoch [26/300], Step [42/225], Training Accuracy: 69.8289%, Training Loss: 0.6542%\n",
      "Epoch [26/300], Step [43/225], Training Accuracy: 69.6948%, Training Loss: 0.6559%\n",
      "Epoch [26/300], Step [44/225], Training Accuracy: 69.8153%, Training Loss: 0.6535%\n",
      "Epoch [26/300], Step [45/225], Training Accuracy: 70.0000%, Training Loss: 0.6521%\n",
      "Epoch [26/300], Step [46/225], Training Accuracy: 69.9389%, Training Loss: 0.6515%\n",
      "Epoch [26/300], Step [47/225], Training Accuracy: 69.8803%, Training Loss: 0.6514%\n",
      "Epoch [26/300], Step [48/225], Training Accuracy: 69.6289%, Training Loss: 0.6519%\n",
      "Epoch [26/300], Step [49/225], Training Accuracy: 69.7704%, Training Loss: 0.6510%\n",
      "Epoch [26/300], Step [50/225], Training Accuracy: 69.7500%, Training Loss: 0.6521%\n",
      "Epoch [26/300], Step [51/225], Training Accuracy: 69.9142%, Training Loss: 0.6506%\n",
      "Epoch [26/300], Step [52/225], Training Accuracy: 69.9820%, Training Loss: 0.6500%\n",
      "Epoch [26/300], Step [53/225], Training Accuracy: 69.9882%, Training Loss: 0.6501%\n",
      "Epoch [26/300], Step [54/225], Training Accuracy: 69.8785%, Training Loss: 0.6511%\n",
      "Epoch [26/300], Step [55/225], Training Accuracy: 69.7727%, Training Loss: 0.6520%\n",
      "Epoch [26/300], Step [56/225], Training Accuracy: 69.6987%, Training Loss: 0.6521%\n",
      "Epoch [26/300], Step [57/225], Training Accuracy: 69.7643%, Training Loss: 0.6507%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Step [58/225], Training Accuracy: 69.6121%, Training Loss: 0.6519%\n",
      "Epoch [26/300], Step [59/225], Training Accuracy: 69.6504%, Training Loss: 0.6506%\n",
      "Epoch [26/300], Step [60/225], Training Accuracy: 69.7396%, Training Loss: 0.6498%\n",
      "Epoch [26/300], Step [61/225], Training Accuracy: 69.7746%, Training Loss: 0.6512%\n",
      "Epoch [26/300], Step [62/225], Training Accuracy: 69.8589%, Training Loss: 0.6502%\n",
      "Epoch [26/300], Step [63/225], Training Accuracy: 69.7421%, Training Loss: 0.6517%\n",
      "Epoch [26/300], Step [64/225], Training Accuracy: 69.8486%, Training Loss: 0.6499%\n",
      "Epoch [26/300], Step [65/225], Training Accuracy: 69.9038%, Training Loss: 0.6487%\n",
      "Epoch [26/300], Step [66/225], Training Accuracy: 70.1231%, Training Loss: 0.6464%\n",
      "Epoch [26/300], Step [67/225], Training Accuracy: 70.1026%, Training Loss: 0.6458%\n",
      "Epoch [26/300], Step [68/225], Training Accuracy: 70.0827%, Training Loss: 0.6469%\n",
      "Epoch [26/300], Step [69/225], Training Accuracy: 69.9275%, Training Loss: 0.6488%\n",
      "Epoch [26/300], Step [70/225], Training Accuracy: 69.9107%, Training Loss: 0.6491%\n",
      "Epoch [26/300], Step [71/225], Training Accuracy: 70.0704%, Training Loss: 0.6471%\n",
      "Epoch [26/300], Step [72/225], Training Accuracy: 70.1172%, Training Loss: 0.6460%\n",
      "Epoch [26/300], Step [73/225], Training Accuracy: 70.0985%, Training Loss: 0.6450%\n",
      "Epoch [26/300], Step [74/225], Training Accuracy: 70.0802%, Training Loss: 0.6441%\n",
      "Epoch [26/300], Step [75/225], Training Accuracy: 70.1667%, Training Loss: 0.6435%\n",
      "Epoch [26/300], Step [76/225], Training Accuracy: 70.0863%, Training Loss: 0.6450%\n",
      "Epoch [26/300], Step [77/225], Training Accuracy: 70.0487%, Training Loss: 0.6441%\n",
      "Epoch [26/300], Step [78/225], Training Accuracy: 70.0120%, Training Loss: 0.6439%\n",
      "Epoch [26/300], Step [79/225], Training Accuracy: 70.1543%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [80/225], Training Accuracy: 70.2148%, Training Loss: 0.6422%\n",
      "Epoch [26/300], Step [81/225], Training Accuracy: 70.2739%, Training Loss: 0.6414%\n",
      "Epoch [26/300], Step [82/225], Training Accuracy: 70.3125%, Training Loss: 0.6406%\n",
      "Epoch [26/300], Step [83/225], Training Accuracy: 70.2184%, Training Loss: 0.6404%\n",
      "Epoch [26/300], Step [84/225], Training Accuracy: 70.2567%, Training Loss: 0.6406%\n",
      "Epoch [26/300], Step [85/225], Training Accuracy: 70.3125%, Training Loss: 0.6389%\n",
      "Epoch [26/300], Step [86/225], Training Accuracy: 70.3125%, Training Loss: 0.6389%\n",
      "Epoch [26/300], Step [87/225], Training Accuracy: 70.3484%, Training Loss: 0.6390%\n",
      "Epoch [26/300], Step [88/225], Training Accuracy: 70.3480%, Training Loss: 0.6393%\n",
      "Epoch [26/300], Step [89/225], Training Accuracy: 70.3301%, Training Loss: 0.6402%\n",
      "Epoch [26/300], Step [90/225], Training Accuracy: 70.2604%, Training Loss: 0.6417%\n",
      "Epoch [26/300], Step [91/225], Training Accuracy: 70.1923%, Training Loss: 0.6416%\n",
      "Epoch [26/300], Step [92/225], Training Accuracy: 70.1087%, Training Loss: 0.6418%\n",
      "Epoch [26/300], Step [93/225], Training Accuracy: 70.1445%, Training Loss: 0.6414%\n",
      "Epoch [26/300], Step [94/225], Training Accuracy: 70.1795%, Training Loss: 0.6405%\n",
      "Epoch [26/300], Step [95/225], Training Accuracy: 70.2467%, Training Loss: 0.6402%\n",
      "Epoch [26/300], Step [96/225], Training Accuracy: 70.2474%, Training Loss: 0.6396%\n",
      "Epoch [26/300], Step [97/225], Training Accuracy: 70.2481%, Training Loss: 0.6394%\n",
      "Epoch [26/300], Step [98/225], Training Accuracy: 70.2009%, Training Loss: 0.6405%\n",
      "Epoch [26/300], Step [99/225], Training Accuracy: 70.2178%, Training Loss: 0.6405%\n",
      "Epoch [26/300], Step [100/225], Training Accuracy: 70.1094%, Training Loss: 0.6423%\n",
      "Epoch [26/300], Step [101/225], Training Accuracy: 70.1114%, Training Loss: 0.6424%\n",
      "Epoch [26/300], Step [102/225], Training Accuracy: 70.0368%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [103/225], Training Accuracy: 70.0394%, Training Loss: 0.6423%\n",
      "Epoch [26/300], Step [104/225], Training Accuracy: 69.9820%, Training Loss: 0.6429%\n",
      "Epoch [26/300], Step [105/225], Training Accuracy: 69.9851%, Training Loss: 0.6420%\n",
      "Epoch [26/300], Step [106/225], Training Accuracy: 70.0029%, Training Loss: 0.6417%\n",
      "Epoch [26/300], Step [107/225], Training Accuracy: 69.9474%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [108/225], Training Accuracy: 69.9219%, Training Loss: 0.6429%\n",
      "Epoch [26/300], Step [109/225], Training Accuracy: 69.8968%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [110/225], Training Accuracy: 69.9148%, Training Loss: 0.6419%\n",
      "Epoch [26/300], Step [111/225], Training Accuracy: 69.9465%, Training Loss: 0.6411%\n",
      "Epoch [26/300], Step [112/225], Training Accuracy: 69.9219%, Training Loss: 0.6415%\n",
      "Epoch [26/300], Step [113/225], Training Accuracy: 69.8977%, Training Loss: 0.6416%\n",
      "Epoch [26/300], Step [114/225], Training Accuracy: 69.9013%, Training Loss: 0.6418%\n",
      "Epoch [26/300], Step [115/225], Training Accuracy: 69.9457%, Training Loss: 0.6418%\n",
      "Epoch [26/300], Step [116/225], Training Accuracy: 69.9892%, Training Loss: 0.6413%\n",
      "Epoch [26/300], Step [117/225], Training Accuracy: 69.9119%, Training Loss: 0.6424%\n",
      "Epoch [26/300], Step [118/225], Training Accuracy: 69.8226%, Training Loss: 0.6432%\n",
      "Epoch [26/300], Step [119/225], Training Accuracy: 69.8136%, Training Loss: 0.6428%\n",
      "Epoch [26/300], Step [120/225], Training Accuracy: 69.8698%, Training Loss: 0.6423%\n",
      "Epoch [26/300], Step [121/225], Training Accuracy: 69.7960%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [122/225], Training Accuracy: 69.8258%, Training Loss: 0.6426%\n",
      "Epoch [26/300], Step [123/225], Training Accuracy: 69.8044%, Training Loss: 0.6424%\n",
      "Epoch [26/300], Step [124/225], Training Accuracy: 69.8463%, Training Loss: 0.6418%\n",
      "Epoch [26/300], Step [125/225], Training Accuracy: 69.8500%, Training Loss: 0.6417%\n",
      "Epoch [26/300], Step [126/225], Training Accuracy: 69.8289%, Training Loss: 0.6417%\n",
      "Epoch [26/300], Step [127/225], Training Accuracy: 69.8450%, Training Loss: 0.6422%\n",
      "Epoch [26/300], Step [128/225], Training Accuracy: 69.7754%, Training Loss: 0.6438%\n",
      "Epoch [26/300], Step [129/225], Training Accuracy: 69.7917%, Training Loss: 0.6439%\n",
      "Epoch [26/300], Step [130/225], Training Accuracy: 69.7596%, Training Loss: 0.6440%\n",
      "Epoch [26/300], Step [131/225], Training Accuracy: 69.7638%, Training Loss: 0.6441%\n",
      "Epoch [26/300], Step [132/225], Training Accuracy: 69.6851%, Training Loss: 0.6443%\n",
      "Epoch [26/300], Step [133/225], Training Accuracy: 69.6194%, Training Loss: 0.6446%\n",
      "Epoch [26/300], Step [134/225], Training Accuracy: 69.5312%, Training Loss: 0.6471%\n",
      "Epoch [26/300], Step [135/225], Training Accuracy: 69.5139%, Training Loss: 0.6475%\n",
      "Epoch [26/300], Step [136/225], Training Accuracy: 69.4968%, Training Loss: 0.6473%\n",
      "Epoch [26/300], Step [137/225], Training Accuracy: 69.4343%, Training Loss: 0.6475%\n",
      "Epoch [26/300], Step [138/225], Training Accuracy: 69.4973%, Training Loss: 0.6463%\n",
      "Epoch [26/300], Step [139/225], Training Accuracy: 69.4919%, Training Loss: 0.6465%\n",
      "Epoch [26/300], Step [140/225], Training Accuracy: 69.5312%, Training Loss: 0.6459%\n",
      "Epoch [26/300], Step [141/225], Training Accuracy: 69.4925%, Training Loss: 0.6459%\n",
      "Epoch [26/300], Step [142/225], Training Accuracy: 69.5202%, Training Loss: 0.6454%\n",
      "Epoch [26/300], Step [143/225], Training Accuracy: 69.5258%, Training Loss: 0.6457%\n",
      "Epoch [26/300], Step [144/225], Training Accuracy: 69.5204%, Training Loss: 0.6457%\n",
      "Epoch [26/300], Step [145/225], Training Accuracy: 69.5259%, Training Loss: 0.6453%\n",
      "Epoch [26/300], Step [146/225], Training Accuracy: 69.5527%, Training Loss: 0.6451%\n",
      "Epoch [26/300], Step [147/225], Training Accuracy: 69.5259%, Training Loss: 0.6455%\n",
      "Epoch [26/300], Step [148/225], Training Accuracy: 69.5735%, Training Loss: 0.6450%\n",
      "Epoch [26/300], Step [149/225], Training Accuracy: 69.5994%, Training Loss: 0.6453%\n",
      "Epoch [26/300], Step [150/225], Training Accuracy: 69.5729%, Training Loss: 0.6455%\n",
      "Epoch [26/300], Step [151/225], Training Accuracy: 69.6296%, Training Loss: 0.6450%\n",
      "Epoch [26/300], Step [152/225], Training Accuracy: 69.5621%, Training Loss: 0.6459%\n",
      "Epoch [26/300], Step [153/225], Training Accuracy: 69.5466%, Training Loss: 0.6459%\n",
      "Epoch [26/300], Step [154/225], Training Accuracy: 69.5312%, Training Loss: 0.6460%\n",
      "Epoch [26/300], Step [155/225], Training Accuracy: 69.5060%, Training Loss: 0.6467%\n",
      "Epoch [26/300], Step [156/225], Training Accuracy: 69.4812%, Training Loss: 0.6470%\n",
      "Epoch [26/300], Step [157/225], Training Accuracy: 69.4367%, Training Loss: 0.6474%\n",
      "Epoch [26/300], Step [158/225], Training Accuracy: 69.3730%, Training Loss: 0.6477%\n",
      "Epoch [26/300], Step [159/225], Training Accuracy: 69.3396%, Training Loss: 0.6485%\n",
      "Epoch [26/300], Step [160/225], Training Accuracy: 69.3555%, Training Loss: 0.6485%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Step [161/225], Training Accuracy: 69.3420%, Training Loss: 0.6485%\n",
      "Epoch [26/300], Step [162/225], Training Accuracy: 69.4541%, Training Loss: 0.6470%\n",
      "Epoch [26/300], Step [163/225], Training Accuracy: 69.4689%, Training Loss: 0.6469%\n",
      "Epoch [26/300], Step [164/225], Training Accuracy: 69.5598%, Training Loss: 0.6462%\n",
      "Epoch [26/300], Step [165/225], Training Accuracy: 69.5549%, Training Loss: 0.6459%\n",
      "Epoch [26/300], Step [166/225], Training Accuracy: 69.6160%, Training Loss: 0.6456%\n",
      "Epoch [26/300], Step [167/225], Training Accuracy: 69.6388%, Training Loss: 0.6454%\n",
      "Epoch [26/300], Step [168/225], Training Accuracy: 69.6708%, Training Loss: 0.6460%\n",
      "Epoch [26/300], Step [169/225], Training Accuracy: 69.6191%, Training Loss: 0.6464%\n",
      "Epoch [26/300], Step [170/225], Training Accuracy: 69.5956%, Training Loss: 0.6465%\n",
      "Epoch [26/300], Step [171/225], Training Accuracy: 69.5724%, Training Loss: 0.6466%\n",
      "Epoch [26/300], Step [172/225], Training Accuracy: 69.5948%, Training Loss: 0.6463%\n",
      "Epoch [26/300], Step [173/225], Training Accuracy: 69.5809%, Training Loss: 0.6464%\n",
      "Epoch [26/300], Step [174/225], Training Accuracy: 69.5851%, Training Loss: 0.6463%\n",
      "Epoch [26/300], Step [175/225], Training Accuracy: 69.6518%, Training Loss: 0.6455%\n",
      "Epoch [26/300], Step [176/225], Training Accuracy: 69.6467%, Training Loss: 0.6454%\n",
      "Epoch [26/300], Step [177/225], Training Accuracy: 69.6593%, Training Loss: 0.6460%\n",
      "Epoch [26/300], Step [178/225], Training Accuracy: 69.6541%, Training Loss: 0.6458%\n",
      "Epoch [26/300], Step [179/225], Training Accuracy: 69.7015%, Training Loss: 0.6453%\n",
      "Epoch [26/300], Step [180/225], Training Accuracy: 69.6875%, Training Loss: 0.6456%\n",
      "Epoch [26/300], Step [181/225], Training Accuracy: 69.6392%, Training Loss: 0.6461%\n",
      "Epoch [26/300], Step [182/225], Training Accuracy: 69.6343%, Training Loss: 0.6467%\n",
      "Epoch [26/300], Step [183/225], Training Accuracy: 69.6380%, Training Loss: 0.6467%\n",
      "Epoch [26/300], Step [184/225], Training Accuracy: 69.6501%, Training Loss: 0.6461%\n",
      "Epoch [26/300], Step [185/225], Training Accuracy: 69.6959%, Training Loss: 0.6456%\n",
      "Epoch [26/300], Step [186/225], Training Accuracy: 69.7413%, Training Loss: 0.6454%\n",
      "Epoch [26/300], Step [187/225], Training Accuracy: 69.7360%, Training Loss: 0.6451%\n",
      "Epoch [26/300], Step [188/225], Training Accuracy: 69.7307%, Training Loss: 0.6445%\n",
      "Epoch [26/300], Step [189/225], Training Accuracy: 69.7255%, Training Loss: 0.6445%\n",
      "Epoch [26/300], Step [190/225], Training Accuracy: 69.7204%, Training Loss: 0.6446%\n",
      "Epoch [26/300], Step [191/225], Training Accuracy: 69.7071%, Training Loss: 0.6447%\n",
      "Epoch [26/300], Step [192/225], Training Accuracy: 69.7673%, Training Loss: 0.6439%\n",
      "Epoch [26/300], Step [193/225], Training Accuracy: 69.7296%, Training Loss: 0.6444%\n",
      "Epoch [26/300], Step [194/225], Training Accuracy: 69.7487%, Training Loss: 0.6440%\n",
      "Epoch [26/300], Step [195/225], Training Accuracy: 69.7676%, Training Loss: 0.6439%\n",
      "Epoch [26/300], Step [196/225], Training Accuracy: 69.7305%, Training Loss: 0.6442%\n",
      "Epoch [26/300], Step [197/225], Training Accuracy: 69.7176%, Training Loss: 0.6443%\n",
      "Epoch [26/300], Step [198/225], Training Accuracy: 69.7601%, Training Loss: 0.6435%\n",
      "Epoch [26/300], Step [199/225], Training Accuracy: 69.7393%, Training Loss: 0.6436%\n",
      "Epoch [26/300], Step [200/225], Training Accuracy: 69.7266%, Training Loss: 0.6437%\n",
      "Epoch [26/300], Step [201/225], Training Accuracy: 69.7139%, Training Loss: 0.6441%\n",
      "Epoch [26/300], Step [202/225], Training Accuracy: 69.7324%, Training Loss: 0.6438%\n",
      "Epoch [26/300], Step [203/225], Training Accuracy: 69.7660%, Training Loss: 0.6432%\n",
      "Epoch [26/300], Step [204/225], Training Accuracy: 69.7840%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [205/225], Training Accuracy: 69.7942%, Training Loss: 0.6424%\n",
      "Epoch [26/300], Step [206/225], Training Accuracy: 69.7816%, Training Loss: 0.6429%\n",
      "Epoch [26/300], Step [207/225], Training Accuracy: 69.7766%, Training Loss: 0.6431%\n",
      "Epoch [26/300], Step [208/225], Training Accuracy: 69.8392%, Training Loss: 0.6425%\n",
      "Epoch [26/300], Step [209/225], Training Accuracy: 69.8415%, Training Loss: 0.6426%\n",
      "Epoch [26/300], Step [210/225], Training Accuracy: 69.8214%, Training Loss: 0.6429%\n",
      "Epoch [26/300], Step [211/225], Training Accuracy: 69.8830%, Training Loss: 0.6422%\n",
      "Epoch [26/300], Step [212/225], Training Accuracy: 69.8777%, Training Loss: 0.6420%\n",
      "Epoch [26/300], Step [213/225], Training Accuracy: 69.8577%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [214/225], Training Accuracy: 69.8598%, Training Loss: 0.6425%\n",
      "Epoch [26/300], Step [215/225], Training Accuracy: 69.8474%, Training Loss: 0.6426%\n",
      "Epoch [26/300], Step [216/225], Training Accuracy: 69.8568%, Training Loss: 0.6426%\n",
      "Epoch [26/300], Step [217/225], Training Accuracy: 69.8661%, Training Loss: 0.6427%\n",
      "Epoch [26/300], Step [218/225], Training Accuracy: 69.8323%, Training Loss: 0.6429%\n",
      "Epoch [26/300], Step [219/225], Training Accuracy: 69.8059%, Training Loss: 0.6429%\n",
      "Epoch [26/300], Step [220/225], Training Accuracy: 69.8011%, Training Loss: 0.6430%\n",
      "Epoch [26/300], Step [221/225], Training Accuracy: 69.7610%, Training Loss: 0.6434%\n",
      "Epoch [26/300], Step [222/225], Training Accuracy: 69.7706%, Training Loss: 0.6433%\n",
      "Epoch [26/300], Step [223/225], Training Accuracy: 69.7450%, Training Loss: 0.6438%\n",
      "Epoch [26/300], Step [224/225], Training Accuracy: 69.7126%, Training Loss: 0.6439%\n",
      "Epoch [26/300], Step [225/225], Training Accuracy: 69.6984%, Training Loss: 0.6438%\n",
      "Epoch [27/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.5134%\n",
      "Epoch [27/300], Step [2/225], Training Accuracy: 74.2188%, Training Loss: 0.5533%\n",
      "Epoch [27/300], Step [3/225], Training Accuracy: 74.4792%, Training Loss: 0.5778%\n",
      "Epoch [27/300], Step [4/225], Training Accuracy: 72.2656%, Training Loss: 0.6112%\n",
      "Epoch [27/300], Step [5/225], Training Accuracy: 73.4375%, Training Loss: 0.6028%\n",
      "Epoch [27/300], Step [6/225], Training Accuracy: 73.4375%, Training Loss: 0.6127%\n",
      "Epoch [27/300], Step [7/225], Training Accuracy: 73.6607%, Training Loss: 0.6119%\n",
      "Epoch [27/300], Step [8/225], Training Accuracy: 72.2656%, Training Loss: 0.6238%\n",
      "Epoch [27/300], Step [9/225], Training Accuracy: 71.8750%, Training Loss: 0.6292%\n",
      "Epoch [27/300], Step [10/225], Training Accuracy: 70.7812%, Training Loss: 0.6488%\n",
      "Epoch [27/300], Step [11/225], Training Accuracy: 70.7386%, Training Loss: 0.6588%\n",
      "Epoch [27/300], Step [12/225], Training Accuracy: 70.5729%, Training Loss: 0.6579%\n",
      "Epoch [27/300], Step [13/225], Training Accuracy: 71.1538%, Training Loss: 0.6475%\n",
      "Epoch [27/300], Step [14/225], Training Accuracy: 70.8705%, Training Loss: 0.6474%\n",
      "Epoch [27/300], Step [15/225], Training Accuracy: 71.1458%, Training Loss: 0.6473%\n",
      "Epoch [27/300], Step [16/225], Training Accuracy: 71.3867%, Training Loss: 0.6438%\n",
      "Epoch [27/300], Step [17/225], Training Accuracy: 71.2316%, Training Loss: 0.6408%\n",
      "Epoch [27/300], Step [18/225], Training Accuracy: 71.0938%, Training Loss: 0.6409%\n",
      "Epoch [27/300], Step [19/225], Training Accuracy: 70.9704%, Training Loss: 0.6407%\n",
      "Epoch [27/300], Step [20/225], Training Accuracy: 71.0938%, Training Loss: 0.6359%\n",
      "Epoch [27/300], Step [21/225], Training Accuracy: 71.2798%, Training Loss: 0.6307%\n",
      "Epoch [27/300], Step [22/225], Training Accuracy: 70.5256%, Training Loss: 0.6421%\n",
      "Epoch [27/300], Step [23/225], Training Accuracy: 70.9918%, Training Loss: 0.6367%\n",
      "Epoch [27/300], Step [24/225], Training Accuracy: 70.6380%, Training Loss: 0.6406%\n",
      "Epoch [27/300], Step [25/225], Training Accuracy: 70.6875%, Training Loss: 0.6388%\n",
      "Epoch [27/300], Step [26/225], Training Accuracy: 70.4327%, Training Loss: 0.6405%\n",
      "Epoch [27/300], Step [27/225], Training Accuracy: 70.6019%, Training Loss: 0.6374%\n",
      "Epoch [27/300], Step [28/225], Training Accuracy: 71.0379%, Training Loss: 0.6306%\n",
      "Epoch [27/300], Step [29/225], Training Accuracy: 71.0129%, Training Loss: 0.6322%\n",
      "Epoch [27/300], Step [30/225], Training Accuracy: 71.0938%, Training Loss: 0.6309%\n",
      "Epoch [27/300], Step [31/225], Training Accuracy: 70.5645%, Training Loss: 0.6374%\n",
      "Epoch [27/300], Step [32/225], Training Accuracy: 70.4102%, Training Loss: 0.6397%\n",
      "Epoch [27/300], Step [33/225], Training Accuracy: 70.3598%, Training Loss: 0.6407%\n",
      "Epoch [27/300], Step [34/225], Training Accuracy: 70.1746%, Training Loss: 0.6402%\n",
      "Epoch [27/300], Step [35/225], Training Accuracy: 70.2679%, Training Loss: 0.6400%\n",
      "Epoch [27/300], Step [36/225], Training Accuracy: 70.4427%, Training Loss: 0.6378%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Step [37/225], Training Accuracy: 70.4392%, Training Loss: 0.6384%\n",
      "Epoch [27/300], Step [38/225], Training Accuracy: 70.4359%, Training Loss: 0.6385%\n",
      "Epoch [27/300], Step [39/225], Training Accuracy: 70.5128%, Training Loss: 0.6381%\n",
      "Epoch [27/300], Step [40/225], Training Accuracy: 70.5469%, Training Loss: 0.6392%\n",
      "Epoch [27/300], Step [41/225], Training Accuracy: 70.5793%, Training Loss: 0.6397%\n",
      "Epoch [27/300], Step [42/225], Training Accuracy: 70.6101%, Training Loss: 0.6396%\n",
      "Epoch [27/300], Step [43/225], Training Accuracy: 70.5669%, Training Loss: 0.6418%\n",
      "Epoch [27/300], Step [44/225], Training Accuracy: 70.7386%, Training Loss: 0.6409%\n",
      "Epoch [27/300], Step [45/225], Training Accuracy: 70.7639%, Training Loss: 0.6401%\n",
      "Epoch [27/300], Step [46/225], Training Accuracy: 70.8220%, Training Loss: 0.6395%\n",
      "Epoch [27/300], Step [47/225], Training Accuracy: 70.6449%, Training Loss: 0.6400%\n",
      "Epoch [27/300], Step [48/225], Training Accuracy: 70.3451%, Training Loss: 0.6417%\n",
      "Epoch [27/300], Step [49/225], Training Accuracy: 70.5038%, Training Loss: 0.6415%\n",
      "Epoch [27/300], Step [50/225], Training Accuracy: 70.5000%, Training Loss: 0.6418%\n",
      "Epoch [27/300], Step [51/225], Training Accuracy: 70.5576%, Training Loss: 0.6415%\n",
      "Epoch [27/300], Step [52/225], Training Accuracy: 70.6731%, Training Loss: 0.6409%\n",
      "Epoch [27/300], Step [53/225], Training Accuracy: 70.6368%, Training Loss: 0.6415%\n",
      "Epoch [27/300], Step [54/225], Training Accuracy: 70.3993%, Training Loss: 0.6423%\n",
      "Epoch [27/300], Step [55/225], Training Accuracy: 70.3125%, Training Loss: 0.6438%\n",
      "Epoch [27/300], Step [56/225], Training Accuracy: 70.1451%, Training Loss: 0.6457%\n",
      "Epoch [27/300], Step [57/225], Training Accuracy: 70.1754%, Training Loss: 0.6449%\n",
      "Epoch [27/300], Step [58/225], Training Accuracy: 70.0970%, Training Loss: 0.6462%\n",
      "Epoch [27/300], Step [59/225], Training Accuracy: 70.1536%, Training Loss: 0.6457%\n",
      "Epoch [27/300], Step [60/225], Training Accuracy: 70.2865%, Training Loss: 0.6449%\n",
      "Epoch [27/300], Step [61/225], Training Accuracy: 70.2357%, Training Loss: 0.6451%\n",
      "Epoch [27/300], Step [62/225], Training Accuracy: 70.1361%, Training Loss: 0.6472%\n",
      "Epoch [27/300], Step [63/225], Training Accuracy: 70.0397%, Training Loss: 0.6491%\n",
      "Epoch [27/300], Step [64/225], Training Accuracy: 70.0684%, Training Loss: 0.6477%\n",
      "Epoch [27/300], Step [65/225], Training Accuracy: 70.0962%, Training Loss: 0.6482%\n",
      "Epoch [27/300], Step [66/225], Training Accuracy: 70.1941%, Training Loss: 0.6465%\n",
      "Epoch [27/300], Step [67/225], Training Accuracy: 70.1959%, Training Loss: 0.6456%\n",
      "Epoch [27/300], Step [68/225], Training Accuracy: 70.2206%, Training Loss: 0.6460%\n",
      "Epoch [27/300], Step [69/225], Training Accuracy: 70.0181%, Training Loss: 0.6500%\n",
      "Epoch [27/300], Step [70/225], Training Accuracy: 70.0000%, Training Loss: 0.6499%\n",
      "Epoch [27/300], Step [71/225], Training Accuracy: 70.0924%, Training Loss: 0.6483%\n",
      "Epoch [27/300], Step [72/225], Training Accuracy: 70.2040%, Training Loss: 0.6472%\n",
      "Epoch [27/300], Step [73/225], Training Accuracy: 70.0985%, Training Loss: 0.6484%\n",
      "Epoch [27/300], Step [74/225], Training Accuracy: 69.9747%, Training Loss: 0.6484%\n",
      "Epoch [27/300], Step [75/225], Training Accuracy: 70.0000%, Training Loss: 0.6472%\n",
      "Epoch [27/300], Step [76/225], Training Accuracy: 69.7985%, Training Loss: 0.6503%\n",
      "Epoch [27/300], Step [77/225], Training Accuracy: 69.8661%, Training Loss: 0.6495%\n",
      "Epoch [27/300], Step [78/225], Training Accuracy: 69.8117%, Training Loss: 0.6500%\n",
      "Epoch [27/300], Step [79/225], Training Accuracy: 69.6994%, Training Loss: 0.6497%\n",
      "Epoch [27/300], Step [80/225], Training Accuracy: 69.7461%, Training Loss: 0.6493%\n",
      "Epoch [27/300], Step [81/225], Training Accuracy: 69.8302%, Training Loss: 0.6481%\n",
      "Epoch [27/300], Step [82/225], Training Accuracy: 69.9123%, Training Loss: 0.6468%\n",
      "Epoch [27/300], Step [83/225], Training Accuracy: 69.9736%, Training Loss: 0.6456%\n",
      "Epoch [27/300], Step [84/225], Training Accuracy: 70.0521%, Training Loss: 0.6454%\n",
      "Epoch [27/300], Step [85/225], Training Accuracy: 70.0919%, Training Loss: 0.6438%\n",
      "Epoch [27/300], Step [86/225], Training Accuracy: 69.9673%, Training Loss: 0.6441%\n",
      "Epoch [27/300], Step [87/225], Training Accuracy: 70.0072%, Training Loss: 0.6442%\n",
      "Epoch [27/300], Step [88/225], Training Accuracy: 69.9041%, Training Loss: 0.6453%\n",
      "Epoch [27/300], Step [89/225], Training Accuracy: 69.9614%, Training Loss: 0.6449%\n",
      "Epoch [27/300], Step [90/225], Training Accuracy: 69.8611%, Training Loss: 0.6467%\n",
      "Epoch [27/300], Step [91/225], Training Accuracy: 69.8146%, Training Loss: 0.6476%\n",
      "Epoch [27/300], Step [92/225], Training Accuracy: 69.8200%, Training Loss: 0.6481%\n",
      "Epoch [27/300], Step [93/225], Training Accuracy: 69.9261%, Training Loss: 0.6466%\n",
      "Epoch [27/300], Step [94/225], Training Accuracy: 70.0299%, Training Loss: 0.6458%\n",
      "Epoch [27/300], Step [95/225], Training Accuracy: 70.0658%, Training Loss: 0.6463%\n",
      "Epoch [27/300], Step [96/225], Training Accuracy: 70.1335%, Training Loss: 0.6450%\n",
      "Epoch [27/300], Step [97/225], Training Accuracy: 70.1192%, Training Loss: 0.6452%\n",
      "Epoch [27/300], Step [98/225], Training Accuracy: 69.9777%, Training Loss: 0.6460%\n",
      "Epoch [27/300], Step [99/225], Training Accuracy: 69.9495%, Training Loss: 0.6467%\n",
      "Epoch [27/300], Step [100/225], Training Accuracy: 69.8750%, Training Loss: 0.6473%\n",
      "Epoch [27/300], Step [101/225], Training Accuracy: 69.9103%, Training Loss: 0.6468%\n",
      "Epoch [27/300], Step [102/225], Training Accuracy: 69.8376%, Training Loss: 0.6473%\n",
      "Epoch [27/300], Step [103/225], Training Accuracy: 69.8877%, Training Loss: 0.6466%\n",
      "Epoch [27/300], Step [104/225], Training Accuracy: 69.8017%, Training Loss: 0.6478%\n",
      "Epoch [27/300], Step [105/225], Training Accuracy: 69.8661%, Training Loss: 0.6468%\n",
      "Epoch [27/300], Step [106/225], Training Accuracy: 69.8850%, Training Loss: 0.6473%\n",
      "Epoch [27/300], Step [107/225], Training Accuracy: 69.8598%, Training Loss: 0.6483%\n",
      "Epoch [27/300], Step [108/225], Training Accuracy: 69.7627%, Training Loss: 0.6485%\n",
      "Epoch [27/300], Step [109/225], Training Accuracy: 69.7391%, Training Loss: 0.6487%\n",
      "Epoch [27/300], Step [110/225], Training Accuracy: 69.7727%, Training Loss: 0.6482%\n",
      "Epoch [27/300], Step [111/225], Training Accuracy: 69.7776%, Training Loss: 0.6475%\n",
      "Epoch [27/300], Step [112/225], Training Accuracy: 69.8103%, Training Loss: 0.6474%\n",
      "Epoch [27/300], Step [113/225], Training Accuracy: 69.8424%, Training Loss: 0.6472%\n",
      "Epoch [27/300], Step [114/225], Training Accuracy: 69.8465%, Training Loss: 0.6469%\n",
      "Epoch [27/300], Step [115/225], Training Accuracy: 69.9185%, Training Loss: 0.6453%\n",
      "Epoch [27/300], Step [116/225], Training Accuracy: 69.9219%, Training Loss: 0.6447%\n",
      "Epoch [27/300], Step [117/225], Training Accuracy: 69.9519%, Training Loss: 0.6453%\n",
      "Epoch [27/300], Step [118/225], Training Accuracy: 69.8888%, Training Loss: 0.6458%\n",
      "Epoch [27/300], Step [119/225], Training Accuracy: 69.8529%, Training Loss: 0.6464%\n",
      "Epoch [27/300], Step [120/225], Training Accuracy: 69.8568%, Training Loss: 0.6461%\n",
      "Epoch [27/300], Step [121/225], Training Accuracy: 69.7960%, Training Loss: 0.6472%\n",
      "Epoch [27/300], Step [122/225], Training Accuracy: 69.7746%, Training Loss: 0.6474%\n",
      "Epoch [27/300], Step [123/225], Training Accuracy: 69.7409%, Training Loss: 0.6477%\n",
      "Epoch [27/300], Step [124/225], Training Accuracy: 69.7203%, Training Loss: 0.6477%\n",
      "Epoch [27/300], Step [125/225], Training Accuracy: 69.6625%, Training Loss: 0.6483%\n",
      "Epoch [27/300], Step [126/225], Training Accuracy: 69.7421%, Training Loss: 0.6478%\n",
      "Epoch [27/300], Step [127/225], Training Accuracy: 69.7835%, Training Loss: 0.6480%\n",
      "Epoch [27/300], Step [128/225], Training Accuracy: 69.7144%, Training Loss: 0.6489%\n",
      "Epoch [27/300], Step [129/225], Training Accuracy: 69.7069%, Training Loss: 0.6491%\n",
      "Epoch [27/300], Step [130/225], Training Accuracy: 69.7596%, Training Loss: 0.6489%\n",
      "Epoch [27/300], Step [131/225], Training Accuracy: 69.7758%, Training Loss: 0.6484%\n",
      "Epoch [27/300], Step [132/225], Training Accuracy: 69.7562%, Training Loss: 0.6487%\n",
      "Epoch [27/300], Step [133/225], Training Accuracy: 69.6898%, Training Loss: 0.6487%\n",
      "Epoch [27/300], Step [134/225], Training Accuracy: 69.6595%, Training Loss: 0.6496%\n",
      "Epoch [27/300], Step [135/225], Training Accuracy: 69.6991%, Training Loss: 0.6494%\n",
      "Epoch [27/300], Step [136/225], Training Accuracy: 69.7495%, Training Loss: 0.6485%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Step [137/225], Training Accuracy: 69.7879%, Training Loss: 0.6480%\n",
      "Epoch [27/300], Step [138/225], Training Accuracy: 69.8483%, Training Loss: 0.6468%\n",
      "Epoch [27/300], Step [139/225], Training Accuracy: 69.8629%, Training Loss: 0.6469%\n",
      "Epoch [27/300], Step [140/225], Training Accuracy: 69.9107%, Training Loss: 0.6462%\n",
      "Epoch [27/300], Step [141/225], Training Accuracy: 69.9025%, Training Loss: 0.6460%\n",
      "Epoch [27/300], Step [142/225], Training Accuracy: 69.8834%, Training Loss: 0.6458%\n",
      "Epoch [27/300], Step [143/225], Training Accuracy: 69.9301%, Training Loss: 0.6455%\n",
      "Epoch [27/300], Step [144/225], Training Accuracy: 69.9002%, Training Loss: 0.6462%\n",
      "Epoch [27/300], Step [145/225], Training Accuracy: 69.9246%, Training Loss: 0.6458%\n",
      "Epoch [27/300], Step [146/225], Training Accuracy: 69.9058%, Training Loss: 0.6459%\n",
      "Epoch [27/300], Step [147/225], Training Accuracy: 69.8554%, Training Loss: 0.6459%\n",
      "Epoch [27/300], Step [148/225], Training Accuracy: 69.9219%, Training Loss: 0.6454%\n",
      "Epoch [27/300], Step [149/225], Training Accuracy: 69.9140%, Training Loss: 0.6455%\n",
      "Epoch [27/300], Step [150/225], Training Accuracy: 69.9062%, Training Loss: 0.6455%\n",
      "Epoch [27/300], Step [151/225], Training Accuracy: 69.9710%, Training Loss: 0.6446%\n",
      "Epoch [27/300], Step [152/225], Training Accuracy: 69.9322%, Training Loss: 0.6452%\n",
      "Epoch [27/300], Step [153/225], Training Accuracy: 69.8938%, Training Loss: 0.6450%\n",
      "Epoch [27/300], Step [154/225], Training Accuracy: 69.9168%, Training Loss: 0.6448%\n",
      "Epoch [27/300], Step [155/225], Training Accuracy: 69.8790%, Training Loss: 0.6453%\n",
      "Epoch [27/300], Step [156/225], Training Accuracy: 69.8417%, Training Loss: 0.6457%\n",
      "Epoch [27/300], Step [157/225], Training Accuracy: 69.8746%, Training Loss: 0.6461%\n",
      "Epoch [27/300], Step [158/225], Training Accuracy: 69.7884%, Training Loss: 0.6465%\n",
      "Epoch [27/300], Step [159/225], Training Accuracy: 69.7622%, Training Loss: 0.6468%\n",
      "Epoch [27/300], Step [160/225], Training Accuracy: 69.8047%, Training Loss: 0.6462%\n",
      "Epoch [27/300], Step [161/225], Training Accuracy: 69.7787%, Training Loss: 0.6463%\n",
      "Epoch [27/300], Step [162/225], Training Accuracy: 69.8592%, Training Loss: 0.6452%\n",
      "Epoch [27/300], Step [163/225], Training Accuracy: 69.8236%, Training Loss: 0.6451%\n",
      "Epoch [27/300], Step [164/225], Training Accuracy: 69.8647%, Training Loss: 0.6439%\n",
      "Epoch [27/300], Step [165/225], Training Accuracy: 69.8674%, Training Loss: 0.6437%\n",
      "Epoch [27/300], Step [166/225], Training Accuracy: 69.9078%, Training Loss: 0.6435%\n",
      "Epoch [27/300], Step [167/225], Training Accuracy: 69.8915%, Training Loss: 0.6440%\n",
      "Epoch [27/300], Step [168/225], Training Accuracy: 69.9033%, Training Loss: 0.6441%\n",
      "Epoch [27/300], Step [169/225], Training Accuracy: 69.8595%, Training Loss: 0.6445%\n",
      "Epoch [27/300], Step [170/225], Training Accuracy: 69.8346%, Training Loss: 0.6447%\n",
      "Epoch [27/300], Step [171/225], Training Accuracy: 69.8556%, Training Loss: 0.6449%\n",
      "Epoch [27/300], Step [172/225], Training Accuracy: 69.8492%, Training Loss: 0.6447%\n",
      "Epoch [27/300], Step [173/225], Training Accuracy: 69.7796%, Training Loss: 0.6453%\n",
      "Epoch [27/300], Step [174/225], Training Accuracy: 69.7917%, Training Loss: 0.6455%\n",
      "Epoch [27/300], Step [175/225], Training Accuracy: 69.8214%, Training Loss: 0.6451%\n",
      "Epoch [27/300], Step [176/225], Training Accuracy: 69.8509%, Training Loss: 0.6445%\n",
      "Epoch [27/300], Step [177/225], Training Accuracy: 69.8711%, Training Loss: 0.6446%\n",
      "Epoch [27/300], Step [178/225], Training Accuracy: 69.8999%, Training Loss: 0.6440%\n",
      "Epoch [27/300], Step [179/225], Training Accuracy: 69.9197%, Training Loss: 0.6436%\n",
      "Epoch [27/300], Step [180/225], Training Accuracy: 69.9219%, Training Loss: 0.6438%\n",
      "Epoch [27/300], Step [181/225], Training Accuracy: 69.9154%, Training Loss: 0.6439%\n",
      "Epoch [27/300], Step [182/225], Training Accuracy: 69.8832%, Training Loss: 0.6439%\n",
      "Epoch [27/300], Step [183/225], Training Accuracy: 69.9197%, Training Loss: 0.6437%\n",
      "Epoch [27/300], Step [184/225], Training Accuracy: 69.9474%, Training Loss: 0.6432%\n",
      "Epoch [27/300], Step [185/225], Training Accuracy: 70.0253%, Training Loss: 0.6423%\n",
      "Epoch [27/300], Step [186/225], Training Accuracy: 70.0185%, Training Loss: 0.6417%\n",
      "Epoch [27/300], Step [187/225], Training Accuracy: 69.9783%, Training Loss: 0.6421%\n",
      "Epoch [27/300], Step [188/225], Training Accuracy: 70.0465%, Training Loss: 0.6412%\n",
      "Epoch [27/300], Step [189/225], Training Accuracy: 70.1058%, Training Loss: 0.6407%\n",
      "Epoch [27/300], Step [190/225], Training Accuracy: 70.1069%, Training Loss: 0.6408%\n",
      "Epoch [27/300], Step [191/225], Training Accuracy: 70.1080%, Training Loss: 0.6407%\n",
      "Epoch [27/300], Step [192/225], Training Accuracy: 70.1660%, Training Loss: 0.6399%\n",
      "Epoch [27/300], Step [193/225], Training Accuracy: 70.1344%, Training Loss: 0.6406%\n",
      "Epoch [27/300], Step [194/225], Training Accuracy: 70.1353%, Training Loss: 0.6405%\n",
      "Epoch [27/300], Step [195/225], Training Accuracy: 70.1683%, Training Loss: 0.6402%\n",
      "Epoch [27/300], Step [196/225], Training Accuracy: 70.1531%, Training Loss: 0.6410%\n",
      "Epoch [27/300], Step [197/225], Training Accuracy: 70.1777%, Training Loss: 0.6407%\n",
      "Epoch [27/300], Step [198/225], Training Accuracy: 70.2336%, Training Loss: 0.6398%\n",
      "Epoch [27/300], Step [199/225], Training Accuracy: 70.2418%, Training Loss: 0.6395%\n",
      "Epoch [27/300], Step [200/225], Training Accuracy: 70.2422%, Training Loss: 0.6392%\n",
      "Epoch [27/300], Step [201/225], Training Accuracy: 70.2425%, Training Loss: 0.6394%\n",
      "Epoch [27/300], Step [202/225], Training Accuracy: 70.2738%, Training Loss: 0.6391%\n",
      "Epoch [27/300], Step [203/225], Training Accuracy: 70.3202%, Training Loss: 0.6386%\n",
      "Epoch [27/300], Step [204/225], Training Accuracy: 70.3278%, Training Loss: 0.6388%\n",
      "Epoch [27/300], Step [205/225], Training Accuracy: 70.3582%, Training Loss: 0.6385%\n",
      "Epoch [27/300], Step [206/225], Training Accuracy: 70.3883%, Training Loss: 0.6386%\n",
      "Epoch [27/300], Step [207/225], Training Accuracy: 70.3351%, Training Loss: 0.6393%\n",
      "Epoch [27/300], Step [208/225], Training Accuracy: 70.3576%, Training Loss: 0.6388%\n",
      "Epoch [27/300], Step [209/225], Training Accuracy: 70.3648%, Training Loss: 0.6392%\n",
      "Epoch [27/300], Step [210/225], Training Accuracy: 70.3274%, Training Loss: 0.6391%\n",
      "Epoch [27/300], Step [211/225], Training Accuracy: 70.3866%, Training Loss: 0.6385%\n",
      "Epoch [27/300], Step [212/225], Training Accuracy: 70.3494%, Training Loss: 0.6393%\n",
      "Epoch [27/300], Step [213/225], Training Accuracy: 70.3198%, Training Loss: 0.6398%\n",
      "Epoch [27/300], Step [214/225], Training Accuracy: 70.3417%, Training Loss: 0.6397%\n",
      "Epoch [27/300], Step [215/225], Training Accuracy: 70.3198%, Training Loss: 0.6399%\n",
      "Epoch [27/300], Step [216/225], Training Accuracy: 70.2980%, Training Loss: 0.6401%\n",
      "Epoch [27/300], Step [217/225], Training Accuracy: 70.2981%, Training Loss: 0.6402%\n",
      "Epoch [27/300], Step [218/225], Training Accuracy: 70.2910%, Training Loss: 0.6403%\n",
      "Epoch [27/300], Step [219/225], Training Accuracy: 70.2911%, Training Loss: 0.6405%\n",
      "Epoch [27/300], Step [220/225], Training Accuracy: 70.2912%, Training Loss: 0.6405%\n",
      "Epoch [27/300], Step [221/225], Training Accuracy: 70.2347%, Training Loss: 0.6411%\n",
      "Epoch [27/300], Step [222/225], Training Accuracy: 70.2492%, Training Loss: 0.6407%\n",
      "Epoch [27/300], Step [223/225], Training Accuracy: 70.2074%, Training Loss: 0.6413%\n",
      "Epoch [27/300], Step [224/225], Training Accuracy: 70.2288%, Training Loss: 0.6408%\n",
      "Epoch [27/300], Step [225/225], Training Accuracy: 70.1918%, Training Loss: 0.6412%\n",
      "Epoch [28/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.4993%\n",
      "Epoch [28/300], Step [2/225], Training Accuracy: 72.6562%, Training Loss: 0.5912%\n",
      "Epoch [28/300], Step [3/225], Training Accuracy: 71.3542%, Training Loss: 0.5976%\n",
      "Epoch [28/300], Step [4/225], Training Accuracy: 71.8750%, Training Loss: 0.6016%\n",
      "Epoch [28/300], Step [5/225], Training Accuracy: 70.9375%, Training Loss: 0.6097%\n",
      "Epoch [28/300], Step [6/225], Training Accuracy: 71.3542%, Training Loss: 0.6257%\n",
      "Epoch [28/300], Step [7/225], Training Accuracy: 70.9821%, Training Loss: 0.6263%\n",
      "Epoch [28/300], Step [8/225], Training Accuracy: 70.1172%, Training Loss: 0.6257%\n",
      "Epoch [28/300], Step [9/225], Training Accuracy: 69.2708%, Training Loss: 0.6425%\n",
      "Epoch [28/300], Step [10/225], Training Accuracy: 68.7500%, Training Loss: 0.6531%\n",
      "Epoch [28/300], Step [11/225], Training Accuracy: 70.0284%, Training Loss: 0.6435%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Step [12/225], Training Accuracy: 70.3125%, Training Loss: 0.6461%\n",
      "Epoch [28/300], Step [13/225], Training Accuracy: 70.5529%, Training Loss: 0.6367%\n",
      "Epoch [28/300], Step [14/225], Training Accuracy: 71.0938%, Training Loss: 0.6331%\n",
      "Epoch [28/300], Step [15/225], Training Accuracy: 71.1458%, Training Loss: 0.6355%\n",
      "Epoch [28/300], Step [16/225], Training Accuracy: 71.0938%, Training Loss: 0.6380%\n",
      "Epoch [28/300], Step [17/225], Training Accuracy: 71.3235%, Training Loss: 0.6397%\n",
      "Epoch [28/300], Step [18/225], Training Accuracy: 71.4410%, Training Loss: 0.6381%\n",
      "Epoch [28/300], Step [19/225], Training Accuracy: 71.3816%, Training Loss: 0.6376%\n",
      "Epoch [28/300], Step [20/225], Training Accuracy: 71.4844%, Training Loss: 0.6344%\n",
      "Epoch [28/300], Step [21/225], Training Accuracy: 71.9494%, Training Loss: 0.6273%\n",
      "Epoch [28/300], Step [22/225], Training Accuracy: 71.6619%, Training Loss: 0.6317%\n",
      "Epoch [28/300], Step [23/225], Training Accuracy: 72.2826%, Training Loss: 0.6255%\n",
      "Epoch [28/300], Step [24/225], Training Accuracy: 72.2005%, Training Loss: 0.6257%\n",
      "Epoch [28/300], Step [25/225], Training Accuracy: 72.3750%, Training Loss: 0.6219%\n",
      "Epoch [28/300], Step [26/225], Training Accuracy: 71.8750%, Training Loss: 0.6228%\n",
      "Epoch [28/300], Step [27/225], Training Accuracy: 71.8750%, Training Loss: 0.6206%\n",
      "Epoch [28/300], Step [28/225], Training Accuracy: 72.3214%, Training Loss: 0.6140%\n",
      "Epoch [28/300], Step [29/225], Training Accuracy: 72.4138%, Training Loss: 0.6155%\n",
      "Epoch [28/300], Step [30/225], Training Accuracy: 72.6042%, Training Loss: 0.6125%\n",
      "Epoch [28/300], Step [31/225], Training Accuracy: 72.0766%, Training Loss: 0.6229%\n",
      "Epoch [28/300], Step [32/225], Training Accuracy: 71.8750%, Training Loss: 0.6226%\n",
      "Epoch [28/300], Step [33/225], Training Accuracy: 72.0170%, Training Loss: 0.6185%\n",
      "Epoch [28/300], Step [34/225], Training Accuracy: 72.1048%, Training Loss: 0.6189%\n",
      "Epoch [28/300], Step [35/225], Training Accuracy: 72.2321%, Training Loss: 0.6169%\n",
      "Epoch [28/300], Step [36/225], Training Accuracy: 72.3090%, Training Loss: 0.6147%\n",
      "Epoch [28/300], Step [37/225], Training Accuracy: 72.3395%, Training Loss: 0.6132%\n",
      "Epoch [28/300], Step [38/225], Training Accuracy: 72.3684%, Training Loss: 0.6133%\n",
      "Epoch [28/300], Step [39/225], Training Accuracy: 72.4359%, Training Loss: 0.6139%\n",
      "Epoch [28/300], Step [40/225], Training Accuracy: 72.5000%, Training Loss: 0.6126%\n",
      "Epoch [28/300], Step [41/225], Training Accuracy: 72.4466%, Training Loss: 0.6148%\n",
      "Epoch [28/300], Step [42/225], Training Accuracy: 72.3586%, Training Loss: 0.6159%\n",
      "Epoch [28/300], Step [43/225], Training Accuracy: 72.2384%, Training Loss: 0.6178%\n",
      "Epoch [28/300], Step [44/225], Training Accuracy: 72.4787%, Training Loss: 0.6157%\n",
      "Epoch [28/300], Step [45/225], Training Accuracy: 72.4653%, Training Loss: 0.6153%\n",
      "Epoch [28/300], Step [46/225], Training Accuracy: 72.3845%, Training Loss: 0.6138%\n",
      "Epoch [28/300], Step [47/225], Training Accuracy: 72.2407%, Training Loss: 0.6141%\n",
      "Epoch [28/300], Step [48/225], Training Accuracy: 72.2005%, Training Loss: 0.6151%\n",
      "Epoch [28/300], Step [49/225], Training Accuracy: 72.2895%, Training Loss: 0.6127%\n",
      "Epoch [28/300], Step [50/225], Training Accuracy: 72.2812%, Training Loss: 0.6128%\n",
      "Epoch [28/300], Step [51/225], Training Accuracy: 72.4265%, Training Loss: 0.6113%\n",
      "Epoch [28/300], Step [52/225], Training Accuracy: 72.5962%, Training Loss: 0.6099%\n",
      "Epoch [28/300], Step [53/225], Training Accuracy: 72.4646%, Training Loss: 0.6114%\n",
      "Epoch [28/300], Step [54/225], Training Accuracy: 72.2801%, Training Loss: 0.6146%\n",
      "Epoch [28/300], Step [55/225], Training Accuracy: 72.1023%, Training Loss: 0.6168%\n",
      "Epoch [28/300], Step [56/225], Training Accuracy: 72.0982%, Training Loss: 0.6174%\n",
      "Epoch [28/300], Step [57/225], Training Accuracy: 72.0395%, Training Loss: 0.6172%\n",
      "Epoch [28/300], Step [58/225], Training Accuracy: 71.8481%, Training Loss: 0.6187%\n",
      "Epoch [28/300], Step [59/225], Training Accuracy: 71.8485%, Training Loss: 0.6180%\n",
      "Epoch [28/300], Step [60/225], Training Accuracy: 71.9271%, Training Loss: 0.6167%\n",
      "Epoch [28/300], Step [61/225], Training Accuracy: 71.8750%, Training Loss: 0.6174%\n",
      "Epoch [28/300], Step [62/225], Training Accuracy: 71.7490%, Training Loss: 0.6195%\n",
      "Epoch [28/300], Step [63/225], Training Accuracy: 71.6270%, Training Loss: 0.6227%\n",
      "Epoch [28/300], Step [64/225], Training Accuracy: 71.7773%, Training Loss: 0.6202%\n",
      "Epoch [28/300], Step [65/225], Training Accuracy: 71.7788%, Training Loss: 0.6196%\n",
      "Epoch [28/300], Step [66/225], Training Accuracy: 71.8750%, Training Loss: 0.6182%\n",
      "Epoch [28/300], Step [67/225], Training Accuracy: 71.8750%, Training Loss: 0.6183%\n",
      "Epoch [28/300], Step [68/225], Training Accuracy: 71.7142%, Training Loss: 0.6196%\n",
      "Epoch [28/300], Step [69/225], Training Accuracy: 71.6033%, Training Loss: 0.6205%\n",
      "Epoch [28/300], Step [70/225], Training Accuracy: 71.4732%, Training Loss: 0.6212%\n",
      "Epoch [28/300], Step [71/225], Training Accuracy: 71.3908%, Training Loss: 0.6222%\n",
      "Epoch [28/300], Step [72/225], Training Accuracy: 71.3325%, Training Loss: 0.6222%\n",
      "Epoch [28/300], Step [73/225], Training Accuracy: 71.2543%, Training Loss: 0.6225%\n",
      "Epoch [28/300], Step [74/225], Training Accuracy: 71.3049%, Training Loss: 0.6222%\n",
      "Epoch [28/300], Step [75/225], Training Accuracy: 71.3750%, Training Loss: 0.6216%\n",
      "Epoch [28/300], Step [76/225], Training Accuracy: 71.2993%, Training Loss: 0.6235%\n",
      "Epoch [28/300], Step [77/225], Training Accuracy: 71.3271%, Training Loss: 0.6226%\n",
      "Epoch [28/300], Step [78/225], Training Accuracy: 71.2340%, Training Loss: 0.6233%\n",
      "Epoch [28/300], Step [79/225], Training Accuracy: 71.2816%, Training Loss: 0.6224%\n",
      "Epoch [28/300], Step [80/225], Training Accuracy: 71.2891%, Training Loss: 0.6235%\n",
      "Epoch [28/300], Step [81/225], Training Accuracy: 71.2384%, Training Loss: 0.6233%\n",
      "Epoch [28/300], Step [82/225], Training Accuracy: 71.3605%, Training Loss: 0.6215%\n",
      "Epoch [28/300], Step [83/225], Training Accuracy: 71.3291%, Training Loss: 0.6212%\n",
      "Epoch [28/300], Step [84/225], Training Accuracy: 71.3542%, Training Loss: 0.6206%\n",
      "Epoch [28/300], Step [85/225], Training Accuracy: 71.4154%, Training Loss: 0.6194%\n",
      "Epoch [28/300], Step [86/225], Training Accuracy: 71.4208%, Training Loss: 0.6193%\n",
      "Epoch [28/300], Step [87/225], Training Accuracy: 71.4260%, Training Loss: 0.6192%\n",
      "Epoch [28/300], Step [88/225], Training Accuracy: 71.4489%, Training Loss: 0.6191%\n",
      "Epoch [28/300], Step [89/225], Training Accuracy: 71.4712%, Training Loss: 0.6191%\n",
      "Epoch [28/300], Step [90/225], Training Accuracy: 71.4236%, Training Loss: 0.6205%\n",
      "Epoch [28/300], Step [91/225], Training Accuracy: 71.3942%, Training Loss: 0.6208%\n",
      "Epoch [28/300], Step [92/225], Training Accuracy: 71.3485%, Training Loss: 0.6212%\n",
      "Epoch [28/300], Step [93/225], Training Accuracy: 71.3878%, Training Loss: 0.6203%\n",
      "Epoch [28/300], Step [94/225], Training Accuracy: 71.3431%, Training Loss: 0.6200%\n",
      "Epoch [28/300], Step [95/225], Training Accuracy: 71.4145%, Training Loss: 0.6203%\n",
      "Epoch [28/300], Step [96/225], Training Accuracy: 71.3704%, Training Loss: 0.6198%\n",
      "Epoch [28/300], Step [97/225], Training Accuracy: 71.4562%, Training Loss: 0.6189%\n",
      "Epoch [28/300], Step [98/225], Training Accuracy: 71.3648%, Training Loss: 0.6205%\n",
      "Epoch [28/300], Step [99/225], Training Accuracy: 71.3542%, Training Loss: 0.6211%\n",
      "Epoch [28/300], Step [100/225], Training Accuracy: 71.2344%, Training Loss: 0.6229%\n",
      "Epoch [28/300], Step [101/225], Training Accuracy: 71.2562%, Training Loss: 0.6234%\n",
      "Epoch [28/300], Step [102/225], Training Accuracy: 71.1857%, Training Loss: 0.6242%\n",
      "Epoch [28/300], Step [103/225], Training Accuracy: 71.1924%, Training Loss: 0.6238%\n",
      "Epoch [28/300], Step [104/225], Training Accuracy: 71.1238%, Training Loss: 0.6252%\n",
      "Epoch [28/300], Step [105/225], Training Accuracy: 71.1905%, Training Loss: 0.6240%\n",
      "Epoch [28/300], Step [106/225], Training Accuracy: 71.1675%, Training Loss: 0.6247%\n",
      "Epoch [28/300], Step [107/225], Training Accuracy: 71.0426%, Training Loss: 0.6261%\n",
      "Epoch [28/300], Step [108/225], Training Accuracy: 71.0359%, Training Loss: 0.6265%\n",
      "Epoch [28/300], Step [109/225], Training Accuracy: 70.9576%, Training Loss: 0.6266%\n",
      "Epoch [28/300], Step [110/225], Training Accuracy: 70.8949%, Training Loss: 0.6268%\n",
      "Epoch [28/300], Step [111/225], Training Accuracy: 70.8896%, Training Loss: 0.6263%\n",
      "Epoch [28/300], Step [112/225], Training Accuracy: 70.8426%, Training Loss: 0.6270%\n",
      "Epoch [28/300], Step [113/225], Training Accuracy: 70.8518%, Training Loss: 0.6273%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Step [114/225], Training Accuracy: 70.8607%, Training Loss: 0.6272%\n",
      "Epoch [28/300], Step [115/225], Training Accuracy: 70.9103%, Training Loss: 0.6266%\n",
      "Epoch [28/300], Step [116/225], Training Accuracy: 70.9052%, Training Loss: 0.6258%\n",
      "Epoch [28/300], Step [117/225], Training Accuracy: 70.8333%, Training Loss: 0.6268%\n",
      "Epoch [28/300], Step [118/225], Training Accuracy: 70.7495%, Training Loss: 0.6277%\n",
      "Epoch [28/300], Step [119/225], Training Accuracy: 70.7195%, Training Loss: 0.6278%\n",
      "Epoch [28/300], Step [120/225], Training Accuracy: 70.7292%, Training Loss: 0.6281%\n",
      "Epoch [28/300], Step [121/225], Training Accuracy: 70.6224%, Training Loss: 0.6290%\n",
      "Epoch [28/300], Step [122/225], Training Accuracy: 70.6839%, Training Loss: 0.6292%\n",
      "Epoch [28/300], Step [123/225], Training Accuracy: 70.6301%, Training Loss: 0.6293%\n",
      "Epoch [28/300], Step [124/225], Training Accuracy: 70.6149%, Training Loss: 0.6293%\n",
      "Epoch [28/300], Step [125/225], Training Accuracy: 70.6000%, Training Loss: 0.6298%\n",
      "Epoch [28/300], Step [126/225], Training Accuracy: 70.5605%, Training Loss: 0.6298%\n",
      "Epoch [28/300], Step [127/225], Training Accuracy: 70.5709%, Training Loss: 0.6302%\n",
      "Epoch [28/300], Step [128/225], Training Accuracy: 70.5078%, Training Loss: 0.6317%\n",
      "Epoch [28/300], Step [129/225], Training Accuracy: 70.4821%, Training Loss: 0.6323%\n",
      "Epoch [28/300], Step [130/225], Training Accuracy: 70.4688%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [131/225], Training Accuracy: 70.5033%, Training Loss: 0.6322%\n",
      "Epoch [28/300], Step [132/225], Training Accuracy: 70.4664%, Training Loss: 0.6326%\n",
      "Epoch [28/300], Step [133/225], Training Accuracy: 70.4417%, Training Loss: 0.6323%\n",
      "Epoch [28/300], Step [134/225], Training Accuracy: 70.4291%, Training Loss: 0.6333%\n",
      "Epoch [28/300], Step [135/225], Training Accuracy: 70.4977%, Training Loss: 0.6330%\n",
      "Epoch [28/300], Step [136/225], Training Accuracy: 70.5308%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [137/225], Training Accuracy: 70.5064%, Training Loss: 0.6328%\n",
      "Epoch [28/300], Step [138/225], Training Accuracy: 70.6069%, Training Loss: 0.6316%\n",
      "Epoch [28/300], Step [139/225], Training Accuracy: 70.5710%, Training Loss: 0.6319%\n",
      "Epoch [28/300], Step [140/225], Training Accuracy: 70.5804%, Training Loss: 0.6318%\n",
      "Epoch [28/300], Step [141/225], Training Accuracy: 70.5230%, Training Loss: 0.6329%\n",
      "Epoch [28/300], Step [142/225], Training Accuracy: 70.5216%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [143/225], Training Accuracy: 70.4983%, Training Loss: 0.6329%\n",
      "Epoch [28/300], Step [144/225], Training Accuracy: 70.4644%, Training Loss: 0.6331%\n",
      "Epoch [28/300], Step [145/225], Training Accuracy: 70.4526%, Training Loss: 0.6330%\n",
      "Epoch [28/300], Step [146/225], Training Accuracy: 70.4302%, Training Loss: 0.6332%\n",
      "Epoch [28/300], Step [147/225], Training Accuracy: 70.3975%, Training Loss: 0.6335%\n",
      "Epoch [28/300], Step [148/225], Training Accuracy: 70.4497%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [149/225], Training Accuracy: 70.4593%, Training Loss: 0.6322%\n",
      "Epoch [28/300], Step [150/225], Training Accuracy: 70.4479%, Training Loss: 0.6320%\n",
      "Epoch [28/300], Step [151/225], Training Accuracy: 70.4781%, Training Loss: 0.6319%\n",
      "Epoch [28/300], Step [152/225], Training Accuracy: 70.4461%, Training Loss: 0.6328%\n",
      "Epoch [28/300], Step [153/225], Training Accuracy: 70.4657%, Training Loss: 0.6324%\n",
      "Epoch [28/300], Step [154/225], Training Accuracy: 70.5154%, Training Loss: 0.6323%\n",
      "Epoch [28/300], Step [155/225], Training Accuracy: 70.4335%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [156/225], Training Accuracy: 70.4427%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [157/225], Training Accuracy: 70.4220%, Training Loss: 0.6334%\n",
      "Epoch [28/300], Step [158/225], Training Accuracy: 70.3422%, Training Loss: 0.6345%\n",
      "Epoch [28/300], Step [159/225], Training Accuracy: 70.3322%, Training Loss: 0.6349%\n",
      "Epoch [28/300], Step [160/225], Training Accuracy: 70.3613%, Training Loss: 0.6345%\n",
      "Epoch [28/300], Step [161/225], Training Accuracy: 70.3319%, Training Loss: 0.6347%\n",
      "Epoch [28/300], Step [162/225], Training Accuracy: 70.4282%, Training Loss: 0.6333%\n",
      "Epoch [28/300], Step [163/225], Training Accuracy: 70.4467%, Training Loss: 0.6333%\n",
      "Epoch [28/300], Step [164/225], Training Accuracy: 70.4840%, Training Loss: 0.6324%\n",
      "Epoch [28/300], Step [165/225], Training Accuracy: 70.5114%, Training Loss: 0.6319%\n",
      "Epoch [28/300], Step [166/225], Training Accuracy: 70.5196%, Training Loss: 0.6317%\n",
      "Epoch [28/300], Step [167/225], Training Accuracy: 70.5090%, Training Loss: 0.6322%\n",
      "Epoch [28/300], Step [168/225], Training Accuracy: 70.4799%, Training Loss: 0.6327%\n",
      "Epoch [28/300], Step [169/225], Training Accuracy: 70.4327%, Training Loss: 0.6331%\n",
      "Epoch [28/300], Step [170/225], Training Accuracy: 70.4044%, Training Loss: 0.6333%\n",
      "Epoch [28/300], Step [171/225], Training Accuracy: 70.4130%, Training Loss: 0.6333%\n",
      "Epoch [28/300], Step [172/225], Training Accuracy: 70.4488%, Training Loss: 0.6331%\n",
      "Epoch [28/300], Step [173/225], Training Accuracy: 70.4570%, Training Loss: 0.6329%\n",
      "Epoch [28/300], Step [174/225], Training Accuracy: 70.4741%, Training Loss: 0.6329%\n",
      "Epoch [28/300], Step [175/225], Training Accuracy: 70.4911%, Training Loss: 0.6324%\n",
      "Epoch [28/300], Step [176/225], Training Accuracy: 70.5167%, Training Loss: 0.6322%\n",
      "Epoch [28/300], Step [177/225], Training Accuracy: 70.5420%, Training Loss: 0.6322%\n",
      "Epoch [28/300], Step [178/225], Training Accuracy: 70.5407%, Training Loss: 0.6322%\n",
      "Epoch [28/300], Step [179/225], Training Accuracy: 70.5918%, Training Loss: 0.6314%\n",
      "Epoch [28/300], Step [180/225], Training Accuracy: 70.5990%, Training Loss: 0.6311%\n",
      "Epoch [28/300], Step [181/225], Training Accuracy: 70.5801%, Training Loss: 0.6314%\n",
      "Epoch [28/300], Step [182/225], Training Accuracy: 70.5786%, Training Loss: 0.6320%\n",
      "Epoch [28/300], Step [183/225], Training Accuracy: 70.5686%, Training Loss: 0.6319%\n",
      "Epoch [28/300], Step [184/225], Training Accuracy: 70.6012%, Training Loss: 0.6314%\n",
      "Epoch [28/300], Step [185/225], Training Accuracy: 70.6250%, Training Loss: 0.6309%\n",
      "Epoch [28/300], Step [186/225], Training Accuracy: 70.6233%, Training Loss: 0.6305%\n",
      "Epoch [28/300], Step [187/225], Training Accuracy: 70.6217%, Training Loss: 0.6307%\n",
      "Epoch [28/300], Step [188/225], Training Accuracy: 70.6533%, Training Loss: 0.6303%\n",
      "Epoch [28/300], Step [189/225], Training Accuracy: 70.6928%, Training Loss: 0.6296%\n",
      "Epoch [28/300], Step [190/225], Training Accuracy: 70.6990%, Training Loss: 0.6299%\n",
      "Epoch [28/300], Step [191/225], Training Accuracy: 70.7052%, Training Loss: 0.6298%\n",
      "Epoch [28/300], Step [192/225], Training Accuracy: 70.7357%, Training Loss: 0.6292%\n",
      "Epoch [28/300], Step [193/225], Training Accuracy: 70.6849%, Training Loss: 0.6297%\n",
      "Epoch [28/300], Step [194/225], Training Accuracy: 70.6830%, Training Loss: 0.6298%\n",
      "Epoch [28/300], Step [195/225], Training Accuracy: 70.7131%, Training Loss: 0.6296%\n",
      "Epoch [28/300], Step [196/225], Training Accuracy: 70.6952%, Training Loss: 0.6300%\n",
      "Epoch [28/300], Step [197/225], Training Accuracy: 70.6694%, Training Loss: 0.6300%\n",
      "Epoch [28/300], Step [198/225], Training Accuracy: 70.7307%, Training Loss: 0.6292%\n",
      "Epoch [28/300], Step [199/225], Training Accuracy: 70.7208%, Training Loss: 0.6290%\n",
      "Epoch [28/300], Step [200/225], Training Accuracy: 70.6875%, Training Loss: 0.6291%\n",
      "Epoch [28/300], Step [201/225], Training Accuracy: 70.6934%, Training Loss: 0.6290%\n",
      "Epoch [28/300], Step [202/225], Training Accuracy: 70.7302%, Training Loss: 0.6286%\n",
      "Epoch [28/300], Step [203/225], Training Accuracy: 70.7589%, Training Loss: 0.6285%\n",
      "Epoch [28/300], Step [204/225], Training Accuracy: 70.7414%, Training Loss: 0.6284%\n",
      "Epoch [28/300], Step [205/225], Training Accuracy: 70.8308%, Training Loss: 0.6275%\n",
      "Epoch [28/300], Step [206/225], Training Accuracy: 70.8131%, Training Loss: 0.6281%\n",
      "Epoch [28/300], Step [207/225], Training Accuracy: 70.7880%, Training Loss: 0.6283%\n",
      "Epoch [28/300], Step [208/225], Training Accuracy: 70.8233%, Training Loss: 0.6277%\n",
      "Epoch [28/300], Step [209/225], Training Accuracy: 70.8358%, Training Loss: 0.6280%\n",
      "Epoch [28/300], Step [210/225], Training Accuracy: 70.8482%, Training Loss: 0.6280%\n",
      "Epoch [28/300], Step [211/225], Training Accuracy: 70.9049%, Training Loss: 0.6276%\n",
      "Epoch [28/300], Step [212/225], Training Accuracy: 70.8726%, Training Loss: 0.6278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Step [213/225], Training Accuracy: 70.8700%, Training Loss: 0.6286%\n",
      "Epoch [28/300], Step [214/225], Training Accuracy: 70.9039%, Training Loss: 0.6282%\n",
      "Epoch [28/300], Step [215/225], Training Accuracy: 70.9084%, Training Loss: 0.6281%\n",
      "Epoch [28/300], Step [216/225], Training Accuracy: 70.9129%, Training Loss: 0.6281%\n",
      "Epoch [28/300], Step [217/225], Training Accuracy: 70.9389%, Training Loss: 0.6282%\n",
      "Epoch [28/300], Step [218/225], Training Accuracy: 70.9146%, Training Loss: 0.6284%\n",
      "Epoch [28/300], Step [219/225], Training Accuracy: 70.9261%, Training Loss: 0.6286%\n",
      "Epoch [28/300], Step [220/225], Training Accuracy: 70.9020%, Training Loss: 0.6291%\n",
      "Epoch [28/300], Step [221/225], Training Accuracy: 70.8428%, Training Loss: 0.6296%\n",
      "Epoch [28/300], Step [222/225], Training Accuracy: 70.8615%, Training Loss: 0.6293%\n",
      "Epoch [28/300], Step [223/225], Training Accuracy: 70.8240%, Training Loss: 0.6296%\n",
      "Epoch [28/300], Step [224/225], Training Accuracy: 70.8287%, Training Loss: 0.6295%\n",
      "Epoch [28/300], Step [225/225], Training Accuracy: 70.7824%, Training Loss: 0.6300%\n",
      "Epoch [29/300], Step [1/225], Training Accuracy: 71.8750%, Training Loss: 0.5274%\n",
      "Epoch [29/300], Step [2/225], Training Accuracy: 72.6562%, Training Loss: 0.5627%\n",
      "Epoch [29/300], Step [3/225], Training Accuracy: 70.3125%, Training Loss: 0.6025%\n",
      "Epoch [29/300], Step [4/225], Training Accuracy: 71.0938%, Training Loss: 0.6039%\n",
      "Epoch [29/300], Step [5/225], Training Accuracy: 72.5000%, Training Loss: 0.5926%\n",
      "Epoch [29/300], Step [6/225], Training Accuracy: 72.1354%, Training Loss: 0.6031%\n",
      "Epoch [29/300], Step [7/225], Training Accuracy: 72.3214%, Training Loss: 0.6071%\n",
      "Epoch [29/300], Step [8/225], Training Accuracy: 71.2891%, Training Loss: 0.6115%\n",
      "Epoch [29/300], Step [9/225], Training Accuracy: 70.4861%, Training Loss: 0.6240%\n",
      "Epoch [29/300], Step [10/225], Training Accuracy: 70.1562%, Training Loss: 0.6327%\n",
      "Epoch [29/300], Step [11/225], Training Accuracy: 70.7386%, Training Loss: 0.6291%\n",
      "Epoch [29/300], Step [12/225], Training Accuracy: 70.7031%, Training Loss: 0.6283%\n",
      "Epoch [29/300], Step [13/225], Training Accuracy: 70.9135%, Training Loss: 0.6251%\n",
      "Epoch [29/300], Step [14/225], Training Accuracy: 71.2054%, Training Loss: 0.6182%\n",
      "Epoch [29/300], Step [15/225], Training Accuracy: 71.2500%, Training Loss: 0.6180%\n",
      "Epoch [29/300], Step [16/225], Training Accuracy: 71.3867%, Training Loss: 0.6139%\n",
      "Epoch [29/300], Step [17/225], Training Accuracy: 71.8750%, Training Loss: 0.6065%\n",
      "Epoch [29/300], Step [18/225], Training Accuracy: 71.6146%, Training Loss: 0.6086%\n",
      "Epoch [29/300], Step [19/225], Training Accuracy: 71.7105%, Training Loss: 0.6060%\n",
      "Epoch [29/300], Step [20/225], Training Accuracy: 71.7188%, Training Loss: 0.6052%\n",
      "Epoch [29/300], Step [21/225], Training Accuracy: 71.9494%, Training Loss: 0.5999%\n",
      "Epoch [29/300], Step [22/225], Training Accuracy: 71.5199%, Training Loss: 0.6085%\n",
      "Epoch [29/300], Step [23/225], Training Accuracy: 71.6712%, Training Loss: 0.6036%\n",
      "Epoch [29/300], Step [24/225], Training Accuracy: 71.6146%, Training Loss: 0.6060%\n",
      "Epoch [29/300], Step [25/225], Training Accuracy: 71.7500%, Training Loss: 0.6070%\n",
      "Epoch [29/300], Step [26/225], Training Accuracy: 71.4543%, Training Loss: 0.6084%\n",
      "Epoch [29/300], Step [27/225], Training Accuracy: 71.6435%, Training Loss: 0.6078%\n",
      "Epoch [29/300], Step [28/225], Training Accuracy: 72.0424%, Training Loss: 0.6032%\n",
      "Epoch [29/300], Step [29/225], Training Accuracy: 71.9289%, Training Loss: 0.6015%\n",
      "Epoch [29/300], Step [30/225], Training Accuracy: 71.9792%, Training Loss: 0.5992%\n",
      "Epoch [29/300], Step [31/225], Training Accuracy: 71.3710%, Training Loss: 0.6094%\n",
      "Epoch [29/300], Step [32/225], Training Accuracy: 70.9473%, Training Loss: 0.6105%\n",
      "Epoch [29/300], Step [33/225], Training Accuracy: 71.0701%, Training Loss: 0.6082%\n",
      "Epoch [29/300], Step [34/225], Training Accuracy: 71.0018%, Training Loss: 0.6083%\n",
      "Epoch [29/300], Step [35/225], Training Accuracy: 71.1161%, Training Loss: 0.6072%\n",
      "Epoch [29/300], Step [36/225], Training Accuracy: 71.1372%, Training Loss: 0.6068%\n",
      "Epoch [29/300], Step [37/225], Training Accuracy: 71.2416%, Training Loss: 0.6066%\n",
      "Epoch [29/300], Step [38/225], Training Accuracy: 71.1760%, Training Loss: 0.6085%\n",
      "Epoch [29/300], Step [39/225], Training Accuracy: 71.1939%, Training Loss: 0.6096%\n",
      "Epoch [29/300], Step [40/225], Training Accuracy: 71.1719%, Training Loss: 0.6104%\n",
      "Epoch [29/300], Step [41/225], Training Accuracy: 71.2271%, Training Loss: 0.6103%\n",
      "Epoch [29/300], Step [42/225], Training Accuracy: 71.1682%, Training Loss: 0.6101%\n",
      "Epoch [29/300], Step [43/225], Training Accuracy: 70.9666%, Training Loss: 0.6118%\n",
      "Epoch [29/300], Step [44/225], Training Accuracy: 71.2003%, Training Loss: 0.6090%\n",
      "Epoch [29/300], Step [45/225], Training Accuracy: 71.1806%, Training Loss: 0.6086%\n",
      "Epoch [29/300], Step [46/225], Training Accuracy: 71.1957%, Training Loss: 0.6068%\n",
      "Epoch [29/300], Step [47/225], Training Accuracy: 71.0771%, Training Loss: 0.6073%\n",
      "Epoch [29/300], Step [48/225], Training Accuracy: 71.0938%, Training Loss: 0.6073%\n",
      "Epoch [29/300], Step [49/225], Training Accuracy: 71.1097%, Training Loss: 0.6056%\n",
      "Epoch [29/300], Step [50/225], Training Accuracy: 71.0312%, Training Loss: 0.6051%\n",
      "Epoch [29/300], Step [51/225], Training Accuracy: 71.1091%, Training Loss: 0.6049%\n",
      "Epoch [29/300], Step [52/225], Training Accuracy: 71.0938%, Training Loss: 0.6047%\n",
      "Epoch [29/300], Step [53/225], Training Accuracy: 71.1380%, Training Loss: 0.6057%\n",
      "Epoch [29/300], Step [54/225], Training Accuracy: 71.0069%, Training Loss: 0.6068%\n",
      "Epoch [29/300], Step [55/225], Training Accuracy: 70.7670%, Training Loss: 0.6105%\n",
      "Epoch [29/300], Step [56/225], Training Accuracy: 70.7031%, Training Loss: 0.6109%\n",
      "Epoch [29/300], Step [57/225], Training Accuracy: 70.6963%, Training Loss: 0.6106%\n",
      "Epoch [29/300], Step [58/225], Training Accuracy: 70.6358%, Training Loss: 0.6111%\n",
      "Epoch [29/300], Step [59/225], Training Accuracy: 70.6833%, Training Loss: 0.6114%\n",
      "Epoch [29/300], Step [60/225], Training Accuracy: 70.7552%, Training Loss: 0.6098%\n",
      "Epoch [29/300], Step [61/225], Training Accuracy: 70.6455%, Training Loss: 0.6108%\n",
      "Epoch [29/300], Step [62/225], Training Accuracy: 70.5645%, Training Loss: 0.6123%\n",
      "Epoch [29/300], Step [63/225], Training Accuracy: 70.4613%, Training Loss: 0.6155%\n",
      "Epoch [29/300], Step [64/225], Training Accuracy: 70.5566%, Training Loss: 0.6144%\n",
      "Epoch [29/300], Step [65/225], Training Accuracy: 70.6250%, Training Loss: 0.6137%\n",
      "Epoch [29/300], Step [66/225], Training Accuracy: 70.7150%, Training Loss: 0.6129%\n",
      "Epoch [29/300], Step [67/225], Training Accuracy: 70.7789%, Training Loss: 0.6129%\n",
      "Epoch [29/300], Step [68/225], Training Accuracy: 70.6801%, Training Loss: 0.6134%\n",
      "Epoch [29/300], Step [69/225], Training Accuracy: 70.6069%, Training Loss: 0.6133%\n",
      "Epoch [29/300], Step [70/225], Training Accuracy: 70.5357%, Training Loss: 0.6147%\n",
      "Epoch [29/300], Step [71/225], Training Accuracy: 70.5326%, Training Loss: 0.6135%\n",
      "Epoch [29/300], Step [72/225], Training Accuracy: 70.5078%, Training Loss: 0.6129%\n",
      "Epoch [29/300], Step [73/225], Training Accuracy: 70.5265%, Training Loss: 0.6130%\n",
      "Epoch [29/300], Step [74/225], Training Accuracy: 70.5659%, Training Loss: 0.6126%\n",
      "Epoch [29/300], Step [75/225], Training Accuracy: 70.6458%, Training Loss: 0.6117%\n",
      "Epoch [29/300], Step [76/225], Training Accuracy: 70.5592%, Training Loss: 0.6135%\n",
      "Epoch [29/300], Step [77/225], Training Accuracy: 70.6778%, Training Loss: 0.6126%\n",
      "Epoch [29/300], Step [78/225], Training Accuracy: 70.6731%, Training Loss: 0.6124%\n",
      "Epoch [29/300], Step [79/225], Training Accuracy: 70.7081%, Training Loss: 0.6117%\n",
      "Epoch [29/300], Step [80/225], Training Accuracy: 70.7812%, Training Loss: 0.6115%\n",
      "Epoch [29/300], Step [81/225], Training Accuracy: 70.8526%, Training Loss: 0.6107%\n",
      "Epoch [29/300], Step [82/225], Training Accuracy: 70.9985%, Training Loss: 0.6094%\n",
      "Epoch [29/300], Step [83/225], Training Accuracy: 71.0843%, Training Loss: 0.6085%\n",
      "Epoch [29/300], Step [84/225], Training Accuracy: 71.0193%, Training Loss: 0.6080%\n",
      "Epoch [29/300], Step [85/225], Training Accuracy: 71.0294%, Training Loss: 0.6072%\n",
      "Epoch [29/300], Step [86/225], Training Accuracy: 70.9847%, Training Loss: 0.6077%\n",
      "Epoch [29/300], Step [87/225], Training Accuracy: 70.8333%, Training Loss: 0.6102%\n",
      "Epoch [29/300], Step [88/225], Training Accuracy: 70.7741%, Training Loss: 0.6107%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [89/225], Training Accuracy: 70.8216%, Training Loss: 0.6120%\n",
      "Epoch [29/300], Step [90/225], Training Accuracy: 70.8160%, Training Loss: 0.6124%\n",
      "Epoch [29/300], Step [91/225], Training Accuracy: 70.7589%, Training Loss: 0.6126%\n",
      "Epoch [29/300], Step [92/225], Training Accuracy: 70.7371%, Training Loss: 0.6126%\n",
      "Epoch [29/300], Step [93/225], Training Accuracy: 70.8165%, Training Loss: 0.6117%\n",
      "Epoch [29/300], Step [94/225], Training Accuracy: 70.8943%, Training Loss: 0.6113%\n",
      "Epoch [29/300], Step [95/225], Training Accuracy: 70.9211%, Training Loss: 0.6117%\n",
      "Epoch [29/300], Step [96/225], Training Accuracy: 70.9961%, Training Loss: 0.6103%\n",
      "Epoch [29/300], Step [97/225], Training Accuracy: 70.9890%, Training Loss: 0.6102%\n",
      "Epoch [29/300], Step [98/225], Training Accuracy: 70.9184%, Training Loss: 0.6113%\n",
      "Epoch [29/300], Step [99/225], Training Accuracy: 70.8807%, Training Loss: 0.6120%\n",
      "Epoch [29/300], Step [100/225], Training Accuracy: 70.8750%, Training Loss: 0.6137%\n",
      "Epoch [29/300], Step [101/225], Training Accuracy: 70.9623%, Training Loss: 0.6130%\n",
      "Epoch [29/300], Step [102/225], Training Accuracy: 70.8487%, Training Loss: 0.6141%\n",
      "Epoch [29/300], Step [103/225], Training Accuracy: 70.8890%, Training Loss: 0.6137%\n",
      "Epoch [29/300], Step [104/225], Training Accuracy: 70.7933%, Training Loss: 0.6152%\n",
      "Epoch [29/300], Step [105/225], Training Accuracy: 70.8036%, Training Loss: 0.6144%\n",
      "Epoch [29/300], Step [106/225], Training Accuracy: 70.8284%, Training Loss: 0.6150%\n",
      "Epoch [29/300], Step [107/225], Training Accuracy: 70.8236%, Training Loss: 0.6155%\n",
      "Epoch [29/300], Step [108/225], Training Accuracy: 70.7755%, Training Loss: 0.6157%\n",
      "Epoch [29/300], Step [109/225], Training Accuracy: 70.7856%, Training Loss: 0.6157%\n",
      "Epoch [29/300], Step [110/225], Training Accuracy: 70.8097%, Training Loss: 0.6151%\n",
      "Epoch [29/300], Step [111/225], Training Accuracy: 70.7630%, Training Loss: 0.6146%\n",
      "Epoch [29/300], Step [112/225], Training Accuracy: 70.7868%, Training Loss: 0.6150%\n",
      "Epoch [29/300], Step [113/225], Training Accuracy: 70.7965%, Training Loss: 0.6157%\n",
      "Epoch [29/300], Step [114/225], Training Accuracy: 70.8059%, Training Loss: 0.6156%\n",
      "Epoch [29/300], Step [115/225], Training Accuracy: 70.7880%, Training Loss: 0.6163%\n",
      "Epoch [29/300], Step [116/225], Training Accuracy: 70.7974%, Training Loss: 0.6163%\n",
      "Epoch [29/300], Step [117/225], Training Accuracy: 70.7131%, Training Loss: 0.6175%\n",
      "Epoch [29/300], Step [118/225], Training Accuracy: 70.6700%, Training Loss: 0.6182%\n",
      "Epoch [29/300], Step [119/225], Training Accuracy: 70.6539%, Training Loss: 0.6181%\n",
      "Epoch [29/300], Step [120/225], Training Accuracy: 70.5859%, Training Loss: 0.6188%\n",
      "Epoch [29/300], Step [121/225], Training Accuracy: 70.5320%, Training Loss: 0.6197%\n",
      "Epoch [29/300], Step [122/225], Training Accuracy: 70.5686%, Training Loss: 0.6196%\n",
      "Epoch [29/300], Step [123/225], Training Accuracy: 70.5539%, Training Loss: 0.6197%\n",
      "Epoch [29/300], Step [124/225], Training Accuracy: 70.5645%, Training Loss: 0.6196%\n",
      "Epoch [29/300], Step [125/225], Training Accuracy: 70.5500%, Training Loss: 0.6195%\n",
      "Epoch [29/300], Step [126/225], Training Accuracy: 70.5481%, Training Loss: 0.6193%\n",
      "Epoch [29/300], Step [127/225], Training Accuracy: 70.5094%, Training Loss: 0.6199%\n",
      "Epoch [29/300], Step [128/225], Training Accuracy: 70.5078%, Training Loss: 0.6209%\n",
      "Epoch [29/300], Step [129/225], Training Accuracy: 70.4821%, Training Loss: 0.6214%\n",
      "Epoch [29/300], Step [130/225], Training Accuracy: 70.4928%, Training Loss: 0.6214%\n",
      "Epoch [29/300], Step [131/225], Training Accuracy: 70.5630%, Training Loss: 0.6209%\n",
      "Epoch [29/300], Step [132/225], Training Accuracy: 70.5848%, Training Loss: 0.6210%\n",
      "Epoch [29/300], Step [133/225], Training Accuracy: 70.6180%, Training Loss: 0.6211%\n",
      "Epoch [29/300], Step [134/225], Training Accuracy: 70.5340%, Training Loss: 0.6218%\n",
      "Epoch [29/300], Step [135/225], Training Accuracy: 70.5208%, Training Loss: 0.6222%\n",
      "Epoch [29/300], Step [136/225], Training Accuracy: 70.5193%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [137/225], Training Accuracy: 70.5178%, Training Loss: 0.6221%\n",
      "Epoch [29/300], Step [138/225], Training Accuracy: 70.5729%, Training Loss: 0.6214%\n",
      "Epoch [29/300], Step [139/225], Training Accuracy: 70.5710%, Training Loss: 0.6222%\n",
      "Epoch [29/300], Step [140/225], Training Accuracy: 70.5915%, Training Loss: 0.6222%\n",
      "Epoch [29/300], Step [141/225], Training Accuracy: 70.5895%, Training Loss: 0.6223%\n",
      "Epoch [29/300], Step [142/225], Training Accuracy: 70.6096%, Training Loss: 0.6217%\n",
      "Epoch [29/300], Step [143/225], Training Accuracy: 70.5857%, Training Loss: 0.6218%\n",
      "Epoch [29/300], Step [144/225], Training Accuracy: 70.5838%, Training Loss: 0.6221%\n",
      "Epoch [29/300], Step [145/225], Training Accuracy: 70.5927%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [146/225], Training Accuracy: 70.5801%, Training Loss: 0.6217%\n",
      "Epoch [29/300], Step [147/225], Training Accuracy: 70.5357%, Training Loss: 0.6222%\n",
      "Epoch [29/300], Step [148/225], Training Accuracy: 70.5870%, Training Loss: 0.6214%\n",
      "Epoch [29/300], Step [149/225], Training Accuracy: 70.5852%, Training Loss: 0.6215%\n",
      "Epoch [29/300], Step [150/225], Training Accuracy: 70.5312%, Training Loss: 0.6219%\n",
      "Epoch [29/300], Step [151/225], Training Accuracy: 70.5815%, Training Loss: 0.6213%\n",
      "Epoch [29/300], Step [152/225], Training Accuracy: 70.5592%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [153/225], Training Accuracy: 70.5372%, Training Loss: 0.6227%\n",
      "Epoch [29/300], Step [154/225], Training Accuracy: 70.5763%, Training Loss: 0.6222%\n",
      "Epoch [29/300], Step [155/225], Training Accuracy: 70.5645%, Training Loss: 0.6223%\n",
      "Epoch [29/300], Step [156/225], Training Accuracy: 70.5729%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [157/225], Training Accuracy: 70.6011%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [158/225], Training Accuracy: 70.4806%, Training Loss: 0.6232%\n",
      "Epoch [29/300], Step [159/225], Training Accuracy: 70.4108%, Training Loss: 0.6242%\n",
      "Epoch [29/300], Step [160/225], Training Accuracy: 70.4395%, Training Loss: 0.6237%\n",
      "Epoch [29/300], Step [161/225], Training Accuracy: 70.4484%, Training Loss: 0.6234%\n",
      "Epoch [29/300], Step [162/225], Training Accuracy: 70.4668%, Training Loss: 0.6227%\n",
      "Epoch [29/300], Step [163/225], Training Accuracy: 70.4371%, Training Loss: 0.6227%\n",
      "Epoch [29/300], Step [164/225], Training Accuracy: 70.5030%, Training Loss: 0.6214%\n",
      "Epoch [29/300], Step [165/225], Training Accuracy: 70.5114%, Training Loss: 0.6209%\n",
      "Epoch [29/300], Step [166/225], Training Accuracy: 70.5008%, Training Loss: 0.6208%\n",
      "Epoch [29/300], Step [167/225], Training Accuracy: 70.4809%, Training Loss: 0.6215%\n",
      "Epoch [29/300], Step [168/225], Training Accuracy: 70.4706%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [169/225], Training Accuracy: 70.4234%, Training Loss: 0.6225%\n",
      "Epoch [29/300], Step [170/225], Training Accuracy: 70.3952%, Training Loss: 0.6228%\n",
      "Epoch [29/300], Step [171/225], Training Accuracy: 70.4130%, Training Loss: 0.6228%\n",
      "Epoch [29/300], Step [172/225], Training Accuracy: 70.4124%, Training Loss: 0.6225%\n",
      "Epoch [29/300], Step [173/225], Training Accuracy: 70.3486%, Training Loss: 0.6231%\n",
      "Epoch [29/300], Step [174/225], Training Accuracy: 70.3843%, Training Loss: 0.6225%\n",
      "Epoch [29/300], Step [175/225], Training Accuracy: 70.3661%, Training Loss: 0.6220%\n",
      "Epoch [29/300], Step [176/225], Training Accuracy: 70.3746%, Training Loss: 0.6219%\n",
      "Epoch [29/300], Step [177/225], Training Accuracy: 70.3919%, Training Loss: 0.6221%\n",
      "Epoch [29/300], Step [178/225], Training Accuracy: 70.4091%, Training Loss: 0.6218%\n",
      "Epoch [29/300], Step [179/225], Training Accuracy: 70.4347%, Training Loss: 0.6213%\n",
      "Epoch [29/300], Step [180/225], Training Accuracy: 70.4253%, Training Loss: 0.6212%\n",
      "Epoch [29/300], Step [181/225], Training Accuracy: 70.4334%, Training Loss: 0.6213%\n",
      "Epoch [29/300], Step [182/225], Training Accuracy: 70.3984%, Training Loss: 0.6214%\n",
      "Epoch [29/300], Step [183/225], Training Accuracy: 70.3979%, Training Loss: 0.6215%\n",
      "Epoch [29/300], Step [184/225], Training Accuracy: 70.4484%, Training Loss: 0.6213%\n",
      "Epoch [29/300], Step [185/225], Training Accuracy: 70.4561%, Training Loss: 0.6210%\n",
      "Epoch [29/300], Step [186/225], Training Accuracy: 70.4637%, Training Loss: 0.6205%\n",
      "Epoch [29/300], Step [187/225], Training Accuracy: 70.4295%, Training Loss: 0.6205%\n",
      "Epoch [29/300], Step [188/225], Training Accuracy: 70.4621%, Training Loss: 0.6200%\n",
      "Epoch [29/300], Step [189/225], Training Accuracy: 70.4696%, Training Loss: 0.6194%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [190/225], Training Accuracy: 70.4688%, Training Loss: 0.6195%\n",
      "Epoch [29/300], Step [191/225], Training Accuracy: 70.4598%, Training Loss: 0.6197%\n",
      "Epoch [29/300], Step [192/225], Training Accuracy: 70.4997%, Training Loss: 0.6194%\n",
      "Epoch [29/300], Step [193/225], Training Accuracy: 70.4582%, Training Loss: 0.6199%\n",
      "Epoch [29/300], Step [194/225], Training Accuracy: 70.4736%, Training Loss: 0.6197%\n",
      "Epoch [29/300], Step [195/225], Training Accuracy: 70.4968%, Training Loss: 0.6196%\n",
      "Epoch [29/300], Step [196/225], Training Accuracy: 70.4959%, Training Loss: 0.6197%\n",
      "Epoch [29/300], Step [197/225], Training Accuracy: 70.4711%, Training Loss: 0.6202%\n",
      "Epoch [29/300], Step [198/225], Training Accuracy: 70.5177%, Training Loss: 0.6193%\n",
      "Epoch [29/300], Step [199/225], Training Accuracy: 70.5716%, Training Loss: 0.6189%\n",
      "Epoch [29/300], Step [200/225], Training Accuracy: 70.5547%, Training Loss: 0.6190%\n",
      "Epoch [29/300], Step [201/225], Training Accuracy: 70.5846%, Training Loss: 0.6188%\n",
      "Epoch [29/300], Step [202/225], Training Accuracy: 70.6296%, Training Loss: 0.6182%\n",
      "Epoch [29/300], Step [203/225], Training Accuracy: 70.6435%, Training Loss: 0.6184%\n",
      "Epoch [29/300], Step [204/225], Training Accuracy: 70.6495%, Training Loss: 0.6186%\n",
      "Epoch [29/300], Step [205/225], Training Accuracy: 70.7165%, Training Loss: 0.6179%\n",
      "Epoch [29/300], Step [206/225], Training Accuracy: 70.7297%, Training Loss: 0.6183%\n",
      "Epoch [29/300], Step [207/225], Training Accuracy: 70.7201%, Training Loss: 0.6186%\n",
      "Epoch [29/300], Step [208/225], Training Accuracy: 70.7707%, Training Loss: 0.6179%\n",
      "Epoch [29/300], Step [209/225], Training Accuracy: 70.7611%, Training Loss: 0.6180%\n",
      "Epoch [29/300], Step [210/225], Training Accuracy: 70.7515%, Training Loss: 0.6183%\n",
      "Epoch [29/300], Step [211/225], Training Accuracy: 70.7790%, Training Loss: 0.6177%\n",
      "Epoch [29/300], Step [212/225], Training Accuracy: 70.7547%, Training Loss: 0.6180%\n",
      "Epoch [29/300], Step [213/225], Training Accuracy: 70.7600%, Training Loss: 0.6182%\n",
      "Epoch [29/300], Step [214/225], Training Accuracy: 70.7652%, Training Loss: 0.6183%\n",
      "Epoch [29/300], Step [215/225], Training Accuracy: 70.7631%, Training Loss: 0.6184%\n",
      "Epoch [29/300], Step [216/225], Training Accuracy: 70.7465%, Training Loss: 0.6187%\n",
      "Epoch [29/300], Step [217/225], Training Accuracy: 70.7517%, Training Loss: 0.6188%\n",
      "Epoch [29/300], Step [218/225], Training Accuracy: 70.7210%, Training Loss: 0.6189%\n",
      "Epoch [29/300], Step [219/225], Training Accuracy: 70.7763%, Training Loss: 0.6187%\n",
      "Epoch [29/300], Step [220/225], Training Accuracy: 70.7599%, Training Loss: 0.6189%\n",
      "Epoch [29/300], Step [221/225], Training Accuracy: 70.7296%, Training Loss: 0.6191%\n",
      "Epoch [29/300], Step [222/225], Training Accuracy: 70.7066%, Training Loss: 0.6189%\n",
      "Epoch [29/300], Step [223/225], Training Accuracy: 70.6418%, Training Loss: 0.6196%\n",
      "Epoch [29/300], Step [224/225], Training Accuracy: 70.6334%, Training Loss: 0.6198%\n",
      "Epoch [29/300], Step [225/225], Training Accuracy: 70.6017%, Training Loss: 0.6200%\n",
      "Epoch [30/300], Step [1/225], Training Accuracy: 71.8750%, Training Loss: 0.5249%\n",
      "Epoch [30/300], Step [2/225], Training Accuracy: 68.7500%, Training Loss: 0.6030%\n",
      "Epoch [30/300], Step [3/225], Training Accuracy: 71.3542%, Training Loss: 0.6049%\n",
      "Epoch [30/300], Step [4/225], Training Accuracy: 71.8750%, Training Loss: 0.6109%\n",
      "Epoch [30/300], Step [5/225], Training Accuracy: 71.8750%, Training Loss: 0.6056%\n",
      "Epoch [30/300], Step [6/225], Training Accuracy: 71.8750%, Training Loss: 0.6198%\n",
      "Epoch [30/300], Step [7/225], Training Accuracy: 72.0982%, Training Loss: 0.6145%\n",
      "Epoch [30/300], Step [8/225], Training Accuracy: 71.6797%, Training Loss: 0.6255%\n",
      "Epoch [30/300], Step [9/225], Training Accuracy: 70.8333%, Training Loss: 0.6393%\n",
      "Epoch [30/300], Step [10/225], Training Accuracy: 70.7812%, Training Loss: 0.6465%\n",
      "Epoch [30/300], Step [11/225], Training Accuracy: 71.4489%, Training Loss: 0.6407%\n",
      "Epoch [30/300], Step [12/225], Training Accuracy: 70.8333%, Training Loss: 0.6377%\n",
      "Epoch [30/300], Step [13/225], Training Accuracy: 71.6346%, Training Loss: 0.6233%\n",
      "Epoch [30/300], Step [14/225], Training Accuracy: 71.7634%, Training Loss: 0.6208%\n",
      "Epoch [30/300], Step [15/225], Training Accuracy: 71.4583%, Training Loss: 0.6227%\n",
      "Epoch [30/300], Step [16/225], Training Accuracy: 71.1914%, Training Loss: 0.6274%\n",
      "Epoch [30/300], Step [17/225], Training Accuracy: 70.9559%, Training Loss: 0.6277%\n",
      "Epoch [30/300], Step [18/225], Training Accuracy: 70.6597%, Training Loss: 0.6323%\n",
      "Epoch [30/300], Step [19/225], Training Accuracy: 70.7237%, Training Loss: 0.6300%\n",
      "Epoch [30/300], Step [20/225], Training Accuracy: 70.8594%, Training Loss: 0.6328%\n",
      "Epoch [30/300], Step [21/225], Training Accuracy: 71.2054%, Training Loss: 0.6267%\n",
      "Epoch [30/300], Step [22/225], Training Accuracy: 71.0227%, Training Loss: 0.6266%\n",
      "Epoch [30/300], Step [23/225], Training Accuracy: 71.4674%, Training Loss: 0.6231%\n",
      "Epoch [30/300], Step [24/225], Training Accuracy: 71.1589%, Training Loss: 0.6317%\n",
      "Epoch [30/300], Step [25/225], Training Accuracy: 71.1875%, Training Loss: 0.6289%\n",
      "Epoch [30/300], Step [26/225], Training Accuracy: 70.9736%, Training Loss: 0.6287%\n",
      "Epoch [30/300], Step [27/225], Training Accuracy: 71.0648%, Training Loss: 0.6269%\n",
      "Epoch [30/300], Step [28/225], Training Accuracy: 71.3170%, Training Loss: 0.6223%\n",
      "Epoch [30/300], Step [29/225], Training Accuracy: 71.2284%, Training Loss: 0.6206%\n",
      "Epoch [30/300], Step [30/225], Training Accuracy: 71.4062%, Training Loss: 0.6197%\n",
      "Epoch [30/300], Step [31/225], Training Accuracy: 71.0181%, Training Loss: 0.6275%\n",
      "Epoch [30/300], Step [32/225], Training Accuracy: 71.0938%, Training Loss: 0.6241%\n",
      "Epoch [30/300], Step [33/225], Training Accuracy: 71.4962%, Training Loss: 0.6203%\n",
      "Epoch [30/300], Step [34/225], Training Accuracy: 71.1857%, Training Loss: 0.6227%\n",
      "Epoch [30/300], Step [35/225], Training Accuracy: 71.2946%, Training Loss: 0.6225%\n",
      "Epoch [30/300], Step [36/225], Training Accuracy: 71.3542%, Training Loss: 0.6210%\n",
      "Epoch [30/300], Step [37/225], Training Accuracy: 71.3682%, Training Loss: 0.6204%\n",
      "Epoch [30/300], Step [38/225], Training Accuracy: 71.3816%, Training Loss: 0.6199%\n",
      "Epoch [30/300], Step [39/225], Training Accuracy: 71.4343%, Training Loss: 0.6217%\n",
      "Epoch [30/300], Step [40/225], Training Accuracy: 71.4844%, Training Loss: 0.6210%\n",
      "Epoch [30/300], Step [41/225], Training Accuracy: 71.4939%, Training Loss: 0.6221%\n",
      "Epoch [30/300], Step [42/225], Training Accuracy: 71.3914%, Training Loss: 0.6217%\n",
      "Epoch [30/300], Step [43/225], Training Accuracy: 71.1846%, Training Loss: 0.6240%\n",
      "Epoch [30/300], Step [44/225], Training Accuracy: 71.4844%, Training Loss: 0.6207%\n",
      "Epoch [30/300], Step [45/225], Training Accuracy: 71.5972%, Training Loss: 0.6187%\n",
      "Epoch [30/300], Step [46/225], Training Accuracy: 71.8071%, Training Loss: 0.6146%\n",
      "Epoch [30/300], Step [47/225], Training Accuracy: 71.4761%, Training Loss: 0.6168%\n",
      "Epoch [30/300], Step [48/225], Training Accuracy: 71.3542%, Training Loss: 0.6176%\n",
      "Epoch [30/300], Step [49/225], Training Accuracy: 71.4923%, Training Loss: 0.6164%\n",
      "Epoch [30/300], Step [50/225], Training Accuracy: 71.5625%, Training Loss: 0.6160%\n",
      "Epoch [30/300], Step [51/225], Training Accuracy: 71.6912%, Training Loss: 0.6147%\n",
      "Epoch [30/300], Step [52/225], Training Accuracy: 71.8149%, Training Loss: 0.6124%\n",
      "Epoch [30/300], Step [53/225], Training Accuracy: 71.9045%, Training Loss: 0.6113%\n",
      "Epoch [30/300], Step [54/225], Training Accuracy: 71.9039%, Training Loss: 0.6111%\n",
      "Epoch [30/300], Step [55/225], Training Accuracy: 71.8466%, Training Loss: 0.6117%\n",
      "Epoch [30/300], Step [56/225], Training Accuracy: 71.7634%, Training Loss: 0.6120%\n",
      "Epoch [30/300], Step [57/225], Training Accuracy: 71.7654%, Training Loss: 0.6111%\n",
      "Epoch [30/300], Step [58/225], Training Accuracy: 71.8481%, Training Loss: 0.6118%\n",
      "Epoch [30/300], Step [59/225], Training Accuracy: 71.7956%, Training Loss: 0.6112%\n",
      "Epoch [30/300], Step [60/225], Training Accuracy: 71.8490%, Training Loss: 0.6106%\n",
      "Epoch [30/300], Step [61/225], Training Accuracy: 71.7982%, Training Loss: 0.6107%\n",
      "Epoch [30/300], Step [62/225], Training Accuracy: 71.7994%, Training Loss: 0.6113%\n",
      "Epoch [30/300], Step [63/225], Training Accuracy: 71.7510%, Training Loss: 0.6128%\n",
      "Epoch [30/300], Step [64/225], Training Accuracy: 71.8994%, Training Loss: 0.6123%\n",
      "Epoch [30/300], Step [65/225], Training Accuracy: 71.8750%, Training Loss: 0.6129%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Step [66/225], Training Accuracy: 71.9697%, Training Loss: 0.6110%\n",
      "Epoch [30/300], Step [67/225], Training Accuracy: 71.9450%, Training Loss: 0.6107%\n",
      "Epoch [30/300], Step [68/225], Training Accuracy: 72.0129%, Training Loss: 0.6119%\n",
      "Epoch [30/300], Step [69/225], Training Accuracy: 71.8976%, Training Loss: 0.6130%\n",
      "Epoch [30/300], Step [70/225], Training Accuracy: 71.8750%, Training Loss: 0.6133%\n",
      "Epoch [30/300], Step [71/225], Training Accuracy: 71.8530%, Training Loss: 0.6132%\n",
      "Epoch [30/300], Step [72/225], Training Accuracy: 71.8533%, Training Loss: 0.6129%\n",
      "Epoch [30/300], Step [73/225], Training Accuracy: 71.8964%, Training Loss: 0.6122%\n",
      "Epoch [30/300], Step [74/225], Training Accuracy: 71.8750%, Training Loss: 0.6115%\n",
      "Epoch [30/300], Step [75/225], Training Accuracy: 72.0000%, Training Loss: 0.6110%\n",
      "Epoch [30/300], Step [76/225], Training Accuracy: 71.9367%, Training Loss: 0.6131%\n",
      "Epoch [30/300], Step [77/225], Training Accuracy: 72.0779%, Training Loss: 0.6122%\n",
      "Epoch [30/300], Step [78/225], Training Accuracy: 71.9952%, Training Loss: 0.6119%\n",
      "Epoch [30/300], Step [79/225], Training Accuracy: 72.0332%, Training Loss: 0.6116%\n",
      "Epoch [30/300], Step [80/225], Training Accuracy: 72.0703%, Training Loss: 0.6113%\n",
      "Epoch [30/300], Step [81/225], Training Accuracy: 72.1065%, Training Loss: 0.6106%\n",
      "Epoch [30/300], Step [82/225], Training Accuracy: 72.1227%, Training Loss: 0.6096%\n",
      "Epoch [30/300], Step [83/225], Training Accuracy: 72.1197%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [84/225], Training Accuracy: 72.0796%, Training Loss: 0.6087%\n",
      "Epoch [30/300], Step [85/225], Training Accuracy: 72.1140%, Training Loss: 0.6078%\n",
      "Epoch [30/300], Step [86/225], Training Accuracy: 72.0930%, Training Loss: 0.6081%\n",
      "Epoch [30/300], Step [87/225], Training Accuracy: 72.0905%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [88/225], Training Accuracy: 72.0170%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [89/225], Training Accuracy: 72.0330%, Training Loss: 0.6092%\n",
      "Epoch [30/300], Step [90/225], Training Accuracy: 71.9792%, Training Loss: 0.6105%\n",
      "Epoch [30/300], Step [91/225], Training Accuracy: 71.9609%, Training Loss: 0.6115%\n",
      "Epoch [30/300], Step [92/225], Training Accuracy: 71.9769%, Training Loss: 0.6114%\n",
      "Epoch [30/300], Step [93/225], Training Accuracy: 72.0934%, Training Loss: 0.6096%\n",
      "Epoch [30/300], Step [94/225], Training Accuracy: 72.0911%, Training Loss: 0.6095%\n",
      "Epoch [30/300], Step [95/225], Training Accuracy: 72.1217%, Training Loss: 0.6096%\n",
      "Epoch [30/300], Step [96/225], Training Accuracy: 72.0866%, Training Loss: 0.6089%\n",
      "Epoch [30/300], Step [97/225], Training Accuracy: 72.0683%, Training Loss: 0.6086%\n",
      "Epoch [30/300], Step [98/225], Training Accuracy: 71.9547%, Training Loss: 0.6102%\n",
      "Epoch [30/300], Step [99/225], Training Accuracy: 71.9697%, Training Loss: 0.6109%\n",
      "Epoch [30/300], Step [100/225], Training Accuracy: 71.9062%, Training Loss: 0.6124%\n",
      "Epoch [30/300], Step [101/225], Training Accuracy: 71.9524%, Training Loss: 0.6133%\n",
      "Epoch [30/300], Step [102/225], Training Accuracy: 71.8903%, Training Loss: 0.6135%\n",
      "Epoch [30/300], Step [103/225], Training Accuracy: 71.9508%, Training Loss: 0.6131%\n",
      "Epoch [30/300], Step [104/225], Training Accuracy: 71.8450%, Training Loss: 0.6140%\n",
      "Epoch [30/300], Step [105/225], Training Accuracy: 71.9345%, Training Loss: 0.6128%\n",
      "Epoch [30/300], Step [106/225], Training Accuracy: 72.0077%, Training Loss: 0.6120%\n",
      "Epoch [30/300], Step [107/225], Training Accuracy: 71.8896%, Training Loss: 0.6136%\n",
      "Epoch [30/300], Step [108/225], Training Accuracy: 71.8316%, Training Loss: 0.6144%\n",
      "Epoch [30/300], Step [109/225], Training Accuracy: 71.8177%, Training Loss: 0.6137%\n",
      "Epoch [30/300], Step [110/225], Training Accuracy: 71.7898%, Training Loss: 0.6134%\n",
      "Epoch [30/300], Step [111/225], Training Accuracy: 71.7342%, Training Loss: 0.6133%\n",
      "Epoch [30/300], Step [112/225], Training Accuracy: 71.7494%, Training Loss: 0.6130%\n",
      "Epoch [30/300], Step [113/225], Training Accuracy: 71.8335%, Training Loss: 0.6125%\n",
      "Epoch [30/300], Step [114/225], Training Accuracy: 71.8339%, Training Loss: 0.6124%\n",
      "Epoch [30/300], Step [115/225], Training Accuracy: 71.9158%, Training Loss: 0.6113%\n",
      "Epoch [30/300], Step [116/225], Training Accuracy: 71.9423%, Training Loss: 0.6108%\n",
      "Epoch [30/300], Step [117/225], Training Accuracy: 71.8483%, Training Loss: 0.6126%\n",
      "Epoch [30/300], Step [118/225], Training Accuracy: 71.7956%, Training Loss: 0.6136%\n",
      "Epoch [30/300], Step [119/225], Training Accuracy: 71.7568%, Training Loss: 0.6144%\n",
      "Epoch [30/300], Step [120/225], Training Accuracy: 71.7708%, Training Loss: 0.6140%\n",
      "Epoch [30/300], Step [121/225], Training Accuracy: 71.7330%, Training Loss: 0.6142%\n",
      "Epoch [30/300], Step [122/225], Training Accuracy: 71.7085%, Training Loss: 0.6146%\n",
      "Epoch [30/300], Step [123/225], Training Accuracy: 71.6590%, Training Loss: 0.6153%\n",
      "Epoch [30/300], Step [124/225], Training Accuracy: 71.6104%, Training Loss: 0.6151%\n",
      "Epoch [30/300], Step [125/225], Training Accuracy: 71.6000%, Training Loss: 0.6154%\n",
      "Epoch [30/300], Step [126/225], Training Accuracy: 71.6022%, Training Loss: 0.6151%\n",
      "Epoch [30/300], Step [127/225], Training Accuracy: 71.5428%, Training Loss: 0.6158%\n",
      "Epoch [30/300], Step [128/225], Training Accuracy: 71.5088%, Training Loss: 0.6173%\n",
      "Epoch [30/300], Step [129/225], Training Accuracy: 71.5237%, Training Loss: 0.6176%\n",
      "Epoch [30/300], Step [130/225], Training Accuracy: 71.4784%, Training Loss: 0.6179%\n",
      "Epoch [30/300], Step [131/225], Training Accuracy: 71.5410%, Training Loss: 0.6173%\n",
      "Epoch [30/300], Step [132/225], Training Accuracy: 71.5436%, Training Loss: 0.6180%\n",
      "Epoch [30/300], Step [133/225], Training Accuracy: 71.4756%, Training Loss: 0.6183%\n",
      "Epoch [30/300], Step [134/225], Training Accuracy: 71.4319%, Training Loss: 0.6191%\n",
      "Epoch [30/300], Step [135/225], Training Accuracy: 71.4005%, Training Loss: 0.6193%\n",
      "Epoch [30/300], Step [136/225], Training Accuracy: 71.4499%, Training Loss: 0.6186%\n",
      "Epoch [30/300], Step [137/225], Training Accuracy: 71.4416%, Training Loss: 0.6184%\n",
      "Epoch [30/300], Step [138/225], Training Accuracy: 71.5127%, Training Loss: 0.6173%\n",
      "Epoch [30/300], Step [139/225], Training Accuracy: 71.5378%, Training Loss: 0.6173%\n",
      "Epoch [30/300], Step [140/225], Training Accuracy: 71.5625%, Training Loss: 0.6170%\n",
      "Epoch [30/300], Step [141/225], Training Accuracy: 71.5869%, Training Loss: 0.6170%\n",
      "Epoch [30/300], Step [142/225], Training Accuracy: 71.5889%, Training Loss: 0.6170%\n",
      "Epoch [30/300], Step [143/225], Training Accuracy: 71.5691%, Training Loss: 0.6169%\n",
      "Epoch [30/300], Step [144/225], Training Accuracy: 71.5278%, Training Loss: 0.6175%\n",
      "Epoch [30/300], Step [145/225], Training Accuracy: 71.5409%, Training Loss: 0.6169%\n",
      "Epoch [30/300], Step [146/225], Training Accuracy: 71.5646%, Training Loss: 0.6167%\n",
      "Epoch [30/300], Step [147/225], Training Accuracy: 71.5880%, Training Loss: 0.6165%\n",
      "Epoch [30/300], Step [148/225], Training Accuracy: 71.6533%, Training Loss: 0.6155%\n",
      "Epoch [30/300], Step [149/225], Training Accuracy: 71.6862%, Training Loss: 0.6151%\n",
      "Epoch [30/300], Step [150/225], Training Accuracy: 71.6562%, Training Loss: 0.6150%\n",
      "Epoch [30/300], Step [151/225], Training Accuracy: 71.7094%, Training Loss: 0.6144%\n",
      "Epoch [30/300], Step [152/225], Training Accuracy: 71.7002%, Training Loss: 0.6152%\n",
      "Epoch [30/300], Step [153/225], Training Accuracy: 71.6912%, Training Loss: 0.6152%\n",
      "Epoch [30/300], Step [154/225], Training Accuracy: 71.7330%, Training Loss: 0.6150%\n",
      "Epoch [30/300], Step [155/225], Training Accuracy: 71.7036%, Training Loss: 0.6160%\n",
      "Epoch [30/300], Step [156/225], Training Accuracy: 71.7047%, Training Loss: 0.6163%\n",
      "Epoch [30/300], Step [157/225], Training Accuracy: 71.6959%, Training Loss: 0.6165%\n",
      "Epoch [30/300], Step [158/225], Training Accuracy: 71.6574%, Training Loss: 0.6169%\n",
      "Epoch [30/300], Step [159/225], Training Accuracy: 71.5998%, Training Loss: 0.6177%\n",
      "Epoch [30/300], Step [160/225], Training Accuracy: 71.6113%, Training Loss: 0.6174%\n",
      "Epoch [30/300], Step [161/225], Training Accuracy: 71.5839%, Training Loss: 0.6174%\n",
      "Epoch [30/300], Step [162/225], Training Accuracy: 71.7110%, Training Loss: 0.6160%\n",
      "Epoch [30/300], Step [163/225], Training Accuracy: 71.7312%, Training Loss: 0.6160%\n",
      "Epoch [30/300], Step [164/225], Training Accuracy: 71.7607%, Training Loss: 0.6151%\n",
      "Epoch [30/300], Step [165/225], Training Accuracy: 71.7803%, Training Loss: 0.6148%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Step [166/225], Training Accuracy: 71.8185%, Training Loss: 0.6144%\n",
      "Epoch [30/300], Step [167/225], Training Accuracy: 71.8095%, Training Loss: 0.6152%\n",
      "Epoch [30/300], Step [168/225], Training Accuracy: 71.8192%, Training Loss: 0.6150%\n",
      "Epoch [30/300], Step [169/225], Training Accuracy: 71.8288%, Training Loss: 0.6148%\n",
      "Epoch [30/300], Step [170/225], Training Accuracy: 71.8474%, Training Loss: 0.6146%\n",
      "Epoch [30/300], Step [171/225], Training Accuracy: 71.8567%, Training Loss: 0.6144%\n",
      "Epoch [30/300], Step [172/225], Training Accuracy: 71.8659%, Training Loss: 0.6142%\n",
      "Epoch [30/300], Step [173/225], Training Accuracy: 71.8569%, Training Loss: 0.6143%\n",
      "Epoch [30/300], Step [174/225], Training Accuracy: 71.8481%, Training Loss: 0.6142%\n",
      "Epoch [30/300], Step [175/225], Training Accuracy: 71.8571%, Training Loss: 0.6138%\n",
      "Epoch [30/300], Step [176/225], Training Accuracy: 71.8129%, Training Loss: 0.6136%\n",
      "Epoch [30/300], Step [177/225], Training Accuracy: 71.8309%, Training Loss: 0.6138%\n",
      "Epoch [30/300], Step [178/225], Training Accuracy: 71.8750%, Training Loss: 0.6133%\n",
      "Epoch [30/300], Step [179/225], Training Accuracy: 71.9361%, Training Loss: 0.6123%\n",
      "Epoch [30/300], Step [180/225], Training Accuracy: 71.9184%, Training Loss: 0.6121%\n",
      "Epoch [30/300], Step [181/225], Training Accuracy: 71.9095%, Training Loss: 0.6120%\n",
      "Epoch [30/300], Step [182/225], Training Accuracy: 71.9265%, Training Loss: 0.6123%\n",
      "Epoch [30/300], Step [183/225], Training Accuracy: 71.9262%, Training Loss: 0.6125%\n",
      "Epoch [30/300], Step [184/225], Training Accuracy: 71.9514%, Training Loss: 0.6120%\n",
      "Epoch [30/300], Step [185/225], Training Accuracy: 71.9341%, Training Loss: 0.6117%\n",
      "Epoch [30/300], Step [186/225], Training Accuracy: 71.9422%, Training Loss: 0.6111%\n",
      "Epoch [30/300], Step [187/225], Training Accuracy: 71.9084%, Training Loss: 0.6114%\n",
      "Epoch [30/300], Step [188/225], Training Accuracy: 71.9332%, Training Loss: 0.6109%\n",
      "Epoch [30/300], Step [189/225], Training Accuracy: 71.9494%, Training Loss: 0.6106%\n",
      "Epoch [30/300], Step [190/225], Training Accuracy: 71.9572%, Training Loss: 0.6111%\n",
      "Epoch [30/300], Step [191/225], Training Accuracy: 71.9486%, Training Loss: 0.6110%\n",
      "Epoch [30/300], Step [192/225], Training Accuracy: 71.9889%, Training Loss: 0.6104%\n",
      "Epoch [30/300], Step [193/225], Training Accuracy: 71.9964%, Training Loss: 0.6105%\n",
      "Epoch [30/300], Step [194/225], Training Accuracy: 72.0039%, Training Loss: 0.6105%\n",
      "Epoch [30/300], Step [195/225], Training Accuracy: 72.0192%, Training Loss: 0.6103%\n",
      "Epoch [30/300], Step [196/225], Training Accuracy: 72.0105%, Training Loss: 0.6104%\n",
      "Epoch [30/300], Step [197/225], Training Accuracy: 72.0257%, Training Loss: 0.6102%\n",
      "Epoch [30/300], Step [198/225], Training Accuracy: 72.1039%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [199/225], Training Accuracy: 72.1184%, Training Loss: 0.6086%\n",
      "Epoch [30/300], Step [200/225], Training Accuracy: 72.0781%, Training Loss: 0.6089%\n",
      "Epoch [30/300], Step [201/225], Training Accuracy: 72.0693%, Training Loss: 0.6093%\n",
      "Epoch [30/300], Step [202/225], Training Accuracy: 72.0838%, Training Loss: 0.6091%\n",
      "Epoch [30/300], Step [203/225], Training Accuracy: 72.1136%, Training Loss: 0.6089%\n",
      "Epoch [30/300], Step [204/225], Training Accuracy: 72.0971%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [205/225], Training Accuracy: 72.1418%, Training Loss: 0.6083%\n",
      "Epoch [30/300], Step [206/225], Training Accuracy: 72.1329%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [207/225], Training Accuracy: 72.1392%, Training Loss: 0.6092%\n",
      "Epoch [30/300], Step [208/225], Training Accuracy: 72.1304%, Training Loss: 0.6094%\n",
      "Epoch [30/300], Step [209/225], Training Accuracy: 72.1441%, Training Loss: 0.6094%\n",
      "Epoch [30/300], Step [210/225], Training Accuracy: 72.0759%, Training Loss: 0.6096%\n",
      "Epoch [30/300], Step [211/225], Training Accuracy: 72.1194%, Training Loss: 0.6090%\n",
      "Epoch [30/300], Step [212/225], Training Accuracy: 72.1108%, Training Loss: 0.6092%\n",
      "Epoch [30/300], Step [213/225], Training Accuracy: 72.0877%, Training Loss: 0.6101%\n",
      "Epoch [30/300], Step [214/225], Training Accuracy: 72.1013%, Training Loss: 0.6100%\n",
      "Epoch [30/300], Step [215/225], Training Accuracy: 72.0930%, Training Loss: 0.6101%\n",
      "Epoch [30/300], Step [216/225], Training Accuracy: 72.0703%, Training Loss: 0.6105%\n",
      "Epoch [30/300], Step [217/225], Training Accuracy: 72.0622%, Training Loss: 0.6110%\n",
      "Epoch [30/300], Step [218/225], Training Accuracy: 72.0470%, Training Loss: 0.6110%\n",
      "Epoch [30/300], Step [219/225], Training Accuracy: 72.0320%, Training Loss: 0.6113%\n",
      "Epoch [30/300], Step [220/225], Training Accuracy: 72.0526%, Training Loss: 0.6111%\n",
      "Epoch [30/300], Step [221/225], Training Accuracy: 72.0305%, Training Loss: 0.6115%\n",
      "Epoch [30/300], Step [222/225], Training Accuracy: 72.0510%, Training Loss: 0.6112%\n",
      "Epoch [30/300], Step [223/225], Training Accuracy: 71.9941%, Training Loss: 0.6120%\n",
      "Epoch [30/300], Step [224/225], Training Accuracy: 71.9657%, Training Loss: 0.6124%\n",
      "Epoch [30/300], Step [225/225], Training Accuracy: 71.9219%, Training Loss: 0.6126%\n",
      "Epoch [31/300], Step [1/225], Training Accuracy: 73.4375%, Training Loss: 0.5995%\n",
      "Epoch [31/300], Step [2/225], Training Accuracy: 74.2188%, Training Loss: 0.5945%\n",
      "Epoch [31/300], Step [3/225], Training Accuracy: 73.4375%, Training Loss: 0.5983%\n",
      "Epoch [31/300], Step [4/225], Training Accuracy: 73.8281%, Training Loss: 0.5998%\n",
      "Epoch [31/300], Step [5/225], Training Accuracy: 73.7500%, Training Loss: 0.5872%\n",
      "Epoch [31/300], Step [6/225], Training Accuracy: 73.1771%, Training Loss: 0.5984%\n",
      "Epoch [31/300], Step [7/225], Training Accuracy: 74.3304%, Training Loss: 0.5884%\n",
      "Epoch [31/300], Step [8/225], Training Accuracy: 73.8281%, Training Loss: 0.5883%\n",
      "Epoch [31/300], Step [9/225], Training Accuracy: 72.2222%, Training Loss: 0.6038%\n",
      "Epoch [31/300], Step [10/225], Training Accuracy: 72.5000%, Training Loss: 0.6105%\n",
      "Epoch [31/300], Step [11/225], Training Accuracy: 72.8693%, Training Loss: 0.6025%\n",
      "Epoch [31/300], Step [12/225], Training Accuracy: 72.6562%, Training Loss: 0.6010%\n",
      "Epoch [31/300], Step [13/225], Training Accuracy: 72.9567%, Training Loss: 0.5895%\n",
      "Epoch [31/300], Step [14/225], Training Accuracy: 73.2143%, Training Loss: 0.5838%\n",
      "Epoch [31/300], Step [15/225], Training Accuracy: 73.5417%, Training Loss: 0.5804%\n",
      "Epoch [31/300], Step [16/225], Training Accuracy: 73.2422%, Training Loss: 0.5808%\n",
      "Epoch [31/300], Step [17/225], Training Accuracy: 73.5294%, Training Loss: 0.5754%\n",
      "Epoch [31/300], Step [18/225], Training Accuracy: 73.0903%, Training Loss: 0.5840%\n",
      "Epoch [31/300], Step [19/225], Training Accuracy: 72.6151%, Training Loss: 0.5894%\n",
      "Epoch [31/300], Step [20/225], Training Accuracy: 72.5781%, Training Loss: 0.5945%\n",
      "Epoch [31/300], Step [21/225], Training Accuracy: 72.3958%, Training Loss: 0.5945%\n",
      "Epoch [31/300], Step [22/225], Training Accuracy: 72.3011%, Training Loss: 0.6000%\n",
      "Epoch [31/300], Step [23/225], Training Accuracy: 72.5543%, Training Loss: 0.5964%\n",
      "Epoch [31/300], Step [24/225], Training Accuracy: 72.5911%, Training Loss: 0.6046%\n",
      "Epoch [31/300], Step [25/225], Training Accuracy: 72.5000%, Training Loss: 0.6051%\n",
      "Epoch [31/300], Step [26/225], Training Accuracy: 72.1755%, Training Loss: 0.6070%\n",
      "Epoch [31/300], Step [27/225], Training Accuracy: 72.2222%, Training Loss: 0.6039%\n",
      "Epoch [31/300], Step [28/225], Training Accuracy: 72.5446%, Training Loss: 0.5977%\n",
      "Epoch [31/300], Step [29/225], Training Accuracy: 72.5216%, Training Loss: 0.5999%\n",
      "Epoch [31/300], Step [30/225], Training Accuracy: 72.6042%, Training Loss: 0.6006%\n",
      "Epoch [31/300], Step [31/225], Training Accuracy: 72.2278%, Training Loss: 0.6107%\n",
      "Epoch [31/300], Step [32/225], Training Accuracy: 71.9238%, Training Loss: 0.6111%\n",
      "Epoch [31/300], Step [33/225], Training Accuracy: 72.0644%, Training Loss: 0.6080%\n",
      "Epoch [31/300], Step [34/225], Training Accuracy: 72.1048%, Training Loss: 0.6085%\n",
      "Epoch [31/300], Step [35/225], Training Accuracy: 72.2321%, Training Loss: 0.6068%\n",
      "Epoch [31/300], Step [36/225], Training Accuracy: 72.0920%, Training Loss: 0.6069%\n",
      "Epoch [31/300], Step [37/225], Training Accuracy: 72.1284%, Training Loss: 0.6058%\n",
      "Epoch [31/300], Step [38/225], Training Accuracy: 71.9161%, Training Loss: 0.6072%\n",
      "Epoch [31/300], Step [39/225], Training Accuracy: 71.9952%, Training Loss: 0.6078%\n",
      "Epoch [31/300], Step [40/225], Training Accuracy: 71.9922%, Training Loss: 0.6075%\n",
      "Epoch [31/300], Step [41/225], Training Accuracy: 72.0274%, Training Loss: 0.6086%\n",
      "Epoch [31/300], Step [42/225], Training Accuracy: 72.1726%, Training Loss: 0.6067%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Step [43/225], Training Accuracy: 72.1657%, Training Loss: 0.6074%\n",
      "Epoch [31/300], Step [44/225], Training Accuracy: 72.3366%, Training Loss: 0.6054%\n",
      "Epoch [31/300], Step [45/225], Training Accuracy: 72.4306%, Training Loss: 0.6045%\n",
      "Epoch [31/300], Step [46/225], Training Accuracy: 72.3845%, Training Loss: 0.6041%\n",
      "Epoch [31/300], Step [47/225], Training Accuracy: 72.1742%, Training Loss: 0.6052%\n",
      "Epoch [31/300], Step [48/225], Training Accuracy: 72.0052%, Training Loss: 0.6067%\n",
      "Epoch [31/300], Step [49/225], Training Accuracy: 72.0663%, Training Loss: 0.6054%\n",
      "Epoch [31/300], Step [50/225], Training Accuracy: 72.1250%, Training Loss: 0.6039%\n",
      "Epoch [31/300], Step [51/225], Training Accuracy: 72.2426%, Training Loss: 0.6043%\n",
      "Epoch [31/300], Step [52/225], Training Accuracy: 72.2656%, Training Loss: 0.6029%\n",
      "Epoch [31/300], Step [53/225], Training Accuracy: 72.1698%, Training Loss: 0.6042%\n",
      "Epoch [31/300], Step [54/225], Training Accuracy: 72.0486%, Training Loss: 0.6051%\n",
      "Epoch [31/300], Step [55/225], Training Accuracy: 72.0739%, Training Loss: 0.6077%\n",
      "Epoch [31/300], Step [56/225], Training Accuracy: 72.0982%, Training Loss: 0.6078%\n",
      "Epoch [31/300], Step [57/225], Training Accuracy: 72.0669%, Training Loss: 0.6075%\n",
      "Epoch [31/300], Step [58/225], Training Accuracy: 72.0097%, Training Loss: 0.6084%\n",
      "Epoch [31/300], Step [59/225], Training Accuracy: 71.9280%, Training Loss: 0.6081%\n",
      "Epoch [31/300], Step [60/225], Training Accuracy: 71.9271%, Training Loss: 0.6077%\n",
      "Epoch [31/300], Step [61/225], Training Accuracy: 71.9006%, Training Loss: 0.6083%\n",
      "Epoch [31/300], Step [62/225], Training Accuracy: 71.8498%, Training Loss: 0.6090%\n",
      "Epoch [31/300], Step [63/225], Training Accuracy: 71.7510%, Training Loss: 0.6114%\n",
      "Epoch [31/300], Step [64/225], Training Accuracy: 71.8506%, Training Loss: 0.6092%\n",
      "Epoch [31/300], Step [65/225], Training Accuracy: 71.8029%, Training Loss: 0.6097%\n",
      "Epoch [31/300], Step [66/225], Training Accuracy: 71.9223%, Training Loss: 0.6078%\n",
      "Epoch [31/300], Step [67/225], Training Accuracy: 71.9683%, Training Loss: 0.6074%\n",
      "Epoch [31/300], Step [68/225], Training Accuracy: 71.9439%, Training Loss: 0.6084%\n",
      "Epoch [31/300], Step [69/225], Training Accuracy: 71.9203%, Training Loss: 0.6094%\n",
      "Epoch [31/300], Step [70/225], Training Accuracy: 71.8080%, Training Loss: 0.6097%\n",
      "Epoch [31/300], Step [71/225], Training Accuracy: 71.8970%, Training Loss: 0.6082%\n",
      "Epoch [31/300], Step [72/225], Training Accuracy: 71.9618%, Training Loss: 0.6074%\n",
      "Epoch [31/300], Step [73/225], Training Accuracy: 71.9392%, Training Loss: 0.6067%\n",
      "Epoch [31/300], Step [74/225], Training Accuracy: 71.9806%, Training Loss: 0.6060%\n",
      "Epoch [31/300], Step [75/225], Training Accuracy: 71.8958%, Training Loss: 0.6070%\n",
      "Epoch [31/300], Step [76/225], Training Accuracy: 71.8544%, Training Loss: 0.6083%\n",
      "Epoch [31/300], Step [77/225], Training Accuracy: 71.8750%, Training Loss: 0.6083%\n",
      "Epoch [31/300], Step [78/225], Training Accuracy: 71.7748%, Training Loss: 0.6082%\n",
      "Epoch [31/300], Step [79/225], Training Accuracy: 71.8552%, Training Loss: 0.6074%\n",
      "Epoch [31/300], Step [80/225], Training Accuracy: 71.9531%, Training Loss: 0.6069%\n",
      "Epoch [31/300], Step [81/225], Training Accuracy: 71.9522%, Training Loss: 0.6068%\n",
      "Epoch [31/300], Step [82/225], Training Accuracy: 72.0084%, Training Loss: 0.6057%\n",
      "Epoch [31/300], Step [83/225], Training Accuracy: 71.9691%, Training Loss: 0.6054%\n",
      "Epoch [31/300], Step [84/225], Training Accuracy: 71.9680%, Training Loss: 0.6046%\n",
      "Epoch [31/300], Step [85/225], Training Accuracy: 71.9853%, Training Loss: 0.6037%\n",
      "Epoch [31/300], Step [86/225], Training Accuracy: 71.9840%, Training Loss: 0.6040%\n",
      "Epoch [31/300], Step [87/225], Training Accuracy: 71.9468%, Training Loss: 0.6041%\n",
      "Epoch [31/300], Step [88/225], Training Accuracy: 71.9105%, Training Loss: 0.6041%\n",
      "Epoch [31/300], Step [89/225], Training Accuracy: 71.8223%, Training Loss: 0.6048%\n",
      "Epoch [31/300], Step [90/225], Training Accuracy: 71.7708%, Training Loss: 0.6050%\n",
      "Epoch [31/300], Step [91/225], Training Accuracy: 71.7205%, Training Loss: 0.6047%\n",
      "Epoch [31/300], Step [92/225], Training Accuracy: 71.7052%, Training Loss: 0.6049%\n",
      "Epoch [31/300], Step [93/225], Training Accuracy: 71.7238%, Training Loss: 0.6039%\n",
      "Epoch [31/300], Step [94/225], Training Accuracy: 71.7753%, Training Loss: 0.6040%\n",
      "Epoch [31/300], Step [95/225], Training Accuracy: 71.6941%, Training Loss: 0.6050%\n",
      "Epoch [31/300], Step [96/225], Training Accuracy: 71.7285%, Training Loss: 0.6038%\n",
      "Epoch [31/300], Step [97/225], Training Accuracy: 71.6495%, Training Loss: 0.6038%\n",
      "Epoch [31/300], Step [98/225], Training Accuracy: 71.6199%, Training Loss: 0.6050%\n",
      "Epoch [31/300], Step [99/225], Training Accuracy: 71.6067%, Training Loss: 0.6055%\n",
      "Epoch [31/300], Step [100/225], Training Accuracy: 71.5781%, Training Loss: 0.6063%\n",
      "Epoch [31/300], Step [101/225], Training Accuracy: 71.5965%, Training Loss: 0.6061%\n",
      "Epoch [31/300], Step [102/225], Training Accuracy: 71.5227%, Training Loss: 0.6070%\n",
      "Epoch [31/300], Step [103/225], Training Accuracy: 71.5413%, Training Loss: 0.6062%\n",
      "Epoch [31/300], Step [104/225], Training Accuracy: 71.4994%, Training Loss: 0.6073%\n",
      "Epoch [31/300], Step [105/225], Training Accuracy: 71.4583%, Training Loss: 0.6067%\n",
      "Epoch [31/300], Step [106/225], Training Accuracy: 71.4917%, Training Loss: 0.6067%\n",
      "Epoch [31/300], Step [107/225], Training Accuracy: 71.4515%, Training Loss: 0.6075%\n",
      "Epoch [31/300], Step [108/225], Training Accuracy: 71.4410%, Training Loss: 0.6080%\n",
      "Epoch [31/300], Step [109/225], Training Accuracy: 71.4163%, Training Loss: 0.6080%\n",
      "Epoch [31/300], Step [110/225], Training Accuracy: 71.4489%, Training Loss: 0.6077%\n",
      "Epoch [31/300], Step [111/225], Training Accuracy: 71.4668%, Training Loss: 0.6079%\n",
      "Epoch [31/300], Step [112/225], Training Accuracy: 71.4844%, Training Loss: 0.6078%\n",
      "Epoch [31/300], Step [113/225], Training Accuracy: 71.5293%, Training Loss: 0.6071%\n",
      "Epoch [31/300], Step [114/225], Training Accuracy: 71.5461%, Training Loss: 0.6069%\n",
      "Epoch [31/300], Step [115/225], Training Accuracy: 71.5489%, Training Loss: 0.6062%\n",
      "Epoch [31/300], Step [116/225], Training Accuracy: 71.5517%, Training Loss: 0.6063%\n",
      "Epoch [31/300], Step [117/225], Training Accuracy: 71.4343%, Training Loss: 0.6086%\n",
      "Epoch [31/300], Step [118/225], Training Accuracy: 71.3718%, Training Loss: 0.6087%\n",
      "Epoch [31/300], Step [119/225], Training Accuracy: 71.4023%, Training Loss: 0.6085%\n",
      "Epoch [31/300], Step [120/225], Training Accuracy: 71.3932%, Training Loss: 0.6086%\n",
      "Epoch [31/300], Step [121/225], Training Accuracy: 71.2810%, Training Loss: 0.6092%\n",
      "Epoch [31/300], Step [122/225], Training Accuracy: 71.3371%, Training Loss: 0.6092%\n",
      "Epoch [31/300], Step [123/225], Training Accuracy: 71.3542%, Training Loss: 0.6090%\n",
      "Epoch [31/300], Step [124/225], Training Accuracy: 71.3080%, Training Loss: 0.6100%\n",
      "Epoch [31/300], Step [125/225], Training Accuracy: 71.2625%, Training Loss: 0.6106%\n",
      "Epoch [31/300], Step [126/225], Training Accuracy: 71.2426%, Training Loss: 0.6103%\n",
      "Epoch [31/300], Step [127/225], Training Accuracy: 71.2721%, Training Loss: 0.6101%\n",
      "Epoch [31/300], Step [128/225], Training Accuracy: 71.3135%, Training Loss: 0.6108%\n",
      "Epoch [31/300], Step [129/225], Training Accuracy: 71.2573%, Training Loss: 0.6117%\n",
      "Epoch [31/300], Step [130/225], Training Accuracy: 71.1779%, Training Loss: 0.6123%\n",
      "Epoch [31/300], Step [131/225], Training Accuracy: 71.1713%, Training Loss: 0.6123%\n",
      "Epoch [31/300], Step [132/225], Training Accuracy: 71.1411%, Training Loss: 0.6127%\n",
      "Epoch [31/300], Step [133/225], Training Accuracy: 71.1114%, Training Loss: 0.6130%\n",
      "Epoch [31/300], Step [134/225], Training Accuracy: 71.1054%, Training Loss: 0.6143%\n",
      "Epoch [31/300], Step [135/225], Training Accuracy: 71.1343%, Training Loss: 0.6142%\n",
      "Epoch [31/300], Step [136/225], Training Accuracy: 71.1857%, Training Loss: 0.6138%\n",
      "Epoch [31/300], Step [137/225], Training Accuracy: 71.1565%, Training Loss: 0.6136%\n",
      "Epoch [31/300], Step [138/225], Training Accuracy: 71.2409%, Training Loss: 0.6123%\n",
      "Epoch [31/300], Step [139/225], Training Accuracy: 71.2005%, Training Loss: 0.6130%\n",
      "Epoch [31/300], Step [140/225], Training Accuracy: 71.2165%, Training Loss: 0.6135%\n",
      "Epoch [31/300], Step [141/225], Training Accuracy: 71.2212%, Training Loss: 0.6133%\n",
      "Epoch [31/300], Step [142/225], Training Accuracy: 71.2148%, Training Loss: 0.6133%\n",
      "Epoch [31/300], Step [143/225], Training Accuracy: 71.1976%, Training Loss: 0.6135%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Step [144/225], Training Accuracy: 71.1263%, Training Loss: 0.6144%\n",
      "Epoch [31/300], Step [145/225], Training Accuracy: 71.1315%, Training Loss: 0.6147%\n",
      "Epoch [31/300], Step [146/225], Training Accuracy: 71.1259%, Training Loss: 0.6149%\n",
      "Epoch [31/300], Step [147/225], Training Accuracy: 71.0991%, Training Loss: 0.6150%\n",
      "Epoch [31/300], Step [148/225], Training Accuracy: 71.1254%, Training Loss: 0.6145%\n",
      "Epoch [31/300], Step [149/225], Training Accuracy: 71.1095%, Training Loss: 0.6146%\n",
      "Epoch [31/300], Step [150/225], Training Accuracy: 71.0729%, Training Loss: 0.6149%\n",
      "Epoch [31/300], Step [151/225], Training Accuracy: 71.1403%, Training Loss: 0.6141%\n",
      "Epoch [31/300], Step [152/225], Training Accuracy: 71.0938%, Training Loss: 0.6150%\n",
      "Epoch [31/300], Step [153/225], Training Accuracy: 71.0989%, Training Loss: 0.6151%\n",
      "Epoch [31/300], Step [154/225], Training Accuracy: 71.1546%, Training Loss: 0.6145%\n",
      "Epoch [31/300], Step [155/225], Training Accuracy: 71.0988%, Training Loss: 0.6153%\n",
      "Epoch [31/300], Step [156/225], Training Accuracy: 71.1138%, Training Loss: 0.6156%\n",
      "Epoch [31/300], Step [157/225], Training Accuracy: 71.0987%, Training Loss: 0.6158%\n",
      "Epoch [31/300], Step [158/225], Training Accuracy: 71.0443%, Training Loss: 0.6166%\n",
      "Epoch [31/300], Step [159/225], Training Accuracy: 70.9513%, Training Loss: 0.6178%\n",
      "Epoch [31/300], Step [160/225], Training Accuracy: 71.0059%, Training Loss: 0.6168%\n",
      "Epoch [31/300], Step [161/225], Training Accuracy: 70.9724%, Training Loss: 0.6168%\n",
      "Epoch [31/300], Step [162/225], Training Accuracy: 71.0359%, Training Loss: 0.6163%\n",
      "Epoch [31/300], Step [163/225], Training Accuracy: 71.0602%, Training Loss: 0.6157%\n",
      "Epoch [31/300], Step [164/225], Training Accuracy: 71.0938%, Training Loss: 0.6156%\n",
      "Epoch [31/300], Step [165/225], Training Accuracy: 71.1364%, Training Loss: 0.6153%\n",
      "Epoch [31/300], Step [166/225], Training Accuracy: 71.1691%, Training Loss: 0.6150%\n",
      "Epoch [31/300], Step [167/225], Training Accuracy: 71.1733%, Training Loss: 0.6152%\n",
      "Epoch [31/300], Step [168/225], Training Accuracy: 71.1775%, Training Loss: 0.6152%\n",
      "Epoch [31/300], Step [169/225], Training Accuracy: 71.1816%, Training Loss: 0.6155%\n",
      "Epoch [31/300], Step [170/225], Training Accuracy: 71.1673%, Training Loss: 0.6161%\n",
      "Epoch [31/300], Step [171/225], Training Accuracy: 71.2171%, Training Loss: 0.6158%\n",
      "Epoch [31/300], Step [172/225], Training Accuracy: 71.2118%, Training Loss: 0.6159%\n",
      "Epoch [31/300], Step [173/225], Training Accuracy: 71.1615%, Training Loss: 0.6162%\n",
      "Epoch [31/300], Step [174/225], Training Accuracy: 71.1656%, Training Loss: 0.6167%\n",
      "Epoch [31/300], Step [175/225], Training Accuracy: 71.2232%, Training Loss: 0.6162%\n",
      "Epoch [31/300], Step [176/225], Training Accuracy: 71.2447%, Training Loss: 0.6156%\n",
      "Epoch [31/300], Step [177/225], Training Accuracy: 71.2659%, Training Loss: 0.6161%\n",
      "Epoch [31/300], Step [178/225], Training Accuracy: 71.2518%, Training Loss: 0.6157%\n",
      "Epoch [31/300], Step [179/225], Training Accuracy: 71.3163%, Training Loss: 0.6149%\n",
      "Epoch [31/300], Step [180/225], Training Accuracy: 71.3021%, Training Loss: 0.6154%\n",
      "Epoch [31/300], Step [181/225], Training Accuracy: 71.2707%, Training Loss: 0.6153%\n",
      "Epoch [31/300], Step [182/225], Training Accuracy: 71.2483%, Training Loss: 0.6155%\n",
      "Epoch [31/300], Step [183/225], Training Accuracy: 71.2346%, Training Loss: 0.6158%\n",
      "Epoch [31/300], Step [184/225], Training Accuracy: 71.2551%, Training Loss: 0.6152%\n",
      "Epoch [31/300], Step [185/225], Training Accuracy: 71.2416%, Training Loss: 0.6146%\n",
      "Epoch [31/300], Step [186/225], Training Accuracy: 71.2702%, Training Loss: 0.6140%\n",
      "Epoch [31/300], Step [187/225], Training Accuracy: 71.2483%, Training Loss: 0.6143%\n",
      "Epoch [31/300], Step [188/225], Training Accuracy: 71.2766%, Training Loss: 0.6137%\n",
      "Epoch [31/300], Step [189/225], Training Accuracy: 71.2798%, Training Loss: 0.6140%\n",
      "Epoch [31/300], Step [190/225], Training Accuracy: 71.2253%, Training Loss: 0.6147%\n",
      "Epoch [31/300], Step [191/225], Training Accuracy: 71.1796%, Training Loss: 0.6151%\n",
      "Epoch [31/300], Step [192/225], Training Accuracy: 71.2321%, Training Loss: 0.6145%\n",
      "Epoch [31/300], Step [193/225], Training Accuracy: 71.2354%, Training Loss: 0.6145%\n",
      "Epoch [31/300], Step [194/225], Training Accuracy: 71.2629%, Training Loss: 0.6143%\n",
      "Epoch [31/300], Step [195/225], Training Accuracy: 71.2981%, Training Loss: 0.6140%\n",
      "Epoch [31/300], Step [196/225], Training Accuracy: 71.2612%, Training Loss: 0.6149%\n",
      "Epoch [31/300], Step [197/225], Training Accuracy: 71.2722%, Training Loss: 0.6150%\n",
      "Epoch [31/300], Step [198/225], Training Accuracy: 71.3384%, Training Loss: 0.6142%\n",
      "Epoch [31/300], Step [199/225], Training Accuracy: 71.3803%, Training Loss: 0.6137%\n",
      "Epoch [31/300], Step [200/225], Training Accuracy: 71.3906%, Training Loss: 0.6132%\n",
      "Epoch [31/300], Step [201/225], Training Accuracy: 71.3853%, Training Loss: 0.6138%\n",
      "Epoch [31/300], Step [202/225], Training Accuracy: 71.4186%, Training Loss: 0.6133%\n",
      "Epoch [31/300], Step [203/225], Training Accuracy: 71.4286%, Training Loss: 0.6131%\n",
      "Epoch [31/300], Step [204/225], Training Accuracy: 71.4308%, Training Loss: 0.6131%\n",
      "Epoch [31/300], Step [205/225], Training Accuracy: 71.5091%, Training Loss: 0.6121%\n",
      "Epoch [31/300], Step [206/225], Training Accuracy: 71.4958%, Training Loss: 0.6124%\n",
      "Epoch [31/300], Step [207/225], Training Accuracy: 71.5051%, Training Loss: 0.6125%\n",
      "Epoch [31/300], Step [208/225], Training Accuracy: 71.5445%, Training Loss: 0.6118%\n",
      "Epoch [31/300], Step [209/225], Training Accuracy: 71.5311%, Training Loss: 0.6120%\n",
      "Epoch [31/300], Step [210/225], Training Accuracy: 71.4955%, Training Loss: 0.6127%\n",
      "Epoch [31/300], Step [211/225], Training Accuracy: 71.5344%, Training Loss: 0.6121%\n",
      "Epoch [31/300], Step [212/225], Training Accuracy: 71.4917%, Training Loss: 0.6127%\n",
      "Epoch [31/300], Step [213/225], Training Accuracy: 71.4789%, Training Loss: 0.6130%\n",
      "Epoch [31/300], Step [214/225], Training Accuracy: 71.4734%, Training Loss: 0.6128%\n",
      "Epoch [31/300], Step [215/225], Training Accuracy: 71.4753%, Training Loss: 0.6128%\n",
      "Epoch [31/300], Step [216/225], Training Accuracy: 71.4771%, Training Loss: 0.6125%\n",
      "Epoch [31/300], Step [217/225], Training Accuracy: 71.4718%, Training Loss: 0.6124%\n",
      "Epoch [31/300], Step [218/225], Training Accuracy: 71.4378%, Training Loss: 0.6125%\n",
      "Epoch [31/300], Step [219/225], Training Accuracy: 71.4469%, Training Loss: 0.6125%\n",
      "Epoch [31/300], Step [220/225], Training Accuracy: 71.4489%, Training Loss: 0.6131%\n",
      "Epoch [31/300], Step [221/225], Training Accuracy: 71.4225%, Training Loss: 0.6138%\n",
      "Epoch [31/300], Step [222/225], Training Accuracy: 71.4597%, Training Loss: 0.6135%\n",
      "Epoch [31/300], Step [223/225], Training Accuracy: 71.4196%, Training Loss: 0.6139%\n",
      "Epoch [31/300], Step [224/225], Training Accuracy: 71.4076%, Training Loss: 0.6139%\n",
      "Epoch [31/300], Step [225/225], Training Accuracy: 71.3869%, Training Loss: 0.6141%\n",
      "Epoch [32/300], Step [1/225], Training Accuracy: 78.1250%, Training Loss: 0.5136%\n",
      "Epoch [32/300], Step [2/225], Training Accuracy: 74.2188%, Training Loss: 0.5567%\n",
      "Epoch [32/300], Step [3/225], Training Accuracy: 73.4375%, Training Loss: 0.5660%\n",
      "Epoch [32/300], Step [4/225], Training Accuracy: 75.0000%, Training Loss: 0.5572%\n",
      "Epoch [32/300], Step [5/225], Training Accuracy: 75.0000%, Training Loss: 0.5592%\n",
      "Epoch [32/300], Step [6/225], Training Accuracy: 74.4792%, Training Loss: 0.5772%\n",
      "Epoch [32/300], Step [7/225], Training Accuracy: 74.3304%, Training Loss: 0.5848%\n",
      "Epoch [32/300], Step [8/225], Training Accuracy: 72.6562%, Training Loss: 0.5989%\n",
      "Epoch [32/300], Step [9/225], Training Accuracy: 71.3542%, Training Loss: 0.6192%\n",
      "Epoch [32/300], Step [10/225], Training Accuracy: 70.6250%, Training Loss: 0.6281%\n",
      "Epoch [32/300], Step [11/225], Training Accuracy: 71.3068%, Training Loss: 0.6234%\n",
      "Epoch [32/300], Step [12/225], Training Accuracy: 70.7031%, Training Loss: 0.6258%\n",
      "Epoch [32/300], Step [13/225], Training Accuracy: 71.2740%, Training Loss: 0.6155%\n",
      "Epoch [32/300], Step [14/225], Training Accuracy: 71.2054%, Training Loss: 0.6132%\n",
      "Epoch [32/300], Step [15/225], Training Accuracy: 71.3542%, Training Loss: 0.6145%\n",
      "Epoch [32/300], Step [16/225], Training Accuracy: 71.7773%, Training Loss: 0.6104%\n",
      "Epoch [32/300], Step [17/225], Training Accuracy: 71.5993%, Training Loss: 0.6095%\n",
      "Epoch [32/300], Step [18/225], Training Accuracy: 71.5278%, Training Loss: 0.6101%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Step [19/225], Training Accuracy: 71.7105%, Training Loss: 0.6108%\n",
      "Epoch [32/300], Step [20/225], Training Accuracy: 71.7969%, Training Loss: 0.6074%\n",
      "Epoch [32/300], Step [21/225], Training Accuracy: 72.2470%, Training Loss: 0.6029%\n",
      "Epoch [32/300], Step [22/225], Training Accuracy: 71.8750%, Training Loss: 0.6085%\n",
      "Epoch [32/300], Step [23/225], Training Accuracy: 72.2826%, Training Loss: 0.6023%\n",
      "Epoch [32/300], Step [24/225], Training Accuracy: 72.2656%, Training Loss: 0.6064%\n",
      "Epoch [32/300], Step [25/225], Training Accuracy: 72.6875%, Training Loss: 0.6033%\n",
      "Epoch [32/300], Step [26/225], Training Accuracy: 72.4760%, Training Loss: 0.6045%\n",
      "Epoch [32/300], Step [27/225], Training Accuracy: 72.5116%, Training Loss: 0.6028%\n",
      "Epoch [32/300], Step [28/225], Training Accuracy: 72.9353%, Training Loss: 0.5970%\n",
      "Epoch [32/300], Step [29/225], Training Accuracy: 72.7909%, Training Loss: 0.5971%\n",
      "Epoch [32/300], Step [30/225], Training Accuracy: 72.9167%, Training Loss: 0.5954%\n",
      "Epoch [32/300], Step [31/225], Training Accuracy: 72.3790%, Training Loss: 0.6048%\n",
      "Epoch [32/300], Step [32/225], Training Accuracy: 72.2168%, Training Loss: 0.6076%\n",
      "Epoch [32/300], Step [33/225], Training Accuracy: 72.3011%, Training Loss: 0.6060%\n",
      "Epoch [32/300], Step [34/225], Training Accuracy: 72.2886%, Training Loss: 0.6057%\n",
      "Epoch [32/300], Step [35/225], Training Accuracy: 72.3214%, Training Loss: 0.6037%\n",
      "Epoch [32/300], Step [36/225], Training Accuracy: 72.6128%, Training Loss: 0.6002%\n",
      "Epoch [32/300], Step [37/225], Training Accuracy: 72.5084%, Training Loss: 0.6004%\n",
      "Epoch [32/300], Step [38/225], Training Accuracy: 72.4918%, Training Loss: 0.6011%\n",
      "Epoch [32/300], Step [39/225], Training Accuracy: 72.5160%, Training Loss: 0.6008%\n",
      "Epoch [32/300], Step [40/225], Training Accuracy: 72.5391%, Training Loss: 0.6014%\n",
      "Epoch [32/300], Step [41/225], Training Accuracy: 72.3704%, Training Loss: 0.6043%\n",
      "Epoch [32/300], Step [42/225], Training Accuracy: 72.4330%, Training Loss: 0.6038%\n",
      "Epoch [32/300], Step [43/225], Training Accuracy: 72.4564%, Training Loss: 0.6047%\n",
      "Epoch [32/300], Step [44/225], Training Accuracy: 72.5852%, Training Loss: 0.6031%\n",
      "Epoch [32/300], Step [45/225], Training Accuracy: 72.6389%, Training Loss: 0.6016%\n",
      "Epoch [32/300], Step [46/225], Training Accuracy: 72.7582%, Training Loss: 0.5994%\n",
      "Epoch [32/300], Step [47/225], Training Accuracy: 72.7726%, Training Loss: 0.5994%\n",
      "Epoch [32/300], Step [48/225], Training Accuracy: 72.6237%, Training Loss: 0.6007%\n",
      "Epoch [32/300], Step [49/225], Training Accuracy: 72.6403%, Training Loss: 0.6005%\n",
      "Epoch [32/300], Step [50/225], Training Accuracy: 72.5000%, Training Loss: 0.6007%\n",
      "Epoch [32/300], Step [51/225], Training Accuracy: 72.4877%, Training Loss: 0.6008%\n",
      "Epoch [32/300], Step [52/225], Training Accuracy: 72.5962%, Training Loss: 0.6000%\n",
      "Epoch [32/300], Step [53/225], Training Accuracy: 72.6415%, Training Loss: 0.6000%\n",
      "Epoch [32/300], Step [54/225], Training Accuracy: 72.5984%, Training Loss: 0.6004%\n",
      "Epoch [32/300], Step [55/225], Training Accuracy: 72.4716%, Training Loss: 0.6031%\n",
      "Epoch [32/300], Step [56/225], Training Accuracy: 72.4609%, Training Loss: 0.6033%\n",
      "Epoch [32/300], Step [57/225], Training Accuracy: 72.5329%, Training Loss: 0.6013%\n",
      "Epoch [32/300], Step [58/225], Training Accuracy: 72.4677%, Training Loss: 0.6030%\n",
      "Epoch [32/300], Step [59/225], Training Accuracy: 72.5106%, Training Loss: 0.6025%\n",
      "Epoch [32/300], Step [60/225], Training Accuracy: 72.5781%, Training Loss: 0.6012%\n",
      "Epoch [32/300], Step [61/225], Training Accuracy: 72.4641%, Training Loss: 0.6037%\n",
      "Epoch [32/300], Step [62/225], Training Accuracy: 72.3286%, Training Loss: 0.6051%\n",
      "Epoch [32/300], Step [63/225], Training Accuracy: 72.2470%, Training Loss: 0.6066%\n",
      "Epoch [32/300], Step [64/225], Training Accuracy: 72.2656%, Training Loss: 0.6056%\n",
      "Epoch [32/300], Step [65/225], Training Accuracy: 72.3077%, Training Loss: 0.6054%\n",
      "Epoch [32/300], Step [66/225], Training Accuracy: 72.4905%, Training Loss: 0.6039%\n",
      "Epoch [32/300], Step [67/225], Training Accuracy: 72.4813%, Training Loss: 0.6039%\n",
      "Epoch [32/300], Step [68/225], Training Accuracy: 72.4494%, Training Loss: 0.6044%\n",
      "Epoch [32/300], Step [69/225], Training Accuracy: 72.4638%, Training Loss: 0.6044%\n",
      "Epoch [32/300], Step [70/225], Training Accuracy: 72.3884%, Training Loss: 0.6044%\n",
      "Epoch [32/300], Step [71/225], Training Accuracy: 72.3812%, Training Loss: 0.6040%\n",
      "Epoch [32/300], Step [72/225], Training Accuracy: 72.3741%, Training Loss: 0.6036%\n",
      "Epoch [32/300], Step [73/225], Training Accuracy: 72.2603%, Training Loss: 0.6038%\n",
      "Epoch [32/300], Step [74/225], Training Accuracy: 72.0861%, Training Loss: 0.6043%\n",
      "Epoch [32/300], Step [75/225], Training Accuracy: 72.1875%, Training Loss: 0.6034%\n",
      "Epoch [32/300], Step [76/225], Training Accuracy: 72.1217%, Training Loss: 0.6044%\n",
      "Epoch [32/300], Step [77/225], Training Accuracy: 72.1591%, Training Loss: 0.6036%\n",
      "Epoch [32/300], Step [78/225], Training Accuracy: 72.1354%, Training Loss: 0.6036%\n",
      "Epoch [32/300], Step [79/225], Training Accuracy: 72.1519%, Training Loss: 0.6031%\n",
      "Epoch [32/300], Step [80/225], Training Accuracy: 72.1680%, Training Loss: 0.6033%\n",
      "Epoch [32/300], Step [81/225], Training Accuracy: 72.2029%, Training Loss: 0.6028%\n",
      "Epoch [32/300], Step [82/225], Training Accuracy: 72.2370%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [83/225], Training Accuracy: 72.2703%, Training Loss: 0.6013%\n",
      "Epoch [32/300], Step [84/225], Training Accuracy: 72.2842%, Training Loss: 0.6001%\n",
      "Epoch [32/300], Step [85/225], Training Accuracy: 72.2978%, Training Loss: 0.5994%\n",
      "Epoch [32/300], Step [86/225], Training Accuracy: 72.4019%, Training Loss: 0.5982%\n",
      "Epoch [32/300], Step [87/225], Training Accuracy: 72.3599%, Training Loss: 0.5988%\n",
      "Epoch [32/300], Step [88/225], Training Accuracy: 72.3366%, Training Loss: 0.6002%\n",
      "Epoch [32/300], Step [89/225], Training Accuracy: 72.3139%, Training Loss: 0.6008%\n",
      "Epoch [32/300], Step [90/225], Training Accuracy: 72.3438%, Training Loss: 0.6011%\n",
      "Epoch [32/300], Step [91/225], Training Accuracy: 72.2184%, Training Loss: 0.6025%\n",
      "Epoch [32/300], Step [92/225], Training Accuracy: 72.2147%, Training Loss: 0.6027%\n",
      "Epoch [32/300], Step [93/225], Training Accuracy: 72.2950%, Training Loss: 0.6016%\n",
      "Epoch [32/300], Step [94/225], Training Accuracy: 72.3072%, Training Loss: 0.6015%\n",
      "Epoch [32/300], Step [95/225], Training Accuracy: 72.3026%, Training Loss: 0.6021%\n",
      "Epoch [32/300], Step [96/225], Training Accuracy: 72.2982%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [97/225], Training Accuracy: 72.3421%, Training Loss: 0.6013%\n",
      "Epoch [32/300], Step [98/225], Training Accuracy: 72.2258%, Training Loss: 0.6034%\n",
      "Epoch [32/300], Step [99/225], Training Accuracy: 72.2222%, Training Loss: 0.6038%\n",
      "Epoch [32/300], Step [100/225], Training Accuracy: 72.1094%, Training Loss: 0.6046%\n",
      "Epoch [32/300], Step [101/225], Training Accuracy: 72.1535%, Training Loss: 0.6045%\n",
      "Epoch [32/300], Step [102/225], Training Accuracy: 72.0435%, Training Loss: 0.6047%\n",
      "Epoch [32/300], Step [103/225], Training Accuracy: 72.0874%, Training Loss: 0.6041%\n",
      "Epoch [32/300], Step [104/225], Training Accuracy: 71.9201%, Training Loss: 0.6064%\n",
      "Epoch [32/300], Step [105/225], Training Accuracy: 71.9792%, Training Loss: 0.6052%\n",
      "Epoch [32/300], Step [106/225], Training Accuracy: 71.9634%, Training Loss: 0.6049%\n",
      "Epoch [32/300], Step [107/225], Training Accuracy: 71.8896%, Training Loss: 0.6060%\n",
      "Epoch [32/300], Step [108/225], Training Accuracy: 71.8750%, Training Loss: 0.6061%\n",
      "Epoch [32/300], Step [109/225], Training Accuracy: 71.9323%, Training Loss: 0.6057%\n",
      "Epoch [32/300], Step [110/225], Training Accuracy: 71.9034%, Training Loss: 0.6054%\n",
      "Epoch [32/300], Step [111/225], Training Accuracy: 71.8750%, Training Loss: 0.6053%\n",
      "Epoch [32/300], Step [112/225], Training Accuracy: 71.8610%, Training Loss: 0.6052%\n",
      "Epoch [32/300], Step [113/225], Training Accuracy: 71.8612%, Training Loss: 0.6055%\n",
      "Epoch [32/300], Step [114/225], Training Accuracy: 71.8613%, Training Loss: 0.6053%\n",
      "Epoch [32/300], Step [115/225], Training Accuracy: 71.9022%, Training Loss: 0.6049%\n",
      "Epoch [32/300], Step [116/225], Training Accuracy: 71.8750%, Training Loss: 0.6046%\n",
      "Epoch [32/300], Step [117/225], Training Accuracy: 71.9151%, Training Loss: 0.6053%\n",
      "Epoch [32/300], Step [118/225], Training Accuracy: 71.8618%, Training Loss: 0.6059%\n",
      "Epoch [32/300], Step [119/225], Training Accuracy: 71.8881%, Training Loss: 0.6059%\n",
      "Epoch [32/300], Step [120/225], Training Accuracy: 71.9401%, Training Loss: 0.6055%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Step [121/225], Training Accuracy: 71.8879%, Training Loss: 0.6057%\n",
      "Epoch [32/300], Step [122/225], Training Accuracy: 71.8878%, Training Loss: 0.6058%\n",
      "Epoch [32/300], Step [123/225], Training Accuracy: 71.8369%, Training Loss: 0.6059%\n",
      "Epoch [32/300], Step [124/225], Training Accuracy: 71.8624%, Training Loss: 0.6053%\n",
      "Epoch [32/300], Step [125/225], Training Accuracy: 71.8750%, Training Loss: 0.6060%\n",
      "Epoch [32/300], Step [126/225], Training Accuracy: 71.9246%, Training Loss: 0.6061%\n",
      "Epoch [32/300], Step [127/225], Training Accuracy: 71.9365%, Training Loss: 0.6061%\n",
      "Epoch [32/300], Step [128/225], Training Accuracy: 71.8872%, Training Loss: 0.6082%\n",
      "Epoch [32/300], Step [129/225], Training Accuracy: 71.8992%, Training Loss: 0.6082%\n",
      "Epoch [32/300], Step [130/225], Training Accuracy: 71.8870%, Training Loss: 0.6083%\n",
      "Epoch [32/300], Step [131/225], Training Accuracy: 71.9466%, Training Loss: 0.6078%\n",
      "Epoch [32/300], Step [132/225], Training Accuracy: 71.9815%, Training Loss: 0.6074%\n",
      "Epoch [32/300], Step [133/225], Training Accuracy: 71.9455%, Training Loss: 0.6085%\n",
      "Epoch [32/300], Step [134/225], Training Accuracy: 71.9566%, Training Loss: 0.6085%\n",
      "Epoch [32/300], Step [135/225], Training Accuracy: 71.9792%, Training Loss: 0.6082%\n",
      "Epoch [32/300], Step [136/225], Training Accuracy: 72.0129%, Training Loss: 0.6080%\n",
      "Epoch [32/300], Step [137/225], Training Accuracy: 72.0119%, Training Loss: 0.6077%\n",
      "Epoch [32/300], Step [138/225], Training Accuracy: 72.0675%, Training Loss: 0.6065%\n",
      "Epoch [32/300], Step [139/225], Training Accuracy: 72.0661%, Training Loss: 0.6068%\n",
      "Epoch [32/300], Step [140/225], Training Accuracy: 72.1429%, Training Loss: 0.6062%\n",
      "Epoch [32/300], Step [141/225], Training Accuracy: 72.1742%, Training Loss: 0.6058%\n",
      "Epoch [32/300], Step [142/225], Training Accuracy: 72.1941%, Training Loss: 0.6052%\n",
      "Epoch [32/300], Step [143/225], Training Accuracy: 72.2356%, Training Loss: 0.6049%\n",
      "Epoch [32/300], Step [144/225], Training Accuracy: 72.2222%, Training Loss: 0.6045%\n",
      "Epoch [32/300], Step [145/225], Training Accuracy: 72.2091%, Training Loss: 0.6042%\n",
      "Epoch [32/300], Step [146/225], Training Accuracy: 72.2068%, Training Loss: 0.6042%\n",
      "Epoch [32/300], Step [147/225], Training Accuracy: 72.1939%, Training Loss: 0.6045%\n",
      "Epoch [32/300], Step [148/225], Training Accuracy: 72.2551%, Training Loss: 0.6034%\n",
      "Epoch [32/300], Step [149/225], Training Accuracy: 72.2420%, Training Loss: 0.6035%\n",
      "Epoch [32/300], Step [150/225], Training Accuracy: 72.2500%, Training Loss: 0.6031%\n",
      "Epoch [32/300], Step [151/225], Training Accuracy: 72.3096%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [152/225], Training Accuracy: 72.3067%, Training Loss: 0.6024%\n",
      "Epoch [32/300], Step [153/225], Training Accuracy: 72.2835%, Training Loss: 0.6024%\n",
      "Epoch [32/300], Step [154/225], Training Accuracy: 72.3011%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [155/225], Training Accuracy: 72.2480%, Training Loss: 0.6026%\n",
      "Epoch [32/300], Step [156/225], Training Accuracy: 72.2055%, Training Loss: 0.6031%\n",
      "Epoch [32/300], Step [157/225], Training Accuracy: 72.1935%, Training Loss: 0.6031%\n",
      "Epoch [32/300], Step [158/225], Training Accuracy: 72.0926%, Training Loss: 0.6044%\n",
      "Epoch [32/300], Step [159/225], Training Accuracy: 72.0814%, Training Loss: 0.6046%\n",
      "Epoch [32/300], Step [160/225], Training Accuracy: 72.0996%, Training Loss: 0.6043%\n",
      "Epoch [32/300], Step [161/225], Training Accuracy: 72.0885%, Training Loss: 0.6040%\n",
      "Epoch [32/300], Step [162/225], Training Accuracy: 72.1547%, Training Loss: 0.6029%\n",
      "Epoch [32/300], Step [163/225], Training Accuracy: 72.1338%, Training Loss: 0.6030%\n",
      "Epoch [32/300], Step [164/225], Training Accuracy: 72.1513%, Training Loss: 0.6023%\n",
      "Epoch [32/300], Step [165/225], Training Accuracy: 72.1780%, Training Loss: 0.6019%\n",
      "Epoch [32/300], Step [166/225], Training Accuracy: 72.1950%, Training Loss: 0.6014%\n",
      "Epoch [32/300], Step [167/225], Training Accuracy: 72.1557%, Training Loss: 0.6018%\n",
      "Epoch [32/300], Step [168/225], Training Accuracy: 72.1540%, Training Loss: 0.6018%\n",
      "Epoch [32/300], Step [169/225], Training Accuracy: 72.1339%, Training Loss: 0.6022%\n",
      "Epoch [32/300], Step [170/225], Training Accuracy: 72.1324%, Training Loss: 0.6022%\n",
      "Epoch [32/300], Step [171/225], Training Accuracy: 72.1674%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [172/225], Training Accuracy: 72.1839%, Training Loss: 0.6018%\n",
      "Epoch [32/300], Step [173/225], Training Accuracy: 72.1730%, Training Loss: 0.6017%\n",
      "Epoch [32/300], Step [174/225], Training Accuracy: 72.2073%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [175/225], Training Accuracy: 72.2143%, Training Loss: 0.6020%\n",
      "Epoch [32/300], Step [176/225], Training Accuracy: 72.2301%, Training Loss: 0.6015%\n",
      "Epoch [32/300], Step [177/225], Training Accuracy: 72.2281%, Training Loss: 0.6018%\n",
      "Epoch [32/300], Step [178/225], Training Accuracy: 72.2349%, Training Loss: 0.6016%\n",
      "Epoch [32/300], Step [179/225], Training Accuracy: 72.3202%, Training Loss: 0.6008%\n",
      "Epoch [32/300], Step [180/225], Training Accuracy: 72.3438%, Training Loss: 0.6005%\n",
      "Epoch [32/300], Step [181/225], Training Accuracy: 72.3239%, Training Loss: 0.6007%\n",
      "Epoch [32/300], Step [182/225], Training Accuracy: 72.3815%, Training Loss: 0.6003%\n",
      "Epoch [32/300], Step [183/225], Training Accuracy: 72.4129%, Training Loss: 0.6003%\n",
      "Epoch [32/300], Step [184/225], Training Accuracy: 72.4270%, Training Loss: 0.5997%\n",
      "Epoch [32/300], Step [185/225], Training Accuracy: 72.4662%, Training Loss: 0.5994%\n",
      "Epoch [32/300], Step [186/225], Training Accuracy: 72.4546%, Training Loss: 0.5989%\n",
      "Epoch [32/300], Step [187/225], Training Accuracy: 72.4515%, Training Loss: 0.5992%\n",
      "Epoch [32/300], Step [188/225], Training Accuracy: 72.4734%, Training Loss: 0.5988%\n",
      "Epoch [32/300], Step [189/225], Training Accuracy: 72.4868%, Training Loss: 0.5985%\n",
      "Epoch [32/300], Step [190/225], Training Accuracy: 72.4836%, Training Loss: 0.5981%\n",
      "Epoch [32/300], Step [191/225], Training Accuracy: 72.4722%, Training Loss: 0.5982%\n",
      "Epoch [32/300], Step [192/225], Training Accuracy: 72.4691%, Training Loss: 0.5977%\n",
      "Epoch [32/300], Step [193/225], Training Accuracy: 72.4417%, Training Loss: 0.5980%\n",
      "Epoch [32/300], Step [194/225], Training Accuracy: 72.4630%, Training Loss: 0.5976%\n",
      "Epoch [32/300], Step [195/225], Training Accuracy: 72.4760%, Training Loss: 0.5978%\n",
      "Epoch [32/300], Step [196/225], Training Accuracy: 72.4330%, Training Loss: 0.5980%\n",
      "Epoch [32/300], Step [197/225], Training Accuracy: 72.3985%, Training Loss: 0.5981%\n",
      "Epoch [32/300], Step [198/225], Training Accuracy: 72.4353%, Training Loss: 0.5972%\n",
      "Epoch [32/300], Step [199/225], Training Accuracy: 72.4246%, Training Loss: 0.5969%\n",
      "Epoch [32/300], Step [200/225], Training Accuracy: 72.3750%, Training Loss: 0.5973%\n",
      "Epoch [32/300], Step [201/225], Training Accuracy: 72.4036%, Training Loss: 0.5969%\n",
      "Epoch [32/300], Step [202/225], Training Accuracy: 72.4397%, Training Loss: 0.5963%\n",
      "Epoch [32/300], Step [203/225], Training Accuracy: 72.4523%, Training Loss: 0.5963%\n",
      "Epoch [32/300], Step [204/225], Training Accuracy: 72.4648%, Training Loss: 0.5962%\n",
      "Epoch [32/300], Step [205/225], Training Accuracy: 72.4924%, Training Loss: 0.5956%\n",
      "Epoch [32/300], Step [206/225], Training Accuracy: 72.5121%, Training Loss: 0.5955%\n",
      "Epoch [32/300], Step [207/225], Training Accuracy: 72.5166%, Training Loss: 0.5958%\n",
      "Epoch [32/300], Step [208/225], Training Accuracy: 72.5661%, Training Loss: 0.5951%\n",
      "Epoch [32/300], Step [209/225], Training Accuracy: 72.6002%, Training Loss: 0.5950%\n",
      "Epoch [32/300], Step [210/225], Training Accuracy: 72.5893%, Training Loss: 0.5953%\n",
      "Epoch [32/300], Step [211/225], Training Accuracy: 72.6377%, Training Loss: 0.5945%\n",
      "Epoch [32/300], Step [212/225], Training Accuracy: 72.6268%, Training Loss: 0.5949%\n",
      "Epoch [32/300], Step [213/225], Training Accuracy: 72.5866%, Training Loss: 0.5954%\n",
      "Epoch [32/300], Step [214/225], Training Accuracy: 72.5832%, Training Loss: 0.5952%\n",
      "Epoch [32/300], Step [215/225], Training Accuracy: 72.5872%, Training Loss: 0.5952%\n",
      "Epoch [32/300], Step [216/225], Training Accuracy: 72.5839%, Training Loss: 0.5952%\n",
      "Epoch [32/300], Step [217/225], Training Accuracy: 72.5878%, Training Loss: 0.5950%\n",
      "Epoch [32/300], Step [218/225], Training Accuracy: 72.5559%, Training Loss: 0.5953%\n",
      "Epoch [32/300], Step [219/225], Training Accuracy: 72.5599%, Training Loss: 0.5953%\n",
      "Epoch [32/300], Step [220/225], Training Accuracy: 72.5639%, Training Loss: 0.5954%\n",
      "Epoch [32/300], Step [221/225], Training Accuracy: 72.4972%, Training Loss: 0.5959%\n",
      "Epoch [32/300], Step [222/225], Training Accuracy: 72.5014%, Training Loss: 0.5955%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Step [223/225], Training Accuracy: 72.4566%, Training Loss: 0.5962%\n",
      "Epoch [32/300], Step [224/225], Training Accuracy: 72.4400%, Training Loss: 0.5962%\n",
      "Epoch [32/300], Step [225/225], Training Accuracy: 72.4152%, Training Loss: 0.5966%\n",
      "Epoch [33/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.5950%\n",
      "Epoch [33/300], Step [2/225], Training Accuracy: 72.6562%, Training Loss: 0.6123%\n",
      "Epoch [33/300], Step [3/225], Training Accuracy: 71.3542%, Training Loss: 0.6217%\n",
      "Epoch [33/300], Step [4/225], Training Accuracy: 71.8750%, Training Loss: 0.6217%\n",
      "Epoch [33/300], Step [5/225], Training Accuracy: 72.8125%, Training Loss: 0.6013%\n",
      "Epoch [33/300], Step [6/225], Training Accuracy: 72.6562%, Training Loss: 0.6047%\n",
      "Epoch [33/300], Step [7/225], Training Accuracy: 72.3214%, Training Loss: 0.5960%\n",
      "Epoch [33/300], Step [8/225], Training Accuracy: 73.0469%, Training Loss: 0.5921%\n",
      "Epoch [33/300], Step [9/225], Training Accuracy: 72.0486%, Training Loss: 0.6046%\n",
      "Epoch [33/300], Step [10/225], Training Accuracy: 71.8750%, Training Loss: 0.6155%\n",
      "Epoch [33/300], Step [11/225], Training Accuracy: 72.0170%, Training Loss: 0.6145%\n",
      "Epoch [33/300], Step [12/225], Training Accuracy: 71.8750%, Training Loss: 0.6157%\n",
      "Epoch [33/300], Step [13/225], Training Accuracy: 71.7548%, Training Loss: 0.6084%\n",
      "Epoch [33/300], Step [14/225], Training Accuracy: 71.8750%, Training Loss: 0.6068%\n",
      "Epoch [33/300], Step [15/225], Training Accuracy: 72.2917%, Training Loss: 0.6022%\n",
      "Epoch [33/300], Step [16/225], Training Accuracy: 72.4609%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [17/225], Training Accuracy: 72.7941%, Training Loss: 0.5958%\n",
      "Epoch [33/300], Step [18/225], Training Accuracy: 72.3958%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [19/225], Training Accuracy: 72.3684%, Training Loss: 0.5983%\n",
      "Epoch [33/300], Step [20/225], Training Accuracy: 72.4219%, Training Loss: 0.5994%\n",
      "Epoch [33/300], Step [21/225], Training Accuracy: 72.6190%, Training Loss: 0.5936%\n",
      "Epoch [33/300], Step [22/225], Training Accuracy: 72.3011%, Training Loss: 0.5978%\n",
      "Epoch [33/300], Step [23/225], Training Accuracy: 72.4185%, Training Loss: 0.5944%\n",
      "Epoch [33/300], Step [24/225], Training Accuracy: 72.3958%, Training Loss: 0.5964%\n",
      "Epoch [33/300], Step [25/225], Training Accuracy: 72.6875%, Training Loss: 0.5937%\n",
      "Epoch [33/300], Step [26/225], Training Accuracy: 72.7163%, Training Loss: 0.5923%\n",
      "Epoch [33/300], Step [27/225], Training Accuracy: 72.8009%, Training Loss: 0.5905%\n",
      "Epoch [33/300], Step [28/225], Training Accuracy: 73.1027%, Training Loss: 0.5852%\n",
      "Epoch [33/300], Step [29/225], Training Accuracy: 72.9526%, Training Loss: 0.5867%\n",
      "Epoch [33/300], Step [30/225], Training Accuracy: 73.0208%, Training Loss: 0.5850%\n",
      "Epoch [33/300], Step [31/225], Training Accuracy: 72.6310%, Training Loss: 0.5936%\n",
      "Epoch [33/300], Step [32/225], Training Accuracy: 72.6562%, Training Loss: 0.5923%\n",
      "Epoch [33/300], Step [33/225], Training Accuracy: 72.7746%, Training Loss: 0.5888%\n",
      "Epoch [33/300], Step [34/225], Training Accuracy: 72.7022%, Training Loss: 0.5903%\n",
      "Epoch [33/300], Step [35/225], Training Accuracy: 72.7232%, Training Loss: 0.5890%\n",
      "Epoch [33/300], Step [36/225], Training Accuracy: 72.6997%, Training Loss: 0.5903%\n",
      "Epoch [33/300], Step [37/225], Training Accuracy: 72.6351%, Training Loss: 0.5916%\n",
      "Epoch [33/300], Step [38/225], Training Accuracy: 72.6974%, Training Loss: 0.5915%\n",
      "Epoch [33/300], Step [39/225], Training Accuracy: 72.6763%, Training Loss: 0.5914%\n",
      "Epoch [33/300], Step [40/225], Training Accuracy: 72.6953%, Training Loss: 0.5919%\n",
      "Epoch [33/300], Step [41/225], Training Accuracy: 72.6753%, Training Loss: 0.5929%\n",
      "Epoch [33/300], Step [42/225], Training Accuracy: 72.6190%, Training Loss: 0.5915%\n",
      "Epoch [33/300], Step [43/225], Training Accuracy: 72.3110%, Training Loss: 0.5934%\n",
      "Epoch [33/300], Step [44/225], Training Accuracy: 72.4432%, Training Loss: 0.5917%\n",
      "Epoch [33/300], Step [45/225], Training Accuracy: 72.4653%, Training Loss: 0.5909%\n",
      "Epoch [33/300], Step [46/225], Training Accuracy: 72.4524%, Training Loss: 0.5912%\n",
      "Epoch [33/300], Step [47/225], Training Accuracy: 72.2407%, Training Loss: 0.5926%\n",
      "Epoch [33/300], Step [48/225], Training Accuracy: 72.2005%, Training Loss: 0.5931%\n",
      "Epoch [33/300], Step [49/225], Training Accuracy: 72.3214%, Training Loss: 0.5911%\n",
      "Epoch [33/300], Step [50/225], Training Accuracy: 72.3125%, Training Loss: 0.5912%\n",
      "Epoch [33/300], Step [51/225], Training Accuracy: 72.4265%, Training Loss: 0.5906%\n",
      "Epoch [33/300], Step [52/225], Training Accuracy: 72.6262%, Training Loss: 0.5887%\n",
      "Epoch [33/300], Step [53/225], Training Accuracy: 72.7300%, Training Loss: 0.5895%\n",
      "Epoch [33/300], Step [54/225], Training Accuracy: 72.6562%, Training Loss: 0.5913%\n",
      "Epoch [33/300], Step [55/225], Training Accuracy: 72.5284%, Training Loss: 0.5932%\n",
      "Epoch [33/300], Step [56/225], Training Accuracy: 72.5167%, Training Loss: 0.5934%\n",
      "Epoch [33/300], Step [57/225], Training Accuracy: 72.4781%, Training Loss: 0.5932%\n",
      "Epoch [33/300], Step [58/225], Training Accuracy: 72.4407%, Training Loss: 0.5950%\n",
      "Epoch [33/300], Step [59/225], Training Accuracy: 72.4311%, Training Loss: 0.5949%\n",
      "Epoch [33/300], Step [60/225], Training Accuracy: 72.5000%, Training Loss: 0.5936%\n",
      "Epoch [33/300], Step [61/225], Training Accuracy: 72.4385%, Training Loss: 0.5949%\n",
      "Epoch [33/300], Step [62/225], Training Accuracy: 72.3790%, Training Loss: 0.5951%\n",
      "Epoch [33/300], Step [63/225], Training Accuracy: 72.3462%, Training Loss: 0.5970%\n",
      "Epoch [33/300], Step [64/225], Training Accuracy: 72.4854%, Training Loss: 0.5945%\n",
      "Epoch [33/300], Step [65/225], Training Accuracy: 72.4519%, Training Loss: 0.5951%\n",
      "Epoch [33/300], Step [66/225], Training Accuracy: 72.5379%, Training Loss: 0.5935%\n",
      "Epoch [33/300], Step [67/225], Training Accuracy: 72.4813%, Training Loss: 0.5937%\n",
      "Epoch [33/300], Step [68/225], Training Accuracy: 72.3116%, Training Loss: 0.5951%\n",
      "Epoch [33/300], Step [69/225], Training Accuracy: 72.1920%, Training Loss: 0.5962%\n",
      "Epoch [33/300], Step [70/225], Training Accuracy: 72.1205%, Training Loss: 0.5966%\n",
      "Epoch [33/300], Step [71/225], Training Accuracy: 72.1171%, Training Loss: 0.5954%\n",
      "Epoch [33/300], Step [72/225], Training Accuracy: 72.1788%, Training Loss: 0.5950%\n",
      "Epoch [33/300], Step [73/225], Training Accuracy: 72.1961%, Training Loss: 0.5953%\n",
      "Epoch [33/300], Step [74/225], Training Accuracy: 72.2762%, Training Loss: 0.5943%\n",
      "Epoch [33/300], Step [75/225], Training Accuracy: 72.3333%, Training Loss: 0.5942%\n",
      "Epoch [33/300], Step [76/225], Training Accuracy: 72.2039%, Training Loss: 0.5970%\n",
      "Epoch [33/300], Step [77/225], Training Accuracy: 72.1794%, Training Loss: 0.5973%\n",
      "Epoch [33/300], Step [78/225], Training Accuracy: 72.1554%, Training Loss: 0.5987%\n",
      "Epoch [33/300], Step [79/225], Training Accuracy: 72.0926%, Training Loss: 0.5982%\n",
      "Epoch [33/300], Step [80/225], Training Accuracy: 72.0898%, Training Loss: 0.5987%\n",
      "Epoch [33/300], Step [81/225], Training Accuracy: 72.1258%, Training Loss: 0.5983%\n",
      "Epoch [33/300], Step [82/225], Training Accuracy: 72.2561%, Training Loss: 0.5971%\n",
      "Epoch [33/300], Step [83/225], Training Accuracy: 72.2327%, Training Loss: 0.5974%\n",
      "Epoch [33/300], Step [84/225], Training Accuracy: 72.2842%, Training Loss: 0.5964%\n",
      "Epoch [33/300], Step [85/225], Training Accuracy: 72.3346%, Training Loss: 0.5951%\n",
      "Epoch [33/300], Step [86/225], Training Accuracy: 72.3837%, Training Loss: 0.5947%\n",
      "Epoch [33/300], Step [87/225], Training Accuracy: 72.3779%, Training Loss: 0.5949%\n",
      "Epoch [33/300], Step [88/225], Training Accuracy: 72.3011%, Training Loss: 0.5963%\n",
      "Epoch [33/300], Step [89/225], Training Accuracy: 72.3139%, Training Loss: 0.5962%\n",
      "Epoch [33/300], Step [90/225], Training Accuracy: 72.2917%, Training Loss: 0.5974%\n",
      "Epoch [33/300], Step [91/225], Training Accuracy: 72.2184%, Training Loss: 0.5981%\n",
      "Epoch [33/300], Step [92/225], Training Accuracy: 72.1977%, Training Loss: 0.5986%\n",
      "Epoch [33/300], Step [93/225], Training Accuracy: 72.2950%, Training Loss: 0.5973%\n",
      "Epoch [33/300], Step [94/225], Training Accuracy: 72.2573%, Training Loss: 0.5970%\n",
      "Epoch [33/300], Step [95/225], Training Accuracy: 72.2039%, Training Loss: 0.5981%\n",
      "Epoch [33/300], Step [96/225], Training Accuracy: 72.2493%, Training Loss: 0.5973%\n",
      "Epoch [33/300], Step [97/225], Training Accuracy: 72.2777%, Training Loss: 0.5965%\n",
      "Epoch [33/300], Step [98/225], Training Accuracy: 72.2736%, Training Loss: 0.5973%\n",
      "Epoch [33/300], Step [99/225], Training Accuracy: 72.3327%, Training Loss: 0.5968%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Step [100/225], Training Accuracy: 72.2812%, Training Loss: 0.5981%\n",
      "Epoch [33/300], Step [101/225], Training Accuracy: 72.2772%, Training Loss: 0.5985%\n",
      "Epoch [33/300], Step [102/225], Training Accuracy: 72.2120%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [103/225], Training Accuracy: 72.2542%, Training Loss: 0.5981%\n",
      "Epoch [33/300], Step [104/225], Training Accuracy: 72.2055%, Training Loss: 0.5983%\n",
      "Epoch [33/300], Step [105/225], Training Accuracy: 72.2024%, Training Loss: 0.5974%\n",
      "Epoch [33/300], Step [106/225], Training Accuracy: 72.2583%, Training Loss: 0.5969%\n",
      "Epoch [33/300], Step [107/225], Training Accuracy: 72.1817%, Training Loss: 0.5976%\n",
      "Epoch [33/300], Step [108/225], Training Accuracy: 72.1788%, Training Loss: 0.5989%\n",
      "Epoch [33/300], Step [109/225], Training Accuracy: 72.1044%, Training Loss: 0.5994%\n",
      "Epoch [33/300], Step [110/225], Training Accuracy: 72.0739%, Training Loss: 0.5994%\n",
      "Epoch [33/300], Step [111/225], Training Accuracy: 72.0298%, Training Loss: 0.5989%\n",
      "Epoch [33/300], Step [112/225], Training Accuracy: 72.0145%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [113/225], Training Accuracy: 72.1101%, Training Loss: 0.5979%\n",
      "Epoch [33/300], Step [114/225], Training Accuracy: 72.1491%, Training Loss: 0.5973%\n",
      "Epoch [33/300], Step [115/225], Training Accuracy: 72.1060%, Training Loss: 0.5973%\n",
      "Epoch [33/300], Step [116/225], Training Accuracy: 72.0636%, Training Loss: 0.5977%\n",
      "Epoch [33/300], Step [117/225], Training Accuracy: 71.9818%, Training Loss: 0.5985%\n",
      "Epoch [33/300], Step [118/225], Training Accuracy: 71.9677%, Training Loss: 0.5986%\n",
      "Epoch [33/300], Step [119/225], Training Accuracy: 71.9932%, Training Loss: 0.5985%\n",
      "Epoch [33/300], Step [120/225], Training Accuracy: 72.0312%, Training Loss: 0.5982%\n",
      "Epoch [33/300], Step [121/225], Training Accuracy: 71.9654%, Training Loss: 0.5986%\n",
      "Epoch [33/300], Step [122/225], Training Accuracy: 71.9518%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [123/225], Training Accuracy: 71.9512%, Training Loss: 0.5991%\n",
      "Epoch [33/300], Step [124/225], Training Accuracy: 71.9254%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [125/225], Training Accuracy: 71.8875%, Training Loss: 0.5988%\n",
      "Epoch [33/300], Step [126/225], Training Accuracy: 71.9122%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [127/225], Training Accuracy: 71.9119%, Training Loss: 0.5992%\n",
      "Epoch [33/300], Step [128/225], Training Accuracy: 71.8628%, Training Loss: 0.6005%\n",
      "Epoch [33/300], Step [129/225], Training Accuracy: 71.8387%, Training Loss: 0.6010%\n",
      "Epoch [33/300], Step [130/225], Training Accuracy: 71.8510%, Training Loss: 0.6014%\n",
      "Epoch [33/300], Step [131/225], Training Accuracy: 71.8750%, Training Loss: 0.6008%\n",
      "Epoch [33/300], Step [132/225], Training Accuracy: 71.9105%, Training Loss: 0.6006%\n",
      "Epoch [33/300], Step [133/225], Training Accuracy: 71.8867%, Training Loss: 0.6006%\n",
      "Epoch [33/300], Step [134/225], Training Accuracy: 71.8517%, Training Loss: 0.6017%\n",
      "Epoch [33/300], Step [135/225], Training Accuracy: 71.8287%, Training Loss: 0.6016%\n",
      "Epoch [33/300], Step [136/225], Training Accuracy: 71.8980%, Training Loss: 0.6010%\n",
      "Epoch [33/300], Step [137/225], Training Accuracy: 71.9092%, Training Loss: 0.6007%\n",
      "Epoch [33/300], Step [138/225], Training Accuracy: 71.9995%, Training Loss: 0.5994%\n",
      "Epoch [33/300], Step [139/225], Training Accuracy: 71.9762%, Training Loss: 0.6002%\n",
      "Epoch [33/300], Step [140/225], Training Accuracy: 72.0089%, Training Loss: 0.5998%\n",
      "Epoch [33/300], Step [141/225], Training Accuracy: 71.9526%, Training Loss: 0.6001%\n",
      "Epoch [33/300], Step [142/225], Training Accuracy: 71.9850%, Training Loss: 0.5997%\n",
      "Epoch [33/300], Step [143/225], Training Accuracy: 72.0061%, Training Loss: 0.5996%\n",
      "Epoch [33/300], Step [144/225], Training Accuracy: 71.9835%, Training Loss: 0.5998%\n",
      "Epoch [33/300], Step [145/225], Training Accuracy: 71.9935%, Training Loss: 0.5999%\n",
      "Epoch [33/300], Step [146/225], Training Accuracy: 72.0034%, Training Loss: 0.6002%\n",
      "Epoch [33/300], Step [147/225], Training Accuracy: 71.9600%, Training Loss: 0.6006%\n",
      "Epoch [33/300], Step [148/225], Training Accuracy: 71.9700%, Training Loss: 0.6007%\n",
      "Epoch [33/300], Step [149/225], Training Accuracy: 71.9904%, Training Loss: 0.6006%\n",
      "Epoch [33/300], Step [150/225], Training Accuracy: 71.9688%, Training Loss: 0.6005%\n",
      "Epoch [33/300], Step [151/225], Training Accuracy: 72.0302%, Training Loss: 0.6001%\n",
      "Epoch [33/300], Step [152/225], Training Accuracy: 71.9778%, Training Loss: 0.6011%\n",
      "Epoch [33/300], Step [153/225], Training Accuracy: 71.9669%, Training Loss: 0.6013%\n",
      "Epoch [33/300], Step [154/225], Training Accuracy: 72.0170%, Training Loss: 0.6009%\n",
      "Epoch [33/300], Step [155/225], Training Accuracy: 72.0060%, Training Loss: 0.6012%\n",
      "Epoch [33/300], Step [156/225], Training Accuracy: 71.9551%, Training Loss: 0.6021%\n",
      "Epoch [33/300], Step [157/225], Training Accuracy: 71.9646%, Training Loss: 0.6019%\n",
      "Epoch [33/300], Step [158/225], Training Accuracy: 71.9146%, Training Loss: 0.6025%\n",
      "Epoch [33/300], Step [159/225], Training Accuracy: 71.8652%, Training Loss: 0.6032%\n",
      "Epoch [33/300], Step [160/225], Training Accuracy: 71.8555%, Training Loss: 0.6032%\n",
      "Epoch [33/300], Step [161/225], Training Accuracy: 71.8362%, Training Loss: 0.6030%\n",
      "Epoch [33/300], Step [162/225], Training Accuracy: 71.9329%, Training Loss: 0.6017%\n",
      "Epoch [33/300], Step [163/225], Training Accuracy: 71.9709%, Training Loss: 0.6016%\n",
      "Epoch [33/300], Step [164/225], Training Accuracy: 72.0370%, Training Loss: 0.6008%\n",
      "Epoch [33/300], Step [165/225], Training Accuracy: 72.0265%, Training Loss: 0.6008%\n",
      "Epoch [33/300], Step [166/225], Training Accuracy: 72.0350%, Training Loss: 0.6005%\n",
      "Epoch [33/300], Step [167/225], Training Accuracy: 72.0060%, Training Loss: 0.6008%\n",
      "Epoch [33/300], Step [168/225], Training Accuracy: 72.0052%, Training Loss: 0.6009%\n",
      "Epoch [33/300], Step [169/225], Training Accuracy: 72.0044%, Training Loss: 0.6012%\n",
      "Epoch [33/300], Step [170/225], Training Accuracy: 72.0588%, Training Loss: 0.6010%\n",
      "Epoch [33/300], Step [171/225], Training Accuracy: 72.0943%, Training Loss: 0.6008%\n",
      "Epoch [33/300], Step [172/225], Training Accuracy: 72.0839%, Training Loss: 0.6009%\n",
      "Epoch [33/300], Step [173/225], Training Accuracy: 72.1098%, Training Loss: 0.6007%\n",
      "Epoch [33/300], Step [174/225], Training Accuracy: 72.1085%, Training Loss: 0.6008%\n",
      "Epoch [33/300], Step [175/225], Training Accuracy: 72.1339%, Training Loss: 0.6001%\n",
      "Epoch [33/300], Step [176/225], Training Accuracy: 72.1413%, Training Loss: 0.5998%\n",
      "Epoch [33/300], Step [177/225], Training Accuracy: 72.1575%, Training Loss: 0.5994%\n",
      "Epoch [33/300], Step [178/225], Training Accuracy: 72.1735%, Training Loss: 0.5993%\n",
      "Epoch [33/300], Step [179/225], Training Accuracy: 72.1805%, Training Loss: 0.5988%\n",
      "Epoch [33/300], Step [180/225], Training Accuracy: 72.1788%, Training Loss: 0.5986%\n",
      "Epoch [33/300], Step [181/225], Training Accuracy: 72.1599%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [182/225], Training Accuracy: 72.1497%, Training Loss: 0.5993%\n",
      "Epoch [33/300], Step [183/225], Training Accuracy: 72.1226%, Training Loss: 0.5997%\n",
      "Epoch [33/300], Step [184/225], Training Accuracy: 72.1213%, Training Loss: 0.5990%\n",
      "Epoch [33/300], Step [185/225], Training Accuracy: 72.1622%, Training Loss: 0.5984%\n",
      "Epoch [33/300], Step [186/225], Training Accuracy: 72.1690%, Training Loss: 0.5979%\n",
      "Epoch [33/300], Step [187/225], Training Accuracy: 72.1591%, Training Loss: 0.5982%\n",
      "Epoch [33/300], Step [188/225], Training Accuracy: 72.1991%, Training Loss: 0.5974%\n",
      "Epoch [33/300], Step [189/225], Training Accuracy: 72.1974%, Training Loss: 0.5977%\n",
      "Epoch [33/300], Step [190/225], Training Accuracy: 72.2286%, Training Loss: 0.5977%\n",
      "Epoch [33/300], Step [191/225], Training Accuracy: 72.2677%, Training Loss: 0.5972%\n",
      "Epoch [33/300], Step [192/225], Training Accuracy: 72.3145%, Training Loss: 0.5966%\n",
      "Epoch [33/300], Step [193/225], Training Accuracy: 72.3122%, Training Loss: 0.5969%\n",
      "Epoch [33/300], Step [194/225], Training Accuracy: 72.3341%, Training Loss: 0.5969%\n",
      "Epoch [33/300], Step [195/225], Training Accuracy: 72.3478%, Training Loss: 0.5969%\n",
      "Epoch [33/300], Step [196/225], Training Accuracy: 72.3055%, Training Loss: 0.5972%\n",
      "Epoch [33/300], Step [197/225], Training Accuracy: 72.3112%, Training Loss: 0.5971%\n",
      "Epoch [33/300], Step [198/225], Training Accuracy: 72.3248%, Training Loss: 0.5964%\n",
      "Epoch [33/300], Step [199/225], Training Accuracy: 72.3618%, Training Loss: 0.5961%\n",
      "Epoch [33/300], Step [200/225], Training Accuracy: 72.3594%, Training Loss: 0.5963%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Step [201/225], Training Accuracy: 72.3570%, Training Loss: 0.5962%\n",
      "Epoch [33/300], Step [202/225], Training Accuracy: 72.3700%, Training Loss: 0.5957%\n",
      "Epoch [33/300], Step [203/225], Training Accuracy: 72.3984%, Training Loss: 0.5953%\n",
      "Epoch [33/300], Step [204/225], Training Accuracy: 72.4341%, Training Loss: 0.5951%\n",
      "Epoch [33/300], Step [205/225], Training Accuracy: 72.4619%, Training Loss: 0.5946%\n",
      "Epoch [33/300], Step [206/225], Training Accuracy: 72.5121%, Training Loss: 0.5943%\n",
      "Epoch [33/300], Step [207/225], Training Accuracy: 72.5317%, Training Loss: 0.5943%\n",
      "Epoch [33/300], Step [208/225], Training Accuracy: 72.5586%, Training Loss: 0.5937%\n",
      "Epoch [33/300], Step [209/225], Training Accuracy: 72.5478%, Training Loss: 0.5939%\n",
      "Epoch [33/300], Step [210/225], Training Accuracy: 72.5521%, Training Loss: 0.5943%\n",
      "Epoch [33/300], Step [211/225], Training Accuracy: 72.6007%, Training Loss: 0.5939%\n",
      "Epoch [33/300], Step [212/225], Training Accuracy: 72.5678%, Training Loss: 0.5943%\n",
      "Epoch [33/300], Step [213/225], Training Accuracy: 72.5279%, Training Loss: 0.5945%\n",
      "Epoch [33/300], Step [214/225], Training Accuracy: 72.5321%, Training Loss: 0.5942%\n",
      "Epoch [33/300], Step [215/225], Training Accuracy: 72.5291%, Training Loss: 0.5941%\n",
      "Epoch [33/300], Step [216/225], Training Accuracy: 72.5260%, Training Loss: 0.5942%\n",
      "Epoch [33/300], Step [217/225], Training Accuracy: 72.5302%, Training Loss: 0.5950%\n",
      "Epoch [33/300], Step [218/225], Training Accuracy: 72.5774%, Training Loss: 0.5948%\n",
      "Epoch [33/300], Step [219/225], Training Accuracy: 72.5742%, Training Loss: 0.5947%\n",
      "Epoch [33/300], Step [220/225], Training Accuracy: 72.5426%, Training Loss: 0.5950%\n",
      "Epoch [33/300], Step [221/225], Training Accuracy: 72.5113%, Training Loss: 0.5954%\n",
      "Epoch [33/300], Step [222/225], Training Accuracy: 72.5436%, Training Loss: 0.5950%\n",
      "Epoch [33/300], Step [223/225], Training Accuracy: 72.5056%, Training Loss: 0.5959%\n",
      "Epoch [33/300], Step [224/225], Training Accuracy: 72.4958%, Training Loss: 0.5960%\n",
      "Epoch [33/300], Step [225/225], Training Accuracy: 72.5056%, Training Loss: 0.5957%\n",
      "Epoch [34/300], Step [1/225], Training Accuracy: 78.1250%, Training Loss: 0.5006%\n",
      "Epoch [34/300], Step [2/225], Training Accuracy: 72.6562%, Training Loss: 0.5549%\n",
      "Epoch [34/300], Step [3/225], Training Accuracy: 71.8750%, Training Loss: 0.5825%\n",
      "Epoch [34/300], Step [4/225], Training Accuracy: 72.2656%, Training Loss: 0.5966%\n",
      "Epoch [34/300], Step [5/225], Training Accuracy: 74.0625%, Training Loss: 0.5826%\n",
      "Epoch [34/300], Step [6/225], Training Accuracy: 73.1771%, Training Loss: 0.5803%\n",
      "Epoch [34/300], Step [7/225], Training Accuracy: 72.7679%, Training Loss: 0.5833%\n",
      "Epoch [34/300], Step [8/225], Training Accuracy: 73.0469%, Training Loss: 0.5855%\n",
      "Epoch [34/300], Step [9/225], Training Accuracy: 72.2222%, Training Loss: 0.6047%\n",
      "Epoch [34/300], Step [10/225], Training Accuracy: 72.3438%, Training Loss: 0.6052%\n",
      "Epoch [34/300], Step [11/225], Training Accuracy: 72.7273%, Training Loss: 0.6064%\n",
      "Epoch [34/300], Step [12/225], Training Accuracy: 72.7865%, Training Loss: 0.6075%\n",
      "Epoch [34/300], Step [13/225], Training Accuracy: 73.3173%, Training Loss: 0.5950%\n",
      "Epoch [34/300], Step [14/225], Training Accuracy: 73.7723%, Training Loss: 0.5886%\n",
      "Epoch [34/300], Step [15/225], Training Accuracy: 74.0625%, Training Loss: 0.5847%\n",
      "Epoch [34/300], Step [16/225], Training Accuracy: 74.7070%, Training Loss: 0.5788%\n",
      "Epoch [34/300], Step [17/225], Training Accuracy: 74.7243%, Training Loss: 0.5766%\n",
      "Epoch [34/300], Step [18/225], Training Accuracy: 74.4792%, Training Loss: 0.5786%\n",
      "Epoch [34/300], Step [19/225], Training Accuracy: 74.7533%, Training Loss: 0.5774%\n",
      "Epoch [34/300], Step [20/225], Training Accuracy: 74.6094%, Training Loss: 0.5736%\n",
      "Epoch [34/300], Step [21/225], Training Accuracy: 74.7024%, Training Loss: 0.5700%\n",
      "Epoch [34/300], Step [22/225], Training Accuracy: 74.3608%, Training Loss: 0.5751%\n",
      "Epoch [34/300], Step [23/225], Training Accuracy: 74.7283%, Training Loss: 0.5690%\n",
      "Epoch [34/300], Step [24/225], Training Accuracy: 74.5443%, Training Loss: 0.5697%\n",
      "Epoch [34/300], Step [25/225], Training Accuracy: 74.6875%, Training Loss: 0.5706%\n",
      "Epoch [34/300], Step [26/225], Training Accuracy: 74.5793%, Training Loss: 0.5726%\n",
      "Epoch [34/300], Step [27/225], Training Accuracy: 74.2477%, Training Loss: 0.5746%\n",
      "Epoch [34/300], Step [28/225], Training Accuracy: 74.3862%, Training Loss: 0.5692%\n",
      "Epoch [34/300], Step [29/225], Training Accuracy: 74.1918%, Training Loss: 0.5700%\n",
      "Epoch [34/300], Step [30/225], Training Accuracy: 74.2708%, Training Loss: 0.5699%\n",
      "Epoch [34/300], Step [31/225], Training Accuracy: 73.6895%, Training Loss: 0.5776%\n",
      "Epoch [34/300], Step [32/225], Training Accuracy: 73.7305%, Training Loss: 0.5779%\n",
      "Epoch [34/300], Step [33/225], Training Accuracy: 73.7689%, Training Loss: 0.5760%\n",
      "Epoch [34/300], Step [34/225], Training Accuracy: 73.5294%, Training Loss: 0.5765%\n",
      "Epoch [34/300], Step [35/225], Training Accuracy: 73.6161%, Training Loss: 0.5761%\n",
      "Epoch [34/300], Step [36/225], Training Accuracy: 73.6979%, Training Loss: 0.5747%\n",
      "Epoch [34/300], Step [37/225], Training Accuracy: 73.6064%, Training Loss: 0.5729%\n",
      "Epoch [34/300], Step [38/225], Training Accuracy: 73.4786%, Training Loss: 0.5731%\n",
      "Epoch [34/300], Step [39/225], Training Accuracy: 73.4375%, Training Loss: 0.5742%\n",
      "Epoch [34/300], Step [40/225], Training Accuracy: 73.4375%, Training Loss: 0.5744%\n",
      "Epoch [34/300], Step [41/225], Training Accuracy: 73.3232%, Training Loss: 0.5770%\n",
      "Epoch [34/300], Step [42/225], Training Accuracy: 73.4003%, Training Loss: 0.5763%\n",
      "Epoch [34/300], Step [43/225], Training Accuracy: 73.2922%, Training Loss: 0.5780%\n",
      "Epoch [34/300], Step [44/225], Training Accuracy: 73.3310%, Training Loss: 0.5771%\n",
      "Epoch [34/300], Step [45/225], Training Accuracy: 73.2986%, Training Loss: 0.5771%\n",
      "Epoch [34/300], Step [46/225], Training Accuracy: 73.3696%, Training Loss: 0.5754%\n",
      "Epoch [34/300], Step [47/225], Training Accuracy: 73.2380%, Training Loss: 0.5792%\n",
      "Epoch [34/300], Step [48/225], Training Accuracy: 73.2096%, Training Loss: 0.5811%\n",
      "Epoch [34/300], Step [49/225], Training Accuracy: 73.3418%, Training Loss: 0.5797%\n",
      "Epoch [34/300], Step [50/225], Training Accuracy: 73.4375%, Training Loss: 0.5784%\n",
      "Epoch [34/300], Step [51/225], Training Accuracy: 73.4375%, Training Loss: 0.5777%\n",
      "Epoch [34/300], Step [52/225], Training Accuracy: 73.4976%, Training Loss: 0.5766%\n",
      "Epoch [34/300], Step [53/225], Training Accuracy: 73.4670%, Training Loss: 0.5779%\n",
      "Epoch [34/300], Step [54/225], Training Accuracy: 73.3507%, Training Loss: 0.5786%\n",
      "Epoch [34/300], Step [55/225], Training Accuracy: 73.1534%, Training Loss: 0.5806%\n",
      "Epoch [34/300], Step [56/225], Training Accuracy: 73.1585%, Training Loss: 0.5809%\n",
      "Epoch [34/300], Step [57/225], Training Accuracy: 73.1634%, Training Loss: 0.5806%\n",
      "Epoch [34/300], Step [58/225], Training Accuracy: 73.0603%, Training Loss: 0.5801%\n",
      "Epoch [34/300], Step [59/225], Training Accuracy: 73.1727%, Training Loss: 0.5784%\n",
      "Epoch [34/300], Step [60/225], Training Accuracy: 73.2031%, Training Loss: 0.5769%\n",
      "Epoch [34/300], Step [61/225], Training Accuracy: 73.2070%, Training Loss: 0.5781%\n",
      "Epoch [34/300], Step [62/225], Training Accuracy: 73.0847%, Training Loss: 0.5791%\n",
      "Epoch [34/300], Step [63/225], Training Accuracy: 72.9415%, Training Loss: 0.5807%\n",
      "Epoch [34/300], Step [64/225], Training Accuracy: 72.9492%, Training Loss: 0.5798%\n",
      "Epoch [34/300], Step [65/225], Training Accuracy: 72.8846%, Training Loss: 0.5798%\n",
      "Epoch [34/300], Step [66/225], Training Accuracy: 73.0587%, Training Loss: 0.5773%\n",
      "Epoch [34/300], Step [67/225], Training Accuracy: 73.0177%, Training Loss: 0.5769%\n",
      "Epoch [34/300], Step [68/225], Training Accuracy: 72.9320%, Training Loss: 0.5775%\n",
      "Epoch [34/300], Step [69/225], Training Accuracy: 72.7582%, Training Loss: 0.5803%\n",
      "Epoch [34/300], Step [70/225], Training Accuracy: 72.6786%, Training Loss: 0.5802%\n",
      "Epoch [34/300], Step [71/225], Training Accuracy: 72.6232%, Training Loss: 0.5795%\n",
      "Epoch [34/300], Step [72/225], Training Accuracy: 72.6128%, Training Loss: 0.5788%\n",
      "Epoch [34/300], Step [73/225], Training Accuracy: 72.5385%, Training Loss: 0.5798%\n",
      "Epoch [34/300], Step [74/225], Training Accuracy: 72.4662%, Training Loss: 0.5803%\n",
      "Epoch [34/300], Step [75/225], Training Accuracy: 72.4792%, Training Loss: 0.5795%\n",
      "Epoch [34/300], Step [76/225], Training Accuracy: 72.4301%, Training Loss: 0.5805%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Step [77/225], Training Accuracy: 72.5244%, Training Loss: 0.5799%\n",
      "Epoch [34/300], Step [78/225], Training Accuracy: 72.3758%, Training Loss: 0.5811%\n",
      "Epoch [34/300], Step [79/225], Training Accuracy: 72.3892%, Training Loss: 0.5800%\n",
      "Epoch [34/300], Step [80/225], Training Accuracy: 72.4023%, Training Loss: 0.5802%\n",
      "Epoch [34/300], Step [81/225], Training Accuracy: 72.4730%, Training Loss: 0.5792%\n",
      "Epoch [34/300], Step [82/225], Training Accuracy: 72.5229%, Training Loss: 0.5784%\n",
      "Epoch [34/300], Step [83/225], Training Accuracy: 72.4962%, Training Loss: 0.5779%\n",
      "Epoch [34/300], Step [84/225], Training Accuracy: 72.5632%, Training Loss: 0.5772%\n",
      "Epoch [34/300], Step [85/225], Training Accuracy: 72.6103%, Training Loss: 0.5762%\n",
      "Epoch [34/300], Step [86/225], Training Accuracy: 72.6017%, Training Loss: 0.5769%\n",
      "Epoch [34/300], Step [87/225], Training Accuracy: 72.5754%, Training Loss: 0.5774%\n",
      "Epoch [34/300], Step [88/225], Training Accuracy: 72.5675%, Training Loss: 0.5774%\n",
      "Epoch [34/300], Step [89/225], Training Accuracy: 72.5772%, Training Loss: 0.5777%\n",
      "Epoch [34/300], Step [90/225], Training Accuracy: 72.6389%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [91/225], Training Accuracy: 72.6305%, Training Loss: 0.5783%\n",
      "Epoch [34/300], Step [92/225], Training Accuracy: 72.6562%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [93/225], Training Accuracy: 72.7151%, Training Loss: 0.5775%\n",
      "Epoch [34/300], Step [94/225], Training Accuracy: 72.6895%, Training Loss: 0.5774%\n",
      "Epoch [34/300], Step [95/225], Training Accuracy: 72.6480%, Training Loss: 0.5778%\n",
      "Epoch [34/300], Step [96/225], Training Accuracy: 72.6888%, Training Loss: 0.5768%\n",
      "Epoch [34/300], Step [97/225], Training Accuracy: 72.7287%, Training Loss: 0.5763%\n",
      "Epoch [34/300], Step [98/225], Training Accuracy: 72.7360%, Training Loss: 0.5774%\n",
      "Epoch [34/300], Step [99/225], Training Accuracy: 72.7431%, Training Loss: 0.5783%\n",
      "Epoch [34/300], Step [100/225], Training Accuracy: 72.6250%, Training Loss: 0.5803%\n",
      "Epoch [34/300], Step [101/225], Training Accuracy: 72.6485%, Training Loss: 0.5802%\n",
      "Epoch [34/300], Step [102/225], Training Accuracy: 72.5184%, Training Loss: 0.5820%\n",
      "Epoch [34/300], Step [103/225], Training Accuracy: 72.5576%, Training Loss: 0.5817%\n",
      "Epoch [34/300], Step [104/225], Training Accuracy: 72.5060%, Training Loss: 0.5831%\n",
      "Epoch [34/300], Step [105/225], Training Accuracy: 72.5446%, Training Loss: 0.5823%\n",
      "Epoch [34/300], Step [106/225], Training Accuracy: 72.6268%, Training Loss: 0.5823%\n",
      "Epoch [34/300], Step [107/225], Training Accuracy: 72.5467%, Training Loss: 0.5839%\n",
      "Epoch [34/300], Step [108/225], Training Accuracy: 72.5260%, Training Loss: 0.5839%\n",
      "Epoch [34/300], Step [109/225], Training Accuracy: 72.5057%, Training Loss: 0.5840%\n",
      "Epoch [34/300], Step [110/225], Training Accuracy: 72.4574%, Training Loss: 0.5842%\n",
      "Epoch [34/300], Step [111/225], Training Accuracy: 72.5366%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [112/225], Training Accuracy: 72.5307%, Training Loss: 0.5839%\n",
      "Epoch [34/300], Step [113/225], Training Accuracy: 72.5249%, Training Loss: 0.5840%\n",
      "Epoch [34/300], Step [114/225], Training Accuracy: 72.5055%, Training Loss: 0.5842%\n",
      "Epoch [34/300], Step [115/225], Training Accuracy: 72.6223%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [116/225], Training Accuracy: 72.6024%, Training Loss: 0.5832%\n",
      "Epoch [34/300], Step [117/225], Training Accuracy: 72.5027%, Training Loss: 0.5844%\n",
      "Epoch [34/300], Step [118/225], Training Accuracy: 72.5106%, Training Loss: 0.5841%\n",
      "Epoch [34/300], Step [119/225], Training Accuracy: 72.5053%, Training Loss: 0.5844%\n",
      "Epoch [34/300], Step [120/225], Training Accuracy: 72.5260%, Training Loss: 0.5842%\n",
      "Epoch [34/300], Step [121/225], Training Accuracy: 72.5207%, Training Loss: 0.5847%\n",
      "Epoch [34/300], Step [122/225], Training Accuracy: 72.5154%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [123/225], Training Accuracy: 72.4721%, Training Loss: 0.5856%\n",
      "Epoch [34/300], Step [124/225], Training Accuracy: 72.4798%, Training Loss: 0.5855%\n",
      "Epoch [34/300], Step [125/225], Training Accuracy: 72.4875%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [126/225], Training Accuracy: 72.5322%, Training Loss: 0.5849%\n",
      "Epoch [34/300], Step [127/225], Training Accuracy: 72.5025%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [128/225], Training Accuracy: 72.3999%, Training Loss: 0.5872%\n",
      "Epoch [34/300], Step [129/225], Training Accuracy: 72.3958%, Training Loss: 0.5873%\n",
      "Epoch [34/300], Step [130/225], Training Accuracy: 72.3678%, Training Loss: 0.5873%\n",
      "Epoch [34/300], Step [131/225], Training Accuracy: 72.3879%, Training Loss: 0.5869%\n",
      "Epoch [34/300], Step [132/225], Training Accuracy: 72.3958%, Training Loss: 0.5869%\n",
      "Epoch [34/300], Step [133/225], Training Accuracy: 72.4154%, Training Loss: 0.5864%\n",
      "Epoch [34/300], Step [134/225], Training Accuracy: 72.4114%, Training Loss: 0.5880%\n",
      "Epoch [34/300], Step [135/225], Training Accuracy: 72.4190%, Training Loss: 0.5877%\n",
      "Epoch [34/300], Step [136/225], Training Accuracy: 72.4494%, Training Loss: 0.5871%\n",
      "Epoch [34/300], Step [137/225], Training Accuracy: 72.4224%, Training Loss: 0.5871%\n",
      "Epoch [34/300], Step [138/225], Training Accuracy: 72.5317%, Training Loss: 0.5856%\n",
      "Epoch [34/300], Step [139/225], Training Accuracy: 72.5382%, Training Loss: 0.5858%\n",
      "Epoch [34/300], Step [140/225], Training Accuracy: 72.6116%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [141/225], Training Accuracy: 72.6175%, Training Loss: 0.5856%\n",
      "Epoch [34/300], Step [142/225], Training Accuracy: 72.6122%, Training Loss: 0.5854%\n",
      "Epoch [34/300], Step [143/225], Training Accuracy: 72.6071%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [144/225], Training Accuracy: 72.6020%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [145/225], Training Accuracy: 72.6185%, Training Loss: 0.5851%\n",
      "Epoch [34/300], Step [146/225], Training Accuracy: 72.6348%, Training Loss: 0.5850%\n",
      "Epoch [34/300], Step [147/225], Training Accuracy: 72.6084%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [148/225], Training Accuracy: 72.6457%, Training Loss: 0.5847%\n",
      "Epoch [34/300], Step [149/225], Training Accuracy: 72.7139%, Training Loss: 0.5844%\n",
      "Epoch [34/300], Step [150/225], Training Accuracy: 72.6875%, Training Loss: 0.5845%\n",
      "Epoch [34/300], Step [151/225], Training Accuracy: 72.7442%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [152/225], Training Accuracy: 72.7590%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [153/225], Training Accuracy: 72.7328%, Training Loss: 0.5834%\n",
      "Epoch [34/300], Step [154/225], Training Accuracy: 72.7577%, Training Loss: 0.5832%\n",
      "Epoch [34/300], Step [155/225], Training Accuracy: 72.7218%, Training Loss: 0.5839%\n",
      "Epoch [34/300], Step [156/225], Training Accuracy: 72.6963%, Training Loss: 0.5846%\n",
      "Epoch [34/300], Step [157/225], Training Accuracy: 72.7309%, Training Loss: 0.5845%\n",
      "Epoch [34/300], Step [158/225], Training Accuracy: 72.6661%, Training Loss: 0.5854%\n",
      "Epoch [34/300], Step [159/225], Training Accuracy: 72.6317%, Training Loss: 0.5860%\n",
      "Epoch [34/300], Step [160/225], Training Accuracy: 72.6660%, Training Loss: 0.5854%\n",
      "Epoch [34/300], Step [161/225], Training Accuracy: 72.6417%, Training Loss: 0.5853%\n",
      "Epoch [34/300], Step [162/225], Training Accuracy: 72.6852%, Training Loss: 0.5848%\n",
      "Epoch [34/300], Step [163/225], Training Accuracy: 72.7186%, Training Loss: 0.5842%\n",
      "Epoch [34/300], Step [164/225], Training Accuracy: 72.7420%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [165/225], Training Accuracy: 72.7652%, Training Loss: 0.5833%\n",
      "Epoch [34/300], Step [166/225], Training Accuracy: 72.7410%, Training Loss: 0.5832%\n",
      "Epoch [34/300], Step [167/225], Training Accuracy: 72.7264%, Training Loss: 0.5837%\n",
      "Epoch [34/300], Step [168/225], Training Accuracy: 72.6935%, Training Loss: 0.5844%\n",
      "Epoch [34/300], Step [169/225], Training Accuracy: 72.6146%, Training Loss: 0.5849%\n",
      "Epoch [34/300], Step [170/225], Training Accuracy: 72.6103%, Training Loss: 0.5851%\n",
      "Epoch [34/300], Step [171/225], Training Accuracy: 72.6334%, Training Loss: 0.5851%\n",
      "Epoch [34/300], Step [172/225], Training Accuracy: 72.6199%, Training Loss: 0.5851%\n",
      "Epoch [34/300], Step [173/225], Training Accuracy: 72.6156%, Training Loss: 0.5849%\n",
      "Epoch [34/300], Step [174/225], Training Accuracy: 72.6203%, Training Loss: 0.5847%\n",
      "Epoch [34/300], Step [175/225], Training Accuracy: 72.6696%, Training Loss: 0.5841%\n",
      "Epoch [34/300], Step [176/225], Training Accuracy: 72.6918%, Training Loss: 0.5838%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Step [177/225], Training Accuracy: 72.7225%, Training Loss: 0.5839%\n",
      "Epoch [34/300], Step [178/225], Training Accuracy: 72.7001%, Training Loss: 0.5840%\n",
      "Epoch [34/300], Step [179/225], Training Accuracy: 72.7566%, Training Loss: 0.5834%\n",
      "Epoch [34/300], Step [180/225], Training Accuracy: 72.7604%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [181/225], Training Accuracy: 72.7469%, Training Loss: 0.5837%\n",
      "Epoch [34/300], Step [182/225], Training Accuracy: 72.6992%, Training Loss: 0.5840%\n",
      "Epoch [34/300], Step [183/225], Training Accuracy: 72.7032%, Training Loss: 0.5840%\n",
      "Epoch [34/300], Step [184/225], Training Accuracy: 72.7327%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [185/225], Training Accuracy: 72.7111%, Training Loss: 0.5836%\n",
      "Epoch [34/300], Step [186/225], Training Accuracy: 72.7571%, Training Loss: 0.5831%\n",
      "Epoch [34/300], Step [187/225], Training Accuracy: 72.7774%, Training Loss: 0.5832%\n",
      "Epoch [34/300], Step [188/225], Training Accuracy: 72.8142%, Training Loss: 0.5827%\n",
      "Epoch [34/300], Step [189/225], Training Accuracy: 72.8588%, Training Loss: 0.5825%\n",
      "Epoch [34/300], Step [190/225], Training Accuracy: 72.8701%, Training Loss: 0.5826%\n",
      "Epoch [34/300], Step [191/225], Training Accuracy: 72.8649%, Training Loss: 0.5827%\n",
      "Epoch [34/300], Step [192/225], Training Accuracy: 72.9411%, Training Loss: 0.5818%\n",
      "Epoch [34/300], Step [193/225], Training Accuracy: 72.9437%, Training Loss: 0.5819%\n",
      "Epoch [34/300], Step [194/225], Training Accuracy: 72.9462%, Training Loss: 0.5819%\n",
      "Epoch [34/300], Step [195/225], Training Accuracy: 72.9487%, Training Loss: 0.5816%\n",
      "Epoch [34/300], Step [196/225], Training Accuracy: 72.9353%, Training Loss: 0.5817%\n",
      "Epoch [34/300], Step [197/225], Training Accuracy: 72.9854%, Training Loss: 0.5812%\n",
      "Epoch [34/300], Step [198/225], Training Accuracy: 73.0587%, Training Loss: 0.5802%\n",
      "Epoch [34/300], Step [199/225], Training Accuracy: 73.0449%, Training Loss: 0.5802%\n",
      "Epoch [34/300], Step [200/225], Training Accuracy: 73.0469%, Training Loss: 0.5802%\n",
      "Epoch [34/300], Step [201/225], Training Accuracy: 73.0410%, Training Loss: 0.5803%\n",
      "Epoch [34/300], Step [202/225], Training Accuracy: 73.0585%, Training Loss: 0.5796%\n",
      "Epoch [34/300], Step [203/225], Training Accuracy: 73.1219%, Training Loss: 0.5791%\n",
      "Epoch [34/300], Step [204/225], Training Accuracy: 73.1081%, Training Loss: 0.5792%\n",
      "Epoch [34/300], Step [205/225], Training Accuracy: 73.1631%, Training Loss: 0.5786%\n",
      "Epoch [34/300], Step [206/225], Training Accuracy: 73.1948%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [207/225], Training Accuracy: 73.2035%, Training Loss: 0.5786%\n",
      "Epoch [34/300], Step [208/225], Training Accuracy: 73.2347%, Training Loss: 0.5781%\n",
      "Epoch [34/300], Step [209/225], Training Accuracy: 73.2207%, Training Loss: 0.5782%\n",
      "Epoch [34/300], Step [210/225], Training Accuracy: 73.1994%, Training Loss: 0.5784%\n",
      "Epoch [34/300], Step [211/225], Training Accuracy: 73.2450%, Training Loss: 0.5777%\n",
      "Epoch [34/300], Step [212/225], Training Accuracy: 73.1869%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [213/225], Training Accuracy: 73.2174%, Training Loss: 0.5786%\n",
      "Epoch [34/300], Step [214/225], Training Accuracy: 73.2185%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [215/225], Training Accuracy: 73.2195%, Training Loss: 0.5783%\n",
      "Epoch [34/300], Step [216/225], Training Accuracy: 73.2422%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [217/225], Training Accuracy: 73.2791%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [218/225], Training Accuracy: 73.2583%, Training Loss: 0.5785%\n",
      "Epoch [34/300], Step [219/225], Training Accuracy: 73.2520%, Training Loss: 0.5787%\n",
      "Epoch [34/300], Step [220/225], Training Accuracy: 73.2812%, Training Loss: 0.5786%\n",
      "Epoch [34/300], Step [221/225], Training Accuracy: 73.2466%, Training Loss: 0.5791%\n",
      "Epoch [34/300], Step [222/225], Training Accuracy: 73.2545%, Training Loss: 0.5788%\n",
      "Epoch [34/300], Step [223/225], Training Accuracy: 73.1853%, Training Loss: 0.5795%\n",
      "Epoch [34/300], Step [224/225], Training Accuracy: 73.1934%, Training Loss: 0.5795%\n",
      "Epoch [34/300], Step [225/225], Training Accuracy: 73.1795%, Training Loss: 0.5794%\n",
      "Epoch [35/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.5941%\n",
      "Epoch [35/300], Step [2/225], Training Accuracy: 76.5625%, Training Loss: 0.5773%\n",
      "Epoch [35/300], Step [3/225], Training Accuracy: 73.4375%, Training Loss: 0.6098%\n",
      "Epoch [35/300], Step [4/225], Training Accuracy: 74.2188%, Training Loss: 0.5974%\n",
      "Epoch [35/300], Step [5/225], Training Accuracy: 74.6875%, Training Loss: 0.5854%\n",
      "Epoch [35/300], Step [6/225], Training Accuracy: 73.4375%, Training Loss: 0.5984%\n",
      "Epoch [35/300], Step [7/225], Training Accuracy: 73.8839%, Training Loss: 0.5879%\n",
      "Epoch [35/300], Step [8/225], Training Accuracy: 74.2188%, Training Loss: 0.5881%\n",
      "Epoch [35/300], Step [9/225], Training Accuracy: 73.6111%, Training Loss: 0.6076%\n",
      "Epoch [35/300], Step [10/225], Training Accuracy: 72.9688%, Training Loss: 0.6280%\n",
      "Epoch [35/300], Step [11/225], Training Accuracy: 72.8693%, Training Loss: 0.6239%\n",
      "Epoch [35/300], Step [12/225], Training Accuracy: 72.5260%, Training Loss: 0.6244%\n",
      "Epoch [35/300], Step [13/225], Training Accuracy: 73.5577%, Training Loss: 0.6088%\n",
      "Epoch [35/300], Step [14/225], Training Accuracy: 73.6607%, Training Loss: 0.6091%\n",
      "Epoch [35/300], Step [15/225], Training Accuracy: 73.8542%, Training Loss: 0.6115%\n",
      "Epoch [35/300], Step [16/225], Training Accuracy: 74.0234%, Training Loss: 0.6094%\n",
      "Epoch [35/300], Step [17/225], Training Accuracy: 74.2647%, Training Loss: 0.6072%\n",
      "Epoch [35/300], Step [18/225], Training Accuracy: 74.3924%, Training Loss: 0.6077%\n",
      "Epoch [35/300], Step [19/225], Training Accuracy: 74.5066%, Training Loss: 0.6067%\n",
      "Epoch [35/300], Step [20/225], Training Accuracy: 74.6094%, Training Loss: 0.6030%\n",
      "Epoch [35/300], Step [21/225], Training Accuracy: 75.0000%, Training Loss: 0.5943%\n",
      "Epoch [35/300], Step [22/225], Training Accuracy: 74.8580%, Training Loss: 0.5973%\n",
      "Epoch [35/300], Step [23/225], Training Accuracy: 74.6603%, Training Loss: 0.5943%\n",
      "Epoch [35/300], Step [24/225], Training Accuracy: 74.2188%, Training Loss: 0.6017%\n",
      "Epoch [35/300], Step [25/225], Training Accuracy: 74.5625%, Training Loss: 0.5985%\n",
      "Epoch [35/300], Step [26/225], Training Accuracy: 74.2188%, Training Loss: 0.6019%\n",
      "Epoch [35/300], Step [27/225], Training Accuracy: 74.1898%, Training Loss: 0.6005%\n",
      "Epoch [35/300], Step [28/225], Training Accuracy: 74.3304%, Training Loss: 0.5939%\n",
      "Epoch [35/300], Step [29/225], Training Accuracy: 74.2996%, Training Loss: 0.5940%\n",
      "Epoch [35/300], Step [30/225], Training Accuracy: 74.4271%, Training Loss: 0.5923%\n",
      "Epoch [35/300], Step [31/225], Training Accuracy: 74.0423%, Training Loss: 0.5974%\n",
      "Epoch [35/300], Step [32/225], Training Accuracy: 74.1211%, Training Loss: 0.5943%\n",
      "Epoch [35/300], Step [33/225], Training Accuracy: 74.2424%, Training Loss: 0.5910%\n",
      "Epoch [35/300], Step [34/225], Training Accuracy: 74.1728%, Training Loss: 0.5915%\n",
      "Epoch [35/300], Step [35/225], Training Accuracy: 74.3304%, Training Loss: 0.5897%\n",
      "Epoch [35/300], Step [36/225], Training Accuracy: 74.4358%, Training Loss: 0.5889%\n",
      "Epoch [35/300], Step [37/225], Training Accuracy: 74.4932%, Training Loss: 0.5875%\n",
      "Epoch [35/300], Step [38/225], Training Accuracy: 74.3832%, Training Loss: 0.5866%\n",
      "Epoch [35/300], Step [39/225], Training Accuracy: 74.3590%, Training Loss: 0.5871%\n",
      "Epoch [35/300], Step [40/225], Training Accuracy: 74.4531%, Training Loss: 0.5840%\n",
      "Epoch [35/300], Step [41/225], Training Accuracy: 74.4284%, Training Loss: 0.5848%\n",
      "Epoch [35/300], Step [42/225], Training Accuracy: 74.5164%, Training Loss: 0.5849%\n",
      "Epoch [35/300], Step [43/225], Training Accuracy: 74.4549%, Training Loss: 0.5856%\n",
      "Epoch [35/300], Step [44/225], Training Accuracy: 74.6094%, Training Loss: 0.5826%\n",
      "Epoch [35/300], Step [45/225], Training Accuracy: 74.6528%, Training Loss: 0.5823%\n",
      "Epoch [35/300], Step [46/225], Training Accuracy: 74.7962%, Training Loss: 0.5819%\n",
      "Epoch [35/300], Step [47/225], Training Accuracy: 74.8670%, Training Loss: 0.5805%\n",
      "Epoch [35/300], Step [48/225], Training Accuracy: 74.8047%, Training Loss: 0.5808%\n",
      "Epoch [35/300], Step [49/225], Training Accuracy: 74.8406%, Training Loss: 0.5801%\n",
      "Epoch [35/300], Step [50/225], Training Accuracy: 74.8750%, Training Loss: 0.5798%\n",
      "Epoch [35/300], Step [51/225], Training Accuracy: 74.9081%, Training Loss: 0.5810%\n",
      "Epoch [35/300], Step [52/225], Training Accuracy: 75.0300%, Training Loss: 0.5787%\n",
      "Epoch [35/300], Step [53/225], Training Accuracy: 75.0884%, Training Loss: 0.5778%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Step [54/225], Training Accuracy: 74.9711%, Training Loss: 0.5777%\n",
      "Epoch [35/300], Step [55/225], Training Accuracy: 74.7443%, Training Loss: 0.5799%\n",
      "Epoch [35/300], Step [56/225], Training Accuracy: 74.7210%, Training Loss: 0.5812%\n",
      "Epoch [35/300], Step [57/225], Training Accuracy: 74.6436%, Training Loss: 0.5805%\n",
      "Epoch [35/300], Step [58/225], Training Accuracy: 74.5151%, Training Loss: 0.5858%\n",
      "Epoch [35/300], Step [59/225], Training Accuracy: 74.5763%, Training Loss: 0.5849%\n",
      "Epoch [35/300], Step [60/225], Training Accuracy: 74.5833%, Training Loss: 0.5841%\n",
      "Epoch [35/300], Step [61/225], Training Accuracy: 74.5645%, Training Loss: 0.5847%\n",
      "Epoch [35/300], Step [62/225], Training Accuracy: 74.5464%, Training Loss: 0.5846%\n",
      "Epoch [35/300], Step [63/225], Training Accuracy: 74.3304%, Training Loss: 0.5875%\n",
      "Epoch [35/300], Step [64/225], Training Accuracy: 74.4385%, Training Loss: 0.5855%\n",
      "Epoch [35/300], Step [65/225], Training Accuracy: 74.3750%, Training Loss: 0.5862%\n",
      "Epoch [35/300], Step [66/225], Training Accuracy: 74.2661%, Training Loss: 0.5863%\n",
      "Epoch [35/300], Step [67/225], Training Accuracy: 74.2071%, Training Loss: 0.5864%\n",
      "Epoch [35/300], Step [68/225], Training Accuracy: 74.1268%, Training Loss: 0.5881%\n",
      "Epoch [35/300], Step [69/225], Training Accuracy: 74.0942%, Training Loss: 0.5881%\n",
      "Epoch [35/300], Step [70/225], Training Accuracy: 73.9955%, Training Loss: 0.5879%\n",
      "Epoch [35/300], Step [71/225], Training Accuracy: 74.0537%, Training Loss: 0.5869%\n",
      "Epoch [35/300], Step [72/225], Training Accuracy: 74.0234%, Training Loss: 0.5873%\n",
      "Epoch [35/300], Step [73/225], Training Accuracy: 73.9726%, Training Loss: 0.5872%\n",
      "Epoch [35/300], Step [74/225], Training Accuracy: 73.9020%, Training Loss: 0.5870%\n",
      "Epoch [35/300], Step [75/225], Training Accuracy: 73.8750%, Training Loss: 0.5863%\n",
      "Epoch [35/300], Step [76/225], Training Accuracy: 73.7664%, Training Loss: 0.5869%\n",
      "Epoch [35/300], Step [77/225], Training Accuracy: 73.8028%, Training Loss: 0.5858%\n",
      "Epoch [35/300], Step [78/225], Training Accuracy: 73.6979%, Training Loss: 0.5863%\n",
      "Epoch [35/300], Step [79/225], Training Accuracy: 73.6353%, Training Loss: 0.5866%\n",
      "Epoch [35/300], Step [80/225], Training Accuracy: 73.5938%, Training Loss: 0.5891%\n",
      "Epoch [35/300], Step [81/225], Training Accuracy: 73.5725%, Training Loss: 0.5885%\n",
      "Epoch [35/300], Step [82/225], Training Accuracy: 73.5709%, Training Loss: 0.5878%\n",
      "Epoch [35/300], Step [83/225], Training Accuracy: 73.6069%, Training Loss: 0.5872%\n",
      "Epoch [35/300], Step [84/225], Training Accuracy: 73.6793%, Training Loss: 0.5865%\n",
      "Epoch [35/300], Step [85/225], Training Accuracy: 73.6397%, Training Loss: 0.5857%\n",
      "Epoch [35/300], Step [86/225], Training Accuracy: 73.6374%, Training Loss: 0.5869%\n",
      "Epoch [35/300], Step [87/225], Training Accuracy: 73.5991%, Training Loss: 0.5879%\n",
      "Epoch [35/300], Step [88/225], Training Accuracy: 73.5263%, Training Loss: 0.5884%\n",
      "Epoch [35/300], Step [89/225], Training Accuracy: 73.4551%, Training Loss: 0.5894%\n",
      "Epoch [35/300], Step [90/225], Training Accuracy: 73.4201%, Training Loss: 0.5903%\n",
      "Epoch [35/300], Step [91/225], Training Accuracy: 73.4203%, Training Loss: 0.5893%\n",
      "Epoch [35/300], Step [92/225], Training Accuracy: 73.4035%, Training Loss: 0.5892%\n",
      "Epoch [35/300], Step [93/225], Training Accuracy: 73.4375%, Training Loss: 0.5881%\n",
      "Epoch [35/300], Step [94/225], Training Accuracy: 73.4874%, Training Loss: 0.5871%\n",
      "Epoch [35/300], Step [95/225], Training Accuracy: 73.5526%, Training Loss: 0.5865%\n",
      "Epoch [35/300], Step [96/225], Training Accuracy: 73.5352%, Training Loss: 0.5865%\n",
      "Epoch [35/300], Step [97/225], Training Accuracy: 73.5341%, Training Loss: 0.5867%\n",
      "Epoch [35/300], Step [98/225], Training Accuracy: 73.5013%, Training Loss: 0.5886%\n",
      "Epoch [35/300], Step [99/225], Training Accuracy: 73.4848%, Training Loss: 0.5884%\n",
      "Epoch [35/300], Step [100/225], Training Accuracy: 73.4844%, Training Loss: 0.5897%\n",
      "Epoch [35/300], Step [101/225], Training Accuracy: 73.5458%, Training Loss: 0.5891%\n",
      "Epoch [35/300], Step [102/225], Training Accuracy: 73.4528%, Training Loss: 0.5899%\n",
      "Epoch [35/300], Step [103/225], Training Accuracy: 73.4072%, Training Loss: 0.5902%\n",
      "Epoch [35/300], Step [104/225], Training Accuracy: 73.3323%, Training Loss: 0.5914%\n",
      "Epoch [35/300], Step [105/225], Training Accuracy: 73.3631%, Training Loss: 0.5907%\n",
      "Epoch [35/300], Step [106/225], Training Accuracy: 73.3343%, Training Loss: 0.5910%\n",
      "Epoch [35/300], Step [107/225], Training Accuracy: 73.2769%, Training Loss: 0.5914%\n",
      "Epoch [35/300], Step [108/225], Training Accuracy: 73.2350%, Training Loss: 0.5923%\n",
      "Epoch [35/300], Step [109/225], Training Accuracy: 73.2511%, Training Loss: 0.5913%\n",
      "Epoch [35/300], Step [110/225], Training Accuracy: 73.1960%, Training Loss: 0.5917%\n",
      "Epoch [35/300], Step [111/225], Training Accuracy: 73.1560%, Training Loss: 0.5915%\n",
      "Epoch [35/300], Step [112/225], Training Accuracy: 73.1724%, Training Loss: 0.5919%\n",
      "Epoch [35/300], Step [113/225], Training Accuracy: 73.1886%, Training Loss: 0.5908%\n",
      "Epoch [35/300], Step [114/225], Training Accuracy: 73.2182%, Training Loss: 0.5904%\n",
      "Epoch [35/300], Step [115/225], Training Accuracy: 73.2880%, Training Loss: 0.5897%\n",
      "Epoch [35/300], Step [116/225], Training Accuracy: 73.2893%, Training Loss: 0.5899%\n",
      "Epoch [35/300], Step [117/225], Training Accuracy: 73.1971%, Training Loss: 0.5913%\n",
      "Epoch [35/300], Step [118/225], Training Accuracy: 73.1594%, Training Loss: 0.5915%\n",
      "Epoch [35/300], Step [119/225], Training Accuracy: 73.1092%, Training Loss: 0.5921%\n",
      "Epoch [35/300], Step [120/225], Training Accuracy: 73.1641%, Training Loss: 0.5920%\n",
      "Epoch [35/300], Step [121/225], Training Accuracy: 73.0501%, Training Loss: 0.5936%\n",
      "Epoch [35/300], Step [122/225], Training Accuracy: 73.0405%, Training Loss: 0.5938%\n",
      "Epoch [35/300], Step [123/225], Training Accuracy: 73.0056%, Training Loss: 0.5941%\n",
      "Epoch [35/300], Step [124/225], Training Accuracy: 72.9839%, Training Loss: 0.5943%\n",
      "Epoch [35/300], Step [125/225], Training Accuracy: 72.9750%, Training Loss: 0.5939%\n",
      "Epoch [35/300], Step [126/225], Training Accuracy: 72.9415%, Training Loss: 0.5939%\n",
      "Epoch [35/300], Step [127/225], Training Accuracy: 72.9085%, Training Loss: 0.5941%\n",
      "Epoch [35/300], Step [128/225], Training Accuracy: 72.8271%, Training Loss: 0.5954%\n",
      "Epoch [35/300], Step [129/225], Training Accuracy: 72.8077%, Training Loss: 0.5958%\n",
      "Epoch [35/300], Step [130/225], Training Accuracy: 72.8005%, Training Loss: 0.5959%\n",
      "Epoch [35/300], Step [131/225], Training Accuracy: 72.8650%, Training Loss: 0.5954%\n",
      "Epoch [35/300], Step [132/225], Training Accuracy: 72.8693%, Training Loss: 0.5953%\n",
      "Epoch [35/300], Step [133/225], Training Accuracy: 72.9088%, Training Loss: 0.5947%\n",
      "Epoch [35/300], Step [134/225], Training Accuracy: 72.8661%, Training Loss: 0.5956%\n",
      "Epoch [35/300], Step [135/225], Training Accuracy: 72.8588%, Training Loss: 0.5961%\n",
      "Epoch [35/300], Step [136/225], Training Accuracy: 72.8860%, Training Loss: 0.5954%\n",
      "Epoch [35/300], Step [137/225], Training Accuracy: 72.8102%, Training Loss: 0.5963%\n",
      "Epoch [35/300], Step [138/225], Training Accuracy: 72.8940%, Training Loss: 0.5949%\n",
      "Epoch [35/300], Step [139/225], Training Accuracy: 72.8867%, Training Loss: 0.5952%\n",
      "Epoch [35/300], Step [140/225], Training Accuracy: 73.0134%, Training Loss: 0.5942%\n",
      "Epoch [35/300], Step [141/225], Training Accuracy: 73.0053%, Training Loss: 0.5938%\n",
      "Epoch [35/300], Step [142/225], Training Accuracy: 73.0524%, Training Loss: 0.5931%\n",
      "Epoch [35/300], Step [143/225], Training Accuracy: 73.0878%, Training Loss: 0.5930%\n",
      "Epoch [35/300], Step [144/225], Training Accuracy: 73.0794%, Training Loss: 0.5932%\n",
      "Epoch [35/300], Step [145/225], Training Accuracy: 73.1142%, Training Loss: 0.5927%\n",
      "Epoch [35/300], Step [146/225], Training Accuracy: 73.1057%, Training Loss: 0.5933%\n",
      "Epoch [35/300], Step [147/225], Training Accuracy: 73.0442%, Training Loss: 0.5940%\n",
      "Epoch [35/300], Step [148/225], Training Accuracy: 73.0680%, Training Loss: 0.5939%\n",
      "Epoch [35/300], Step [149/225], Training Accuracy: 73.0495%, Training Loss: 0.5939%\n",
      "Epoch [35/300], Step [150/225], Training Accuracy: 73.0938%, Training Loss: 0.5933%\n",
      "Epoch [35/300], Step [151/225], Training Accuracy: 73.1271%, Training Loss: 0.5930%\n",
      "Epoch [35/300], Step [152/225], Training Accuracy: 73.0572%, Training Loss: 0.5940%\n",
      "Epoch [35/300], Step [153/225], Training Accuracy: 73.0494%, Training Loss: 0.5943%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Step [154/225], Training Accuracy: 73.0519%, Training Loss: 0.5942%\n",
      "Epoch [35/300], Step [155/225], Training Accuracy: 73.0040%, Training Loss: 0.5946%\n",
      "Epoch [35/300], Step [156/225], Training Accuracy: 72.9467%, Training Loss: 0.5952%\n",
      "Epoch [35/300], Step [157/225], Training Accuracy: 72.9100%, Training Loss: 0.5958%\n",
      "Epoch [35/300], Step [158/225], Training Accuracy: 72.8540%, Training Loss: 0.5965%\n",
      "Epoch [35/300], Step [159/225], Training Accuracy: 72.8381%, Training Loss: 0.5967%\n",
      "Epoch [35/300], Step [160/225], Training Accuracy: 72.8809%, Training Loss: 0.5962%\n",
      "Epoch [35/300], Step [161/225], Training Accuracy: 72.8261%, Training Loss: 0.5965%\n",
      "Epoch [35/300], Step [162/225], Training Accuracy: 72.9263%, Training Loss: 0.5955%\n",
      "Epoch [35/300], Step [163/225], Training Accuracy: 72.9294%, Training Loss: 0.5953%\n",
      "Epoch [35/300], Step [164/225], Training Accuracy: 72.9707%, Training Loss: 0.5941%\n",
      "Epoch [35/300], Step [165/225], Training Accuracy: 73.0208%, Training Loss: 0.5936%\n",
      "Epoch [35/300], Step [166/225], Training Accuracy: 73.0328%, Training Loss: 0.5936%\n",
      "Epoch [35/300], Step [167/225], Training Accuracy: 73.0539%, Training Loss: 0.5935%\n",
      "Epoch [35/300], Step [168/225], Training Accuracy: 73.0562%, Training Loss: 0.5944%\n",
      "Epoch [35/300], Step [169/225], Training Accuracy: 73.0214%, Training Loss: 0.5943%\n",
      "Epoch [35/300], Step [170/225], Training Accuracy: 73.0423%, Training Loss: 0.5944%\n",
      "Epoch [35/300], Step [171/225], Training Accuracy: 73.0811%, Training Loss: 0.5938%\n",
      "Epoch [35/300], Step [172/225], Training Accuracy: 73.1014%, Training Loss: 0.5934%\n",
      "Epoch [35/300], Step [173/225], Training Accuracy: 73.0762%, Training Loss: 0.5936%\n",
      "Epoch [35/300], Step [174/225], Training Accuracy: 73.0873%, Training Loss: 0.5938%\n",
      "Epoch [35/300], Step [175/225], Training Accuracy: 73.1250%, Training Loss: 0.5934%\n",
      "Epoch [35/300], Step [176/225], Training Accuracy: 73.1357%, Training Loss: 0.5930%\n",
      "Epoch [35/300], Step [177/225], Training Accuracy: 73.1285%, Training Loss: 0.5929%\n",
      "Epoch [35/300], Step [178/225], Training Accuracy: 73.1303%, Training Loss: 0.5926%\n",
      "Epoch [35/300], Step [179/225], Training Accuracy: 73.1669%, Training Loss: 0.5920%\n",
      "Epoch [35/300], Step [180/225], Training Accuracy: 73.1684%, Training Loss: 0.5921%\n",
      "Epoch [35/300], Step [181/225], Training Accuracy: 73.1785%, Training Loss: 0.5920%\n",
      "Epoch [35/300], Step [182/225], Training Accuracy: 73.1885%, Training Loss: 0.5922%\n",
      "Epoch [35/300], Step [183/225], Training Accuracy: 73.1728%, Training Loss: 0.5925%\n",
      "Epoch [35/300], Step [184/225], Training Accuracy: 73.1827%, Training Loss: 0.5917%\n",
      "Epoch [35/300], Step [185/225], Training Accuracy: 73.2010%, Training Loss: 0.5912%\n",
      "Epoch [35/300], Step [186/225], Training Accuracy: 73.2359%, Training Loss: 0.5905%\n",
      "Epoch [35/300], Step [187/225], Training Accuracy: 73.2620%, Training Loss: 0.5904%\n",
      "Epoch [35/300], Step [188/225], Training Accuracy: 73.2547%, Training Loss: 0.5901%\n",
      "Epoch [35/300], Step [189/225], Training Accuracy: 73.2887%, Training Loss: 0.5895%\n",
      "Epoch [35/300], Step [190/225], Training Accuracy: 73.2648%, Training Loss: 0.5901%\n",
      "Epoch [35/300], Step [191/225], Training Accuracy: 73.2575%, Training Loss: 0.5899%\n",
      "Epoch [35/300], Step [192/225], Training Accuracy: 73.2992%, Training Loss: 0.5891%\n",
      "Epoch [35/300], Step [193/225], Training Accuracy: 73.2351%, Training Loss: 0.5893%\n",
      "Epoch [35/300], Step [194/225], Training Accuracy: 73.2442%, Training Loss: 0.5891%\n",
      "Epoch [35/300], Step [195/225], Training Accuracy: 73.2692%, Training Loss: 0.5888%\n",
      "Epoch [35/300], Step [196/225], Training Accuracy: 73.2621%, Training Loss: 0.5890%\n",
      "Epoch [35/300], Step [197/225], Training Accuracy: 73.2471%, Training Loss: 0.5891%\n",
      "Epoch [35/300], Step [198/225], Training Accuracy: 73.3112%, Training Loss: 0.5882%\n",
      "Epoch [35/300], Step [199/225], Training Accuracy: 73.3197%, Training Loss: 0.5882%\n",
      "Epoch [35/300], Step [200/225], Training Accuracy: 73.3203%, Training Loss: 0.5879%\n",
      "Epoch [35/300], Step [201/225], Training Accuracy: 73.2820%, Training Loss: 0.5883%\n",
      "Epoch [35/300], Step [202/225], Training Accuracy: 73.2673%, Training Loss: 0.5878%\n",
      "Epoch [35/300], Step [203/225], Training Accuracy: 73.2913%, Training Loss: 0.5876%\n",
      "Epoch [35/300], Step [204/225], Training Accuracy: 73.2767%, Training Loss: 0.5874%\n",
      "Epoch [35/300], Step [205/225], Training Accuracy: 73.2851%, Training Loss: 0.5870%\n",
      "Epoch [35/300], Step [206/225], Training Accuracy: 73.3161%, Training Loss: 0.5869%\n",
      "Epoch [35/300], Step [207/225], Training Accuracy: 73.2865%, Training Loss: 0.5870%\n",
      "Epoch [35/300], Step [208/225], Training Accuracy: 73.3023%, Training Loss: 0.5863%\n",
      "Epoch [35/300], Step [209/225], Training Accuracy: 73.3254%, Training Loss: 0.5861%\n",
      "Epoch [35/300], Step [210/225], Training Accuracy: 73.2812%, Training Loss: 0.5869%\n",
      "Epoch [35/300], Step [211/225], Training Accuracy: 73.3264%, Training Loss: 0.5866%\n",
      "Epoch [35/300], Step [212/225], Training Accuracy: 73.3048%, Training Loss: 0.5872%\n",
      "Epoch [35/300], Step [213/225], Training Accuracy: 73.2908%, Training Loss: 0.5876%\n",
      "Epoch [35/300], Step [214/225], Training Accuracy: 73.3134%, Training Loss: 0.5874%\n",
      "Epoch [35/300], Step [215/225], Training Accuracy: 73.2994%, Training Loss: 0.5876%\n",
      "Epoch [35/300], Step [216/225], Training Accuracy: 73.2711%, Training Loss: 0.5877%\n",
      "Epoch [35/300], Step [217/225], Training Accuracy: 73.2791%, Training Loss: 0.5878%\n",
      "Epoch [35/300], Step [218/225], Training Accuracy: 73.2655%, Training Loss: 0.5876%\n",
      "Epoch [35/300], Step [219/225], Training Accuracy: 73.2520%, Training Loss: 0.5874%\n",
      "Epoch [35/300], Step [220/225], Training Accuracy: 73.2599%, Training Loss: 0.5873%\n",
      "Epoch [35/300], Step [221/225], Training Accuracy: 73.2325%, Training Loss: 0.5876%\n",
      "Epoch [35/300], Step [222/225], Training Accuracy: 73.2334%, Training Loss: 0.5874%\n",
      "Epoch [35/300], Step [223/225], Training Accuracy: 73.2273%, Training Loss: 0.5875%\n",
      "Epoch [35/300], Step [224/225], Training Accuracy: 73.2073%, Training Loss: 0.5874%\n",
      "Epoch [35/300], Step [225/225], Training Accuracy: 73.1795%, Training Loss: 0.5874%\n",
      "Epoch [36/300], Step [1/225], Training Accuracy: 62.5000%, Training Loss: 0.6517%\n",
      "Epoch [36/300], Step [2/225], Training Accuracy: 71.0938%, Training Loss: 0.6249%\n",
      "Epoch [36/300], Step [3/225], Training Accuracy: 71.8750%, Training Loss: 0.6303%\n",
      "Epoch [36/300], Step [4/225], Training Accuracy: 72.6562%, Training Loss: 0.6143%\n",
      "Epoch [36/300], Step [5/225], Training Accuracy: 73.1250%, Training Loss: 0.5976%\n",
      "Epoch [36/300], Step [6/225], Training Accuracy: 74.4792%, Training Loss: 0.5841%\n",
      "Epoch [36/300], Step [7/225], Training Accuracy: 74.5536%, Training Loss: 0.5831%\n",
      "Epoch [36/300], Step [8/225], Training Accuracy: 74.0234%, Training Loss: 0.5775%\n",
      "Epoch [36/300], Step [9/225], Training Accuracy: 72.9167%, Training Loss: 0.5822%\n",
      "Epoch [36/300], Step [10/225], Training Accuracy: 72.5000%, Training Loss: 0.5925%\n",
      "Epoch [36/300], Step [11/225], Training Accuracy: 72.7273%, Training Loss: 0.5879%\n",
      "Epoch [36/300], Step [12/225], Training Accuracy: 73.1771%, Training Loss: 0.5859%\n",
      "Epoch [36/300], Step [13/225], Training Accuracy: 73.9183%, Training Loss: 0.5713%\n",
      "Epoch [36/300], Step [14/225], Training Accuracy: 73.9955%, Training Loss: 0.5677%\n",
      "Epoch [36/300], Step [15/225], Training Accuracy: 73.8542%, Training Loss: 0.5763%\n",
      "Epoch [36/300], Step [16/225], Training Accuracy: 74.0234%, Training Loss: 0.5750%\n",
      "Epoch [36/300], Step [17/225], Training Accuracy: 74.2647%, Training Loss: 0.5733%\n",
      "Epoch [36/300], Step [18/225], Training Accuracy: 73.7847%, Training Loss: 0.5766%\n",
      "Epoch [36/300], Step [19/225], Training Accuracy: 73.9309%, Training Loss: 0.5744%\n",
      "Epoch [36/300], Step [20/225], Training Accuracy: 73.8281%, Training Loss: 0.5706%\n",
      "Epoch [36/300], Step [21/225], Training Accuracy: 73.8839%, Training Loss: 0.5649%\n",
      "Epoch [36/300], Step [22/225], Training Accuracy: 73.8636%, Training Loss: 0.5697%\n",
      "Epoch [36/300], Step [23/225], Training Accuracy: 74.0489%, Training Loss: 0.5637%\n",
      "Epoch [36/300], Step [24/225], Training Accuracy: 73.7630%, Training Loss: 0.5648%\n",
      "Epoch [36/300], Step [25/225], Training Accuracy: 73.8125%, Training Loss: 0.5645%\n",
      "Epoch [36/300], Step [26/225], Training Accuracy: 73.7981%, Training Loss: 0.5655%\n",
      "Epoch [36/300], Step [27/225], Training Accuracy: 73.6111%, Training Loss: 0.5669%\n",
      "Epoch [36/300], Step [28/225], Training Accuracy: 73.8839%, Training Loss: 0.5634%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Step [29/225], Training Accuracy: 73.8685%, Training Loss: 0.5645%\n",
      "Epoch [36/300], Step [30/225], Training Accuracy: 73.9062%, Training Loss: 0.5651%\n",
      "Epoch [36/300], Step [31/225], Training Accuracy: 73.4879%, Training Loss: 0.5742%\n",
      "Epoch [36/300], Step [32/225], Training Accuracy: 73.1445%, Training Loss: 0.5764%\n",
      "Epoch [36/300], Step [33/225], Training Accuracy: 73.2955%, Training Loss: 0.5722%\n",
      "Epoch [36/300], Step [34/225], Training Accuracy: 73.2537%, Training Loss: 0.5737%\n",
      "Epoch [36/300], Step [35/225], Training Accuracy: 73.2589%, Training Loss: 0.5742%\n",
      "Epoch [36/300], Step [36/225], Training Accuracy: 73.3941%, Training Loss: 0.5726%\n",
      "Epoch [36/300], Step [37/225], Training Accuracy: 73.4375%, Training Loss: 0.5724%\n",
      "Epoch [36/300], Step [38/225], Training Accuracy: 73.3964%, Training Loss: 0.5730%\n",
      "Epoch [36/300], Step [39/225], Training Accuracy: 73.3574%, Training Loss: 0.5725%\n",
      "Epoch [36/300], Step [40/225], Training Accuracy: 73.4375%, Training Loss: 0.5734%\n",
      "Epoch [36/300], Step [41/225], Training Accuracy: 73.4375%, Training Loss: 0.5745%\n",
      "Epoch [36/300], Step [42/225], Training Accuracy: 73.5491%, Training Loss: 0.5730%\n",
      "Epoch [36/300], Step [43/225], Training Accuracy: 73.3648%, Training Loss: 0.5755%\n",
      "Epoch [36/300], Step [44/225], Training Accuracy: 73.4730%, Training Loss: 0.5736%\n",
      "Epoch [36/300], Step [45/225], Training Accuracy: 73.5069%, Training Loss: 0.5732%\n",
      "Epoch [36/300], Step [46/225], Training Accuracy: 73.5394%, Training Loss: 0.5717%\n",
      "Epoch [36/300], Step [47/225], Training Accuracy: 73.4375%, Training Loss: 0.5722%\n",
      "Epoch [36/300], Step [48/225], Training Accuracy: 73.2747%, Training Loss: 0.5744%\n",
      "Epoch [36/300], Step [49/225], Training Accuracy: 73.3099%, Training Loss: 0.5734%\n",
      "Epoch [36/300], Step [50/225], Training Accuracy: 73.3438%, Training Loss: 0.5742%\n",
      "Epoch [36/300], Step [51/225], Training Accuracy: 73.4069%, Training Loss: 0.5735%\n",
      "Epoch [36/300], Step [52/225], Training Accuracy: 73.4675%, Training Loss: 0.5726%\n",
      "Epoch [36/300], Step [53/225], Training Accuracy: 73.5259%, Training Loss: 0.5728%\n",
      "Epoch [36/300], Step [54/225], Training Accuracy: 73.5243%, Training Loss: 0.5728%\n",
      "Epoch [36/300], Step [55/225], Training Accuracy: 73.3239%, Training Loss: 0.5747%\n",
      "Epoch [36/300], Step [56/225], Training Accuracy: 73.2980%, Training Loss: 0.5754%\n",
      "Epoch [36/300], Step [57/225], Training Accuracy: 73.1634%, Training Loss: 0.5763%\n",
      "Epoch [36/300], Step [58/225], Training Accuracy: 73.1412%, Training Loss: 0.5754%\n",
      "Epoch [36/300], Step [59/225], Training Accuracy: 73.2256%, Training Loss: 0.5742%\n",
      "Epoch [36/300], Step [60/225], Training Accuracy: 73.3073%, Training Loss: 0.5733%\n",
      "Epoch [36/300], Step [61/225], Training Accuracy: 73.2326%, Training Loss: 0.5754%\n",
      "Epoch [36/300], Step [62/225], Training Accuracy: 73.2359%, Training Loss: 0.5762%\n",
      "Epoch [36/300], Step [63/225], Training Accuracy: 73.1151%, Training Loss: 0.5787%\n",
      "Epoch [36/300], Step [64/225], Training Accuracy: 73.1689%, Training Loss: 0.5773%\n",
      "Epoch [36/300], Step [65/225], Training Accuracy: 73.1010%, Training Loss: 0.5777%\n",
      "Epoch [36/300], Step [66/225], Training Accuracy: 73.1771%, Training Loss: 0.5761%\n",
      "Epoch [36/300], Step [67/225], Training Accuracy: 73.0877%, Training Loss: 0.5783%\n",
      "Epoch [36/300], Step [68/225], Training Accuracy: 72.9779%, Training Loss: 0.5799%\n",
      "Epoch [36/300], Step [69/225], Training Accuracy: 72.9393%, Training Loss: 0.5800%\n",
      "Epoch [36/300], Step [70/225], Training Accuracy: 72.9464%, Training Loss: 0.5809%\n",
      "Epoch [36/300], Step [71/225], Training Accuracy: 73.0634%, Training Loss: 0.5801%\n",
      "Epoch [36/300], Step [72/225], Training Accuracy: 73.0035%, Training Loss: 0.5805%\n",
      "Epoch [36/300], Step [73/225], Training Accuracy: 72.9452%, Training Loss: 0.5805%\n",
      "Epoch [36/300], Step [74/225], Training Accuracy: 72.9519%, Training Loss: 0.5805%\n",
      "Epoch [36/300], Step [75/225], Training Accuracy: 72.9167%, Training Loss: 0.5804%\n",
      "Epoch [36/300], Step [76/225], Training Accuracy: 72.8002%, Training Loss: 0.5820%\n",
      "Epoch [36/300], Step [77/225], Training Accuracy: 72.8084%, Training Loss: 0.5813%\n",
      "Epoch [36/300], Step [78/225], Training Accuracy: 72.7564%, Training Loss: 0.5827%\n",
      "Epoch [36/300], Step [79/225], Training Accuracy: 72.7848%, Training Loss: 0.5828%\n",
      "Epoch [36/300], Step [80/225], Training Accuracy: 72.8711%, Training Loss: 0.5821%\n",
      "Epoch [36/300], Step [81/225], Training Accuracy: 72.9552%, Training Loss: 0.5808%\n",
      "Epoch [36/300], Step [82/225], Training Accuracy: 72.9992%, Training Loss: 0.5794%\n",
      "Epoch [36/300], Step [83/225], Training Accuracy: 72.9857%, Training Loss: 0.5792%\n",
      "Epoch [36/300], Step [84/225], Training Accuracy: 72.9911%, Training Loss: 0.5791%\n",
      "Epoch [36/300], Step [85/225], Training Accuracy: 73.0699%, Training Loss: 0.5775%\n",
      "Epoch [36/300], Step [86/225], Training Accuracy: 73.0741%, Training Loss: 0.5771%\n",
      "Epoch [36/300], Step [87/225], Training Accuracy: 73.1322%, Training Loss: 0.5771%\n",
      "Epoch [36/300], Step [88/225], Training Accuracy: 73.0824%, Training Loss: 0.5778%\n",
      "Epoch [36/300], Step [89/225], Training Accuracy: 73.0864%, Training Loss: 0.5783%\n",
      "Epoch [36/300], Step [90/225], Training Accuracy: 73.0382%, Training Loss: 0.5793%\n",
      "Epoch [36/300], Step [91/225], Training Accuracy: 72.9567%, Training Loss: 0.5793%\n",
      "Epoch [36/300], Step [92/225], Training Accuracy: 72.9450%, Training Loss: 0.5787%\n",
      "Epoch [36/300], Step [93/225], Training Accuracy: 73.0175%, Training Loss: 0.5778%\n",
      "Epoch [36/300], Step [94/225], Training Accuracy: 73.0884%, Training Loss: 0.5768%\n",
      "Epoch [36/300], Step [95/225], Training Accuracy: 73.1414%, Training Loss: 0.5779%\n",
      "Epoch [36/300], Step [96/225], Training Accuracy: 73.1283%, Training Loss: 0.5776%\n",
      "Epoch [36/300], Step [97/225], Training Accuracy: 73.0670%, Training Loss: 0.5784%\n",
      "Epoch [36/300], Step [98/225], Training Accuracy: 73.0548%, Training Loss: 0.5789%\n",
      "Epoch [36/300], Step [99/225], Training Accuracy: 73.1061%, Training Loss: 0.5786%\n",
      "Epoch [36/300], Step [100/225], Training Accuracy: 73.0938%, Training Loss: 0.5795%\n",
      "Epoch [36/300], Step [101/225], Training Accuracy: 73.0817%, Training Loss: 0.5795%\n",
      "Epoch [36/300], Step [102/225], Training Accuracy: 73.0545%, Training Loss: 0.5802%\n",
      "Epoch [36/300], Step [103/225], Training Accuracy: 73.1038%, Training Loss: 0.5798%\n",
      "Epoch [36/300], Step [104/225], Training Accuracy: 73.0168%, Training Loss: 0.5813%\n",
      "Epoch [36/300], Step [105/225], Training Accuracy: 73.0506%, Training Loss: 0.5803%\n",
      "Epoch [36/300], Step [106/225], Training Accuracy: 73.0542%, Training Loss: 0.5801%\n",
      "Epoch [36/300], Step [107/225], Training Accuracy: 73.0432%, Training Loss: 0.5808%\n",
      "Epoch [36/300], Step [108/225], Training Accuracy: 73.0324%, Training Loss: 0.5805%\n",
      "Epoch [36/300], Step [109/225], Training Accuracy: 72.9788%, Training Loss: 0.5806%\n",
      "Epoch [36/300], Step [110/225], Training Accuracy: 72.9688%, Training Loss: 0.5806%\n",
      "Epoch [36/300], Step [111/225], Training Accuracy: 73.0152%, Training Loss: 0.5795%\n",
      "Epoch [36/300], Step [112/225], Training Accuracy: 73.0329%, Training Loss: 0.5791%\n",
      "Epoch [36/300], Step [113/225], Training Accuracy: 73.0503%, Training Loss: 0.5792%\n",
      "Epoch [36/300], Step [114/225], Training Accuracy: 72.9578%, Training Loss: 0.5796%\n",
      "Epoch [36/300], Step [115/225], Training Accuracy: 73.0571%, Training Loss: 0.5787%\n",
      "Epoch [36/300], Step [116/225], Training Accuracy: 73.1142%, Training Loss: 0.5783%\n",
      "Epoch [36/300], Step [117/225], Training Accuracy: 73.0502%, Training Loss: 0.5796%\n",
      "Epoch [36/300], Step [118/225], Training Accuracy: 73.0138%, Training Loss: 0.5800%\n",
      "Epoch [36/300], Step [119/225], Training Accuracy: 72.9911%, Training Loss: 0.5801%\n",
      "Epoch [36/300], Step [120/225], Training Accuracy: 73.0078%, Training Loss: 0.5801%\n",
      "Epoch [36/300], Step [121/225], Training Accuracy: 72.8822%, Training Loss: 0.5811%\n",
      "Epoch [36/300], Step [122/225], Training Accuracy: 72.8484%, Training Loss: 0.5813%\n",
      "Epoch [36/300], Step [123/225], Training Accuracy: 72.7896%, Training Loss: 0.5821%\n",
      "Epoch [36/300], Step [124/225], Training Accuracy: 72.8201%, Training Loss: 0.5823%\n",
      "Epoch [36/300], Step [125/225], Training Accuracy: 72.8125%, Training Loss: 0.5826%\n",
      "Epoch [36/300], Step [126/225], Training Accuracy: 72.8299%, Training Loss: 0.5827%\n",
      "Epoch [36/300], Step [127/225], Training Accuracy: 72.8593%, Training Loss: 0.5829%\n",
      "Epoch [36/300], Step [128/225], Training Accuracy: 72.8394%, Training Loss: 0.5837%\n",
      "Epoch [36/300], Step [129/225], Training Accuracy: 72.8319%, Training Loss: 0.5842%\n",
      "Epoch [36/300], Step [130/225], Training Accuracy: 72.8966%, Training Loss: 0.5837%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Step [131/225], Training Accuracy: 72.9485%, Training Loss: 0.5826%\n",
      "Epoch [36/300], Step [132/225], Training Accuracy: 72.9403%, Training Loss: 0.5830%\n",
      "Epoch [36/300], Step [133/225], Training Accuracy: 72.9558%, Training Loss: 0.5834%\n",
      "Epoch [36/300], Step [134/225], Training Accuracy: 72.9011%, Training Loss: 0.5845%\n",
      "Epoch [36/300], Step [135/225], Training Accuracy: 72.9167%, Training Loss: 0.5845%\n",
      "Epoch [36/300], Step [136/225], Training Accuracy: 72.9894%, Training Loss: 0.5834%\n",
      "Epoch [36/300], Step [137/225], Training Accuracy: 72.9699%, Training Loss: 0.5835%\n",
      "Epoch [36/300], Step [138/225], Training Accuracy: 73.0299%, Training Loss: 0.5824%\n",
      "Epoch [36/300], Step [139/225], Training Accuracy: 73.0665%, Training Loss: 0.5825%\n",
      "Epoch [36/300], Step [140/225], Training Accuracy: 73.1138%, Training Loss: 0.5823%\n",
      "Epoch [36/300], Step [141/225], Training Accuracy: 73.1051%, Training Loss: 0.5829%\n",
      "Epoch [36/300], Step [142/225], Training Accuracy: 73.1404%, Training Loss: 0.5825%\n",
      "Epoch [36/300], Step [143/225], Training Accuracy: 73.1316%, Training Loss: 0.5826%\n",
      "Epoch [36/300], Step [144/225], Training Accuracy: 73.1228%, Training Loss: 0.5826%\n",
      "Epoch [36/300], Step [145/225], Training Accuracy: 73.1034%, Training Loss: 0.5828%\n",
      "Epoch [36/300], Step [146/225], Training Accuracy: 73.0843%, Training Loss: 0.5831%\n",
      "Epoch [36/300], Step [147/225], Training Accuracy: 73.1186%, Training Loss: 0.5825%\n",
      "Epoch [36/300], Step [148/225], Training Accuracy: 73.2158%, Training Loss: 0.5813%\n",
      "Epoch [36/300], Step [149/225], Training Accuracy: 73.2278%, Training Loss: 0.5810%\n",
      "Epoch [36/300], Step [150/225], Training Accuracy: 73.2188%, Training Loss: 0.5813%\n",
      "Epoch [36/300], Step [151/225], Training Accuracy: 73.3340%, Training Loss: 0.5801%\n",
      "Epoch [36/300], Step [152/225], Training Accuracy: 73.3244%, Training Loss: 0.5812%\n",
      "Epoch [36/300], Step [153/225], Training Accuracy: 73.3150%, Training Loss: 0.5812%\n",
      "Epoch [36/300], Step [154/225], Training Accuracy: 73.3056%, Training Loss: 0.5810%\n",
      "Epoch [36/300], Step [155/225], Training Accuracy: 73.2863%, Training Loss: 0.5817%\n",
      "Epoch [36/300], Step [156/225], Training Accuracy: 73.2272%, Training Loss: 0.5829%\n",
      "Epoch [36/300], Step [157/225], Training Accuracy: 73.2086%, Training Loss: 0.5831%\n",
      "Epoch [36/300], Step [158/225], Training Accuracy: 73.1804%, Training Loss: 0.5832%\n",
      "Epoch [36/300], Step [159/225], Training Accuracy: 73.1820%, Training Loss: 0.5840%\n",
      "Epoch [36/300], Step [160/225], Training Accuracy: 73.2227%, Training Loss: 0.5835%\n",
      "Epoch [36/300], Step [161/225], Training Accuracy: 73.2725%, Training Loss: 0.5831%\n",
      "Epoch [36/300], Step [162/225], Training Accuracy: 73.3700%, Training Loss: 0.5822%\n",
      "Epoch [36/300], Step [163/225], Training Accuracy: 73.3416%, Training Loss: 0.5825%\n",
      "Epoch [36/300], Step [164/225], Training Accuracy: 73.3708%, Training Loss: 0.5816%\n",
      "Epoch [36/300], Step [165/225], Training Accuracy: 73.4091%, Training Loss: 0.5813%\n",
      "Epoch [36/300], Step [166/225], Training Accuracy: 73.4093%, Training Loss: 0.5811%\n",
      "Epoch [36/300], Step [167/225], Training Accuracy: 73.4375%, Training Loss: 0.5808%\n",
      "Epoch [36/300], Step [168/225], Training Accuracy: 73.4468%, Training Loss: 0.5812%\n",
      "Epoch [36/300], Step [169/225], Training Accuracy: 73.4098%, Training Loss: 0.5818%\n",
      "Epoch [36/300], Step [170/225], Training Accuracy: 73.4375%, Training Loss: 0.5819%\n",
      "Epoch [36/300], Step [171/225], Training Accuracy: 73.4375%, Training Loss: 0.5814%\n",
      "Epoch [36/300], Step [172/225], Training Accuracy: 73.4102%, Training Loss: 0.5814%\n",
      "Epoch [36/300], Step [173/225], Training Accuracy: 73.3743%, Training Loss: 0.5814%\n",
      "Epoch [36/300], Step [174/225], Training Accuracy: 73.3836%, Training Loss: 0.5810%\n",
      "Epoch [36/300], Step [175/225], Training Accuracy: 73.4018%, Training Loss: 0.5809%\n",
      "Epoch [36/300], Step [176/225], Training Accuracy: 73.4109%, Training Loss: 0.5807%\n",
      "Epoch [36/300], Step [177/225], Training Accuracy: 73.4110%, Training Loss: 0.5807%\n",
      "Epoch [36/300], Step [178/225], Training Accuracy: 73.4287%, Training Loss: 0.5804%\n",
      "Epoch [36/300], Step [179/225], Training Accuracy: 73.4550%, Training Loss: 0.5796%\n",
      "Epoch [36/300], Step [180/225], Training Accuracy: 73.4201%, Training Loss: 0.5798%\n",
      "Epoch [36/300], Step [181/225], Training Accuracy: 73.4202%, Training Loss: 0.5796%\n",
      "Epoch [36/300], Step [182/225], Training Accuracy: 73.4633%, Training Loss: 0.5794%\n",
      "Epoch [36/300], Step [183/225], Training Accuracy: 73.4119%, Training Loss: 0.5802%\n",
      "Epoch [36/300], Step [184/225], Training Accuracy: 73.4290%, Training Loss: 0.5798%\n",
      "Epoch [36/300], Step [185/225], Training Accuracy: 73.4628%, Training Loss: 0.5793%\n",
      "Epoch [36/300], Step [186/225], Training Accuracy: 73.4711%, Training Loss: 0.5789%\n",
      "Epoch [36/300], Step [187/225], Training Accuracy: 73.4626%, Training Loss: 0.5793%\n",
      "Epoch [36/300], Step [188/225], Training Accuracy: 73.5040%, Training Loss: 0.5787%\n",
      "Epoch [36/300], Step [189/225], Training Accuracy: 73.5036%, Training Loss: 0.5782%\n",
      "Epoch [36/300], Step [190/225], Training Accuracy: 73.4951%, Training Loss: 0.5785%\n",
      "Epoch [36/300], Step [191/225], Training Accuracy: 73.4702%, Training Loss: 0.5786%\n",
      "Epoch [36/300], Step [192/225], Training Accuracy: 73.5107%, Training Loss: 0.5780%\n",
      "Epoch [36/300], Step [193/225], Training Accuracy: 73.5104%, Training Loss: 0.5781%\n",
      "Epoch [36/300], Step [194/225], Training Accuracy: 73.5019%, Training Loss: 0.5782%\n",
      "Epoch [36/300], Step [195/225], Training Accuracy: 73.5016%, Training Loss: 0.5780%\n",
      "Epoch [36/300], Step [196/225], Training Accuracy: 73.4534%, Training Loss: 0.5784%\n",
      "Epoch [36/300], Step [197/225], Training Accuracy: 73.4296%, Training Loss: 0.5788%\n",
      "Epoch [36/300], Step [198/225], Training Accuracy: 73.4770%, Training Loss: 0.5778%\n",
      "Epoch [36/300], Step [199/225], Training Accuracy: 73.4846%, Training Loss: 0.5780%\n",
      "Epoch [36/300], Step [200/225], Training Accuracy: 73.4375%, Training Loss: 0.5782%\n",
      "Epoch [36/300], Step [201/225], Training Accuracy: 73.3986%, Training Loss: 0.5784%\n",
      "Epoch [36/300], Step [202/225], Training Accuracy: 73.4066%, Training Loss: 0.5782%\n",
      "Epoch [36/300], Step [203/225], Training Accuracy: 73.4683%, Training Loss: 0.5782%\n",
      "Epoch [36/300], Step [204/225], Training Accuracy: 73.4758%, Training Loss: 0.5782%\n",
      "Epoch [36/300], Step [205/225], Training Accuracy: 73.4985%, Training Loss: 0.5775%\n",
      "Epoch [36/300], Step [206/225], Training Accuracy: 73.4678%, Training Loss: 0.5778%\n",
      "Epoch [36/300], Step [207/225], Training Accuracy: 73.4979%, Training Loss: 0.5777%\n",
      "Epoch [36/300], Step [208/225], Training Accuracy: 73.5577%, Training Loss: 0.5772%\n",
      "Epoch [36/300], Step [209/225], Training Accuracy: 73.5571%, Training Loss: 0.5773%\n",
      "Epoch [36/300], Step [210/225], Training Accuracy: 73.5268%, Training Loss: 0.5776%\n",
      "Epoch [36/300], Step [211/225], Training Accuracy: 73.5634%, Training Loss: 0.5771%\n",
      "Epoch [36/300], Step [212/225], Training Accuracy: 73.5333%, Training Loss: 0.5777%\n",
      "Epoch [36/300], Step [213/225], Training Accuracy: 73.5402%, Training Loss: 0.5777%\n",
      "Epoch [36/300], Step [214/225], Training Accuracy: 73.5616%, Training Loss: 0.5774%\n",
      "Epoch [36/300], Step [215/225], Training Accuracy: 73.5756%, Training Loss: 0.5772%\n",
      "Epoch [36/300], Step [216/225], Training Accuracy: 73.5315%, Training Loss: 0.5773%\n",
      "Epoch [36/300], Step [217/225], Training Accuracy: 73.5383%, Training Loss: 0.5774%\n",
      "Epoch [36/300], Step [218/225], Training Accuracy: 73.5235%, Training Loss: 0.5773%\n",
      "Epoch [36/300], Step [219/225], Training Accuracy: 73.4874%, Training Loss: 0.5778%\n",
      "Epoch [36/300], Step [220/225], Training Accuracy: 73.4801%, Training Loss: 0.5781%\n",
      "Epoch [36/300], Step [221/225], Training Accuracy: 73.4234%, Training Loss: 0.5790%\n",
      "Epoch [36/300], Step [222/225], Training Accuracy: 73.4445%, Training Loss: 0.5784%\n",
      "Epoch [36/300], Step [223/225], Training Accuracy: 73.4095%, Training Loss: 0.5788%\n",
      "Epoch [36/300], Step [224/225], Training Accuracy: 73.4305%, Training Loss: 0.5784%\n",
      "Epoch [36/300], Step [225/225], Training Accuracy: 73.4088%, Training Loss: 0.5786%\n",
      "Epoch [37/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.5802%\n",
      "Epoch [37/300], Step [2/225], Training Accuracy: 74.2188%, Training Loss: 0.5370%\n",
      "Epoch [37/300], Step [3/225], Training Accuracy: 73.4375%, Training Loss: 0.5649%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Step [4/225], Training Accuracy: 75.3906%, Training Loss: 0.5386%\n",
      "Epoch [37/300], Step [5/225], Training Accuracy: 75.0000%, Training Loss: 0.5252%\n",
      "Epoch [37/300], Step [6/225], Training Accuracy: 76.0417%, Training Loss: 0.5283%\n",
      "Epoch [37/300], Step [7/225], Training Accuracy: 76.3393%, Training Loss: 0.5240%\n",
      "Epoch [37/300], Step [8/225], Training Accuracy: 75.5859%, Training Loss: 0.5305%\n",
      "Epoch [37/300], Step [9/225], Training Accuracy: 74.4792%, Training Loss: 0.5484%\n",
      "Epoch [37/300], Step [10/225], Training Accuracy: 75.4688%, Training Loss: 0.5494%\n",
      "Epoch [37/300], Step [11/225], Training Accuracy: 75.2841%, Training Loss: 0.5477%\n",
      "Epoch [37/300], Step [12/225], Training Accuracy: 75.9115%, Training Loss: 0.5410%\n",
      "Epoch [37/300], Step [13/225], Training Accuracy: 76.5625%, Training Loss: 0.5272%\n",
      "Epoch [37/300], Step [14/225], Training Accuracy: 76.5625%, Training Loss: 0.5202%\n",
      "Epoch [37/300], Step [15/225], Training Accuracy: 76.5625%, Training Loss: 0.5218%\n",
      "Epoch [37/300], Step [16/225], Training Accuracy: 76.4648%, Training Loss: 0.5195%\n",
      "Epoch [37/300], Step [17/225], Training Accuracy: 76.4706%, Training Loss: 0.5174%\n",
      "Epoch [37/300], Step [18/225], Training Accuracy: 76.1285%, Training Loss: 0.5230%\n",
      "Epoch [37/300], Step [19/225], Training Accuracy: 76.1513%, Training Loss: 0.5227%\n",
      "Epoch [37/300], Step [20/225], Training Accuracy: 75.8594%, Training Loss: 0.5237%\n",
      "Epoch [37/300], Step [21/225], Training Accuracy: 76.1905%, Training Loss: 0.5209%\n",
      "Epoch [37/300], Step [22/225], Training Accuracy: 75.6392%, Training Loss: 0.5284%\n",
      "Epoch [37/300], Step [23/225], Training Accuracy: 75.6793%, Training Loss: 0.5252%\n",
      "Epoch [37/300], Step [24/225], Training Accuracy: 75.2604%, Training Loss: 0.5290%\n",
      "Epoch [37/300], Step [25/225], Training Accuracy: 75.3750%, Training Loss: 0.5315%\n",
      "Epoch [37/300], Step [26/225], Training Accuracy: 75.2404%, Training Loss: 0.5350%\n",
      "Epoch [37/300], Step [27/225], Training Accuracy: 75.0579%, Training Loss: 0.5365%\n",
      "Epoch [37/300], Step [28/225], Training Accuracy: 75.3906%, Training Loss: 0.5323%\n",
      "Epoch [37/300], Step [29/225], Training Accuracy: 75.3233%, Training Loss: 0.5391%\n",
      "Epoch [37/300], Step [30/225], Training Accuracy: 75.2083%, Training Loss: 0.5418%\n",
      "Epoch [37/300], Step [31/225], Training Accuracy: 74.8992%, Training Loss: 0.5528%\n",
      "Epoch [37/300], Step [32/225], Training Accuracy: 74.7559%, Training Loss: 0.5529%\n",
      "Epoch [37/300], Step [33/225], Training Accuracy: 74.8106%, Training Loss: 0.5529%\n",
      "Epoch [37/300], Step [34/225], Training Accuracy: 74.6783%, Training Loss: 0.5526%\n",
      "Epoch [37/300], Step [35/225], Training Accuracy: 74.5536%, Training Loss: 0.5550%\n",
      "Epoch [37/300], Step [36/225], Training Accuracy: 74.6962%, Training Loss: 0.5526%\n",
      "Epoch [37/300], Step [37/225], Training Accuracy: 74.5777%, Training Loss: 0.5522%\n",
      "Epoch [37/300], Step [38/225], Training Accuracy: 74.5888%, Training Loss: 0.5522%\n",
      "Epoch [37/300], Step [39/225], Training Accuracy: 74.5593%, Training Loss: 0.5532%\n",
      "Epoch [37/300], Step [40/225], Training Accuracy: 74.6094%, Training Loss: 0.5525%\n",
      "Epoch [37/300], Step [41/225], Training Accuracy: 74.5427%, Training Loss: 0.5548%\n",
      "Epoch [37/300], Step [42/225], Training Accuracy: 74.5164%, Training Loss: 0.5544%\n",
      "Epoch [37/300], Step [43/225], Training Accuracy: 74.3096%, Training Loss: 0.5555%\n",
      "Epoch [37/300], Step [44/225], Training Accuracy: 74.4318%, Training Loss: 0.5528%\n",
      "Epoch [37/300], Step [45/225], Training Accuracy: 74.5139%, Training Loss: 0.5511%\n",
      "Epoch [37/300], Step [46/225], Training Accuracy: 74.5924%, Training Loss: 0.5506%\n",
      "Epoch [37/300], Step [47/225], Training Accuracy: 74.4348%, Training Loss: 0.5524%\n",
      "Epoch [37/300], Step [48/225], Training Accuracy: 74.1862%, Training Loss: 0.5542%\n",
      "Epoch [37/300], Step [49/225], Training Accuracy: 74.2985%, Training Loss: 0.5526%\n",
      "Epoch [37/300], Step [50/225], Training Accuracy: 74.2812%, Training Loss: 0.5522%\n",
      "Epoch [37/300], Step [51/225], Training Accuracy: 74.3873%, Training Loss: 0.5519%\n",
      "Epoch [37/300], Step [52/225], Training Accuracy: 74.5493%, Training Loss: 0.5508%\n",
      "Epoch [37/300], Step [53/225], Training Accuracy: 74.5578%, Training Loss: 0.5533%\n",
      "Epoch [37/300], Step [54/225], Training Accuracy: 74.5370%, Training Loss: 0.5545%\n",
      "Epoch [37/300], Step [55/225], Training Accuracy: 74.4318%, Training Loss: 0.5555%\n",
      "Epoch [37/300], Step [56/225], Training Accuracy: 74.4141%, Training Loss: 0.5565%\n",
      "Epoch [37/300], Step [57/225], Training Accuracy: 74.3421%, Training Loss: 0.5556%\n",
      "Epoch [37/300], Step [58/225], Training Accuracy: 74.2996%, Training Loss: 0.5561%\n",
      "Epoch [37/300], Step [59/225], Training Accuracy: 74.3114%, Training Loss: 0.5557%\n",
      "Epoch [37/300], Step [60/225], Training Accuracy: 74.2708%, Training Loss: 0.5559%\n",
      "Epoch [37/300], Step [61/225], Training Accuracy: 74.1547%, Training Loss: 0.5579%\n",
      "Epoch [37/300], Step [62/225], Training Accuracy: 74.0927%, Training Loss: 0.5593%\n",
      "Epoch [37/300], Step [63/225], Training Accuracy: 74.0575%, Training Loss: 0.5618%\n",
      "Epoch [37/300], Step [64/225], Training Accuracy: 74.1943%, Training Loss: 0.5601%\n",
      "Epoch [37/300], Step [65/225], Training Accuracy: 74.2788%, Training Loss: 0.5592%\n",
      "Epoch [37/300], Step [66/225], Training Accuracy: 74.3608%, Training Loss: 0.5582%\n",
      "Epoch [37/300], Step [67/225], Training Accuracy: 74.3237%, Training Loss: 0.5586%\n",
      "Epoch [37/300], Step [68/225], Training Accuracy: 74.3107%, Training Loss: 0.5593%\n",
      "Epoch [37/300], Step [69/225], Training Accuracy: 74.3207%, Training Loss: 0.5604%\n",
      "Epoch [37/300], Step [70/225], Training Accuracy: 74.3080%, Training Loss: 0.5604%\n",
      "Epoch [37/300], Step [71/225], Training Accuracy: 74.2518%, Training Loss: 0.5605%\n",
      "Epoch [37/300], Step [72/225], Training Accuracy: 74.1753%, Training Loss: 0.5612%\n",
      "Epoch [37/300], Step [73/225], Training Accuracy: 73.9940%, Training Loss: 0.5628%\n",
      "Epoch [37/300], Step [74/225], Training Accuracy: 73.9443%, Training Loss: 0.5637%\n",
      "Epoch [37/300], Step [75/225], Training Accuracy: 73.9792%, Training Loss: 0.5630%\n",
      "Epoch [37/300], Step [76/225], Training Accuracy: 73.8898%, Training Loss: 0.5646%\n",
      "Epoch [37/300], Step [77/225], Training Accuracy: 73.9448%, Training Loss: 0.5644%\n",
      "Epoch [37/300], Step [78/225], Training Accuracy: 73.9383%, Training Loss: 0.5645%\n",
      "Epoch [37/300], Step [79/225], Training Accuracy: 73.9715%, Training Loss: 0.5641%\n",
      "Epoch [37/300], Step [80/225], Training Accuracy: 73.9648%, Training Loss: 0.5641%\n",
      "Epoch [37/300], Step [81/225], Training Accuracy: 73.9969%, Training Loss: 0.5639%\n",
      "Epoch [37/300], Step [82/225], Training Accuracy: 74.0091%, Training Loss: 0.5627%\n",
      "Epoch [37/300], Step [83/225], Training Accuracy: 73.9646%, Training Loss: 0.5630%\n",
      "Epoch [37/300], Step [84/225], Training Accuracy: 73.9955%, Training Loss: 0.5628%\n",
      "Epoch [37/300], Step [85/225], Training Accuracy: 74.0625%, Training Loss: 0.5615%\n",
      "Epoch [37/300], Step [86/225], Training Accuracy: 74.0007%, Training Loss: 0.5623%\n",
      "Epoch [37/300], Step [87/225], Training Accuracy: 73.9224%, Training Loss: 0.5634%\n",
      "Epoch [37/300], Step [88/225], Training Accuracy: 73.7393%, Training Loss: 0.5650%\n",
      "Epoch [37/300], Step [89/225], Training Accuracy: 73.7535%, Training Loss: 0.5659%\n",
      "Epoch [37/300], Step [90/225], Training Accuracy: 73.7500%, Training Loss: 0.5669%\n",
      "Epoch [37/300], Step [91/225], Training Accuracy: 73.6951%, Training Loss: 0.5669%\n",
      "Epoch [37/300], Step [92/225], Training Accuracy: 73.6583%, Training Loss: 0.5672%\n",
      "Epoch [37/300], Step [93/225], Training Accuracy: 73.6895%, Training Loss: 0.5663%\n",
      "Epoch [37/300], Step [94/225], Training Accuracy: 73.7699%, Training Loss: 0.5653%\n",
      "Epoch [37/300], Step [95/225], Training Accuracy: 73.8158%, Training Loss: 0.5649%\n",
      "Epoch [37/300], Step [96/225], Training Accuracy: 73.7956%, Training Loss: 0.5645%\n",
      "Epoch [37/300], Step [97/225], Training Accuracy: 73.8241%, Training Loss: 0.5639%\n",
      "Epoch [37/300], Step [98/225], Training Accuracy: 73.7245%, Training Loss: 0.5666%\n",
      "Epoch [37/300], Step [99/225], Training Accuracy: 73.6585%, Training Loss: 0.5674%\n",
      "Epoch [37/300], Step [100/225], Training Accuracy: 73.5781%, Training Loss: 0.5695%\n",
      "Epoch [37/300], Step [101/225], Training Accuracy: 73.5767%, Training Loss: 0.5702%\n",
      "Epoch [37/300], Step [102/225], Training Accuracy: 73.5294%, Training Loss: 0.5709%\n",
      "Epoch [37/300], Step [103/225], Training Accuracy: 73.5133%, Training Loss: 0.5707%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Step [104/225], Training Accuracy: 73.4225%, Training Loss: 0.5721%\n",
      "Epoch [37/300], Step [105/225], Training Accuracy: 73.4524%, Training Loss: 0.5722%\n",
      "Epoch [37/300], Step [106/225], Training Accuracy: 73.4670%, Training Loss: 0.5724%\n",
      "Epoch [37/300], Step [107/225], Training Accuracy: 73.3353%, Training Loss: 0.5740%\n",
      "Epoch [37/300], Step [108/225], Training Accuracy: 73.3218%, Training Loss: 0.5744%\n",
      "Epoch [37/300], Step [109/225], Training Accuracy: 73.2942%, Training Loss: 0.5747%\n",
      "Epoch [37/300], Step [110/225], Training Accuracy: 73.3097%, Training Loss: 0.5748%\n",
      "Epoch [37/300], Step [111/225], Training Accuracy: 73.3249%, Training Loss: 0.5743%\n",
      "Epoch [37/300], Step [112/225], Training Accuracy: 73.2980%, Training Loss: 0.5741%\n",
      "Epoch [37/300], Step [113/225], Training Accuracy: 73.2992%, Training Loss: 0.5740%\n",
      "Epoch [37/300], Step [114/225], Training Accuracy: 73.3279%, Training Loss: 0.5734%\n",
      "Epoch [37/300], Step [115/225], Training Accuracy: 73.3560%, Training Loss: 0.5730%\n",
      "Epoch [37/300], Step [116/225], Training Accuracy: 73.3836%, Training Loss: 0.5734%\n",
      "Epoch [37/300], Step [117/225], Training Accuracy: 73.3040%, Training Loss: 0.5755%\n",
      "Epoch [37/300], Step [118/225], Training Accuracy: 73.2654%, Training Loss: 0.5766%\n",
      "Epoch [37/300], Step [119/225], Training Accuracy: 73.2274%, Training Loss: 0.5772%\n",
      "Epoch [37/300], Step [120/225], Training Accuracy: 73.2161%, Training Loss: 0.5775%\n",
      "Epoch [37/300], Step [121/225], Training Accuracy: 73.1276%, Training Loss: 0.5781%\n",
      "Epoch [37/300], Step [122/225], Training Accuracy: 73.0917%, Training Loss: 0.5787%\n",
      "Epoch [37/300], Step [123/225], Training Accuracy: 73.1199%, Training Loss: 0.5793%\n",
      "Epoch [37/300], Step [124/225], Training Accuracy: 73.1225%, Training Loss: 0.5788%\n",
      "Epoch [37/300], Step [125/225], Training Accuracy: 73.1125%, Training Loss: 0.5791%\n",
      "Epoch [37/300], Step [126/225], Training Accuracy: 73.1027%, Training Loss: 0.5798%\n",
      "Epoch [37/300], Step [127/225], Training Accuracy: 73.1053%, Training Loss: 0.5805%\n",
      "Epoch [37/300], Step [128/225], Training Accuracy: 73.0713%, Training Loss: 0.5815%\n",
      "Epoch [37/300], Step [129/225], Training Accuracy: 73.0257%, Training Loss: 0.5823%\n",
      "Epoch [37/300], Step [130/225], Training Accuracy: 73.0529%, Training Loss: 0.5824%\n",
      "Epoch [37/300], Step [131/225], Training Accuracy: 73.1274%, Training Loss: 0.5821%\n",
      "Epoch [37/300], Step [132/225], Training Accuracy: 73.0942%, Training Loss: 0.5824%\n",
      "Epoch [37/300], Step [133/225], Training Accuracy: 73.0498%, Training Loss: 0.5823%\n",
      "Epoch [37/300], Step [134/225], Training Accuracy: 73.0410%, Training Loss: 0.5830%\n",
      "Epoch [37/300], Step [135/225], Training Accuracy: 73.0671%, Training Loss: 0.5829%\n",
      "Epoch [37/300], Step [136/225], Training Accuracy: 73.0469%, Training Loss: 0.5827%\n",
      "Epoch [37/300], Step [137/225], Training Accuracy: 73.0383%, Training Loss: 0.5828%\n",
      "Epoch [37/300], Step [138/225], Training Accuracy: 73.1091%, Training Loss: 0.5816%\n",
      "Epoch [37/300], Step [139/225], Training Accuracy: 73.1003%, Training Loss: 0.5822%\n",
      "Epoch [37/300], Step [140/225], Training Accuracy: 73.1027%, Training Loss: 0.5821%\n",
      "Epoch [37/300], Step [141/225], Training Accuracy: 73.0940%, Training Loss: 0.5822%\n",
      "Epoch [37/300], Step [142/225], Training Accuracy: 73.1184%, Training Loss: 0.5821%\n",
      "Epoch [37/300], Step [143/225], Training Accuracy: 73.1206%, Training Loss: 0.5819%\n",
      "Epoch [37/300], Step [144/225], Training Accuracy: 73.0903%, Training Loss: 0.5817%\n",
      "Epoch [37/300], Step [145/225], Training Accuracy: 73.1142%, Training Loss: 0.5813%\n",
      "Epoch [37/300], Step [146/225], Training Accuracy: 73.1057%, Training Loss: 0.5814%\n",
      "Epoch [37/300], Step [147/225], Training Accuracy: 73.0761%, Training Loss: 0.5820%\n",
      "Epoch [37/300], Step [148/225], Training Accuracy: 73.1419%, Training Loss: 0.5815%\n",
      "Epoch [37/300], Step [149/225], Training Accuracy: 73.2278%, Training Loss: 0.5811%\n",
      "Epoch [37/300], Step [150/225], Training Accuracy: 73.1979%, Training Loss: 0.5813%\n",
      "Epoch [37/300], Step [151/225], Training Accuracy: 73.2305%, Training Loss: 0.5806%\n",
      "Epoch [37/300], Step [152/225], Training Accuracy: 73.2319%, Training Loss: 0.5805%\n",
      "Epoch [37/300], Step [153/225], Training Accuracy: 73.2435%, Training Loss: 0.5806%\n",
      "Epoch [37/300], Step [154/225], Training Accuracy: 73.2853%, Training Loss: 0.5802%\n",
      "Epoch [37/300], Step [155/225], Training Accuracy: 73.2258%, Training Loss: 0.5814%\n",
      "Epoch [37/300], Step [156/225], Training Accuracy: 73.1971%, Training Loss: 0.5819%\n",
      "Epoch [37/300], Step [157/225], Training Accuracy: 73.1986%, Training Loss: 0.5820%\n",
      "Epoch [37/300], Step [158/225], Training Accuracy: 73.1705%, Training Loss: 0.5825%\n",
      "Epoch [37/300], Step [159/225], Training Accuracy: 73.1230%, Training Loss: 0.5829%\n",
      "Epoch [37/300], Step [160/225], Training Accuracy: 73.1250%, Training Loss: 0.5825%\n",
      "Epoch [37/300], Step [161/225], Training Accuracy: 73.1172%, Training Loss: 0.5826%\n",
      "Epoch [37/300], Step [162/225], Training Accuracy: 73.1867%, Training Loss: 0.5818%\n",
      "Epoch [37/300], Step [163/225], Training Accuracy: 73.2074%, Training Loss: 0.5819%\n",
      "Epoch [37/300], Step [164/225], Training Accuracy: 73.2374%, Training Loss: 0.5813%\n",
      "Epoch [37/300], Step [165/225], Training Accuracy: 73.3144%, Training Loss: 0.5806%\n",
      "Epoch [37/300], Step [166/225], Training Accuracy: 73.3528%, Training Loss: 0.5803%\n",
      "Epoch [37/300], Step [167/225], Training Accuracy: 73.3533%, Training Loss: 0.5803%\n",
      "Epoch [37/300], Step [168/225], Training Accuracy: 73.2887%, Training Loss: 0.5810%\n",
      "Epoch [37/300], Step [169/225], Training Accuracy: 73.2711%, Training Loss: 0.5812%\n",
      "Epoch [37/300], Step [170/225], Training Accuracy: 73.2629%, Training Loss: 0.5811%\n",
      "Epoch [37/300], Step [171/225], Training Accuracy: 73.2822%, Training Loss: 0.5809%\n",
      "Epoch [37/300], Step [172/225], Training Accuracy: 73.2740%, Training Loss: 0.5809%\n",
      "Epoch [37/300], Step [173/225], Training Accuracy: 73.2569%, Training Loss: 0.5813%\n",
      "Epoch [37/300], Step [174/225], Training Accuracy: 73.2759%, Training Loss: 0.5807%\n",
      "Epoch [37/300], Step [175/225], Training Accuracy: 73.3125%, Training Loss: 0.5805%\n",
      "Epoch [37/300], Step [176/225], Training Accuracy: 73.3310%, Training Loss: 0.5800%\n",
      "Epoch [37/300], Step [177/225], Training Accuracy: 73.3404%, Training Loss: 0.5797%\n",
      "Epoch [37/300], Step [178/225], Training Accuracy: 73.3322%, Training Loss: 0.5799%\n",
      "Epoch [37/300], Step [179/225], Training Accuracy: 73.3677%, Training Loss: 0.5796%\n",
      "Epoch [37/300], Step [180/225], Training Accuracy: 73.3507%, Training Loss: 0.5796%\n",
      "Epoch [37/300], Step [181/225], Training Accuracy: 73.3857%, Training Loss: 0.5795%\n",
      "Epoch [37/300], Step [182/225], Training Accuracy: 73.3774%, Training Loss: 0.5794%\n",
      "Epoch [37/300], Step [183/225], Training Accuracy: 73.3692%, Training Loss: 0.5796%\n",
      "Epoch [37/300], Step [184/225], Training Accuracy: 73.4035%, Training Loss: 0.5794%\n",
      "Epoch [37/300], Step [185/225], Training Accuracy: 73.4628%, Training Loss: 0.5787%\n",
      "Epoch [37/300], Step [186/225], Training Accuracy: 73.5131%, Training Loss: 0.5781%\n",
      "Epoch [37/300], Step [187/225], Training Accuracy: 73.5043%, Training Loss: 0.5784%\n",
      "Epoch [37/300], Step [188/225], Training Accuracy: 73.5539%, Training Loss: 0.5777%\n",
      "Epoch [37/300], Step [189/225], Training Accuracy: 73.5698%, Training Loss: 0.5774%\n",
      "Epoch [37/300], Step [190/225], Training Accuracy: 73.5691%, Training Loss: 0.5774%\n",
      "Epoch [37/300], Step [191/225], Training Accuracy: 73.5520%, Training Loss: 0.5771%\n",
      "Epoch [37/300], Step [192/225], Training Accuracy: 73.5677%, Training Loss: 0.5763%\n",
      "Epoch [37/300], Step [193/225], Training Accuracy: 73.5347%, Training Loss: 0.5771%\n",
      "Epoch [37/300], Step [194/225], Training Accuracy: 73.5341%, Training Loss: 0.5772%\n",
      "Epoch [37/300], Step [195/225], Training Accuracy: 73.5497%, Training Loss: 0.5769%\n",
      "Epoch [37/300], Step [196/225], Training Accuracy: 73.5411%, Training Loss: 0.5773%\n",
      "Epoch [37/300], Step [197/225], Training Accuracy: 73.5327%, Training Loss: 0.5776%\n",
      "Epoch [37/300], Step [198/225], Training Accuracy: 73.5874%, Training Loss: 0.5765%\n",
      "Epoch [37/300], Step [199/225], Training Accuracy: 73.6102%, Training Loss: 0.5759%\n",
      "Epoch [37/300], Step [200/225], Training Accuracy: 73.5859%, Training Loss: 0.5757%\n",
      "Epoch [37/300], Step [201/225], Training Accuracy: 73.6085%, Training Loss: 0.5756%\n",
      "Epoch [37/300], Step [202/225], Training Accuracy: 73.6541%, Training Loss: 0.5750%\n",
      "Epoch [37/300], Step [203/225], Training Accuracy: 73.7069%, Training Loss: 0.5746%\n",
      "Epoch [37/300], Step [204/225], Training Accuracy: 73.7209%, Training Loss: 0.5743%\n",
      "Epoch [37/300], Step [205/225], Training Accuracy: 73.7652%, Training Loss: 0.5736%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Step [206/225], Training Accuracy: 73.8016%, Training Loss: 0.5732%\n",
      "Epoch [37/300], Step [207/225], Training Accuracy: 73.8225%, Training Loss: 0.5733%\n",
      "Epoch [37/300], Step [208/225], Training Accuracy: 73.8732%, Training Loss: 0.5730%\n",
      "Epoch [37/300], Step [209/225], Training Accuracy: 73.8861%, Training Loss: 0.5727%\n",
      "Epoch [37/300], Step [210/225], Training Accuracy: 73.8839%, Training Loss: 0.5728%\n",
      "Epoch [37/300], Step [211/225], Training Accuracy: 73.8966%, Training Loss: 0.5727%\n",
      "Epoch [37/300], Step [212/225], Training Accuracy: 73.8723%, Training Loss: 0.5731%\n",
      "Epoch [37/300], Step [213/225], Training Accuracy: 73.8703%, Training Loss: 0.5734%\n",
      "Epoch [37/300], Step [214/225], Training Accuracy: 73.8537%, Training Loss: 0.5732%\n",
      "Epoch [37/300], Step [215/225], Training Accuracy: 73.8590%, Training Loss: 0.5730%\n",
      "Epoch [37/300], Step [216/225], Training Accuracy: 73.8426%, Training Loss: 0.5729%\n",
      "Epoch [37/300], Step [217/225], Training Accuracy: 73.8479%, Training Loss: 0.5730%\n",
      "Epoch [37/300], Step [218/225], Training Accuracy: 73.8245%, Training Loss: 0.5735%\n",
      "Epoch [37/300], Step [219/225], Training Accuracy: 73.8228%, Training Loss: 0.5736%\n",
      "Epoch [37/300], Step [220/225], Training Accuracy: 73.8352%, Training Loss: 0.5733%\n",
      "Epoch [37/300], Step [221/225], Training Accuracy: 73.7910%, Training Loss: 0.5743%\n",
      "Epoch [37/300], Step [222/225], Training Accuracy: 73.8176%, Training Loss: 0.5741%\n",
      "Epoch [37/300], Step [223/225], Training Accuracy: 73.7738%, Training Loss: 0.5746%\n",
      "Epoch [37/300], Step [224/225], Training Accuracy: 73.7653%, Training Loss: 0.5743%\n",
      "Epoch [37/300], Step [225/225], Training Accuracy: 73.7146%, Training Loss: 0.5744%\n",
      "Epoch [38/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.5214%\n",
      "Epoch [38/300], Step [2/225], Training Accuracy: 75.0000%, Training Loss: 0.5873%\n",
      "Epoch [38/300], Step [3/225], Training Accuracy: 73.4375%, Training Loss: 0.6102%\n",
      "Epoch [38/300], Step [4/225], Training Accuracy: 75.0000%, Training Loss: 0.5857%\n",
      "Epoch [38/300], Step [5/225], Training Accuracy: 75.0000%, Training Loss: 0.5669%\n",
      "Epoch [38/300], Step [6/225], Training Accuracy: 75.0000%, Training Loss: 0.5631%\n",
      "Epoch [38/300], Step [7/225], Training Accuracy: 76.7857%, Training Loss: 0.5492%\n",
      "Epoch [38/300], Step [8/225], Training Accuracy: 76.9531%, Training Loss: 0.5427%\n",
      "Epoch [38/300], Step [9/225], Training Accuracy: 76.0417%, Training Loss: 0.5549%\n",
      "Epoch [38/300], Step [10/225], Training Accuracy: 75.9375%, Training Loss: 0.5575%\n",
      "Epoch [38/300], Step [11/225], Training Accuracy: 75.8523%, Training Loss: 0.5502%\n",
      "Epoch [38/300], Step [12/225], Training Accuracy: 75.7812%, Training Loss: 0.5521%\n",
      "Epoch [38/300], Step [13/225], Training Accuracy: 76.2019%, Training Loss: 0.5491%\n",
      "Epoch [38/300], Step [14/225], Training Accuracy: 76.0045%, Training Loss: 0.5517%\n",
      "Epoch [38/300], Step [15/225], Training Accuracy: 76.2500%, Training Loss: 0.5496%\n",
      "Epoch [38/300], Step [16/225], Training Accuracy: 75.8789%, Training Loss: 0.5486%\n",
      "Epoch [38/300], Step [17/225], Training Accuracy: 76.1949%, Training Loss: 0.5440%\n",
      "Epoch [38/300], Step [18/225], Training Accuracy: 76.1285%, Training Loss: 0.5448%\n",
      "Epoch [38/300], Step [19/225], Training Accuracy: 76.4803%, Training Loss: 0.5436%\n",
      "Epoch [38/300], Step [20/225], Training Accuracy: 76.4844%, Training Loss: 0.5411%\n",
      "Epoch [38/300], Step [21/225], Training Accuracy: 76.6369%, Training Loss: 0.5401%\n",
      "Epoch [38/300], Step [22/225], Training Accuracy: 76.2074%, Training Loss: 0.5484%\n",
      "Epoch [38/300], Step [23/225], Training Accuracy: 76.6304%, Training Loss: 0.5449%\n",
      "Epoch [38/300], Step [24/225], Training Accuracy: 76.1719%, Training Loss: 0.5499%\n",
      "Epoch [38/300], Step [25/225], Training Accuracy: 76.1250%, Training Loss: 0.5498%\n",
      "Epoch [38/300], Step [26/225], Training Accuracy: 75.7812%, Training Loss: 0.5537%\n",
      "Epoch [38/300], Step [27/225], Training Accuracy: 75.6944%, Training Loss: 0.5536%\n",
      "Epoch [38/300], Step [28/225], Training Accuracy: 76.0045%, Training Loss: 0.5504%\n",
      "Epoch [38/300], Step [29/225], Training Accuracy: 75.8082%, Training Loss: 0.5539%\n",
      "Epoch [38/300], Step [30/225], Training Accuracy: 75.8333%, Training Loss: 0.5537%\n",
      "Epoch [38/300], Step [31/225], Training Accuracy: 75.5544%, Training Loss: 0.5601%\n",
      "Epoch [38/300], Step [32/225], Training Accuracy: 75.6348%, Training Loss: 0.5592%\n",
      "Epoch [38/300], Step [33/225], Training Accuracy: 75.8049%, Training Loss: 0.5554%\n",
      "Epoch [38/300], Step [34/225], Training Accuracy: 75.5515%, Training Loss: 0.5594%\n",
      "Epoch [38/300], Step [35/225], Training Accuracy: 75.4911%, Training Loss: 0.5596%\n",
      "Epoch [38/300], Step [36/225], Training Accuracy: 75.3038%, Training Loss: 0.5607%\n",
      "Epoch [38/300], Step [37/225], Training Accuracy: 75.2956%, Training Loss: 0.5602%\n",
      "Epoch [38/300], Step [38/225], Training Accuracy: 75.3701%, Training Loss: 0.5609%\n",
      "Epoch [38/300], Step [39/225], Training Accuracy: 75.3606%, Training Loss: 0.5632%\n",
      "Epoch [38/300], Step [40/225], Training Accuracy: 75.3125%, Training Loss: 0.5643%\n",
      "Epoch [38/300], Step [41/225], Training Accuracy: 75.2668%, Training Loss: 0.5652%\n",
      "Epoch [38/300], Step [42/225], Training Accuracy: 75.3720%, Training Loss: 0.5643%\n",
      "Epoch [38/300], Step [43/225], Training Accuracy: 75.2180%, Training Loss: 0.5667%\n",
      "Epoch [38/300], Step [44/225], Training Accuracy: 75.2841%, Training Loss: 0.5644%\n",
      "Epoch [38/300], Step [45/225], Training Accuracy: 75.3819%, Training Loss: 0.5619%\n",
      "Epoch [38/300], Step [46/225], Training Accuracy: 75.3057%, Training Loss: 0.5612%\n",
      "Epoch [38/300], Step [47/225], Training Accuracy: 75.2992%, Training Loss: 0.5616%\n",
      "Epoch [38/300], Step [48/225], Training Accuracy: 75.0651%, Training Loss: 0.5638%\n",
      "Epoch [38/300], Step [49/225], Training Accuracy: 75.1913%, Training Loss: 0.5622%\n",
      "Epoch [38/300], Step [50/225], Training Accuracy: 75.1250%, Training Loss: 0.5618%\n",
      "Epoch [38/300], Step [51/225], Training Accuracy: 75.0919%, Training Loss: 0.5615%\n",
      "Epoch [38/300], Step [52/225], Training Accuracy: 75.1803%, Training Loss: 0.5593%\n",
      "Epoch [38/300], Step [53/225], Training Accuracy: 75.0590%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [54/225], Training Accuracy: 75.0000%, Training Loss: 0.5627%\n",
      "Epoch [38/300], Step [55/225], Training Accuracy: 74.8011%, Training Loss: 0.5642%\n",
      "Epoch [38/300], Step [56/225], Training Accuracy: 74.7210%, Training Loss: 0.5642%\n",
      "Epoch [38/300], Step [57/225], Training Accuracy: 74.6985%, Training Loss: 0.5642%\n",
      "Epoch [38/300], Step [58/225], Training Accuracy: 74.7845%, Training Loss: 0.5643%\n",
      "Epoch [38/300], Step [59/225], Training Accuracy: 74.8676%, Training Loss: 0.5637%\n",
      "Epoch [38/300], Step [60/225], Training Accuracy: 74.9219%, Training Loss: 0.5620%\n",
      "Epoch [38/300], Step [61/225], Training Accuracy: 74.7182%, Training Loss: 0.5640%\n",
      "Epoch [38/300], Step [62/225], Training Accuracy: 74.6472%, Training Loss: 0.5648%\n",
      "Epoch [38/300], Step [63/225], Training Accuracy: 74.6032%, Training Loss: 0.5663%\n",
      "Epoch [38/300], Step [64/225], Training Accuracy: 74.8291%, Training Loss: 0.5640%\n",
      "Epoch [38/300], Step [65/225], Training Accuracy: 74.8558%, Training Loss: 0.5635%\n",
      "Epoch [38/300], Step [66/225], Training Accuracy: 74.8343%, Training Loss: 0.5630%\n",
      "Epoch [38/300], Step [67/225], Training Accuracy: 74.6968%, Training Loss: 0.5634%\n",
      "Epoch [38/300], Step [68/225], Training Accuracy: 74.7472%, Training Loss: 0.5630%\n",
      "Epoch [38/300], Step [69/225], Training Accuracy: 74.7056%, Training Loss: 0.5627%\n",
      "Epoch [38/300], Step [70/225], Training Accuracy: 74.6429%, Training Loss: 0.5639%\n",
      "Epoch [38/300], Step [71/225], Training Accuracy: 74.6699%, Training Loss: 0.5629%\n",
      "Epoch [38/300], Step [72/225], Training Accuracy: 74.6311%, Training Loss: 0.5637%\n",
      "Epoch [38/300], Step [73/225], Training Accuracy: 74.6147%, Training Loss: 0.5628%\n",
      "Epoch [38/300], Step [74/225], Training Accuracy: 74.5566%, Training Loss: 0.5623%\n",
      "Epoch [38/300], Step [75/225], Training Accuracy: 74.5417%, Training Loss: 0.5620%\n",
      "Epoch [38/300], Step [76/225], Training Accuracy: 74.4243%, Training Loss: 0.5630%\n",
      "Epoch [38/300], Step [77/225], Training Accuracy: 74.5739%, Training Loss: 0.5616%\n",
      "Epoch [38/300], Step [78/225], Training Accuracy: 74.5192%, Training Loss: 0.5614%\n",
      "Epoch [38/300], Step [79/225], Training Accuracy: 74.6044%, Training Loss: 0.5604%\n",
      "Epoch [38/300], Step [80/225], Training Accuracy: 74.6094%, Training Loss: 0.5612%\n",
      "Epoch [38/300], Step [81/225], Training Accuracy: 74.7106%, Training Loss: 0.5607%\n",
      "Epoch [38/300], Step [82/225], Training Accuracy: 74.7713%, Training Loss: 0.5598%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [83/225], Training Accuracy: 74.7553%, Training Loss: 0.5587%\n",
      "Epoch [38/300], Step [84/225], Training Accuracy: 74.7024%, Training Loss: 0.5591%\n",
      "Epoch [38/300], Step [85/225], Training Accuracy: 74.7610%, Training Loss: 0.5576%\n",
      "Epoch [38/300], Step [86/225], Training Accuracy: 74.6548%, Training Loss: 0.5585%\n",
      "Epoch [38/300], Step [87/225], Training Accuracy: 74.5690%, Training Loss: 0.5591%\n",
      "Epoch [38/300], Step [88/225], Training Accuracy: 74.5561%, Training Loss: 0.5604%\n",
      "Epoch [38/300], Step [89/225], Training Accuracy: 74.5084%, Training Loss: 0.5610%\n",
      "Epoch [38/300], Step [90/225], Training Accuracy: 74.3750%, Training Loss: 0.5639%\n",
      "Epoch [38/300], Step [91/225], Training Accuracy: 74.2788%, Training Loss: 0.5646%\n",
      "Epoch [38/300], Step [92/225], Training Accuracy: 74.3376%, Training Loss: 0.5647%\n",
      "Epoch [38/300], Step [93/225], Training Accuracy: 74.4624%, Training Loss: 0.5641%\n",
      "Epoch [38/300], Step [94/225], Training Accuracy: 74.4681%, Training Loss: 0.5645%\n",
      "Epoch [38/300], Step [95/225], Training Accuracy: 74.5724%, Training Loss: 0.5639%\n",
      "Epoch [38/300], Step [96/225], Training Accuracy: 74.5443%, Training Loss: 0.5627%\n",
      "Epoch [38/300], Step [97/225], Training Accuracy: 74.5329%, Training Loss: 0.5625%\n",
      "Epoch [38/300], Step [98/225], Training Accuracy: 74.4579%, Training Loss: 0.5631%\n",
      "Epoch [38/300], Step [99/225], Training Accuracy: 74.4476%, Training Loss: 0.5629%\n",
      "Epoch [38/300], Step [100/225], Training Accuracy: 74.4062%, Training Loss: 0.5640%\n",
      "Epoch [38/300], Step [101/225], Training Accuracy: 74.3502%, Training Loss: 0.5652%\n",
      "Epoch [38/300], Step [102/225], Training Accuracy: 74.3107%, Training Loss: 0.5651%\n",
      "Epoch [38/300], Step [103/225], Training Accuracy: 74.3932%, Training Loss: 0.5641%\n",
      "Epoch [38/300], Step [104/225], Training Accuracy: 74.3990%, Training Loss: 0.5647%\n",
      "Epoch [38/300], Step [105/225], Training Accuracy: 74.4048%, Training Loss: 0.5638%\n",
      "Epoch [38/300], Step [106/225], Training Accuracy: 74.3956%, Training Loss: 0.5634%\n",
      "Epoch [38/300], Step [107/225], Training Accuracy: 74.3721%, Training Loss: 0.5654%\n",
      "Epoch [38/300], Step [108/225], Training Accuracy: 74.3056%, Training Loss: 0.5658%\n",
      "Epoch [38/300], Step [109/225], Training Accuracy: 74.2976%, Training Loss: 0.5654%\n",
      "Epoch [38/300], Step [110/225], Training Accuracy: 74.2898%, Training Loss: 0.5657%\n",
      "Epoch [38/300], Step [111/225], Training Accuracy: 74.2821%, Training Loss: 0.5661%\n",
      "Epoch [38/300], Step [112/225], Training Accuracy: 74.2467%, Training Loss: 0.5667%\n",
      "Epoch [38/300], Step [113/225], Training Accuracy: 74.2948%, Training Loss: 0.5668%\n",
      "Epoch [38/300], Step [114/225], Training Accuracy: 74.3010%, Training Loss: 0.5665%\n",
      "Epoch [38/300], Step [115/225], Training Accuracy: 74.3478%, Training Loss: 0.5659%\n",
      "Epoch [38/300], Step [116/225], Training Accuracy: 74.3265%, Training Loss: 0.5658%\n",
      "Epoch [38/300], Step [117/225], Training Accuracy: 74.2655%, Training Loss: 0.5672%\n",
      "Epoch [38/300], Step [118/225], Training Accuracy: 74.2320%, Training Loss: 0.5673%\n",
      "Epoch [38/300], Step [119/225], Training Accuracy: 74.2516%, Training Loss: 0.5677%\n",
      "Epoch [38/300], Step [120/225], Training Accuracy: 74.2448%, Training Loss: 0.5680%\n",
      "Epoch [38/300], Step [121/225], Training Accuracy: 74.2510%, Training Loss: 0.5684%\n",
      "Epoch [38/300], Step [122/225], Training Accuracy: 74.2572%, Training Loss: 0.5685%\n",
      "Epoch [38/300], Step [123/225], Training Accuracy: 74.1870%, Training Loss: 0.5687%\n",
      "Epoch [38/300], Step [124/225], Training Accuracy: 74.1809%, Training Loss: 0.5682%\n",
      "Epoch [38/300], Step [125/225], Training Accuracy: 74.1500%, Training Loss: 0.5686%\n",
      "Epoch [38/300], Step [126/225], Training Accuracy: 74.2188%, Training Loss: 0.5684%\n",
      "Epoch [38/300], Step [127/225], Training Accuracy: 74.1757%, Training Loss: 0.5687%\n",
      "Epoch [38/300], Step [128/225], Training Accuracy: 74.0967%, Training Loss: 0.5705%\n",
      "Epoch [38/300], Step [129/225], Training Accuracy: 74.0795%, Training Loss: 0.5707%\n",
      "Epoch [38/300], Step [130/225], Training Accuracy: 74.0505%, Training Loss: 0.5708%\n",
      "Epoch [38/300], Step [131/225], Training Accuracy: 74.0577%, Training Loss: 0.5704%\n",
      "Epoch [38/300], Step [132/225], Training Accuracy: 74.0175%, Training Loss: 0.5706%\n",
      "Epoch [38/300], Step [133/225], Training Accuracy: 74.0014%, Training Loss: 0.5711%\n",
      "Epoch [38/300], Step [134/225], Training Accuracy: 73.9389%, Training Loss: 0.5721%\n",
      "Epoch [38/300], Step [135/225], Training Accuracy: 73.9236%, Training Loss: 0.5721%\n",
      "Epoch [38/300], Step [136/225], Training Accuracy: 73.9890%, Training Loss: 0.5713%\n",
      "Epoch [38/300], Step [137/225], Training Accuracy: 73.9507%, Training Loss: 0.5712%\n",
      "Epoch [38/300], Step [138/225], Training Accuracy: 73.9697%, Training Loss: 0.5707%\n",
      "Epoch [38/300], Step [139/225], Training Accuracy: 73.9209%, Training Loss: 0.5711%\n",
      "Epoch [38/300], Step [140/225], Training Accuracy: 73.9509%, Training Loss: 0.5711%\n",
      "Epoch [38/300], Step [141/225], Training Accuracy: 74.0137%, Training Loss: 0.5704%\n",
      "Epoch [38/300], Step [142/225], Training Accuracy: 74.0097%, Training Loss: 0.5706%\n",
      "Epoch [38/300], Step [143/225], Training Accuracy: 73.9729%, Training Loss: 0.5707%\n",
      "Epoch [38/300], Step [144/225], Training Accuracy: 73.9583%, Training Loss: 0.5707%\n",
      "Epoch [38/300], Step [145/225], Training Accuracy: 73.9547%, Training Loss: 0.5712%\n",
      "Epoch [38/300], Step [146/225], Training Accuracy: 73.9405%, Training Loss: 0.5713%\n",
      "Epoch [38/300], Step [147/225], Training Accuracy: 73.9052%, Training Loss: 0.5715%\n",
      "Epoch [38/300], Step [148/225], Training Accuracy: 73.9443%, Training Loss: 0.5709%\n",
      "Epoch [38/300], Step [149/225], Training Accuracy: 73.9409%, Training Loss: 0.5707%\n",
      "Epoch [38/300], Step [150/225], Training Accuracy: 73.9167%, Training Loss: 0.5708%\n",
      "Epoch [38/300], Step [151/225], Training Accuracy: 73.9963%, Training Loss: 0.5702%\n",
      "Epoch [38/300], Step [152/225], Training Accuracy: 73.9926%, Training Loss: 0.5707%\n",
      "Epoch [38/300], Step [153/225], Training Accuracy: 74.0196%, Training Loss: 0.5703%\n",
      "Epoch [38/300], Step [154/225], Training Accuracy: 74.0869%, Training Loss: 0.5696%\n",
      "Epoch [38/300], Step [155/225], Training Accuracy: 74.0524%, Training Loss: 0.5698%\n",
      "Epoch [38/300], Step [156/225], Training Accuracy: 74.0485%, Training Loss: 0.5696%\n",
      "Epoch [38/300], Step [157/225], Training Accuracy: 74.0247%, Training Loss: 0.5694%\n",
      "Epoch [38/300], Step [158/225], Training Accuracy: 73.9913%, Training Loss: 0.5702%\n",
      "Epoch [38/300], Step [159/225], Training Accuracy: 73.9780%, Training Loss: 0.5708%\n",
      "Epoch [38/300], Step [160/225], Training Accuracy: 73.9746%, Training Loss: 0.5705%\n",
      "Epoch [38/300], Step [161/225], Training Accuracy: 73.9616%, Training Loss: 0.5703%\n",
      "Epoch [38/300], Step [162/225], Training Accuracy: 73.9969%, Training Loss: 0.5695%\n",
      "Epoch [38/300], Step [163/225], Training Accuracy: 74.0127%, Training Loss: 0.5693%\n",
      "Epoch [38/300], Step [164/225], Training Accuracy: 74.0282%, Training Loss: 0.5689%\n",
      "Epoch [38/300], Step [165/225], Training Accuracy: 74.0341%, Training Loss: 0.5687%\n",
      "Epoch [38/300], Step [166/225], Training Accuracy: 74.0305%, Training Loss: 0.5682%\n",
      "Epoch [38/300], Step [167/225], Training Accuracy: 73.9989%, Training Loss: 0.5683%\n",
      "Epoch [38/300], Step [168/225], Training Accuracy: 74.0327%, Training Loss: 0.5681%\n",
      "Epoch [38/300], Step [169/225], Training Accuracy: 73.9737%, Training Loss: 0.5682%\n",
      "Epoch [38/300], Step [170/225], Training Accuracy: 73.9614%, Training Loss: 0.5685%\n",
      "Epoch [38/300], Step [171/225], Training Accuracy: 74.0314%, Training Loss: 0.5680%\n",
      "Epoch [38/300], Step [172/225], Training Accuracy: 74.0371%, Training Loss: 0.5680%\n",
      "Epoch [38/300], Step [173/225], Training Accuracy: 74.0065%, Training Loss: 0.5681%\n",
      "Epoch [38/300], Step [174/225], Training Accuracy: 74.0571%, Training Loss: 0.5677%\n",
      "Epoch [38/300], Step [175/225], Training Accuracy: 74.0804%, Training Loss: 0.5673%\n",
      "Epoch [38/300], Step [176/225], Training Accuracy: 74.1211%, Training Loss: 0.5667%\n",
      "Epoch [38/300], Step [177/225], Training Accuracy: 74.1614%, Training Loss: 0.5665%\n",
      "Epoch [38/300], Step [178/225], Training Accuracy: 74.1661%, Training Loss: 0.5665%\n",
      "Epoch [38/300], Step [179/225], Training Accuracy: 74.2057%, Training Loss: 0.5660%\n",
      "Epoch [38/300], Step [180/225], Training Accuracy: 74.1840%, Training Loss: 0.5660%\n",
      "Epoch [38/300], Step [181/225], Training Accuracy: 74.2058%, Training Loss: 0.5659%\n",
      "Epoch [38/300], Step [182/225], Training Accuracy: 74.2359%, Training Loss: 0.5655%\n",
      "Epoch [38/300], Step [183/225], Training Accuracy: 74.2316%, Training Loss: 0.5657%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [184/225], Training Accuracy: 74.2782%, Training Loss: 0.5650%\n",
      "Epoch [38/300], Step [185/225], Training Accuracy: 74.3159%, Training Loss: 0.5640%\n",
      "Epoch [38/300], Step [186/225], Training Accuracy: 74.3616%, Training Loss: 0.5635%\n",
      "Epoch [38/300], Step [187/225], Training Accuracy: 74.3399%, Training Loss: 0.5640%\n",
      "Epoch [38/300], Step [188/225], Training Accuracy: 74.3767%, Training Loss: 0.5634%\n",
      "Epoch [38/300], Step [189/225], Training Accuracy: 74.3717%, Training Loss: 0.5637%\n",
      "Epoch [38/300], Step [190/225], Training Accuracy: 74.3503%, Training Loss: 0.5639%\n",
      "Epoch [38/300], Step [191/225], Training Accuracy: 74.3292%, Training Loss: 0.5638%\n",
      "Epoch [38/300], Step [192/225], Training Accuracy: 74.3652%, Training Loss: 0.5632%\n",
      "Epoch [38/300], Step [193/225], Training Accuracy: 74.3604%, Training Loss: 0.5635%\n",
      "Epoch [38/300], Step [194/225], Training Accuracy: 74.3396%, Training Loss: 0.5636%\n",
      "Epoch [38/300], Step [195/225], Training Accuracy: 74.3510%, Training Loss: 0.5633%\n",
      "Epoch [38/300], Step [196/225], Training Accuracy: 74.3543%, Training Loss: 0.5634%\n",
      "Epoch [38/300], Step [197/225], Training Accuracy: 74.3496%, Training Loss: 0.5632%\n",
      "Epoch [38/300], Step [198/225], Training Accuracy: 74.4160%, Training Loss: 0.5626%\n",
      "Epoch [38/300], Step [199/225], Training Accuracy: 74.4425%, Training Loss: 0.5621%\n",
      "Epoch [38/300], Step [200/225], Training Accuracy: 74.4609%, Training Loss: 0.5619%\n",
      "Epoch [38/300], Step [201/225], Training Accuracy: 74.4481%, Training Loss: 0.5620%\n",
      "Epoch [38/300], Step [202/225], Training Accuracy: 74.4740%, Training Loss: 0.5617%\n",
      "Epoch [38/300], Step [203/225], Training Accuracy: 74.5151%, Training Loss: 0.5612%\n",
      "Epoch [38/300], Step [204/225], Training Accuracy: 74.5251%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [205/225], Training Accuracy: 74.5122%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [206/225], Training Accuracy: 74.5221%, Training Loss: 0.5613%\n",
      "Epoch [38/300], Step [207/225], Training Accuracy: 74.5546%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [208/225], Training Accuracy: 74.5793%, Training Loss: 0.5605%\n",
      "Epoch [38/300], Step [209/225], Training Accuracy: 74.5888%, Training Loss: 0.5603%\n",
      "Epoch [38/300], Step [210/225], Training Accuracy: 74.5536%, Training Loss: 0.5610%\n",
      "Epoch [38/300], Step [211/225], Training Accuracy: 74.5705%, Training Loss: 0.5606%\n",
      "Epoch [38/300], Step [212/225], Training Accuracy: 74.5430%, Training Loss: 0.5609%\n",
      "Epoch [38/300], Step [213/225], Training Accuracy: 74.5305%, Training Loss: 0.5611%\n",
      "Epoch [38/300], Step [214/225], Training Accuracy: 74.5473%, Training Loss: 0.5607%\n",
      "Epoch [38/300], Step [215/225], Training Accuracy: 74.5494%, Training Loss: 0.5606%\n",
      "Epoch [38/300], Step [216/225], Training Accuracy: 74.5298%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [217/225], Training Accuracy: 74.5176%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [218/225], Training Accuracy: 74.4983%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [219/225], Training Accuracy: 74.4720%, Training Loss: 0.5608%\n",
      "Epoch [38/300], Step [220/225], Training Accuracy: 74.4602%, Training Loss: 0.5609%\n",
      "Epoch [38/300], Step [221/225], Training Accuracy: 74.4202%, Training Loss: 0.5613%\n",
      "Epoch [38/300], Step [222/225], Training Accuracy: 74.4017%, Training Loss: 0.5610%\n",
      "Epoch [38/300], Step [223/225], Training Accuracy: 74.3834%, Training Loss: 0.5613%\n",
      "Epoch [38/300], Step [224/225], Training Accuracy: 74.3862%, Training Loss: 0.5610%\n",
      "Epoch [38/300], Step [225/225], Training Accuracy: 74.3747%, Training Loss: 0.5612%\n",
      "Epoch [39/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.4945%\n",
      "Epoch [39/300], Step [2/225], Training Accuracy: 77.3438%, Training Loss: 0.5188%\n",
      "Epoch [39/300], Step [3/225], Training Accuracy: 74.4792%, Training Loss: 0.5401%\n",
      "Epoch [39/300], Step [4/225], Training Accuracy: 74.2188%, Training Loss: 0.5433%\n",
      "Epoch [39/300], Step [5/225], Training Accuracy: 75.6250%, Training Loss: 0.5413%\n",
      "Epoch [39/300], Step [6/225], Training Accuracy: 75.2604%, Training Loss: 0.5522%\n",
      "Epoch [39/300], Step [7/225], Training Accuracy: 75.6696%, Training Loss: 0.5486%\n",
      "Epoch [39/300], Step [8/225], Training Accuracy: 74.8047%, Training Loss: 0.5580%\n",
      "Epoch [39/300], Step [9/225], Training Accuracy: 74.1319%, Training Loss: 0.5754%\n",
      "Epoch [39/300], Step [10/225], Training Accuracy: 73.9062%, Training Loss: 0.5815%\n",
      "Epoch [39/300], Step [11/225], Training Accuracy: 74.2898%, Training Loss: 0.5696%\n",
      "Epoch [39/300], Step [12/225], Training Accuracy: 73.8281%, Training Loss: 0.5685%\n",
      "Epoch [39/300], Step [13/225], Training Accuracy: 74.7596%, Training Loss: 0.5580%\n",
      "Epoch [39/300], Step [14/225], Training Accuracy: 75.1116%, Training Loss: 0.5517%\n",
      "Epoch [39/300], Step [15/225], Training Accuracy: 75.3125%, Training Loss: 0.5549%\n",
      "Epoch [39/300], Step [16/225], Training Accuracy: 75.3906%, Training Loss: 0.5519%\n",
      "Epoch [39/300], Step [17/225], Training Accuracy: 75.3676%, Training Loss: 0.5508%\n",
      "Epoch [39/300], Step [18/225], Training Accuracy: 74.8264%, Training Loss: 0.5528%\n",
      "Epoch [39/300], Step [19/225], Training Accuracy: 74.9178%, Training Loss: 0.5551%\n",
      "Epoch [39/300], Step [20/225], Training Accuracy: 74.5312%, Training Loss: 0.5554%\n",
      "Epoch [39/300], Step [21/225], Training Accuracy: 75.0744%, Training Loss: 0.5494%\n",
      "Epoch [39/300], Step [22/225], Training Accuracy: 74.7159%, Training Loss: 0.5564%\n",
      "Epoch [39/300], Step [23/225], Training Accuracy: 74.8641%, Training Loss: 0.5529%\n",
      "Epoch [39/300], Step [24/225], Training Accuracy: 74.6094%, Training Loss: 0.5542%\n",
      "Epoch [39/300], Step [25/225], Training Accuracy: 74.6875%, Training Loss: 0.5516%\n",
      "Epoch [39/300], Step [26/225], Training Accuracy: 74.2788%, Training Loss: 0.5565%\n",
      "Epoch [39/300], Step [27/225], Training Accuracy: 74.2477%, Training Loss: 0.5571%\n",
      "Epoch [39/300], Step [28/225], Training Accuracy: 74.4978%, Training Loss: 0.5525%\n",
      "Epoch [39/300], Step [29/225], Training Accuracy: 74.3534%, Training Loss: 0.5552%\n",
      "Epoch [39/300], Step [30/225], Training Accuracy: 74.5312%, Training Loss: 0.5534%\n",
      "Epoch [39/300], Step [31/225], Training Accuracy: 74.2440%, Training Loss: 0.5595%\n",
      "Epoch [39/300], Step [32/225], Training Accuracy: 74.2676%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [33/225], Training Accuracy: 74.4792%, Training Loss: 0.5544%\n",
      "Epoch [39/300], Step [34/225], Training Accuracy: 74.4026%, Training Loss: 0.5544%\n",
      "Epoch [39/300], Step [35/225], Training Accuracy: 74.5089%, Training Loss: 0.5527%\n",
      "Epoch [39/300], Step [36/225], Training Accuracy: 74.4792%, Training Loss: 0.5519%\n",
      "Epoch [39/300], Step [37/225], Training Accuracy: 74.4932%, Training Loss: 0.5529%\n",
      "Epoch [39/300], Step [38/225], Training Accuracy: 74.3832%, Training Loss: 0.5533%\n",
      "Epoch [39/300], Step [39/225], Training Accuracy: 74.3189%, Training Loss: 0.5551%\n",
      "Epoch [39/300], Step [40/225], Training Accuracy: 74.4141%, Training Loss: 0.5541%\n",
      "Epoch [39/300], Step [41/225], Training Accuracy: 74.4284%, Training Loss: 0.5569%\n",
      "Epoch [39/300], Step [42/225], Training Accuracy: 74.4420%, Training Loss: 0.5564%\n",
      "Epoch [39/300], Step [43/225], Training Accuracy: 74.4186%, Training Loss: 0.5562%\n",
      "Epoch [39/300], Step [44/225], Training Accuracy: 74.6094%, Training Loss: 0.5526%\n",
      "Epoch [39/300], Step [45/225], Training Accuracy: 74.6875%, Training Loss: 0.5509%\n",
      "Epoch [39/300], Step [46/225], Training Accuracy: 74.5924%, Training Loss: 0.5520%\n",
      "Epoch [39/300], Step [47/225], Training Accuracy: 74.6343%, Training Loss: 0.5511%\n",
      "Epoch [39/300], Step [48/225], Training Accuracy: 74.6419%, Training Loss: 0.5503%\n",
      "Epoch [39/300], Step [49/225], Training Accuracy: 74.6173%, Training Loss: 0.5492%\n",
      "Epoch [39/300], Step [50/225], Training Accuracy: 74.5625%, Training Loss: 0.5504%\n",
      "Epoch [39/300], Step [51/225], Training Accuracy: 74.6017%, Training Loss: 0.5497%\n",
      "Epoch [39/300], Step [52/225], Training Accuracy: 74.7296%, Training Loss: 0.5478%\n",
      "Epoch [39/300], Step [53/225], Training Accuracy: 74.7936%, Training Loss: 0.5480%\n",
      "Epoch [39/300], Step [54/225], Training Accuracy: 74.7685%, Training Loss: 0.5484%\n",
      "Epoch [39/300], Step [55/225], Training Accuracy: 74.6591%, Training Loss: 0.5504%\n",
      "Epoch [39/300], Step [56/225], Training Accuracy: 74.6373%, Training Loss: 0.5507%\n",
      "Epoch [39/300], Step [57/225], Training Accuracy: 74.5888%, Training Loss: 0.5509%\n",
      "Epoch [39/300], Step [58/225], Training Accuracy: 74.7306%, Training Loss: 0.5493%\n",
      "Epoch [39/300], Step [59/225], Training Accuracy: 74.8411%, Training Loss: 0.5484%\n",
      "Epoch [39/300], Step [60/225], Training Accuracy: 74.8698%, Training Loss: 0.5481%\n",
      "Epoch [39/300], Step [61/225], Training Accuracy: 74.7182%, Training Loss: 0.5505%\n",
      "Epoch [39/300], Step [62/225], Training Accuracy: 74.7480%, Training Loss: 0.5521%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Step [63/225], Training Accuracy: 74.6280%, Training Loss: 0.5543%\n",
      "Epoch [39/300], Step [64/225], Training Accuracy: 74.6094%, Training Loss: 0.5537%\n",
      "Epoch [39/300], Step [65/225], Training Accuracy: 74.5913%, Training Loss: 0.5536%\n",
      "Epoch [39/300], Step [66/225], Training Accuracy: 74.6686%, Training Loss: 0.5523%\n",
      "Epoch [39/300], Step [67/225], Training Accuracy: 74.7435%, Training Loss: 0.5507%\n",
      "Epoch [39/300], Step [68/225], Training Accuracy: 74.6094%, Training Loss: 0.5517%\n",
      "Epoch [39/300], Step [69/225], Training Accuracy: 74.5697%, Training Loss: 0.5519%\n",
      "Epoch [39/300], Step [70/225], Training Accuracy: 74.4866%, Training Loss: 0.5532%\n",
      "Epoch [39/300], Step [71/225], Training Accuracy: 74.4718%, Training Loss: 0.5525%\n",
      "Epoch [39/300], Step [72/225], Training Accuracy: 74.4575%, Training Loss: 0.5530%\n",
      "Epoch [39/300], Step [73/225], Training Accuracy: 74.4221%, Training Loss: 0.5534%\n",
      "Epoch [39/300], Step [74/225], Training Accuracy: 74.4510%, Training Loss: 0.5527%\n",
      "Epoch [39/300], Step [75/225], Training Accuracy: 74.4583%, Training Loss: 0.5525%\n",
      "Epoch [39/300], Step [76/225], Training Accuracy: 74.4449%, Training Loss: 0.5549%\n",
      "Epoch [39/300], Step [77/225], Training Accuracy: 74.4927%, Training Loss: 0.5543%\n",
      "Epoch [39/300], Step [78/225], Training Accuracy: 74.4191%, Training Loss: 0.5554%\n",
      "Epoch [39/300], Step [79/225], Training Accuracy: 74.3473%, Training Loss: 0.5548%\n",
      "Epoch [39/300], Step [80/225], Training Accuracy: 74.3164%, Training Loss: 0.5546%\n",
      "Epoch [39/300], Step [81/225], Training Accuracy: 74.4599%, Training Loss: 0.5530%\n",
      "Epoch [39/300], Step [82/225], Training Accuracy: 74.4855%, Training Loss: 0.5519%\n",
      "Epoch [39/300], Step [83/225], Training Accuracy: 74.5105%, Training Loss: 0.5517%\n",
      "Epoch [39/300], Step [84/225], Training Accuracy: 74.5536%, Training Loss: 0.5510%\n",
      "Epoch [39/300], Step [85/225], Training Accuracy: 74.5772%, Training Loss: 0.5503%\n",
      "Epoch [39/300], Step [86/225], Training Accuracy: 74.5640%, Training Loss: 0.5507%\n",
      "Epoch [39/300], Step [87/225], Training Accuracy: 74.5690%, Training Loss: 0.5508%\n",
      "Epoch [39/300], Step [88/225], Training Accuracy: 74.4318%, Training Loss: 0.5519%\n",
      "Epoch [39/300], Step [89/225], Training Accuracy: 74.3855%, Training Loss: 0.5539%\n",
      "Epoch [39/300], Step [90/225], Training Accuracy: 74.3403%, Training Loss: 0.5550%\n",
      "Epoch [39/300], Step [91/225], Training Accuracy: 74.3819%, Training Loss: 0.5548%\n",
      "Epoch [39/300], Step [92/225], Training Accuracy: 74.4056%, Training Loss: 0.5549%\n",
      "Epoch [39/300], Step [93/225], Training Accuracy: 74.4624%, Training Loss: 0.5538%\n",
      "Epoch [39/300], Step [94/225], Training Accuracy: 74.4348%, Training Loss: 0.5542%\n",
      "Epoch [39/300], Step [95/225], Training Accuracy: 74.3914%, Training Loss: 0.5550%\n",
      "Epoch [39/300], Step [96/225], Training Accuracy: 74.2839%, Training Loss: 0.5555%\n",
      "Epoch [39/300], Step [97/225], Training Accuracy: 74.3073%, Training Loss: 0.5548%\n",
      "Epoch [39/300], Step [98/225], Training Accuracy: 74.2506%, Training Loss: 0.5553%\n",
      "Epoch [39/300], Step [99/225], Training Accuracy: 74.2582%, Training Loss: 0.5556%\n",
      "Epoch [39/300], Step [100/225], Training Accuracy: 74.2500%, Training Loss: 0.5564%\n",
      "Epoch [39/300], Step [101/225], Training Accuracy: 74.1955%, Training Loss: 0.5570%\n",
      "Epoch [39/300], Step [102/225], Training Accuracy: 74.2188%, Training Loss: 0.5570%\n",
      "Epoch [39/300], Step [103/225], Training Accuracy: 74.2870%, Training Loss: 0.5561%\n",
      "Epoch [39/300], Step [104/225], Training Accuracy: 74.1737%, Training Loss: 0.5573%\n",
      "Epoch [39/300], Step [105/225], Training Accuracy: 74.1369%, Training Loss: 0.5572%\n",
      "Epoch [39/300], Step [106/225], Training Accuracy: 74.2482%, Training Loss: 0.5563%\n",
      "Epoch [39/300], Step [107/225], Training Accuracy: 74.1676%, Training Loss: 0.5581%\n",
      "Epoch [39/300], Step [108/225], Training Accuracy: 74.1175%, Training Loss: 0.5584%\n",
      "Epoch [39/300], Step [109/225], Training Accuracy: 74.0826%, Training Loss: 0.5582%\n",
      "Epoch [39/300], Step [110/225], Training Accuracy: 74.1193%, Training Loss: 0.5576%\n",
      "Epoch [39/300], Step [111/225], Training Accuracy: 74.1554%, Training Loss: 0.5570%\n",
      "Epoch [39/300], Step [112/225], Training Accuracy: 74.1769%, Training Loss: 0.5571%\n",
      "Epoch [39/300], Step [113/225], Training Accuracy: 74.1427%, Training Loss: 0.5570%\n",
      "Epoch [39/300], Step [114/225], Training Accuracy: 74.1228%, Training Loss: 0.5576%\n",
      "Epoch [39/300], Step [115/225], Training Accuracy: 74.1576%, Training Loss: 0.5571%\n",
      "Epoch [39/300], Step [116/225], Training Accuracy: 74.0841%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [117/225], Training Accuracy: 74.0518%, Training Loss: 0.5586%\n",
      "Epoch [39/300], Step [118/225], Training Accuracy: 74.0466%, Training Loss: 0.5589%\n",
      "Epoch [39/300], Step [119/225], Training Accuracy: 74.0809%, Training Loss: 0.5586%\n",
      "Epoch [39/300], Step [120/225], Training Accuracy: 74.1276%, Training Loss: 0.5585%\n",
      "Epoch [39/300], Step [121/225], Training Accuracy: 74.0832%, Training Loss: 0.5593%\n",
      "Epoch [39/300], Step [122/225], Training Accuracy: 74.0907%, Training Loss: 0.5597%\n",
      "Epoch [39/300], Step [123/225], Training Accuracy: 74.1108%, Training Loss: 0.5594%\n",
      "Epoch [39/300], Step [124/225], Training Accuracy: 74.0801%, Training Loss: 0.5602%\n",
      "Epoch [39/300], Step [125/225], Training Accuracy: 74.1250%, Training Loss: 0.5598%\n",
      "Epoch [39/300], Step [126/225], Training Accuracy: 74.1319%, Training Loss: 0.5604%\n",
      "Epoch [39/300], Step [127/225], Training Accuracy: 74.1634%, Training Loss: 0.5607%\n",
      "Epoch [39/300], Step [128/225], Training Accuracy: 74.1699%, Training Loss: 0.5615%\n",
      "Epoch [39/300], Step [129/225], Training Accuracy: 74.1764%, Training Loss: 0.5620%\n",
      "Epoch [39/300], Step [130/225], Training Accuracy: 74.1707%, Training Loss: 0.5617%\n",
      "Epoch [39/300], Step [131/225], Training Accuracy: 74.2247%, Training Loss: 0.5609%\n",
      "Epoch [39/300], Step [132/225], Training Accuracy: 74.2543%, Training Loss: 0.5606%\n",
      "Epoch [39/300], Step [133/225], Training Accuracy: 74.2951%, Training Loss: 0.5608%\n",
      "Epoch [39/300], Step [134/225], Training Accuracy: 74.2071%, Training Loss: 0.5623%\n",
      "Epoch [39/300], Step [135/225], Training Accuracy: 74.1782%, Training Loss: 0.5628%\n",
      "Epoch [39/300], Step [136/225], Training Accuracy: 74.2532%, Training Loss: 0.5618%\n",
      "Epoch [39/300], Step [137/225], Training Accuracy: 74.2130%, Training Loss: 0.5622%\n",
      "Epoch [39/300], Step [138/225], Training Accuracy: 74.2527%, Training Loss: 0.5615%\n",
      "Epoch [39/300], Step [139/225], Training Accuracy: 74.2469%, Training Loss: 0.5618%\n",
      "Epoch [39/300], Step [140/225], Training Accuracy: 74.3080%, Training Loss: 0.5612%\n",
      "Epoch [39/300], Step [141/225], Training Accuracy: 74.3240%, Training Loss: 0.5609%\n",
      "Epoch [39/300], Step [142/225], Training Accuracy: 74.3508%, Training Loss: 0.5605%\n",
      "Epoch [39/300], Step [143/225], Training Accuracy: 74.3553%, Training Loss: 0.5608%\n",
      "Epoch [39/300], Step [144/225], Training Accuracy: 74.3707%, Training Loss: 0.5602%\n",
      "Epoch [39/300], Step [145/225], Training Accuracy: 74.4181%, Training Loss: 0.5601%\n",
      "Epoch [39/300], Step [146/225], Training Accuracy: 74.4114%, Training Loss: 0.5605%\n",
      "Epoch [39/300], Step [147/225], Training Accuracy: 74.3941%, Training Loss: 0.5610%\n",
      "Epoch [39/300], Step [148/225], Training Accuracy: 74.4827%, Training Loss: 0.5599%\n",
      "Epoch [39/300], Step [149/225], Training Accuracy: 74.5281%, Training Loss: 0.5593%\n",
      "Epoch [39/300], Step [150/225], Training Accuracy: 74.5312%, Training Loss: 0.5594%\n",
      "Epoch [39/300], Step [151/225], Training Accuracy: 74.5861%, Training Loss: 0.5589%\n",
      "Epoch [39/300], Step [152/225], Training Accuracy: 74.5683%, Training Loss: 0.5592%\n",
      "Epoch [39/300], Step [153/225], Training Accuracy: 74.5609%, Training Loss: 0.5589%\n",
      "Epoch [39/300], Step [154/225], Training Accuracy: 74.6043%, Training Loss: 0.5585%\n",
      "Epoch [39/300], Step [155/225], Training Accuracy: 74.5161%, Training Loss: 0.5593%\n",
      "Epoch [39/300], Step [156/225], Training Accuracy: 74.4792%, Training Loss: 0.5597%\n",
      "Epoch [39/300], Step [157/225], Training Accuracy: 74.5123%, Training Loss: 0.5597%\n",
      "Epoch [39/300], Step [158/225], Training Accuracy: 74.4660%, Training Loss: 0.5605%\n",
      "Epoch [39/300], Step [159/225], Training Accuracy: 74.4300%, Training Loss: 0.5612%\n",
      "Epoch [39/300], Step [160/225], Training Accuracy: 74.4238%, Training Loss: 0.5610%\n",
      "Epoch [39/300], Step [161/225], Training Accuracy: 74.3983%, Training Loss: 0.5609%\n",
      "Epoch [39/300], Step [162/225], Training Accuracy: 74.4406%, Training Loss: 0.5600%\n",
      "Epoch [39/300], Step [163/225], Training Accuracy: 74.4728%, Training Loss: 0.5594%\n",
      "Epoch [39/300], Step [164/225], Training Accuracy: 74.4855%, Training Loss: 0.5588%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Step [165/225], Training Accuracy: 74.5076%, Training Loss: 0.5582%\n",
      "Epoch [39/300], Step [166/225], Training Accuracy: 74.5670%, Training Loss: 0.5576%\n",
      "Epoch [39/300], Step [167/225], Training Accuracy: 74.5977%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [168/225], Training Accuracy: 74.6280%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [169/225], Training Accuracy: 74.6024%, Training Loss: 0.5578%\n",
      "Epoch [39/300], Step [170/225], Training Accuracy: 74.5956%, Training Loss: 0.5579%\n",
      "Epoch [39/300], Step [171/225], Training Accuracy: 74.5888%, Training Loss: 0.5580%\n",
      "Epoch [39/300], Step [172/225], Training Accuracy: 74.5821%, Training Loss: 0.5578%\n",
      "Epoch [39/300], Step [173/225], Training Accuracy: 74.6207%, Training Loss: 0.5573%\n",
      "Epoch [39/300], Step [174/225], Training Accuracy: 74.6318%, Training Loss: 0.5578%\n",
      "Epoch [39/300], Step [175/225], Training Accuracy: 74.6429%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [176/225], Training Accuracy: 74.6804%, Training Loss: 0.5573%\n",
      "Epoch [39/300], Step [177/225], Training Accuracy: 74.6910%, Training Loss: 0.5573%\n",
      "Epoch [39/300], Step [178/225], Training Accuracy: 74.7191%, Training Loss: 0.5574%\n",
      "Epoch [39/300], Step [179/225], Training Accuracy: 74.7643%, Training Loss: 0.5567%\n",
      "Epoch [39/300], Step [180/225], Training Accuracy: 74.7569%, Training Loss: 0.5568%\n",
      "Epoch [39/300], Step [181/225], Training Accuracy: 74.7583%, Training Loss: 0.5569%\n",
      "Epoch [39/300], Step [182/225], Training Accuracy: 74.7424%, Training Loss: 0.5572%\n",
      "Epoch [39/300], Step [183/225], Training Accuracy: 74.7268%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [184/225], Training Accuracy: 74.7283%, Training Loss: 0.5577%\n",
      "Epoch [39/300], Step [185/225], Training Accuracy: 74.7382%, Training Loss: 0.5570%\n",
      "Epoch [39/300], Step [186/225], Training Accuracy: 74.7480%, Training Loss: 0.5567%\n",
      "Epoch [39/300], Step [187/225], Training Accuracy: 74.7577%, Training Loss: 0.5565%\n",
      "Epoch [39/300], Step [188/225], Training Accuracy: 74.7590%, Training Loss: 0.5558%\n",
      "Epoch [39/300], Step [189/225], Training Accuracy: 74.7437%, Training Loss: 0.5557%\n",
      "Epoch [39/300], Step [190/225], Training Accuracy: 74.7286%, Training Loss: 0.5558%\n",
      "Epoch [39/300], Step [191/225], Training Accuracy: 74.7219%, Training Loss: 0.5560%\n",
      "Epoch [39/300], Step [192/225], Training Accuracy: 74.7559%, Training Loss: 0.5555%\n",
      "Epoch [39/300], Step [193/225], Training Accuracy: 74.7409%, Training Loss: 0.5556%\n",
      "Epoch [39/300], Step [194/225], Training Accuracy: 74.6939%, Training Loss: 0.5559%\n",
      "Epoch [39/300], Step [195/225], Training Accuracy: 74.6795%, Training Loss: 0.5558%\n",
      "Epoch [39/300], Step [196/225], Training Accuracy: 74.6173%, Training Loss: 0.5567%\n",
      "Epoch [39/300], Step [197/225], Training Accuracy: 74.5876%, Training Loss: 0.5567%\n",
      "Epoch [39/300], Step [198/225], Training Accuracy: 74.6133%, Training Loss: 0.5558%\n",
      "Epoch [39/300], Step [199/225], Training Accuracy: 74.6467%, Training Loss: 0.5557%\n",
      "Epoch [39/300], Step [200/225], Training Accuracy: 74.6328%, Training Loss: 0.5560%\n",
      "Epoch [39/300], Step [201/225], Training Accuracy: 74.6191%, Training Loss: 0.5562%\n",
      "Epoch [39/300], Step [202/225], Training Accuracy: 74.6597%, Training Loss: 0.5556%\n",
      "Epoch [39/300], Step [203/225], Training Accuracy: 74.6844%, Training Loss: 0.5552%\n",
      "Epoch [39/300], Step [204/225], Training Accuracy: 74.6936%, Training Loss: 0.5550%\n",
      "Epoch [39/300], Step [205/225], Training Accuracy: 74.7027%, Training Loss: 0.5549%\n",
      "Epoch [39/300], Step [206/225], Training Accuracy: 74.6966%, Training Loss: 0.5551%\n",
      "Epoch [39/300], Step [207/225], Training Accuracy: 74.6830%, Training Loss: 0.5559%\n",
      "Epoch [39/300], Step [208/225], Training Accuracy: 74.7296%, Training Loss: 0.5552%\n",
      "Epoch [39/300], Step [209/225], Training Accuracy: 74.7309%, Training Loss: 0.5557%\n",
      "Epoch [39/300], Step [210/225], Training Accuracy: 74.6875%, Training Loss: 0.5563%\n",
      "Epoch [39/300], Step [211/225], Training Accuracy: 74.7038%, Training Loss: 0.5560%\n",
      "Epoch [39/300], Step [212/225], Training Accuracy: 74.6904%, Training Loss: 0.5562%\n",
      "Epoch [39/300], Step [213/225], Training Accuracy: 74.6992%, Training Loss: 0.5563%\n",
      "Epoch [39/300], Step [214/225], Training Accuracy: 74.6933%, Training Loss: 0.5562%\n",
      "Epoch [39/300], Step [215/225], Training Accuracy: 74.6730%, Training Loss: 0.5565%\n",
      "Epoch [39/300], Step [216/225], Training Accuracy: 74.6600%, Training Loss: 0.5566%\n",
      "Epoch [39/300], Step [217/225], Training Accuracy: 74.6760%, Training Loss: 0.5566%\n",
      "Epoch [39/300], Step [218/225], Training Accuracy: 74.6631%, Training Loss: 0.5566%\n",
      "Epoch [39/300], Step [219/225], Training Accuracy: 74.6789%, Training Loss: 0.5563%\n",
      "Epoch [39/300], Step [220/225], Training Accuracy: 74.6520%, Training Loss: 0.5568%\n",
      "Epoch [39/300], Step [221/225], Training Accuracy: 74.6041%, Training Loss: 0.5573%\n",
      "Epoch [39/300], Step [222/225], Training Accuracy: 74.6270%, Training Loss: 0.5570%\n",
      "Epoch [39/300], Step [223/225], Training Accuracy: 74.5936%, Training Loss: 0.5573%\n",
      "Epoch [39/300], Step [224/225], Training Accuracy: 74.5466%, Training Loss: 0.5576%\n",
      "Epoch [39/300], Step [225/225], Training Accuracy: 74.5345%, Training Loss: 0.5575%\n",
      "Epoch [40/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.6205%\n",
      "Epoch [40/300], Step [2/225], Training Accuracy: 76.5625%, Training Loss: 0.5821%\n",
      "Epoch [40/300], Step [3/225], Training Accuracy: 75.0000%, Training Loss: 0.5832%\n",
      "Epoch [40/300], Step [4/225], Training Accuracy: 75.7812%, Training Loss: 0.5628%\n",
      "Epoch [40/300], Step [5/225], Training Accuracy: 75.0000%, Training Loss: 0.5723%\n",
      "Epoch [40/300], Step [6/225], Training Accuracy: 75.0000%, Training Loss: 0.5731%\n",
      "Epoch [40/300], Step [7/225], Training Accuracy: 75.4464%, Training Loss: 0.5805%\n",
      "Epoch [40/300], Step [8/225], Training Accuracy: 75.3906%, Training Loss: 0.5818%\n",
      "Epoch [40/300], Step [9/225], Training Accuracy: 74.1319%, Training Loss: 0.6080%\n",
      "Epoch [40/300], Step [10/225], Training Accuracy: 73.9062%, Training Loss: 0.6108%\n",
      "Epoch [40/300], Step [11/225], Training Accuracy: 74.5739%, Training Loss: 0.6052%\n",
      "Epoch [40/300], Step [12/225], Training Accuracy: 73.6979%, Training Loss: 0.6121%\n",
      "Epoch [40/300], Step [13/225], Training Accuracy: 73.7981%, Training Loss: 0.6009%\n",
      "Epoch [40/300], Step [14/225], Training Accuracy: 73.8839%, Training Loss: 0.5947%\n",
      "Epoch [40/300], Step [15/225], Training Accuracy: 73.3333%, Training Loss: 0.5995%\n",
      "Epoch [40/300], Step [16/225], Training Accuracy: 73.2422%, Training Loss: 0.5972%\n",
      "Epoch [40/300], Step [17/225], Training Accuracy: 73.6213%, Training Loss: 0.5892%\n",
      "Epoch [40/300], Step [18/225], Training Accuracy: 74.0451%, Training Loss: 0.5854%\n",
      "Epoch [40/300], Step [19/225], Training Accuracy: 73.9309%, Training Loss: 0.5824%\n",
      "Epoch [40/300], Step [20/225], Training Accuracy: 74.2188%, Training Loss: 0.5804%\n",
      "Epoch [40/300], Step [21/225], Training Accuracy: 74.9256%, Training Loss: 0.5706%\n",
      "Epoch [40/300], Step [22/225], Training Accuracy: 74.6449%, Training Loss: 0.5774%\n",
      "Epoch [40/300], Step [23/225], Training Accuracy: 74.7962%, Training Loss: 0.5730%\n",
      "Epoch [40/300], Step [24/225], Training Accuracy: 74.5443%, Training Loss: 0.5776%\n",
      "Epoch [40/300], Step [25/225], Training Accuracy: 74.7500%, Training Loss: 0.5781%\n",
      "Epoch [40/300], Step [26/225], Training Accuracy: 74.5793%, Training Loss: 0.5774%\n",
      "Epoch [40/300], Step [27/225], Training Accuracy: 74.4213%, Training Loss: 0.5772%\n",
      "Epoch [40/300], Step [28/225], Training Accuracy: 74.8326%, Training Loss: 0.5701%\n",
      "Epoch [40/300], Step [29/225], Training Accuracy: 74.7845%, Training Loss: 0.5719%\n",
      "Epoch [40/300], Step [30/225], Training Accuracy: 74.8958%, Training Loss: 0.5707%\n",
      "Epoch [40/300], Step [31/225], Training Accuracy: 74.4456%, Training Loss: 0.5801%\n",
      "Epoch [40/300], Step [32/225], Training Accuracy: 74.3164%, Training Loss: 0.5804%\n",
      "Epoch [40/300], Step [33/225], Training Accuracy: 74.3845%, Training Loss: 0.5764%\n",
      "Epoch [40/300], Step [34/225], Training Accuracy: 74.4026%, Training Loss: 0.5794%\n",
      "Epoch [40/300], Step [35/225], Training Accuracy: 74.4196%, Training Loss: 0.5776%\n",
      "Epoch [40/300], Step [36/225], Training Accuracy: 74.3490%, Training Loss: 0.5778%\n",
      "Epoch [40/300], Step [37/225], Training Accuracy: 74.5355%, Training Loss: 0.5759%\n",
      "Epoch [40/300], Step [38/225], Training Accuracy: 74.3421%, Training Loss: 0.5751%\n",
      "Epoch [40/300], Step [39/225], Training Accuracy: 74.4391%, Training Loss: 0.5736%\n",
      "Epoch [40/300], Step [40/225], Training Accuracy: 74.4922%, Training Loss: 0.5718%\n",
      "Epoch [40/300], Step [41/225], Training Accuracy: 74.4665%, Training Loss: 0.5721%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Step [42/225], Training Accuracy: 74.5908%, Training Loss: 0.5703%\n",
      "Epoch [40/300], Step [43/225], Training Accuracy: 74.5276%, Training Loss: 0.5704%\n",
      "Epoch [40/300], Step [44/225], Training Accuracy: 74.8224%, Training Loss: 0.5669%\n",
      "Epoch [40/300], Step [45/225], Training Accuracy: 74.7917%, Training Loss: 0.5674%\n",
      "Epoch [40/300], Step [46/225], Training Accuracy: 74.8302%, Training Loss: 0.5657%\n",
      "Epoch [40/300], Step [47/225], Training Accuracy: 74.8005%, Training Loss: 0.5644%\n",
      "Epoch [40/300], Step [48/225], Training Accuracy: 74.5768%, Training Loss: 0.5665%\n",
      "Epoch [40/300], Step [49/225], Training Accuracy: 74.7130%, Training Loss: 0.5653%\n",
      "Epoch [40/300], Step [50/225], Training Accuracy: 74.7500%, Training Loss: 0.5638%\n",
      "Epoch [40/300], Step [51/225], Training Accuracy: 74.7855%, Training Loss: 0.5634%\n",
      "Epoch [40/300], Step [52/225], Training Accuracy: 74.8197%, Training Loss: 0.5624%\n",
      "Epoch [40/300], Step [53/225], Training Accuracy: 74.6757%, Training Loss: 0.5633%\n",
      "Epoch [40/300], Step [54/225], Training Accuracy: 74.6528%, Training Loss: 0.5639%\n",
      "Epoch [40/300], Step [55/225], Training Accuracy: 74.4886%, Training Loss: 0.5651%\n",
      "Epoch [40/300], Step [56/225], Training Accuracy: 74.5257%, Training Loss: 0.5643%\n",
      "Epoch [40/300], Step [57/225], Training Accuracy: 74.5066%, Training Loss: 0.5639%\n",
      "Epoch [40/300], Step [58/225], Training Accuracy: 74.5959%, Training Loss: 0.5637%\n",
      "Epoch [40/300], Step [59/225], Training Accuracy: 74.6557%, Training Loss: 0.5629%\n",
      "Epoch [40/300], Step [60/225], Training Accuracy: 74.7135%, Training Loss: 0.5617%\n",
      "Epoch [40/300], Step [61/225], Training Accuracy: 74.6926%, Training Loss: 0.5622%\n",
      "Epoch [40/300], Step [62/225], Training Accuracy: 74.6220%, Training Loss: 0.5624%\n",
      "Epoch [40/300], Step [63/225], Training Accuracy: 74.4792%, Training Loss: 0.5647%\n",
      "Epoch [40/300], Step [64/225], Training Accuracy: 74.5605%, Training Loss: 0.5629%\n",
      "Epoch [40/300], Step [65/225], Training Accuracy: 74.6154%, Training Loss: 0.5627%\n",
      "Epoch [40/300], Step [66/225], Training Accuracy: 74.6686%, Training Loss: 0.5618%\n",
      "Epoch [40/300], Step [67/225], Training Accuracy: 74.6035%, Training Loss: 0.5619%\n",
      "Epoch [40/300], Step [68/225], Training Accuracy: 74.6553%, Training Loss: 0.5619%\n",
      "Epoch [40/300], Step [69/225], Training Accuracy: 74.5471%, Training Loss: 0.5636%\n",
      "Epoch [40/300], Step [70/225], Training Accuracy: 74.5312%, Training Loss: 0.5641%\n",
      "Epoch [40/300], Step [71/225], Training Accuracy: 74.5599%, Training Loss: 0.5628%\n",
      "Epoch [40/300], Step [72/225], Training Accuracy: 74.5443%, Training Loss: 0.5623%\n",
      "Epoch [40/300], Step [73/225], Training Accuracy: 74.4863%, Training Loss: 0.5638%\n",
      "Epoch [40/300], Step [74/225], Training Accuracy: 74.4721%, Training Loss: 0.5631%\n",
      "Epoch [40/300], Step [75/225], Training Accuracy: 74.6042%, Training Loss: 0.5617%\n",
      "Epoch [40/300], Step [76/225], Training Accuracy: 74.5477%, Training Loss: 0.5627%\n",
      "Epoch [40/300], Step [77/225], Training Accuracy: 74.6144%, Training Loss: 0.5620%\n",
      "Epoch [40/300], Step [78/225], Training Accuracy: 74.5793%, Training Loss: 0.5624%\n",
      "Epoch [40/300], Step [79/225], Training Accuracy: 74.7033%, Training Loss: 0.5605%\n",
      "Epoch [40/300], Step [80/225], Training Accuracy: 74.8047%, Training Loss: 0.5597%\n",
      "Epoch [40/300], Step [81/225], Training Accuracy: 74.8264%, Training Loss: 0.5584%\n",
      "Epoch [40/300], Step [82/225], Training Accuracy: 74.8857%, Training Loss: 0.5571%\n",
      "Epoch [40/300], Step [83/225], Training Accuracy: 74.8870%, Training Loss: 0.5571%\n",
      "Epoch [40/300], Step [84/225], Training Accuracy: 74.8884%, Training Loss: 0.5565%\n",
      "Epoch [40/300], Step [85/225], Training Accuracy: 74.9265%, Training Loss: 0.5556%\n",
      "Epoch [40/300], Step [86/225], Training Accuracy: 75.0182%, Training Loss: 0.5553%\n",
      "Epoch [40/300], Step [87/225], Training Accuracy: 74.9641%, Training Loss: 0.5567%\n",
      "Epoch [40/300], Step [88/225], Training Accuracy: 74.8047%, Training Loss: 0.5579%\n",
      "Epoch [40/300], Step [89/225], Training Accuracy: 74.7542%, Training Loss: 0.5597%\n",
      "Epoch [40/300], Step [90/225], Training Accuracy: 74.6701%, Training Loss: 0.5621%\n",
      "Epoch [40/300], Step [91/225], Training Accuracy: 74.6566%, Training Loss: 0.5626%\n",
      "Epoch [40/300], Step [92/225], Training Accuracy: 74.6603%, Training Loss: 0.5621%\n",
      "Epoch [40/300], Step [93/225], Training Accuracy: 74.7312%, Training Loss: 0.5607%\n",
      "Epoch [40/300], Step [94/225], Training Accuracy: 74.7839%, Training Loss: 0.5598%\n",
      "Epoch [40/300], Step [95/225], Training Accuracy: 74.7862%, Training Loss: 0.5611%\n",
      "Epoch [40/300], Step [96/225], Training Accuracy: 74.8047%, Training Loss: 0.5601%\n",
      "Epoch [40/300], Step [97/225], Training Accuracy: 74.8067%, Training Loss: 0.5597%\n",
      "Epoch [40/300], Step [98/225], Training Accuracy: 74.7290%, Training Loss: 0.5610%\n",
      "Epoch [40/300], Step [99/225], Training Accuracy: 74.7317%, Training Loss: 0.5622%\n",
      "Epoch [40/300], Step [100/225], Training Accuracy: 74.7031%, Training Loss: 0.5631%\n",
      "Epoch [40/300], Step [101/225], Training Accuracy: 74.7061%, Training Loss: 0.5632%\n",
      "Epoch [40/300], Step [102/225], Training Accuracy: 74.7089%, Training Loss: 0.5632%\n",
      "Epoch [40/300], Step [103/225], Training Accuracy: 74.7118%, Training Loss: 0.5634%\n",
      "Epoch [40/300], Step [104/225], Training Accuracy: 74.6995%, Training Loss: 0.5633%\n",
      "Epoch [40/300], Step [105/225], Training Accuracy: 74.7024%, Training Loss: 0.5624%\n",
      "Epoch [40/300], Step [106/225], Training Accuracy: 74.6167%, Training Loss: 0.5630%\n",
      "Epoch [40/300], Step [107/225], Training Accuracy: 74.5911%, Training Loss: 0.5637%\n",
      "Epoch [40/300], Step [108/225], Training Accuracy: 74.4213%, Training Loss: 0.5654%\n",
      "Epoch [40/300], Step [109/225], Training Accuracy: 74.4696%, Training Loss: 0.5647%\n",
      "Epoch [40/300], Step [110/225], Training Accuracy: 74.4318%, Training Loss: 0.5650%\n",
      "Epoch [40/300], Step [111/225], Training Accuracy: 74.3947%, Training Loss: 0.5649%\n",
      "Epoch [40/300], Step [112/225], Training Accuracy: 74.4978%, Training Loss: 0.5647%\n",
      "Epoch [40/300], Step [113/225], Training Accuracy: 74.5299%, Training Loss: 0.5642%\n",
      "Epoch [40/300], Step [114/225], Training Accuracy: 74.5066%, Training Loss: 0.5640%\n",
      "Epoch [40/300], Step [115/225], Training Accuracy: 74.5652%, Training Loss: 0.5642%\n",
      "Epoch [40/300], Step [116/225], Training Accuracy: 74.5151%, Training Loss: 0.5640%\n",
      "Epoch [40/300], Step [117/225], Training Accuracy: 74.4257%, Training Loss: 0.5652%\n",
      "Epoch [40/300], Step [118/225], Training Accuracy: 74.3644%, Training Loss: 0.5664%\n",
      "Epoch [40/300], Step [119/225], Training Accuracy: 74.3829%, Training Loss: 0.5662%\n",
      "Epoch [40/300], Step [120/225], Training Accuracy: 74.4271%, Training Loss: 0.5657%\n",
      "Epoch [40/300], Step [121/225], Training Accuracy: 74.2769%, Training Loss: 0.5675%\n",
      "Epoch [40/300], Step [122/225], Training Accuracy: 74.2956%, Training Loss: 0.5678%\n",
      "Epoch [40/300], Step [123/225], Training Accuracy: 74.2759%, Training Loss: 0.5683%\n",
      "Epoch [40/300], Step [124/225], Training Accuracy: 74.2188%, Training Loss: 0.5687%\n",
      "Epoch [40/300], Step [125/225], Training Accuracy: 74.1875%, Training Loss: 0.5684%\n",
      "Epoch [40/300], Step [126/225], Training Accuracy: 74.2188%, Training Loss: 0.5680%\n",
      "Epoch [40/300], Step [127/225], Training Accuracy: 74.2126%, Training Loss: 0.5682%\n",
      "Epoch [40/300], Step [128/225], Training Accuracy: 74.1699%, Training Loss: 0.5698%\n",
      "Epoch [40/300], Step [129/225], Training Accuracy: 74.1642%, Training Loss: 0.5701%\n",
      "Epoch [40/300], Step [130/225], Training Accuracy: 74.1947%, Training Loss: 0.5698%\n",
      "Epoch [40/300], Step [131/225], Training Accuracy: 74.2605%, Training Loss: 0.5690%\n",
      "Epoch [40/300], Step [132/225], Training Accuracy: 74.2424%, Training Loss: 0.5691%\n",
      "Epoch [40/300], Step [133/225], Training Accuracy: 74.2599%, Training Loss: 0.5692%\n",
      "Epoch [40/300], Step [134/225], Training Accuracy: 74.2654%, Training Loss: 0.5697%\n",
      "Epoch [40/300], Step [135/225], Training Accuracy: 74.2593%, Training Loss: 0.5701%\n",
      "Epoch [40/300], Step [136/225], Training Accuracy: 74.3107%, Training Loss: 0.5692%\n",
      "Epoch [40/300], Step [137/225], Training Accuracy: 74.3043%, Training Loss: 0.5690%\n",
      "Epoch [40/300], Step [138/225], Training Accuracy: 74.3659%, Training Loss: 0.5679%\n",
      "Epoch [40/300], Step [139/225], Training Accuracy: 74.3255%, Training Loss: 0.5679%\n",
      "Epoch [40/300], Step [140/225], Training Accuracy: 74.3415%, Training Loss: 0.5676%\n",
      "Epoch [40/300], Step [141/225], Training Accuracy: 74.3129%, Training Loss: 0.5684%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Step [142/225], Training Accuracy: 74.3398%, Training Loss: 0.5687%\n",
      "Epoch [40/300], Step [143/225], Training Accuracy: 74.3772%, Training Loss: 0.5684%\n",
      "Epoch [40/300], Step [144/225], Training Accuracy: 74.4141%, Training Loss: 0.5679%\n",
      "Epoch [40/300], Step [145/225], Training Accuracy: 74.4181%, Training Loss: 0.5675%\n",
      "Epoch [40/300], Step [146/225], Training Accuracy: 74.3579%, Training Loss: 0.5677%\n",
      "Epoch [40/300], Step [147/225], Training Accuracy: 74.2878%, Training Loss: 0.5680%\n",
      "Epoch [40/300], Step [148/225], Training Accuracy: 74.3454%, Training Loss: 0.5672%\n",
      "Epoch [40/300], Step [149/225], Training Accuracy: 74.3289%, Training Loss: 0.5671%\n",
      "Epoch [40/300], Step [150/225], Training Accuracy: 74.3333%, Training Loss: 0.5669%\n",
      "Epoch [40/300], Step [151/225], Training Accuracy: 74.3998%, Training Loss: 0.5660%\n",
      "Epoch [40/300], Step [152/225], Training Accuracy: 74.3318%, Training Loss: 0.5667%\n",
      "Epoch [40/300], Step [153/225], Training Accuracy: 74.3158%, Training Loss: 0.5665%\n",
      "Epoch [40/300], Step [154/225], Training Accuracy: 74.3304%, Training Loss: 0.5664%\n",
      "Epoch [40/300], Step [155/225], Training Accuracy: 74.2944%, Training Loss: 0.5671%\n",
      "Epoch [40/300], Step [156/225], Training Accuracy: 74.2288%, Training Loss: 0.5680%\n",
      "Epoch [40/300], Step [157/225], Training Accuracy: 74.2038%, Training Loss: 0.5682%\n",
      "Epoch [40/300], Step [158/225], Training Accuracy: 74.1100%, Training Loss: 0.5693%\n",
      "Epoch [40/300], Step [159/225], Training Accuracy: 74.0566%, Training Loss: 0.5702%\n",
      "Epoch [40/300], Step [160/225], Training Accuracy: 74.0625%, Training Loss: 0.5701%\n",
      "Epoch [40/300], Step [161/225], Training Accuracy: 74.0780%, Training Loss: 0.5701%\n",
      "Epoch [40/300], Step [162/225], Training Accuracy: 74.1223%, Training Loss: 0.5696%\n",
      "Epoch [40/300], Step [163/225], Training Accuracy: 74.1277%, Training Loss: 0.5694%\n",
      "Epoch [40/300], Step [164/225], Training Accuracy: 74.1616%, Training Loss: 0.5688%\n",
      "Epoch [40/300], Step [165/225], Training Accuracy: 74.1572%, Training Loss: 0.5689%\n",
      "Epoch [40/300], Step [166/225], Training Accuracy: 74.1811%, Training Loss: 0.5686%\n",
      "Epoch [40/300], Step [167/225], Training Accuracy: 74.1766%, Training Loss: 0.5683%\n",
      "Epoch [40/300], Step [168/225], Training Accuracy: 74.1815%, Training Loss: 0.5686%\n",
      "Epoch [40/300], Step [169/225], Training Accuracy: 74.1309%, Training Loss: 0.5689%\n",
      "Epoch [40/300], Step [170/225], Training Accuracy: 74.1176%, Training Loss: 0.5692%\n",
      "Epoch [40/300], Step [171/225], Training Accuracy: 74.1411%, Training Loss: 0.5688%\n",
      "Epoch [40/300], Step [172/225], Training Accuracy: 74.1733%, Training Loss: 0.5683%\n",
      "Epoch [40/300], Step [173/225], Training Accuracy: 74.1510%, Training Loss: 0.5687%\n",
      "Epoch [40/300], Step [174/225], Training Accuracy: 74.2188%, Training Loss: 0.5681%\n",
      "Epoch [40/300], Step [175/225], Training Accuracy: 74.2857%, Training Loss: 0.5673%\n",
      "Epoch [40/300], Step [176/225], Training Accuracy: 74.2898%, Training Loss: 0.5668%\n",
      "Epoch [40/300], Step [177/225], Training Accuracy: 74.2938%, Training Loss: 0.5669%\n",
      "Epoch [40/300], Step [178/225], Training Accuracy: 74.2978%, Training Loss: 0.5666%\n",
      "Epoch [40/300], Step [179/225], Training Accuracy: 74.3104%, Training Loss: 0.5663%\n",
      "Epoch [40/300], Step [180/225], Training Accuracy: 74.2882%, Training Loss: 0.5665%\n",
      "Epoch [40/300], Step [181/225], Training Accuracy: 74.2835%, Training Loss: 0.5665%\n",
      "Epoch [40/300], Step [182/225], Training Accuracy: 74.3046%, Training Loss: 0.5662%\n",
      "Epoch [40/300], Step [183/225], Training Accuracy: 74.2572%, Training Loss: 0.5669%\n",
      "Epoch [40/300], Step [184/225], Training Accuracy: 74.2867%, Training Loss: 0.5663%\n",
      "Epoch [40/300], Step [185/225], Training Accuracy: 74.2905%, Training Loss: 0.5659%\n",
      "Epoch [40/300], Step [186/225], Training Accuracy: 74.2860%, Training Loss: 0.5660%\n",
      "Epoch [40/300], Step [187/225], Training Accuracy: 74.2981%, Training Loss: 0.5656%\n",
      "Epoch [40/300], Step [188/225], Training Accuracy: 74.3434%, Training Loss: 0.5651%\n",
      "Epoch [40/300], Step [189/225], Training Accuracy: 74.3882%, Training Loss: 0.5648%\n",
      "Epoch [40/300], Step [190/225], Training Accuracy: 74.3997%, Training Loss: 0.5652%\n",
      "Epoch [40/300], Step [191/225], Training Accuracy: 74.3783%, Training Loss: 0.5655%\n",
      "Epoch [40/300], Step [192/225], Training Accuracy: 74.4385%, Training Loss: 0.5647%\n",
      "Epoch [40/300], Step [193/225], Training Accuracy: 74.4252%, Training Loss: 0.5647%\n",
      "Epoch [40/300], Step [194/225], Training Accuracy: 74.4443%, Training Loss: 0.5648%\n",
      "Epoch [40/300], Step [195/225], Training Accuracy: 74.4471%, Training Loss: 0.5644%\n",
      "Epoch [40/300], Step [196/225], Training Accuracy: 74.3862%, Training Loss: 0.5650%\n",
      "Epoch [40/300], Step [197/225], Training Accuracy: 74.3972%, Training Loss: 0.5652%\n",
      "Epoch [40/300], Step [198/225], Training Accuracy: 74.4871%, Training Loss: 0.5641%\n",
      "Epoch [40/300], Step [199/225], Training Accuracy: 74.4975%, Training Loss: 0.5636%\n",
      "Epoch [40/300], Step [200/225], Training Accuracy: 74.5000%, Training Loss: 0.5633%\n",
      "Epoch [40/300], Step [201/225], Training Accuracy: 74.5103%, Training Loss: 0.5636%\n",
      "Epoch [40/300], Step [202/225], Training Accuracy: 74.5359%, Training Loss: 0.5634%\n",
      "Epoch [40/300], Step [203/225], Training Accuracy: 74.5151%, Training Loss: 0.5639%\n",
      "Epoch [40/300], Step [204/225], Training Accuracy: 74.4945%, Training Loss: 0.5641%\n",
      "Epoch [40/300], Step [205/225], Training Accuracy: 74.5503%, Training Loss: 0.5633%\n",
      "Epoch [40/300], Step [206/225], Training Accuracy: 74.5297%, Training Loss: 0.5634%\n",
      "Epoch [40/300], Step [207/225], Training Accuracy: 74.5396%, Training Loss: 0.5631%\n",
      "Epoch [40/300], Step [208/225], Training Accuracy: 74.5568%, Training Loss: 0.5629%\n",
      "Epoch [40/300], Step [209/225], Training Accuracy: 74.5290%, Training Loss: 0.5631%\n",
      "Epoch [40/300], Step [210/225], Training Accuracy: 74.5312%, Training Loss: 0.5633%\n",
      "Epoch [40/300], Step [211/225], Training Accuracy: 74.5705%, Training Loss: 0.5627%\n",
      "Epoch [40/300], Step [212/225], Training Accuracy: 74.5283%, Training Loss: 0.5633%\n",
      "Epoch [40/300], Step [213/225], Training Accuracy: 74.5452%, Training Loss: 0.5635%\n",
      "Epoch [40/300], Step [214/225], Training Accuracy: 74.5473%, Training Loss: 0.5635%\n",
      "Epoch [40/300], Step [215/225], Training Accuracy: 74.5494%, Training Loss: 0.5635%\n",
      "Epoch [40/300], Step [216/225], Training Accuracy: 74.5443%, Training Loss: 0.5635%\n",
      "Epoch [40/300], Step [217/225], Training Accuracy: 74.5248%, Training Loss: 0.5638%\n",
      "Epoch [40/300], Step [218/225], Training Accuracy: 74.4839%, Training Loss: 0.5639%\n",
      "Epoch [40/300], Step [219/225], Training Accuracy: 74.4720%, Training Loss: 0.5641%\n",
      "Epoch [40/300], Step [220/225], Training Accuracy: 74.4460%, Training Loss: 0.5647%\n",
      "Epoch [40/300], Step [221/225], Training Accuracy: 74.4415%, Training Loss: 0.5648%\n",
      "Epoch [40/300], Step [222/225], Training Accuracy: 74.4158%, Training Loss: 0.5648%\n",
      "Epoch [40/300], Step [223/225], Training Accuracy: 74.3974%, Training Loss: 0.5649%\n",
      "Epoch [40/300], Step [224/225], Training Accuracy: 74.4350%, Training Loss: 0.5643%\n",
      "Epoch [40/300], Step [225/225], Training Accuracy: 74.4163%, Training Loss: 0.5641%\n",
      "Epoch [41/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.5046%\n",
      "Epoch [41/300], Step [2/225], Training Accuracy: 75.0000%, Training Loss: 0.5309%\n",
      "Epoch [41/300], Step [3/225], Training Accuracy: 76.5625%, Training Loss: 0.5158%\n",
      "Epoch [41/300], Step [4/225], Training Accuracy: 74.2188%, Training Loss: 0.5324%\n",
      "Epoch [41/300], Step [5/225], Training Accuracy: 74.0625%, Training Loss: 0.5389%\n",
      "Epoch [41/300], Step [6/225], Training Accuracy: 76.0417%, Training Loss: 0.5355%\n",
      "Epoch [41/300], Step [7/225], Training Accuracy: 77.4554%, Training Loss: 0.5241%\n",
      "Epoch [41/300], Step [8/225], Training Accuracy: 76.5625%, Training Loss: 0.5334%\n",
      "Epoch [41/300], Step [9/225], Training Accuracy: 75.5208%, Training Loss: 0.5437%\n",
      "Epoch [41/300], Step [10/225], Training Accuracy: 76.0938%, Training Loss: 0.5392%\n",
      "Epoch [41/300], Step [11/225], Training Accuracy: 76.7045%, Training Loss: 0.5355%\n",
      "Epoch [41/300], Step [12/225], Training Accuracy: 76.5625%, Training Loss: 0.5383%\n",
      "Epoch [41/300], Step [13/225], Training Accuracy: 76.9231%, Training Loss: 0.5291%\n",
      "Epoch [41/300], Step [14/225], Training Accuracy: 76.6741%, Training Loss: 0.5286%\n",
      "Epoch [41/300], Step [15/225], Training Accuracy: 76.3542%, Training Loss: 0.5307%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Step [16/225], Training Accuracy: 76.5625%, Training Loss: 0.5286%\n",
      "Epoch [41/300], Step [17/225], Training Accuracy: 76.5625%, Training Loss: 0.5280%\n",
      "Epoch [41/300], Step [18/225], Training Accuracy: 76.1285%, Training Loss: 0.5327%\n",
      "Epoch [41/300], Step [19/225], Training Accuracy: 76.0691%, Training Loss: 0.5326%\n",
      "Epoch [41/300], Step [20/225], Training Accuracy: 76.0938%, Training Loss: 0.5333%\n",
      "Epoch [41/300], Step [21/225], Training Accuracy: 76.3393%, Training Loss: 0.5294%\n",
      "Epoch [41/300], Step [22/225], Training Accuracy: 75.9943%, Training Loss: 0.5345%\n",
      "Epoch [41/300], Step [23/225], Training Accuracy: 76.2228%, Training Loss: 0.5309%\n",
      "Epoch [41/300], Step [24/225], Training Accuracy: 75.7812%, Training Loss: 0.5372%\n",
      "Epoch [41/300], Step [25/225], Training Accuracy: 76.1250%, Training Loss: 0.5349%\n",
      "Epoch [41/300], Step [26/225], Training Accuracy: 75.8413%, Training Loss: 0.5380%\n",
      "Epoch [41/300], Step [27/225], Training Accuracy: 75.7523%, Training Loss: 0.5401%\n",
      "Epoch [41/300], Step [28/225], Training Accuracy: 76.0045%, Training Loss: 0.5348%\n",
      "Epoch [41/300], Step [29/225], Training Accuracy: 75.9159%, Training Loss: 0.5369%\n",
      "Epoch [41/300], Step [30/225], Training Accuracy: 76.1458%, Training Loss: 0.5368%\n",
      "Epoch [41/300], Step [31/225], Training Accuracy: 75.5544%, Training Loss: 0.5516%\n",
      "Epoch [41/300], Step [32/225], Training Accuracy: 75.5371%, Training Loss: 0.5514%\n",
      "Epoch [41/300], Step [33/225], Training Accuracy: 75.5208%, Training Loss: 0.5505%\n",
      "Epoch [41/300], Step [34/225], Training Accuracy: 75.5515%, Training Loss: 0.5490%\n",
      "Epoch [41/300], Step [35/225], Training Accuracy: 75.5357%, Training Loss: 0.5483%\n",
      "Epoch [41/300], Step [36/225], Training Accuracy: 75.6510%, Training Loss: 0.5469%\n",
      "Epoch [41/300], Step [37/225], Training Accuracy: 75.5912%, Training Loss: 0.5484%\n",
      "Epoch [41/300], Step [38/225], Training Accuracy: 75.4112%, Training Loss: 0.5498%\n",
      "Epoch [41/300], Step [39/225], Training Accuracy: 75.3606%, Training Loss: 0.5511%\n",
      "Epoch [41/300], Step [40/225], Training Accuracy: 75.3125%, Training Loss: 0.5512%\n",
      "Epoch [41/300], Step [41/225], Training Accuracy: 75.1905%, Training Loss: 0.5524%\n",
      "Epoch [41/300], Step [42/225], Training Accuracy: 75.2976%, Training Loss: 0.5501%\n",
      "Epoch [41/300], Step [43/225], Training Accuracy: 75.2544%, Training Loss: 0.5510%\n",
      "Epoch [41/300], Step [44/225], Training Accuracy: 75.5327%, Training Loss: 0.5472%\n",
      "Epoch [41/300], Step [45/225], Training Accuracy: 75.6250%, Training Loss: 0.5450%\n",
      "Epoch [41/300], Step [46/225], Training Accuracy: 75.6793%, Training Loss: 0.5438%\n",
      "Epoch [41/300], Step [47/225], Training Accuracy: 75.5652%, Training Loss: 0.5451%\n",
      "Epoch [41/300], Step [48/225], Training Accuracy: 75.5208%, Training Loss: 0.5459%\n",
      "Epoch [41/300], Step [49/225], Training Accuracy: 75.6059%, Training Loss: 0.5457%\n",
      "Epoch [41/300], Step [50/225], Training Accuracy: 75.5625%, Training Loss: 0.5447%\n",
      "Epoch [41/300], Step [51/225], Training Accuracy: 75.7047%, Training Loss: 0.5429%\n",
      "Epoch [41/300], Step [52/225], Training Accuracy: 75.6911%, Training Loss: 0.5422%\n",
      "Epoch [41/300], Step [53/225], Training Accuracy: 75.7075%, Training Loss: 0.5428%\n",
      "Epoch [41/300], Step [54/225], Training Accuracy: 75.5787%, Training Loss: 0.5425%\n",
      "Epoch [41/300], Step [55/225], Training Accuracy: 75.6250%, Training Loss: 0.5435%\n",
      "Epoch [41/300], Step [56/225], Training Accuracy: 75.5580%, Training Loss: 0.5439%\n",
      "Epoch [41/300], Step [57/225], Training Accuracy: 75.5482%, Training Loss: 0.5426%\n",
      "Epoch [41/300], Step [58/225], Training Accuracy: 75.5657%, Training Loss: 0.5428%\n",
      "Epoch [41/300], Step [59/225], Training Accuracy: 75.7415%, Training Loss: 0.5414%\n",
      "Epoch [41/300], Step [60/225], Training Accuracy: 75.8073%, Training Loss: 0.5403%\n",
      "Epoch [41/300], Step [61/225], Training Accuracy: 75.7428%, Training Loss: 0.5406%\n",
      "Epoch [41/300], Step [62/225], Training Accuracy: 75.6804%, Training Loss: 0.5434%\n",
      "Epoch [41/300], Step [63/225], Training Accuracy: 75.5704%, Training Loss: 0.5457%\n",
      "Epoch [41/300], Step [64/225], Training Accuracy: 75.6592%, Training Loss: 0.5440%\n",
      "Epoch [41/300], Step [65/225], Training Accuracy: 75.7212%, Training Loss: 0.5434%\n",
      "Epoch [41/300], Step [66/225], Training Accuracy: 75.7339%, Training Loss: 0.5429%\n",
      "Epoch [41/300], Step [67/225], Training Accuracy: 75.6297%, Training Loss: 0.5436%\n",
      "Epoch [41/300], Step [68/225], Training Accuracy: 75.5055%, Training Loss: 0.5448%\n",
      "Epoch [41/300], Step [69/225], Training Accuracy: 75.3850%, Training Loss: 0.5466%\n",
      "Epoch [41/300], Step [70/225], Training Accuracy: 75.4018%, Training Loss: 0.5465%\n",
      "Epoch [41/300], Step [71/225], Training Accuracy: 75.4181%, Training Loss: 0.5456%\n",
      "Epoch [41/300], Step [72/225], Training Accuracy: 75.4557%, Training Loss: 0.5454%\n",
      "Epoch [41/300], Step [73/225], Training Accuracy: 75.3639%, Training Loss: 0.5456%\n",
      "Epoch [41/300], Step [74/225], Training Accuracy: 75.2745%, Training Loss: 0.5468%\n",
      "Epoch [41/300], Step [75/225], Training Accuracy: 75.1875%, Training Loss: 0.5464%\n",
      "Epoch [41/300], Step [76/225], Training Accuracy: 75.0206%, Training Loss: 0.5485%\n",
      "Epoch [41/300], Step [77/225], Training Accuracy: 75.1623%, Training Loss: 0.5466%\n",
      "Epoch [41/300], Step [78/225], Training Accuracy: 75.1002%, Training Loss: 0.5465%\n",
      "Epoch [41/300], Step [79/225], Training Accuracy: 75.0396%, Training Loss: 0.5465%\n",
      "Epoch [41/300], Step [80/225], Training Accuracy: 75.0195%, Training Loss: 0.5469%\n",
      "Epoch [41/300], Step [81/225], Training Accuracy: 75.0579%, Training Loss: 0.5460%\n",
      "Epoch [41/300], Step [82/225], Training Accuracy: 74.9809%, Training Loss: 0.5463%\n",
      "Epoch [41/300], Step [83/225], Training Accuracy: 74.9435%, Training Loss: 0.5469%\n",
      "Epoch [41/300], Step [84/225], Training Accuracy: 74.9256%, Training Loss: 0.5466%\n",
      "Epoch [41/300], Step [85/225], Training Accuracy: 74.9816%, Training Loss: 0.5454%\n",
      "Epoch [41/300], Step [86/225], Training Accuracy: 74.9455%, Training Loss: 0.5451%\n",
      "Epoch [41/300], Step [87/225], Training Accuracy: 74.9282%, Training Loss: 0.5464%\n",
      "Epoch [41/300], Step [88/225], Training Accuracy: 74.8047%, Training Loss: 0.5476%\n",
      "Epoch [41/300], Step [89/225], Training Accuracy: 74.7893%, Training Loss: 0.5478%\n",
      "Epoch [41/300], Step [90/225], Training Accuracy: 74.7396%, Training Loss: 0.5497%\n",
      "Epoch [41/300], Step [91/225], Training Accuracy: 74.6909%, Training Loss: 0.5506%\n",
      "Epoch [41/300], Step [92/225], Training Accuracy: 74.6603%, Training Loss: 0.5510%\n",
      "Epoch [41/300], Step [93/225], Training Accuracy: 74.7480%, Training Loss: 0.5498%\n",
      "Epoch [41/300], Step [94/225], Training Accuracy: 74.7839%, Training Loss: 0.5491%\n",
      "Epoch [41/300], Step [95/225], Training Accuracy: 74.7533%, Training Loss: 0.5500%\n",
      "Epoch [41/300], Step [96/225], Training Accuracy: 74.7884%, Training Loss: 0.5490%\n",
      "Epoch [41/300], Step [97/225], Training Accuracy: 74.7423%, Training Loss: 0.5497%\n",
      "Epoch [41/300], Step [98/225], Training Accuracy: 74.6971%, Training Loss: 0.5505%\n",
      "Epoch [41/300], Step [99/225], Training Accuracy: 74.6843%, Training Loss: 0.5510%\n",
      "Epoch [41/300], Step [100/225], Training Accuracy: 74.6406%, Training Loss: 0.5522%\n",
      "Epoch [41/300], Step [101/225], Training Accuracy: 74.6906%, Training Loss: 0.5526%\n",
      "Epoch [41/300], Step [102/225], Training Accuracy: 74.6017%, Training Loss: 0.5534%\n",
      "Epoch [41/300], Step [103/225], Training Accuracy: 74.6966%, Training Loss: 0.5527%\n",
      "Epoch [41/300], Step [104/225], Training Accuracy: 74.6244%, Training Loss: 0.5542%\n",
      "Epoch [41/300], Step [105/225], Training Accuracy: 74.6726%, Training Loss: 0.5536%\n",
      "Epoch [41/300], Step [106/225], Training Accuracy: 74.6167%, Training Loss: 0.5539%\n",
      "Epoch [41/300], Step [107/225], Training Accuracy: 74.5619%, Training Loss: 0.5552%\n",
      "Epoch [41/300], Step [108/225], Training Accuracy: 74.5515%, Training Loss: 0.5551%\n",
      "Epoch [41/300], Step [109/225], Training Accuracy: 74.5556%, Training Loss: 0.5551%\n",
      "Epoch [41/300], Step [110/225], Training Accuracy: 74.5028%, Training Loss: 0.5557%\n",
      "Epoch [41/300], Step [111/225], Training Accuracy: 74.5214%, Training Loss: 0.5556%\n",
      "Epoch [41/300], Step [112/225], Training Accuracy: 74.4838%, Training Loss: 0.5559%\n",
      "Epoch [41/300], Step [113/225], Training Accuracy: 74.5160%, Training Loss: 0.5552%\n",
      "Epoch [41/300], Step [114/225], Training Accuracy: 74.5340%, Training Loss: 0.5542%\n",
      "Epoch [41/300], Step [115/225], Training Accuracy: 74.5924%, Training Loss: 0.5536%\n",
      "Epoch [41/300], Step [116/225], Training Accuracy: 74.5555%, Training Loss: 0.5540%\n",
      "Epoch [41/300], Step [117/225], Training Accuracy: 74.4925%, Training Loss: 0.5559%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Step [118/225], Training Accuracy: 74.4571%, Training Loss: 0.5564%\n",
      "Epoch [41/300], Step [119/225], Training Accuracy: 74.4879%, Training Loss: 0.5559%\n",
      "Epoch [41/300], Step [120/225], Training Accuracy: 74.4661%, Training Loss: 0.5563%\n",
      "Epoch [41/300], Step [121/225], Training Accuracy: 74.2898%, Training Loss: 0.5587%\n",
      "Epoch [41/300], Step [122/225], Training Accuracy: 74.2828%, Training Loss: 0.5592%\n",
      "Epoch [41/300], Step [123/225], Training Accuracy: 74.2505%, Training Loss: 0.5594%\n",
      "Epoch [41/300], Step [124/225], Training Accuracy: 74.2440%, Training Loss: 0.5585%\n",
      "Epoch [41/300], Step [125/225], Training Accuracy: 74.2875%, Training Loss: 0.5584%\n",
      "Epoch [41/300], Step [126/225], Training Accuracy: 74.3304%, Training Loss: 0.5578%\n",
      "Epoch [41/300], Step [127/225], Training Accuracy: 74.2864%, Training Loss: 0.5586%\n",
      "Epoch [41/300], Step [128/225], Training Accuracy: 74.1943%, Training Loss: 0.5600%\n",
      "Epoch [41/300], Step [129/225], Training Accuracy: 74.2127%, Training Loss: 0.5602%\n",
      "Epoch [41/300], Step [130/225], Training Accuracy: 74.2428%, Training Loss: 0.5599%\n",
      "Epoch [41/300], Step [131/225], Training Accuracy: 74.2963%, Training Loss: 0.5591%\n",
      "Epoch [41/300], Step [132/225], Training Accuracy: 74.2188%, Training Loss: 0.5601%\n",
      "Epoch [41/300], Step [133/225], Training Accuracy: 74.2011%, Training Loss: 0.5603%\n",
      "Epoch [41/300], Step [134/225], Training Accuracy: 74.1721%, Training Loss: 0.5615%\n",
      "Epoch [41/300], Step [135/225], Training Accuracy: 74.1319%, Training Loss: 0.5621%\n",
      "Epoch [41/300], Step [136/225], Training Accuracy: 74.1843%, Training Loss: 0.5616%\n",
      "Epoch [41/300], Step [137/225], Training Accuracy: 74.1560%, Training Loss: 0.5622%\n",
      "Epoch [41/300], Step [138/225], Training Accuracy: 74.2640%, Training Loss: 0.5608%\n",
      "Epoch [41/300], Step [139/225], Training Accuracy: 74.2019%, Training Loss: 0.5618%\n",
      "Epoch [41/300], Step [140/225], Training Accuracy: 74.2299%, Training Loss: 0.5618%\n",
      "Epoch [41/300], Step [141/225], Training Accuracy: 74.2243%, Training Loss: 0.5621%\n",
      "Epoch [41/300], Step [142/225], Training Accuracy: 74.2077%, Training Loss: 0.5618%\n",
      "Epoch [41/300], Step [143/225], Training Accuracy: 74.2351%, Training Loss: 0.5618%\n",
      "Epoch [41/300], Step [144/225], Training Accuracy: 74.1645%, Training Loss: 0.5617%\n",
      "Epoch [41/300], Step [145/225], Training Accuracy: 74.2026%, Training Loss: 0.5613%\n",
      "Epoch [41/300], Step [146/225], Training Accuracy: 74.2295%, Training Loss: 0.5613%\n",
      "Epoch [41/300], Step [147/225], Training Accuracy: 74.1922%, Training Loss: 0.5620%\n",
      "Epoch [41/300], Step [148/225], Training Accuracy: 74.2715%, Training Loss: 0.5616%\n",
      "Epoch [41/300], Step [149/225], Training Accuracy: 74.2659%, Training Loss: 0.5615%\n",
      "Epoch [41/300], Step [150/225], Training Accuracy: 74.2604%, Training Loss: 0.5618%\n",
      "Epoch [41/300], Step [151/225], Training Accuracy: 74.2860%, Training Loss: 0.5612%\n",
      "Epoch [41/300], Step [152/225], Training Accuracy: 74.2701%, Training Loss: 0.5614%\n",
      "Epoch [41/300], Step [153/225], Training Accuracy: 74.3056%, Training Loss: 0.5610%\n",
      "Epoch [41/300], Step [154/225], Training Accuracy: 74.3202%, Training Loss: 0.5606%\n",
      "Epoch [41/300], Step [155/225], Training Accuracy: 74.3448%, Training Loss: 0.5606%\n",
      "Epoch [41/300], Step [156/225], Training Accuracy: 74.3690%, Training Loss: 0.5604%\n",
      "Epoch [41/300], Step [157/225], Training Accuracy: 74.3929%, Training Loss: 0.5601%\n",
      "Epoch [41/300], Step [158/225], Training Accuracy: 74.3671%, Training Loss: 0.5607%\n",
      "Epoch [41/300], Step [159/225], Training Accuracy: 74.3219%, Training Loss: 0.5611%\n",
      "Epoch [41/300], Step [160/225], Training Accuracy: 74.3262%, Training Loss: 0.5609%\n",
      "Epoch [41/300], Step [161/225], Training Accuracy: 74.3207%, Training Loss: 0.5609%\n",
      "Epoch [41/300], Step [162/225], Training Accuracy: 74.3731%, Training Loss: 0.5603%\n",
      "Epoch [41/300], Step [163/225], Training Accuracy: 74.4057%, Training Loss: 0.5601%\n",
      "Epoch [41/300], Step [164/225], Training Accuracy: 74.4474%, Training Loss: 0.5596%\n",
      "Epoch [41/300], Step [165/225], Training Accuracy: 74.4697%, Training Loss: 0.5593%\n",
      "Epoch [41/300], Step [166/225], Training Accuracy: 74.4729%, Training Loss: 0.5590%\n",
      "Epoch [41/300], Step [167/225], Training Accuracy: 74.4667%, Training Loss: 0.5592%\n",
      "Epoch [41/300], Step [168/225], Training Accuracy: 74.4606%, Training Loss: 0.5599%\n",
      "Epoch [41/300], Step [169/225], Training Accuracy: 74.4360%, Training Loss: 0.5600%\n",
      "Epoch [41/300], Step [170/225], Training Accuracy: 74.4393%, Training Loss: 0.5601%\n",
      "Epoch [41/300], Step [171/225], Training Accuracy: 74.4426%, Training Loss: 0.5599%\n",
      "Epoch [41/300], Step [172/225], Training Accuracy: 74.4459%, Training Loss: 0.5599%\n",
      "Epoch [41/300], Step [173/225], Training Accuracy: 74.4039%, Training Loss: 0.5598%\n",
      "Epoch [41/300], Step [174/225], Training Accuracy: 74.4343%, Training Loss: 0.5596%\n",
      "Epoch [41/300], Step [175/225], Training Accuracy: 74.4821%, Training Loss: 0.5590%\n",
      "Epoch [41/300], Step [176/225], Training Accuracy: 74.4673%, Training Loss: 0.5586%\n",
      "Epoch [41/300], Step [177/225], Training Accuracy: 74.4968%, Training Loss: 0.5582%\n",
      "Epoch [41/300], Step [178/225], Training Accuracy: 74.4821%, Training Loss: 0.5582%\n",
      "Epoch [41/300], Step [179/225], Training Accuracy: 74.5024%, Training Loss: 0.5580%\n",
      "Epoch [41/300], Step [180/225], Training Accuracy: 74.5312%, Training Loss: 0.5575%\n",
      "Epoch [41/300], Step [181/225], Training Accuracy: 74.5511%, Training Loss: 0.5575%\n",
      "Epoch [41/300], Step [182/225], Training Accuracy: 74.5450%, Training Loss: 0.5577%\n",
      "Epoch [41/300], Step [183/225], Training Accuracy: 74.5304%, Training Loss: 0.5579%\n",
      "Epoch [41/300], Step [184/225], Training Accuracy: 74.5499%, Training Loss: 0.5571%\n",
      "Epoch [41/300], Step [185/225], Training Accuracy: 74.5861%, Training Loss: 0.5566%\n",
      "Epoch [41/300], Step [186/225], Training Accuracy: 74.5800%, Training Loss: 0.5565%\n",
      "Epoch [41/300], Step [187/225], Training Accuracy: 74.5488%, Training Loss: 0.5565%\n",
      "Epoch [41/300], Step [188/225], Training Accuracy: 74.5678%, Training Loss: 0.5562%\n",
      "Epoch [41/300], Step [189/225], Training Accuracy: 74.6032%, Training Loss: 0.5555%\n",
      "Epoch [41/300], Step [190/225], Training Accuracy: 74.6217%, Training Loss: 0.5552%\n",
      "Epoch [41/300], Step [191/225], Training Accuracy: 74.6237%, Training Loss: 0.5551%\n",
      "Epoch [41/300], Step [192/225], Training Accuracy: 74.6338%, Training Loss: 0.5545%\n",
      "Epoch [41/300], Step [193/225], Training Accuracy: 74.6033%, Training Loss: 0.5547%\n",
      "Epoch [41/300], Step [194/225], Training Accuracy: 74.5892%, Training Loss: 0.5546%\n",
      "Epoch [41/300], Step [195/225], Training Accuracy: 74.5994%, Training Loss: 0.5543%\n",
      "Epoch [41/300], Step [196/225], Training Accuracy: 74.5615%, Training Loss: 0.5548%\n",
      "Epoch [41/300], Step [197/225], Training Accuracy: 74.5638%, Training Loss: 0.5544%\n",
      "Epoch [41/300], Step [198/225], Training Accuracy: 74.6133%, Training Loss: 0.5536%\n",
      "Epoch [41/300], Step [199/225], Training Accuracy: 74.6545%, Training Loss: 0.5534%\n",
      "Epoch [41/300], Step [200/225], Training Accuracy: 74.6406%, Training Loss: 0.5539%\n",
      "Epoch [41/300], Step [201/225], Training Accuracy: 74.6113%, Training Loss: 0.5545%\n",
      "Epoch [41/300], Step [202/225], Training Accuracy: 74.6364%, Training Loss: 0.5540%\n",
      "Epoch [41/300], Step [203/225], Training Accuracy: 74.6382%, Training Loss: 0.5543%\n",
      "Epoch [41/300], Step [204/225], Training Accuracy: 74.6630%, Training Loss: 0.5540%\n",
      "Epoch [41/300], Step [205/225], Training Accuracy: 74.6951%, Training Loss: 0.5532%\n",
      "Epoch [41/300], Step [206/225], Training Accuracy: 74.7042%, Training Loss: 0.5535%\n",
      "Epoch [41/300], Step [207/225], Training Accuracy: 74.7056%, Training Loss: 0.5534%\n",
      "Epoch [41/300], Step [208/225], Training Accuracy: 74.7145%, Training Loss: 0.5531%\n",
      "Epoch [41/300], Step [209/225], Training Accuracy: 74.7309%, Training Loss: 0.5528%\n",
      "Epoch [41/300], Step [210/225], Training Accuracy: 74.7545%, Training Loss: 0.5522%\n",
      "Epoch [41/300], Step [211/225], Training Accuracy: 74.7852%, Training Loss: 0.5518%\n",
      "Epoch [41/300], Step [212/225], Training Accuracy: 74.7863%, Training Loss: 0.5520%\n",
      "Epoch [41/300], Step [213/225], Training Accuracy: 74.7359%, Training Loss: 0.5531%\n",
      "Epoch [41/300], Step [214/225], Training Accuracy: 74.7225%, Training Loss: 0.5528%\n",
      "Epoch [41/300], Step [215/225], Training Accuracy: 74.7093%, Training Loss: 0.5526%\n",
      "Epoch [41/300], Step [216/225], Training Accuracy: 74.6817%, Training Loss: 0.5527%\n",
      "Epoch [41/300], Step [217/225], Training Accuracy: 74.6904%, Training Loss: 0.5525%\n",
      "Epoch [41/300], Step [218/225], Training Accuracy: 74.6488%, Training Loss: 0.5528%\n",
      "Epoch [41/300], Step [219/225], Training Accuracy: 74.6575%, Training Loss: 0.5529%\n",
      "Epoch [41/300], Step [220/225], Training Accuracy: 74.6946%, Training Loss: 0.5524%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Step [221/225], Training Accuracy: 74.6606%, Training Loss: 0.5527%\n",
      "Epoch [41/300], Step [222/225], Training Accuracy: 74.7114%, Training Loss: 0.5523%\n",
      "Epoch [41/300], Step [223/225], Training Accuracy: 74.6707%, Training Loss: 0.5526%\n",
      "Epoch [41/300], Step [224/225], Training Accuracy: 74.6931%, Training Loss: 0.5527%\n",
      "Epoch [41/300], Step [225/225], Training Accuracy: 74.6804%, Training Loss: 0.5526%\n",
      "Epoch [42/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.5376%\n",
      "Epoch [42/300], Step [2/225], Training Accuracy: 71.0938%, Training Loss: 0.5847%\n",
      "Epoch [42/300], Step [3/225], Training Accuracy: 73.4375%, Training Loss: 0.5443%\n",
      "Epoch [42/300], Step [4/225], Training Accuracy: 74.6094%, Training Loss: 0.5435%\n",
      "Epoch [42/300], Step [5/225], Training Accuracy: 74.0625%, Training Loss: 0.5440%\n",
      "Epoch [42/300], Step [6/225], Training Accuracy: 73.9583%, Training Loss: 0.5378%\n",
      "Epoch [42/300], Step [7/225], Training Accuracy: 74.7768%, Training Loss: 0.5321%\n",
      "Epoch [42/300], Step [8/225], Training Accuracy: 75.3906%, Training Loss: 0.5341%\n",
      "Epoch [42/300], Step [9/225], Training Accuracy: 74.6528%, Training Loss: 0.5418%\n",
      "Epoch [42/300], Step [10/225], Training Accuracy: 75.3125%, Training Loss: 0.5402%\n",
      "Epoch [42/300], Step [11/225], Training Accuracy: 75.7102%, Training Loss: 0.5344%\n",
      "Epoch [42/300], Step [12/225], Training Accuracy: 76.0417%, Training Loss: 0.5357%\n",
      "Epoch [42/300], Step [13/225], Training Accuracy: 76.6827%, Training Loss: 0.5264%\n",
      "Epoch [42/300], Step [14/225], Training Accuracy: 76.6741%, Training Loss: 0.5221%\n",
      "Epoch [42/300], Step [15/225], Training Accuracy: 76.8750%, Training Loss: 0.5168%\n",
      "Epoch [42/300], Step [16/225], Training Accuracy: 76.8555%, Training Loss: 0.5142%\n",
      "Epoch [42/300], Step [17/225], Training Accuracy: 76.9301%, Training Loss: 0.5149%\n",
      "Epoch [42/300], Step [18/225], Training Accuracy: 76.5625%, Training Loss: 0.5169%\n",
      "Epoch [42/300], Step [19/225], Training Accuracy: 76.6447%, Training Loss: 0.5145%\n",
      "Epoch [42/300], Step [20/225], Training Accuracy: 76.2500%, Training Loss: 0.5131%\n",
      "Epoch [42/300], Step [21/225], Training Accuracy: 76.6369%, Training Loss: 0.5083%\n",
      "Epoch [42/300], Step [22/225], Training Accuracy: 76.4915%, Training Loss: 0.5144%\n",
      "Epoch [42/300], Step [23/225], Training Accuracy: 76.4266%, Training Loss: 0.5136%\n",
      "Epoch [42/300], Step [24/225], Training Accuracy: 76.3021%, Training Loss: 0.5186%\n",
      "Epoch [42/300], Step [25/225], Training Accuracy: 76.4375%, Training Loss: 0.5190%\n",
      "Epoch [42/300], Step [26/225], Training Accuracy: 76.1418%, Training Loss: 0.5237%\n",
      "Epoch [42/300], Step [27/225], Training Accuracy: 75.9838%, Training Loss: 0.5251%\n",
      "Epoch [42/300], Step [28/225], Training Accuracy: 76.3393%, Training Loss: 0.5202%\n",
      "Epoch [42/300], Step [29/225], Training Accuracy: 76.1853%, Training Loss: 0.5199%\n",
      "Epoch [42/300], Step [30/225], Training Accuracy: 76.2500%, Training Loss: 0.5193%\n",
      "Epoch [42/300], Step [31/225], Training Accuracy: 76.0081%, Training Loss: 0.5244%\n",
      "Epoch [42/300], Step [32/225], Training Accuracy: 75.6836%, Training Loss: 0.5255%\n",
      "Epoch [42/300], Step [33/225], Training Accuracy: 75.7102%, Training Loss: 0.5244%\n",
      "Epoch [42/300], Step [34/225], Training Accuracy: 75.7353%, Training Loss: 0.5254%\n",
      "Epoch [42/300], Step [35/225], Training Accuracy: 75.5804%, Training Loss: 0.5271%\n",
      "Epoch [42/300], Step [36/225], Training Accuracy: 75.4774%, Training Loss: 0.5271%\n",
      "Epoch [42/300], Step [37/225], Training Accuracy: 75.5490%, Training Loss: 0.5274%\n",
      "Epoch [42/300], Step [38/225], Training Accuracy: 75.4523%, Training Loss: 0.5282%\n",
      "Epoch [42/300], Step [39/225], Training Accuracy: 75.5208%, Training Loss: 0.5276%\n",
      "Epoch [42/300], Step [40/225], Training Accuracy: 75.4297%, Training Loss: 0.5291%\n",
      "Epoch [42/300], Step [41/225], Training Accuracy: 75.3430%, Training Loss: 0.5308%\n",
      "Epoch [42/300], Step [42/225], Training Accuracy: 75.3720%, Training Loss: 0.5310%\n",
      "Epoch [42/300], Step [43/225], Training Accuracy: 75.1090%, Training Loss: 0.5347%\n",
      "Epoch [42/300], Step [44/225], Training Accuracy: 75.3551%, Training Loss: 0.5310%\n",
      "Epoch [42/300], Step [45/225], Training Accuracy: 75.2778%, Training Loss: 0.5313%\n",
      "Epoch [42/300], Step [46/225], Training Accuracy: 75.4755%, Training Loss: 0.5285%\n",
      "Epoch [42/300], Step [47/225], Training Accuracy: 75.3324%, Training Loss: 0.5288%\n",
      "Epoch [42/300], Step [48/225], Training Accuracy: 75.1953%, Training Loss: 0.5317%\n",
      "Epoch [42/300], Step [49/225], Training Accuracy: 75.2870%, Training Loss: 0.5305%\n",
      "Epoch [42/300], Step [50/225], Training Accuracy: 75.2812%, Training Loss: 0.5309%\n",
      "Epoch [42/300], Step [51/225], Training Accuracy: 75.3370%, Training Loss: 0.5296%\n",
      "Epoch [42/300], Step [52/225], Training Accuracy: 75.4207%, Training Loss: 0.5286%\n",
      "Epoch [42/300], Step [53/225], Training Accuracy: 75.4127%, Training Loss: 0.5301%\n",
      "Epoch [42/300], Step [54/225], Training Accuracy: 75.3472%, Training Loss: 0.5307%\n",
      "Epoch [42/300], Step [55/225], Training Accuracy: 75.1989%, Training Loss: 0.5330%\n",
      "Epoch [42/300], Step [56/225], Training Accuracy: 75.2790%, Training Loss: 0.5331%\n",
      "Epoch [42/300], Step [57/225], Training Accuracy: 75.2467%, Training Loss: 0.5327%\n",
      "Epoch [42/300], Step [58/225], Training Accuracy: 75.2694%, Training Loss: 0.5336%\n",
      "Epoch [42/300], Step [59/225], Training Accuracy: 75.3443%, Training Loss: 0.5328%\n",
      "Epoch [42/300], Step [60/225], Training Accuracy: 75.4688%, Training Loss: 0.5322%\n",
      "Epoch [42/300], Step [61/225], Training Accuracy: 75.3586%, Training Loss: 0.5347%\n",
      "Epoch [42/300], Step [62/225], Training Accuracy: 75.4536%, Training Loss: 0.5336%\n",
      "Epoch [42/300], Step [63/225], Training Accuracy: 75.2728%, Training Loss: 0.5364%\n",
      "Epoch [42/300], Step [64/225], Training Accuracy: 75.4395%, Training Loss: 0.5344%\n",
      "Epoch [42/300], Step [65/225], Training Accuracy: 75.4567%, Training Loss: 0.5332%\n",
      "Epoch [42/300], Step [66/225], Training Accuracy: 75.5208%, Training Loss: 0.5321%\n",
      "Epoch [42/300], Step [67/225], Training Accuracy: 75.3731%, Training Loss: 0.5338%\n",
      "Epoch [42/300], Step [68/225], Training Accuracy: 75.2528%, Training Loss: 0.5349%\n",
      "Epoch [42/300], Step [69/225], Training Accuracy: 75.1812%, Training Loss: 0.5366%\n",
      "Epoch [42/300], Step [70/225], Training Accuracy: 75.1786%, Training Loss: 0.5367%\n",
      "Epoch [42/300], Step [71/225], Training Accuracy: 75.1100%, Training Loss: 0.5369%\n",
      "Epoch [42/300], Step [72/225], Training Accuracy: 75.1519%, Training Loss: 0.5358%\n",
      "Epoch [42/300], Step [73/225], Training Accuracy: 75.1926%, Training Loss: 0.5355%\n",
      "Epoch [42/300], Step [74/225], Training Accuracy: 75.1478%, Training Loss: 0.5360%\n",
      "Epoch [42/300], Step [75/225], Training Accuracy: 75.0625%, Training Loss: 0.5370%\n",
      "Epoch [42/300], Step [76/225], Training Accuracy: 74.9794%, Training Loss: 0.5390%\n",
      "Epoch [42/300], Step [77/225], Training Accuracy: 75.0609%, Training Loss: 0.5381%\n",
      "Epoch [42/300], Step [78/225], Training Accuracy: 75.0200%, Training Loss: 0.5384%\n",
      "Epoch [42/300], Step [79/225], Training Accuracy: 75.0000%, Training Loss: 0.5389%\n",
      "Epoch [42/300], Step [80/225], Training Accuracy: 74.9414%, Training Loss: 0.5389%\n",
      "Epoch [42/300], Step [81/225], Training Accuracy: 74.9421%, Training Loss: 0.5382%\n",
      "Epoch [42/300], Step [82/225], Training Accuracy: 74.9809%, Training Loss: 0.5370%\n",
      "Epoch [42/300], Step [83/225], Training Accuracy: 75.0188%, Training Loss: 0.5364%\n",
      "Epoch [42/300], Step [84/225], Training Accuracy: 75.0372%, Training Loss: 0.5365%\n",
      "Epoch [42/300], Step [85/225], Training Accuracy: 75.1287%, Training Loss: 0.5356%\n",
      "Epoch [42/300], Step [86/225], Training Accuracy: 75.0727%, Training Loss: 0.5358%\n",
      "Epoch [42/300], Step [87/225], Training Accuracy: 75.0539%, Training Loss: 0.5359%\n",
      "Epoch [42/300], Step [88/225], Training Accuracy: 75.0178%, Training Loss: 0.5372%\n",
      "Epoch [42/300], Step [89/225], Training Accuracy: 75.0176%, Training Loss: 0.5374%\n",
      "Epoch [42/300], Step [90/225], Training Accuracy: 75.0347%, Training Loss: 0.5385%\n",
      "Epoch [42/300], Step [91/225], Training Accuracy: 75.0000%, Training Loss: 0.5388%\n",
      "Epoch [42/300], Step [92/225], Training Accuracy: 74.9660%, Training Loss: 0.5388%\n",
      "Epoch [42/300], Step [93/225], Training Accuracy: 75.0504%, Training Loss: 0.5382%\n",
      "Epoch [42/300], Step [94/225], Training Accuracy: 75.0831%, Training Loss: 0.5372%\n",
      "Epoch [42/300], Step [95/225], Training Accuracy: 75.0329%, Training Loss: 0.5378%\n",
      "Epoch [42/300], Step [96/225], Training Accuracy: 75.0000%, Training Loss: 0.5375%\n",
      "Epoch [42/300], Step [97/225], Training Accuracy: 75.0644%, Training Loss: 0.5366%\n",
      "Epoch [42/300], Step [98/225], Training Accuracy: 74.9841%, Training Loss: 0.5381%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Step [99/225], Training Accuracy: 75.0316%, Training Loss: 0.5384%\n",
      "Epoch [42/300], Step [100/225], Training Accuracy: 75.0156%, Training Loss: 0.5391%\n",
      "Epoch [42/300], Step [101/225], Training Accuracy: 74.9381%, Training Loss: 0.5397%\n",
      "Epoch [42/300], Step [102/225], Training Accuracy: 74.9540%, Training Loss: 0.5398%\n",
      "Epoch [42/300], Step [103/225], Training Accuracy: 74.9848%, Training Loss: 0.5393%\n",
      "Epoch [42/300], Step [104/225], Training Accuracy: 74.8798%, Training Loss: 0.5403%\n",
      "Epoch [42/300], Step [105/225], Training Accuracy: 74.8958%, Training Loss: 0.5394%\n",
      "Epoch [42/300], Step [106/225], Training Accuracy: 74.9410%, Training Loss: 0.5389%\n",
      "Epoch [42/300], Step [107/225], Training Accuracy: 74.8394%, Training Loss: 0.5405%\n",
      "Epoch [42/300], Step [108/225], Training Accuracy: 74.8119%, Training Loss: 0.5411%\n",
      "Epoch [42/300], Step [109/225], Training Accuracy: 74.7993%, Training Loss: 0.5410%\n",
      "Epoch [42/300], Step [110/225], Training Accuracy: 74.7585%, Training Loss: 0.5410%\n",
      "Epoch [42/300], Step [111/225], Training Accuracy: 74.8029%, Training Loss: 0.5408%\n",
      "Epoch [42/300], Step [112/225], Training Accuracy: 74.8465%, Training Loss: 0.5416%\n",
      "Epoch [42/300], Step [113/225], Training Accuracy: 74.9032%, Training Loss: 0.5422%\n",
      "Epoch [42/300], Step [114/225], Training Accuracy: 74.8629%, Training Loss: 0.5428%\n",
      "Epoch [42/300], Step [115/225], Training Accuracy: 74.9321%, Training Loss: 0.5419%\n",
      "Epoch [42/300], Step [116/225], Training Accuracy: 74.9057%, Training Loss: 0.5418%\n",
      "Epoch [42/300], Step [117/225], Training Accuracy: 74.8932%, Training Loss: 0.5424%\n",
      "Epoch [42/300], Step [118/225], Training Accuracy: 74.8808%, Training Loss: 0.5421%\n",
      "Epoch [42/300], Step [119/225], Training Accuracy: 74.9212%, Training Loss: 0.5418%\n",
      "Epoch [42/300], Step [120/225], Training Accuracy: 74.9609%, Training Loss: 0.5412%\n",
      "Epoch [42/300], Step [121/225], Training Accuracy: 74.8321%, Training Loss: 0.5428%\n",
      "Epoch [42/300], Step [122/225], Training Accuracy: 74.8591%, Training Loss: 0.5429%\n",
      "Epoch [42/300], Step [123/225], Training Accuracy: 74.8730%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [124/225], Training Accuracy: 74.8488%, Training Loss: 0.5430%\n",
      "Epoch [42/300], Step [125/225], Training Accuracy: 74.7625%, Training Loss: 0.5436%\n",
      "Epoch [42/300], Step [126/225], Training Accuracy: 74.7396%, Training Loss: 0.5437%\n",
      "Epoch [42/300], Step [127/225], Training Accuracy: 74.7539%, Training Loss: 0.5440%\n",
      "Epoch [42/300], Step [128/225], Training Accuracy: 74.7314%, Training Loss: 0.5449%\n",
      "Epoch [42/300], Step [129/225], Training Accuracy: 74.7093%, Training Loss: 0.5457%\n",
      "Epoch [42/300], Step [130/225], Training Accuracy: 74.6995%, Training Loss: 0.5460%\n",
      "Epoch [42/300], Step [131/225], Training Accuracy: 74.7018%, Training Loss: 0.5459%\n",
      "Epoch [42/300], Step [132/225], Training Accuracy: 74.6922%, Training Loss: 0.5465%\n",
      "Epoch [42/300], Step [133/225], Training Accuracy: 74.6476%, Training Loss: 0.5466%\n",
      "Epoch [42/300], Step [134/225], Training Accuracy: 74.6152%, Training Loss: 0.5470%\n",
      "Epoch [42/300], Step [135/225], Training Accuracy: 74.6181%, Training Loss: 0.5469%\n",
      "Epoch [42/300], Step [136/225], Training Accuracy: 74.5979%, Training Loss: 0.5467%\n",
      "Epoch [42/300], Step [137/225], Training Accuracy: 74.6008%, Training Loss: 0.5468%\n",
      "Epoch [42/300], Step [138/225], Training Accuracy: 74.6150%, Training Loss: 0.5462%\n",
      "Epoch [42/300], Step [139/225], Training Accuracy: 74.6290%, Training Loss: 0.5459%\n",
      "Epoch [42/300], Step [140/225], Training Accuracy: 74.7098%, Training Loss: 0.5450%\n",
      "Epoch [42/300], Step [141/225], Training Accuracy: 74.7230%, Training Loss: 0.5447%\n",
      "Epoch [42/300], Step [142/225], Training Accuracy: 74.7029%, Training Loss: 0.5445%\n",
      "Epoch [42/300], Step [143/225], Training Accuracy: 74.6722%, Training Loss: 0.5449%\n",
      "Epoch [42/300], Step [144/225], Training Accuracy: 74.6745%, Training Loss: 0.5453%\n",
      "Epoch [42/300], Step [145/225], Training Accuracy: 74.7198%, Training Loss: 0.5449%\n",
      "Epoch [42/300], Step [146/225], Training Accuracy: 74.6896%, Training Loss: 0.5452%\n",
      "Epoch [42/300], Step [147/225], Training Accuracy: 74.6705%, Training Loss: 0.5451%\n",
      "Epoch [42/300], Step [148/225], Training Accuracy: 74.6622%, Training Loss: 0.5447%\n",
      "Epoch [42/300], Step [149/225], Training Accuracy: 74.6749%, Training Loss: 0.5446%\n",
      "Epoch [42/300], Step [150/225], Training Accuracy: 74.6875%, Training Loss: 0.5443%\n",
      "Epoch [42/300], Step [151/225], Training Accuracy: 74.7413%, Training Loss: 0.5436%\n",
      "Epoch [42/300], Step [152/225], Training Accuracy: 74.7019%, Training Loss: 0.5446%\n",
      "Epoch [42/300], Step [153/225], Training Accuracy: 74.7141%, Training Loss: 0.5442%\n",
      "Epoch [42/300], Step [154/225], Training Accuracy: 74.7261%, Training Loss: 0.5436%\n",
      "Epoch [42/300], Step [155/225], Training Accuracy: 74.6673%, Training Loss: 0.5443%\n",
      "Epoch [42/300], Step [156/225], Training Accuracy: 74.6494%, Training Loss: 0.5452%\n",
      "Epoch [42/300], Step [157/225], Training Accuracy: 74.6417%, Training Loss: 0.5456%\n",
      "Epoch [42/300], Step [158/225], Training Accuracy: 74.5945%, Training Loss: 0.5458%\n",
      "Epoch [42/300], Step [159/225], Training Accuracy: 74.5774%, Training Loss: 0.5461%\n",
      "Epoch [42/300], Step [160/225], Training Accuracy: 74.6387%, Training Loss: 0.5454%\n",
      "Epoch [42/300], Step [161/225], Training Accuracy: 74.6118%, Training Loss: 0.5459%\n",
      "Epoch [42/300], Step [162/225], Training Accuracy: 74.7396%, Training Loss: 0.5443%\n",
      "Epoch [42/300], Step [163/225], Training Accuracy: 74.7412%, Training Loss: 0.5439%\n",
      "Epoch [42/300], Step [164/225], Training Accuracy: 74.7618%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [165/225], Training Accuracy: 74.7633%, Training Loss: 0.5429%\n",
      "Epoch [42/300], Step [166/225], Training Accuracy: 74.7176%, Training Loss: 0.5434%\n",
      "Epoch [42/300], Step [167/225], Training Accuracy: 74.7287%, Training Loss: 0.5435%\n",
      "Epoch [42/300], Step [168/225], Training Accuracy: 74.7117%, Training Loss: 0.5436%\n",
      "Epoch [42/300], Step [169/225], Training Accuracy: 74.6857%, Training Loss: 0.5439%\n",
      "Epoch [42/300], Step [170/225], Training Accuracy: 74.7059%, Training Loss: 0.5437%\n",
      "Epoch [42/300], Step [171/225], Training Accuracy: 74.7259%, Training Loss: 0.5434%\n",
      "Epoch [42/300], Step [172/225], Training Accuracy: 74.7184%, Training Loss: 0.5432%\n",
      "Epoch [42/300], Step [173/225], Training Accuracy: 74.6749%, Training Loss: 0.5433%\n",
      "Epoch [42/300], Step [174/225], Training Accuracy: 74.6588%, Training Loss: 0.5435%\n",
      "Epoch [42/300], Step [175/225], Training Accuracy: 74.6964%, Training Loss: 0.5430%\n",
      "Epoch [42/300], Step [176/225], Training Accuracy: 74.6626%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [177/225], Training Accuracy: 74.7087%, Training Loss: 0.5428%\n",
      "Epoch [42/300], Step [178/225], Training Accuracy: 74.7015%, Training Loss: 0.5427%\n",
      "Epoch [42/300], Step [179/225], Training Accuracy: 74.7381%, Training Loss: 0.5422%\n",
      "Epoch [42/300], Step [180/225], Training Accuracy: 74.6875%, Training Loss: 0.5430%\n",
      "Epoch [42/300], Step [181/225], Training Accuracy: 74.6202%, Training Loss: 0.5439%\n",
      "Epoch [42/300], Step [182/225], Training Accuracy: 74.5965%, Training Loss: 0.5447%\n",
      "Epoch [42/300], Step [183/225], Training Accuracy: 74.5816%, Training Loss: 0.5449%\n",
      "Epoch [42/300], Step [184/225], Training Accuracy: 74.6603%, Training Loss: 0.5440%\n",
      "Epoch [42/300], Step [185/225], Training Accuracy: 74.6875%, Training Loss: 0.5434%\n",
      "Epoch [42/300], Step [186/225], Training Accuracy: 74.7312%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [187/225], Training Accuracy: 74.7326%, Training Loss: 0.5433%\n",
      "Epoch [42/300], Step [188/225], Training Accuracy: 74.7590%, Training Loss: 0.5430%\n",
      "Epoch [42/300], Step [189/225], Training Accuracy: 74.7685%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [190/225], Training Accuracy: 74.7533%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [191/225], Training Accuracy: 74.7955%, Training Loss: 0.5431%\n",
      "Epoch [42/300], Step [192/225], Training Accuracy: 74.8372%, Training Loss: 0.5425%\n",
      "Epoch [42/300], Step [193/225], Training Accuracy: 74.8381%, Training Loss: 0.5423%\n",
      "Epoch [42/300], Step [194/225], Training Accuracy: 74.8389%, Training Loss: 0.5420%\n",
      "Epoch [42/300], Step [195/225], Training Accuracy: 74.8397%, Training Loss: 0.5418%\n",
      "Epoch [42/300], Step [196/225], Training Accuracy: 74.8087%, Training Loss: 0.5422%\n",
      "Epoch [42/300], Step [197/225], Training Accuracy: 74.8096%, Training Loss: 0.5423%\n",
      "Epoch [42/300], Step [198/225], Training Accuracy: 74.8658%, Training Loss: 0.5413%\n",
      "Epoch [42/300], Step [199/225], Training Accuracy: 74.8744%, Training Loss: 0.5411%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Step [200/225], Training Accuracy: 74.8438%, Training Loss: 0.5416%\n",
      "Epoch [42/300], Step [201/225], Training Accuracy: 74.8678%, Training Loss: 0.5415%\n",
      "Epoch [42/300], Step [202/225], Training Accuracy: 74.8762%, Training Loss: 0.5413%\n",
      "Epoch [42/300], Step [203/225], Training Accuracy: 74.9076%, Training Loss: 0.5415%\n",
      "Epoch [42/300], Step [204/225], Training Accuracy: 74.9157%, Training Loss: 0.5414%\n",
      "Epoch [42/300], Step [205/225], Training Accuracy: 74.9619%, Training Loss: 0.5409%\n",
      "Epoch [42/300], Step [206/225], Training Accuracy: 74.9924%, Training Loss: 0.5410%\n",
      "Epoch [42/300], Step [207/225], Training Accuracy: 75.0226%, Training Loss: 0.5409%\n",
      "Epoch [42/300], Step [208/225], Training Accuracy: 75.0601%, Training Loss: 0.5403%\n",
      "Epoch [42/300], Step [209/225], Training Accuracy: 75.0897%, Training Loss: 0.5401%\n",
      "Epoch [42/300], Step [210/225], Training Accuracy: 75.0670%, Training Loss: 0.5406%\n",
      "Epoch [42/300], Step [211/225], Training Accuracy: 75.1037%, Training Loss: 0.5403%\n",
      "Epoch [42/300], Step [212/225], Training Accuracy: 75.0811%, Training Loss: 0.5411%\n",
      "Epoch [42/300], Step [213/225], Training Accuracy: 75.1027%, Training Loss: 0.5413%\n",
      "Epoch [42/300], Step [214/225], Training Accuracy: 75.1022%, Training Loss: 0.5410%\n",
      "Epoch [42/300], Step [215/225], Training Accuracy: 75.0945%, Training Loss: 0.5412%\n",
      "Epoch [42/300], Step [216/225], Training Accuracy: 75.0579%, Training Loss: 0.5412%\n",
      "Epoch [42/300], Step [217/225], Training Accuracy: 75.0720%, Training Loss: 0.5411%\n",
      "Epoch [42/300], Step [218/225], Training Accuracy: 75.0573%, Training Loss: 0.5412%\n",
      "Epoch [42/300], Step [219/225], Training Accuracy: 75.0571%, Training Loss: 0.5412%\n",
      "Epoch [42/300], Step [220/225], Training Accuracy: 75.0142%, Training Loss: 0.5417%\n",
      "Epoch [42/300], Step [221/225], Training Accuracy: 74.9929%, Training Loss: 0.5421%\n",
      "Epoch [42/300], Step [222/225], Training Accuracy: 75.0211%, Training Loss: 0.5418%\n",
      "Epoch [42/300], Step [223/225], Training Accuracy: 75.0140%, Training Loss: 0.5418%\n",
      "Epoch [42/300], Step [224/225], Training Accuracy: 75.0209%, Training Loss: 0.5415%\n",
      "Epoch [42/300], Step [225/225], Training Accuracy: 74.9861%, Training Loss: 0.5416%\n",
      "Epoch [43/300], Step [1/225], Training Accuracy: 67.1875%, Training Loss: 0.5693%\n",
      "Epoch [43/300], Step [2/225], Training Accuracy: 74.2188%, Training Loss: 0.5502%\n",
      "Epoch [43/300], Step [3/225], Training Accuracy: 76.0417%, Training Loss: 0.5343%\n",
      "Epoch [43/300], Step [4/225], Training Accuracy: 76.5625%, Training Loss: 0.5314%\n",
      "Epoch [43/300], Step [5/225], Training Accuracy: 77.5000%, Training Loss: 0.5316%\n",
      "Epoch [43/300], Step [6/225], Training Accuracy: 77.0833%, Training Loss: 0.5389%\n",
      "Epoch [43/300], Step [7/225], Training Accuracy: 77.6786%, Training Loss: 0.5272%\n",
      "Epoch [43/300], Step [8/225], Training Accuracy: 77.1484%, Training Loss: 0.5279%\n",
      "Epoch [43/300], Step [9/225], Training Accuracy: 76.2153%, Training Loss: 0.5378%\n",
      "Epoch [43/300], Step [10/225], Training Accuracy: 75.7812%, Training Loss: 0.5436%\n",
      "Epoch [43/300], Step [11/225], Training Accuracy: 76.1364%, Training Loss: 0.5337%\n",
      "Epoch [43/300], Step [12/225], Training Accuracy: 75.5208%, Training Loss: 0.5391%\n",
      "Epoch [43/300], Step [13/225], Training Accuracy: 75.3606%, Training Loss: 0.5366%\n",
      "Epoch [43/300], Step [14/225], Training Accuracy: 75.7812%, Training Loss: 0.5279%\n",
      "Epoch [43/300], Step [15/225], Training Accuracy: 76.2500%, Training Loss: 0.5214%\n",
      "Epoch [43/300], Step [16/225], Training Accuracy: 76.3672%, Training Loss: 0.5184%\n",
      "Epoch [43/300], Step [17/225], Training Accuracy: 76.1949%, Training Loss: 0.5179%\n",
      "Epoch [43/300], Step [18/225], Training Accuracy: 76.3021%, Training Loss: 0.5167%\n",
      "Epoch [43/300], Step [19/225], Training Accuracy: 76.0691%, Training Loss: 0.5223%\n",
      "Epoch [43/300], Step [20/225], Training Accuracy: 75.7031%, Training Loss: 0.5254%\n",
      "Epoch [43/300], Step [21/225], Training Accuracy: 75.8929%, Training Loss: 0.5211%\n",
      "Epoch [43/300], Step [22/225], Training Accuracy: 75.6392%, Training Loss: 0.5271%\n",
      "Epoch [43/300], Step [23/225], Training Accuracy: 75.6114%, Training Loss: 0.5249%\n",
      "Epoch [43/300], Step [24/225], Training Accuracy: 75.3255%, Training Loss: 0.5289%\n",
      "Epoch [43/300], Step [25/225], Training Accuracy: 75.6250%, Training Loss: 0.5247%\n",
      "Epoch [43/300], Step [26/225], Training Accuracy: 75.4207%, Training Loss: 0.5251%\n",
      "Epoch [43/300], Step [27/225], Training Accuracy: 75.1157%, Training Loss: 0.5268%\n",
      "Epoch [43/300], Step [28/225], Training Accuracy: 75.4464%, Training Loss: 0.5223%\n",
      "Epoch [43/300], Step [29/225], Training Accuracy: 75.0539%, Training Loss: 0.5246%\n",
      "Epoch [43/300], Step [30/225], Training Accuracy: 75.4688%, Training Loss: 0.5224%\n",
      "Epoch [43/300], Step [31/225], Training Accuracy: 75.3024%, Training Loss: 0.5309%\n",
      "Epoch [43/300], Step [32/225], Training Accuracy: 75.2441%, Training Loss: 0.5319%\n",
      "Epoch [43/300], Step [33/225], Training Accuracy: 75.2367%, Training Loss: 0.5298%\n",
      "Epoch [43/300], Step [34/225], Training Accuracy: 75.1838%, Training Loss: 0.5307%\n",
      "Epoch [43/300], Step [35/225], Training Accuracy: 75.3125%, Training Loss: 0.5300%\n",
      "Epoch [43/300], Step [36/225], Training Accuracy: 75.3472%, Training Loss: 0.5299%\n",
      "Epoch [43/300], Step [37/225], Training Accuracy: 75.2534%, Training Loss: 0.5309%\n",
      "Epoch [43/300], Step [38/225], Training Accuracy: 75.1645%, Training Loss: 0.5308%\n",
      "Epoch [43/300], Step [39/225], Training Accuracy: 75.2404%, Training Loss: 0.5303%\n",
      "Epoch [43/300], Step [40/225], Training Accuracy: 75.3516%, Training Loss: 0.5293%\n",
      "Epoch [43/300], Step [41/225], Training Accuracy: 75.3049%, Training Loss: 0.5295%\n",
      "Epoch [43/300], Step [42/225], Training Accuracy: 75.4092%, Training Loss: 0.5298%\n",
      "Epoch [43/300], Step [43/225], Training Accuracy: 75.4360%, Training Loss: 0.5297%\n",
      "Epoch [43/300], Step [44/225], Training Accuracy: 75.4972%, Training Loss: 0.5279%\n",
      "Epoch [43/300], Step [45/225], Training Accuracy: 75.5556%, Training Loss: 0.5281%\n",
      "Epoch [43/300], Step [46/225], Training Accuracy: 75.6454%, Training Loss: 0.5272%\n",
      "Epoch [43/300], Step [47/225], Training Accuracy: 75.4654%, Training Loss: 0.5280%\n",
      "Epoch [43/300], Step [48/225], Training Accuracy: 75.3581%, Training Loss: 0.5297%\n",
      "Epoch [43/300], Step [49/225], Training Accuracy: 75.3827%, Training Loss: 0.5285%\n",
      "Epoch [43/300], Step [50/225], Training Accuracy: 75.3125%, Training Loss: 0.5294%\n",
      "Epoch [43/300], Step [51/225], Training Accuracy: 75.3064%, Training Loss: 0.5301%\n",
      "Epoch [43/300], Step [52/225], Training Accuracy: 75.3606%, Training Loss: 0.5294%\n",
      "Epoch [43/300], Step [53/225], Training Accuracy: 75.3833%, Training Loss: 0.5314%\n",
      "Epoch [43/300], Step [54/225], Training Accuracy: 75.2894%, Training Loss: 0.5317%\n",
      "Epoch [43/300], Step [55/225], Training Accuracy: 75.2557%, Training Loss: 0.5336%\n",
      "Epoch [43/300], Step [56/225], Training Accuracy: 75.1953%, Training Loss: 0.5350%\n",
      "Epoch [43/300], Step [57/225], Training Accuracy: 75.2193%, Training Loss: 0.5336%\n",
      "Epoch [43/300], Step [58/225], Training Accuracy: 75.2963%, Training Loss: 0.5335%\n",
      "Epoch [43/300], Step [59/225], Training Accuracy: 75.3443%, Training Loss: 0.5336%\n",
      "Epoch [43/300], Step [60/225], Training Accuracy: 75.4948%, Training Loss: 0.5326%\n",
      "Epoch [43/300], Step [61/225], Training Accuracy: 75.3586%, Training Loss: 0.5341%\n",
      "Epoch [43/300], Step [62/225], Training Accuracy: 75.2520%, Training Loss: 0.5349%\n",
      "Epoch [43/300], Step [63/225], Training Accuracy: 75.0744%, Training Loss: 0.5371%\n",
      "Epoch [43/300], Step [64/225], Training Accuracy: 75.2197%, Training Loss: 0.5350%\n",
      "Epoch [43/300], Step [65/225], Training Accuracy: 75.2885%, Training Loss: 0.5342%\n",
      "Epoch [43/300], Step [66/225], Training Accuracy: 75.4261%, Training Loss: 0.5332%\n",
      "Epoch [43/300], Step [67/225], Training Accuracy: 75.4664%, Training Loss: 0.5326%\n",
      "Epoch [43/300], Step [68/225], Training Accuracy: 75.4136%, Training Loss: 0.5338%\n",
      "Epoch [43/300], Step [69/225], Training Accuracy: 75.2491%, Training Loss: 0.5357%\n",
      "Epoch [43/300], Step [70/225], Training Accuracy: 75.2009%, Training Loss: 0.5360%\n",
      "Epoch [43/300], Step [71/225], Training Accuracy: 75.2421%, Training Loss: 0.5353%\n",
      "Epoch [43/300], Step [72/225], Training Accuracy: 75.2387%, Training Loss: 0.5363%\n",
      "Epoch [43/300], Step [73/225], Training Accuracy: 75.2568%, Training Loss: 0.5367%\n",
      "Epoch [43/300], Step [74/225], Training Accuracy: 75.2956%, Training Loss: 0.5358%\n",
      "Epoch [43/300], Step [75/225], Training Accuracy: 75.2500%, Training Loss: 0.5364%\n",
      "Epoch [43/300], Step [76/225], Training Accuracy: 75.2056%, Training Loss: 0.5371%\n",
      "Epoch [43/300], Step [77/225], Training Accuracy: 75.2638%, Training Loss: 0.5367%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Step [78/225], Training Accuracy: 75.2003%, Training Loss: 0.5370%\n",
      "Epoch [43/300], Step [79/225], Training Accuracy: 75.1780%, Training Loss: 0.5380%\n",
      "Epoch [43/300], Step [80/225], Training Accuracy: 75.1953%, Training Loss: 0.5378%\n",
      "Epoch [43/300], Step [81/225], Training Accuracy: 75.2701%, Training Loss: 0.5368%\n",
      "Epoch [43/300], Step [82/225], Training Accuracy: 75.3430%, Training Loss: 0.5361%\n",
      "Epoch [43/300], Step [83/225], Training Accuracy: 75.3200%, Training Loss: 0.5364%\n",
      "Epoch [43/300], Step [84/225], Training Accuracy: 75.2418%, Training Loss: 0.5361%\n",
      "Epoch [43/300], Step [85/225], Training Accuracy: 75.2757%, Training Loss: 0.5352%\n",
      "Epoch [43/300], Step [86/225], Training Accuracy: 75.1999%, Training Loss: 0.5353%\n",
      "Epoch [43/300], Step [87/225], Training Accuracy: 75.1257%, Training Loss: 0.5351%\n",
      "Epoch [43/300], Step [88/225], Training Accuracy: 75.0178%, Training Loss: 0.5366%\n",
      "Epoch [43/300], Step [89/225], Training Accuracy: 74.9649%, Training Loss: 0.5379%\n",
      "Epoch [43/300], Step [90/225], Training Accuracy: 74.9306%, Training Loss: 0.5387%\n",
      "Epoch [43/300], Step [91/225], Training Accuracy: 74.9313%, Training Loss: 0.5389%\n",
      "Epoch [43/300], Step [92/225], Training Accuracy: 74.9321%, Training Loss: 0.5396%\n",
      "Epoch [43/300], Step [93/225], Training Accuracy: 75.0336%, Training Loss: 0.5381%\n",
      "Epoch [43/300], Step [94/225], Training Accuracy: 75.0665%, Training Loss: 0.5375%\n",
      "Epoch [43/300], Step [95/225], Training Accuracy: 75.1151%, Training Loss: 0.5374%\n",
      "Epoch [43/300], Step [96/225], Training Accuracy: 75.1628%, Training Loss: 0.5369%\n",
      "Epoch [43/300], Step [97/225], Training Accuracy: 75.1611%, Training Loss: 0.5373%\n",
      "Epoch [43/300], Step [98/225], Training Accuracy: 75.0638%, Training Loss: 0.5398%\n",
      "Epoch [43/300], Step [99/225], Training Accuracy: 75.0631%, Training Loss: 0.5405%\n",
      "Epoch [43/300], Step [100/225], Training Accuracy: 75.0312%, Training Loss: 0.5418%\n",
      "Epoch [43/300], Step [101/225], Training Accuracy: 75.0464%, Training Loss: 0.5416%\n",
      "Epoch [43/300], Step [102/225], Training Accuracy: 75.0000%, Training Loss: 0.5416%\n",
      "Epoch [43/300], Step [103/225], Training Accuracy: 74.9848%, Training Loss: 0.5412%\n",
      "Epoch [43/300], Step [104/225], Training Accuracy: 74.9099%, Training Loss: 0.5419%\n",
      "Epoch [43/300], Step [105/225], Training Accuracy: 74.9702%, Training Loss: 0.5406%\n",
      "Epoch [43/300], Step [106/225], Training Accuracy: 75.0000%, Training Loss: 0.5412%\n",
      "Epoch [43/300], Step [107/225], Training Accuracy: 74.9124%, Training Loss: 0.5426%\n",
      "Epoch [43/300], Step [108/225], Training Accuracy: 74.9711%, Training Loss: 0.5426%\n",
      "Epoch [43/300], Step [109/225], Training Accuracy: 74.9570%, Training Loss: 0.5426%\n",
      "Epoch [43/300], Step [110/225], Training Accuracy: 74.9148%, Training Loss: 0.5430%\n",
      "Epoch [43/300], Step [111/225], Training Accuracy: 74.8592%, Training Loss: 0.5435%\n",
      "Epoch [43/300], Step [112/225], Training Accuracy: 74.8884%, Training Loss: 0.5434%\n",
      "Epoch [43/300], Step [113/225], Training Accuracy: 74.8894%, Training Loss: 0.5438%\n",
      "Epoch [43/300], Step [114/225], Training Accuracy: 74.8629%, Training Loss: 0.5444%\n",
      "Epoch [43/300], Step [115/225], Training Accuracy: 74.9185%, Training Loss: 0.5435%\n",
      "Epoch [43/300], Step [116/225], Training Accuracy: 74.9327%, Training Loss: 0.5434%\n",
      "Epoch [43/300], Step [117/225], Training Accuracy: 74.8665%, Training Loss: 0.5451%\n",
      "Epoch [43/300], Step [118/225], Training Accuracy: 74.8411%, Training Loss: 0.5453%\n",
      "Epoch [43/300], Step [119/225], Training Accuracy: 74.8424%, Training Loss: 0.5448%\n",
      "Epoch [43/300], Step [120/225], Training Accuracy: 74.8698%, Training Loss: 0.5446%\n",
      "Epoch [43/300], Step [121/225], Training Accuracy: 74.8192%, Training Loss: 0.5450%\n",
      "Epoch [43/300], Step [122/225], Training Accuracy: 74.8079%, Training Loss: 0.5449%\n",
      "Epoch [43/300], Step [123/225], Training Accuracy: 74.7713%, Training Loss: 0.5457%\n",
      "Epoch [43/300], Step [124/225], Training Accuracy: 74.6976%, Training Loss: 0.5467%\n",
      "Epoch [43/300], Step [125/225], Training Accuracy: 74.7000%, Training Loss: 0.5469%\n",
      "Epoch [43/300], Step [126/225], Training Accuracy: 74.6652%, Training Loss: 0.5484%\n",
      "Epoch [43/300], Step [127/225], Training Accuracy: 74.5817%, Training Loss: 0.5495%\n",
      "Epoch [43/300], Step [128/225], Training Accuracy: 74.5605%, Training Loss: 0.5504%\n",
      "Epoch [43/300], Step [129/225], Training Accuracy: 74.5276%, Training Loss: 0.5510%\n",
      "Epoch [43/300], Step [130/225], Training Accuracy: 74.4832%, Training Loss: 0.5511%\n",
      "Epoch [43/300], Step [131/225], Training Accuracy: 74.5348%, Training Loss: 0.5501%\n",
      "Epoch [43/300], Step [132/225], Training Accuracy: 74.5265%, Training Loss: 0.5501%\n",
      "Epoch [43/300], Step [133/225], Training Accuracy: 74.4948%, Training Loss: 0.5496%\n",
      "Epoch [43/300], Step [134/225], Training Accuracy: 74.4753%, Training Loss: 0.5512%\n",
      "Epoch [43/300], Step [135/225], Training Accuracy: 74.4560%, Training Loss: 0.5513%\n",
      "Epoch [43/300], Step [136/225], Training Accuracy: 74.4830%, Training Loss: 0.5506%\n",
      "Epoch [43/300], Step [137/225], Training Accuracy: 74.4297%, Training Loss: 0.5514%\n",
      "Epoch [43/300], Step [138/225], Training Accuracy: 74.5018%, Training Loss: 0.5506%\n",
      "Epoch [43/300], Step [139/225], Training Accuracy: 74.5054%, Training Loss: 0.5502%\n",
      "Epoch [43/300], Step [140/225], Training Accuracy: 74.5871%, Training Loss: 0.5497%\n",
      "Epoch [43/300], Step [141/225], Training Accuracy: 74.5900%, Training Loss: 0.5498%\n",
      "Epoch [43/300], Step [142/225], Training Accuracy: 74.5929%, Training Loss: 0.5495%\n",
      "Epoch [43/300], Step [143/225], Training Accuracy: 74.6285%, Training Loss: 0.5493%\n",
      "Epoch [43/300], Step [144/225], Training Accuracy: 74.6745%, Training Loss: 0.5490%\n",
      "Epoch [43/300], Step [145/225], Training Accuracy: 74.7091%, Training Loss: 0.5482%\n",
      "Epoch [43/300], Step [146/225], Training Accuracy: 74.6682%, Training Loss: 0.5488%\n",
      "Epoch [43/300], Step [147/225], Training Accuracy: 74.6492%, Training Loss: 0.5487%\n",
      "Epoch [43/300], Step [148/225], Training Accuracy: 74.7572%, Training Loss: 0.5473%\n",
      "Epoch [43/300], Step [149/225], Training Accuracy: 74.7588%, Training Loss: 0.5471%\n",
      "Epoch [43/300], Step [150/225], Training Accuracy: 74.7083%, Training Loss: 0.5474%\n",
      "Epoch [43/300], Step [151/225], Training Accuracy: 74.7620%, Training Loss: 0.5468%\n",
      "Epoch [43/300], Step [152/225], Training Accuracy: 74.7636%, Training Loss: 0.5472%\n",
      "Epoch [43/300], Step [153/225], Training Accuracy: 74.7141%, Training Loss: 0.5475%\n",
      "Epoch [43/300], Step [154/225], Training Accuracy: 74.7159%, Training Loss: 0.5473%\n",
      "Epoch [43/300], Step [155/225], Training Accuracy: 74.6774%, Training Loss: 0.5477%\n",
      "Epoch [43/300], Step [156/225], Training Accuracy: 74.6094%, Training Loss: 0.5494%\n",
      "Epoch [43/300], Step [157/225], Training Accuracy: 74.5621%, Training Loss: 0.5505%\n",
      "Epoch [43/300], Step [158/225], Training Accuracy: 74.5352%, Training Loss: 0.5508%\n",
      "Epoch [43/300], Step [159/225], Training Accuracy: 74.4988%, Training Loss: 0.5515%\n",
      "Epoch [43/300], Step [160/225], Training Accuracy: 74.5508%, Training Loss: 0.5509%\n",
      "Epoch [43/300], Step [161/225], Training Accuracy: 74.5633%, Training Loss: 0.5505%\n",
      "Epoch [43/300], Step [162/225], Training Accuracy: 74.6142%, Training Loss: 0.5504%\n",
      "Epoch [43/300], Step [163/225], Training Accuracy: 74.6645%, Training Loss: 0.5502%\n",
      "Epoch [43/300], Step [164/225], Training Accuracy: 74.7142%, Training Loss: 0.5493%\n",
      "Epoch [43/300], Step [165/225], Training Accuracy: 74.7727%, Training Loss: 0.5489%\n",
      "Epoch [43/300], Step [166/225], Training Accuracy: 74.8400%, Training Loss: 0.5487%\n",
      "Epoch [43/300], Step [167/225], Training Accuracy: 74.8129%, Training Loss: 0.5497%\n",
      "Epoch [43/300], Step [168/225], Training Accuracy: 74.8233%, Training Loss: 0.5498%\n",
      "Epoch [43/300], Step [169/225], Training Accuracy: 74.7966%, Training Loss: 0.5504%\n",
      "Epoch [43/300], Step [170/225], Training Accuracy: 74.7794%, Training Loss: 0.5509%\n",
      "Epoch [43/300], Step [171/225], Training Accuracy: 74.7624%, Training Loss: 0.5511%\n",
      "Epoch [43/300], Step [172/225], Training Accuracy: 74.7911%, Training Loss: 0.5504%\n",
      "Epoch [43/300], Step [173/225], Training Accuracy: 74.7832%, Training Loss: 0.5505%\n",
      "Epoch [43/300], Step [174/225], Training Accuracy: 74.8024%, Training Loss: 0.5509%\n",
      "Epoch [43/300], Step [175/225], Training Accuracy: 74.8125%, Training Loss: 0.5506%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Step [176/225], Training Accuracy: 74.8047%, Training Loss: 0.5508%\n",
      "Epoch [43/300], Step [177/225], Training Accuracy: 74.8323%, Training Loss: 0.5504%\n",
      "Epoch [43/300], Step [178/225], Training Accuracy: 74.8683%, Training Loss: 0.5502%\n",
      "Epoch [43/300], Step [179/225], Training Accuracy: 74.9040%, Training Loss: 0.5496%\n",
      "Epoch [43/300], Step [180/225], Training Accuracy: 74.8958%, Training Loss: 0.5494%\n",
      "Epoch [43/300], Step [181/225], Training Accuracy: 74.8878%, Training Loss: 0.5496%\n",
      "Epoch [43/300], Step [182/225], Training Accuracy: 74.8798%, Training Loss: 0.5496%\n",
      "Epoch [43/300], Step [183/225], Training Accuracy: 74.8805%, Training Loss: 0.5497%\n",
      "Epoch [43/300], Step [184/225], Training Accuracy: 74.9490%, Training Loss: 0.5487%\n",
      "Epoch [43/300], Step [185/225], Training Accuracy: 74.9916%, Training Loss: 0.5482%\n",
      "Epoch [43/300], Step [186/225], Training Accuracy: 75.0168%, Training Loss: 0.5475%\n",
      "Epoch [43/300], Step [187/225], Training Accuracy: 75.0251%, Training Loss: 0.5473%\n",
      "Epoch [43/300], Step [188/225], Training Accuracy: 75.0582%, Training Loss: 0.5468%\n",
      "Epoch [43/300], Step [189/225], Training Accuracy: 75.0744%, Training Loss: 0.5463%\n",
      "Epoch [43/300], Step [190/225], Training Accuracy: 75.0905%, Training Loss: 0.5466%\n",
      "Epoch [43/300], Step [191/225], Training Accuracy: 75.0654%, Training Loss: 0.5468%\n",
      "Epoch [43/300], Step [192/225], Training Accuracy: 75.0814%, Training Loss: 0.5462%\n",
      "Epoch [43/300], Step [193/225], Training Accuracy: 75.0648%, Training Loss: 0.5465%\n",
      "Epoch [43/300], Step [194/225], Training Accuracy: 75.0564%, Training Loss: 0.5469%\n",
      "Epoch [43/300], Step [195/225], Training Accuracy: 75.0881%, Training Loss: 0.5465%\n",
      "Epoch [43/300], Step [196/225], Training Accuracy: 75.0638%, Training Loss: 0.5465%\n",
      "Epoch [43/300], Step [197/225], Training Accuracy: 75.0555%, Training Loss: 0.5469%\n",
      "Epoch [43/300], Step [198/225], Training Accuracy: 75.1184%, Training Loss: 0.5459%\n",
      "Epoch [43/300], Step [199/225], Training Accuracy: 75.1178%, Training Loss: 0.5460%\n",
      "Epoch [43/300], Step [200/225], Training Accuracy: 75.1016%, Training Loss: 0.5465%\n",
      "Epoch [43/300], Step [201/225], Training Accuracy: 75.1166%, Training Loss: 0.5464%\n",
      "Epoch [43/300], Step [202/225], Training Accuracy: 75.1779%, Training Loss: 0.5457%\n",
      "Epoch [43/300], Step [203/225], Training Accuracy: 75.2386%, Training Loss: 0.5453%\n",
      "Epoch [43/300], Step [204/225], Training Accuracy: 75.2145%, Training Loss: 0.5455%\n",
      "Epoch [43/300], Step [205/225], Training Accuracy: 75.2591%, Training Loss: 0.5449%\n",
      "Epoch [43/300], Step [206/225], Training Accuracy: 75.2200%, Training Loss: 0.5455%\n",
      "Epoch [43/300], Step [207/225], Training Accuracy: 75.2114%, Training Loss: 0.5456%\n",
      "Epoch [43/300], Step [208/225], Training Accuracy: 75.2329%, Training Loss: 0.5450%\n",
      "Epoch [43/300], Step [209/225], Training Accuracy: 75.2617%, Training Loss: 0.5451%\n",
      "Epoch [43/300], Step [210/225], Training Accuracy: 75.2307%, Training Loss: 0.5455%\n",
      "Epoch [43/300], Step [211/225], Training Accuracy: 75.2370%, Training Loss: 0.5452%\n",
      "Epoch [43/300], Step [212/225], Training Accuracy: 75.2506%, Training Loss: 0.5453%\n",
      "Epoch [43/300], Step [213/225], Training Accuracy: 75.2274%, Training Loss: 0.5459%\n",
      "Epoch [43/300], Step [214/225], Training Accuracy: 75.2409%, Training Loss: 0.5454%\n",
      "Epoch [43/300], Step [215/225], Training Accuracy: 75.2398%, Training Loss: 0.5455%\n",
      "Epoch [43/300], Step [216/225], Training Accuracy: 75.2459%, Training Loss: 0.5454%\n",
      "Epoch [43/300], Step [217/225], Training Accuracy: 75.2304%, Training Loss: 0.5457%\n",
      "Epoch [43/300], Step [218/225], Training Accuracy: 75.2222%, Training Loss: 0.5461%\n",
      "Epoch [43/300], Step [219/225], Training Accuracy: 75.2283%, Training Loss: 0.5462%\n",
      "Epoch [43/300], Step [220/225], Training Accuracy: 75.2273%, Training Loss: 0.5461%\n",
      "Epoch [43/300], Step [221/225], Training Accuracy: 75.2121%, Training Loss: 0.5464%\n",
      "Epoch [43/300], Step [222/225], Training Accuracy: 75.2252%, Training Loss: 0.5462%\n",
      "Epoch [43/300], Step [223/225], Training Accuracy: 75.2172%, Training Loss: 0.5465%\n",
      "Epoch [43/300], Step [224/225], Training Accuracy: 75.2093%, Training Loss: 0.5468%\n",
      "Epoch [43/300], Step [225/225], Training Accuracy: 75.1876%, Training Loss: 0.5468%\n",
      "Epoch [44/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.5191%\n",
      "Epoch [44/300], Step [2/225], Training Accuracy: 79.6875%, Training Loss: 0.4824%\n",
      "Epoch [44/300], Step [3/225], Training Accuracy: 78.6458%, Training Loss: 0.4807%\n",
      "Epoch [44/300], Step [4/225], Training Accuracy: 78.1250%, Training Loss: 0.4988%\n",
      "Epoch [44/300], Step [5/225], Training Accuracy: 78.7500%, Training Loss: 0.5075%\n",
      "Epoch [44/300], Step [6/225], Training Accuracy: 78.3854%, Training Loss: 0.5139%\n",
      "Epoch [44/300], Step [7/225], Training Accuracy: 78.7946%, Training Loss: 0.5116%\n",
      "Epoch [44/300], Step [8/225], Training Accuracy: 78.1250%, Training Loss: 0.5094%\n",
      "Epoch [44/300], Step [9/225], Training Accuracy: 77.2569%, Training Loss: 0.5226%\n",
      "Epoch [44/300], Step [10/225], Training Accuracy: 77.3438%, Training Loss: 0.5304%\n",
      "Epoch [44/300], Step [11/225], Training Accuracy: 77.8409%, Training Loss: 0.5264%\n",
      "Epoch [44/300], Step [12/225], Training Accuracy: 77.0833%, Training Loss: 0.5249%\n",
      "Epoch [44/300], Step [13/225], Training Accuracy: 77.1635%, Training Loss: 0.5195%\n",
      "Epoch [44/300], Step [14/225], Training Accuracy: 77.0089%, Training Loss: 0.5198%\n",
      "Epoch [44/300], Step [15/225], Training Accuracy: 77.1875%, Training Loss: 0.5175%\n",
      "Epoch [44/300], Step [16/225], Training Accuracy: 77.1484%, Training Loss: 0.5171%\n",
      "Epoch [44/300], Step [17/225], Training Accuracy: 76.8382%, Training Loss: 0.5204%\n",
      "Epoch [44/300], Step [18/225], Training Accuracy: 76.2153%, Training Loss: 0.5215%\n",
      "Epoch [44/300], Step [19/225], Training Accuracy: 75.9868%, Training Loss: 0.5232%\n",
      "Epoch [44/300], Step [20/225], Training Accuracy: 76.1719%, Training Loss: 0.5225%\n",
      "Epoch [44/300], Step [21/225], Training Accuracy: 76.5625%, Training Loss: 0.5184%\n",
      "Epoch [44/300], Step [22/225], Training Accuracy: 76.4205%, Training Loss: 0.5206%\n",
      "Epoch [44/300], Step [23/225], Training Accuracy: 76.4266%, Training Loss: 0.5201%\n",
      "Epoch [44/300], Step [24/225], Training Accuracy: 76.3672%, Training Loss: 0.5242%\n",
      "Epoch [44/300], Step [25/225], Training Accuracy: 76.6875%, Training Loss: 0.5218%\n",
      "Epoch [44/300], Step [26/225], Training Accuracy: 76.6226%, Training Loss: 0.5213%\n",
      "Epoch [44/300], Step [27/225], Training Accuracy: 76.5046%, Training Loss: 0.5217%\n",
      "Epoch [44/300], Step [28/225], Training Accuracy: 76.8973%, Training Loss: 0.5160%\n",
      "Epoch [44/300], Step [29/225], Training Accuracy: 76.8858%, Training Loss: 0.5159%\n",
      "Epoch [44/300], Step [30/225], Training Accuracy: 76.8750%, Training Loss: 0.5144%\n",
      "Epoch [44/300], Step [31/225], Training Accuracy: 76.6129%, Training Loss: 0.5217%\n",
      "Epoch [44/300], Step [32/225], Training Accuracy: 76.7090%, Training Loss: 0.5226%\n",
      "Epoch [44/300], Step [33/225], Training Accuracy: 76.6098%, Training Loss: 0.5205%\n",
      "Epoch [44/300], Step [34/225], Training Accuracy: 76.2868%, Training Loss: 0.5234%\n",
      "Epoch [44/300], Step [35/225], Training Accuracy: 76.2500%, Training Loss: 0.5224%\n",
      "Epoch [44/300], Step [36/225], Training Accuracy: 76.1719%, Training Loss: 0.5230%\n",
      "Epoch [44/300], Step [37/225], Training Accuracy: 76.0980%, Training Loss: 0.5223%\n",
      "Epoch [44/300], Step [38/225], Training Accuracy: 76.1513%, Training Loss: 0.5233%\n",
      "Epoch [44/300], Step [39/225], Training Accuracy: 76.2420%, Training Loss: 0.5228%\n",
      "Epoch [44/300], Step [40/225], Training Accuracy: 76.3672%, Training Loss: 0.5218%\n",
      "Epoch [44/300], Step [41/225], Training Accuracy: 76.2195%, Training Loss: 0.5239%\n",
      "Epoch [44/300], Step [42/225], Training Accuracy: 76.2649%, Training Loss: 0.5241%\n",
      "Epoch [44/300], Step [43/225], Training Accuracy: 76.3808%, Training Loss: 0.5247%\n",
      "Epoch [44/300], Step [44/225], Training Accuracy: 76.5980%, Training Loss: 0.5221%\n",
      "Epoch [44/300], Step [45/225], Training Accuracy: 76.7361%, Training Loss: 0.5202%\n",
      "Epoch [44/300], Step [46/225], Training Accuracy: 76.6984%, Training Loss: 0.5199%\n",
      "Epoch [44/300], Step [47/225], Training Accuracy: 76.4960%, Training Loss: 0.5230%\n",
      "Epoch [44/300], Step [48/225], Training Accuracy: 76.4323%, Training Loss: 0.5234%\n",
      "Epoch [44/300], Step [49/225], Training Accuracy: 76.5306%, Training Loss: 0.5214%\n",
      "Epoch [44/300], Step [50/225], Training Accuracy: 76.5000%, Training Loss: 0.5211%\n",
      "Epoch [44/300], Step [51/225], Training Accuracy: 76.5319%, Training Loss: 0.5212%\n",
      "Epoch [44/300], Step [52/225], Training Accuracy: 76.5325%, Training Loss: 0.5212%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Step [53/225], Training Accuracy: 76.6215%, Training Loss: 0.5211%\n",
      "Epoch [44/300], Step [54/225], Training Accuracy: 76.5046%, Training Loss: 0.5227%\n",
      "Epoch [44/300], Step [55/225], Training Accuracy: 76.3636%, Training Loss: 0.5242%\n",
      "Epoch [44/300], Step [56/225], Training Accuracy: 76.3393%, Training Loss: 0.5249%\n",
      "Epoch [44/300], Step [57/225], Training Accuracy: 76.2884%, Training Loss: 0.5249%\n",
      "Epoch [44/300], Step [58/225], Training Accuracy: 76.2662%, Training Loss: 0.5256%\n",
      "Epoch [44/300], Step [59/225], Training Accuracy: 76.3771%, Training Loss: 0.5241%\n",
      "Epoch [44/300], Step [60/225], Training Accuracy: 76.4844%, Training Loss: 0.5222%\n",
      "Epoch [44/300], Step [61/225], Training Accuracy: 76.4857%, Training Loss: 0.5231%\n",
      "Epoch [44/300], Step [62/225], Training Accuracy: 76.4617%, Training Loss: 0.5249%\n",
      "Epoch [44/300], Step [63/225], Training Accuracy: 76.4633%, Training Loss: 0.5270%\n",
      "Epoch [44/300], Step [64/225], Training Accuracy: 76.5137%, Training Loss: 0.5258%\n",
      "Epoch [44/300], Step [65/225], Training Accuracy: 76.5144%, Training Loss: 0.5255%\n",
      "Epoch [44/300], Step [66/225], Training Accuracy: 76.5152%, Training Loss: 0.5247%\n",
      "Epoch [44/300], Step [67/225], Training Accuracy: 76.5858%, Training Loss: 0.5242%\n",
      "Epoch [44/300], Step [68/225], Training Accuracy: 76.4936%, Training Loss: 0.5251%\n",
      "Epoch [44/300], Step [69/225], Training Accuracy: 76.4040%, Training Loss: 0.5255%\n",
      "Epoch [44/300], Step [70/225], Training Accuracy: 76.3616%, Training Loss: 0.5265%\n",
      "Epoch [44/300], Step [71/225], Training Accuracy: 76.2544%, Training Loss: 0.5276%\n",
      "Epoch [44/300], Step [72/225], Training Accuracy: 76.2153%, Training Loss: 0.5285%\n",
      "Epoch [44/300], Step [73/225], Training Accuracy: 76.3485%, Training Loss: 0.5295%\n",
      "Epoch [44/300], Step [74/225], Training Accuracy: 76.2880%, Training Loss: 0.5300%\n",
      "Epoch [44/300], Step [75/225], Training Accuracy: 76.2292%, Training Loss: 0.5292%\n",
      "Epoch [44/300], Step [76/225], Training Accuracy: 76.1513%, Training Loss: 0.5301%\n",
      "Epoch [44/300], Step [77/225], Training Accuracy: 76.2378%, Training Loss: 0.5313%\n",
      "Epoch [44/300], Step [78/225], Training Accuracy: 76.1819%, Training Loss: 0.5325%\n",
      "Epoch [44/300], Step [79/225], Training Accuracy: 76.2263%, Training Loss: 0.5321%\n",
      "Epoch [44/300], Step [80/225], Training Accuracy: 76.2695%, Training Loss: 0.5319%\n",
      "Epoch [44/300], Step [81/225], Training Accuracy: 76.2731%, Training Loss: 0.5311%\n",
      "Epoch [44/300], Step [82/225], Training Accuracy: 76.2957%, Training Loss: 0.5303%\n",
      "Epoch [44/300], Step [83/225], Training Accuracy: 76.2989%, Training Loss: 0.5301%\n",
      "Epoch [44/300], Step [84/225], Training Accuracy: 76.3207%, Training Loss: 0.5297%\n",
      "Epoch [44/300], Step [85/225], Training Accuracy: 76.3603%, Training Loss: 0.5284%\n",
      "Epoch [44/300], Step [86/225], Training Accuracy: 76.3626%, Training Loss: 0.5281%\n",
      "Epoch [44/300], Step [87/225], Training Accuracy: 76.3470%, Training Loss: 0.5302%\n",
      "Epoch [44/300], Step [88/225], Training Accuracy: 76.2784%, Training Loss: 0.5316%\n",
      "Epoch [44/300], Step [89/225], Training Accuracy: 76.1763%, Training Loss: 0.5338%\n",
      "Epoch [44/300], Step [90/225], Training Accuracy: 76.0590%, Training Loss: 0.5348%\n",
      "Epoch [44/300], Step [91/225], Training Accuracy: 76.0474%, Training Loss: 0.5346%\n",
      "Epoch [44/300], Step [92/225], Training Accuracy: 76.0360%, Training Loss: 0.5340%\n",
      "Epoch [44/300], Step [93/225], Training Accuracy: 76.0081%, Training Loss: 0.5337%\n",
      "Epoch [44/300], Step [94/225], Training Accuracy: 76.0306%, Training Loss: 0.5329%\n",
      "Epoch [44/300], Step [95/225], Training Accuracy: 76.0526%, Training Loss: 0.5328%\n",
      "Epoch [44/300], Step [96/225], Training Accuracy: 76.1230%, Training Loss: 0.5321%\n",
      "Epoch [44/300], Step [97/225], Training Accuracy: 76.1437%, Training Loss: 0.5320%\n",
      "Epoch [44/300], Step [98/225], Training Accuracy: 76.1001%, Training Loss: 0.5342%\n",
      "Epoch [44/300], Step [99/225], Training Accuracy: 76.0890%, Training Loss: 0.5355%\n",
      "Epoch [44/300], Step [100/225], Training Accuracy: 75.9375%, Training Loss: 0.5376%\n",
      "Epoch [44/300], Step [101/225], Training Accuracy: 75.9282%, Training Loss: 0.5376%\n",
      "Epoch [44/300], Step [102/225], Training Accuracy: 75.9191%, Training Loss: 0.5379%\n",
      "Epoch [44/300], Step [103/225], Training Accuracy: 75.9102%, Training Loss: 0.5375%\n",
      "Epoch [44/300], Step [104/225], Training Accuracy: 75.8864%, Training Loss: 0.5389%\n",
      "Epoch [44/300], Step [105/225], Training Accuracy: 75.8780%, Training Loss: 0.5380%\n",
      "Epoch [44/300], Step [106/225], Training Accuracy: 75.9434%, Training Loss: 0.5372%\n",
      "Epoch [44/300], Step [107/225], Training Accuracy: 75.9492%, Training Loss: 0.5380%\n",
      "Epoch [44/300], Step [108/225], Training Accuracy: 75.9404%, Training Loss: 0.5381%\n",
      "Epoch [44/300], Step [109/225], Training Accuracy: 75.9604%, Training Loss: 0.5373%\n",
      "Epoch [44/300], Step [110/225], Training Accuracy: 75.9659%, Training Loss: 0.5363%\n",
      "Epoch [44/300], Step [111/225], Training Accuracy: 75.9431%, Training Loss: 0.5364%\n",
      "Epoch [44/300], Step [112/225], Training Accuracy: 75.8929%, Training Loss: 0.5373%\n",
      "Epoch [44/300], Step [113/225], Training Accuracy: 75.9541%, Training Loss: 0.5372%\n",
      "Epoch [44/300], Step [114/225], Training Accuracy: 76.0280%, Training Loss: 0.5365%\n",
      "Epoch [44/300], Step [115/225], Training Accuracy: 76.0598%, Training Loss: 0.5361%\n",
      "Epoch [44/300], Step [116/225], Training Accuracy: 76.0641%, Training Loss: 0.5353%\n",
      "Epoch [44/300], Step [117/225], Training Accuracy: 75.9749%, Training Loss: 0.5361%\n",
      "Epoch [44/300], Step [118/225], Training Accuracy: 75.9534%, Training Loss: 0.5361%\n",
      "Epoch [44/300], Step [119/225], Training Accuracy: 75.9979%, Training Loss: 0.5360%\n",
      "Epoch [44/300], Step [120/225], Training Accuracy: 76.0547%, Training Loss: 0.5355%\n",
      "Epoch [44/300], Step [121/225], Training Accuracy: 75.9685%, Training Loss: 0.5369%\n",
      "Epoch [44/300], Step [122/225], Training Accuracy: 76.0118%, Training Loss: 0.5369%\n",
      "Epoch [44/300], Step [123/225], Training Accuracy: 76.0163%, Training Loss: 0.5376%\n",
      "Epoch [44/300], Step [124/225], Training Accuracy: 76.0333%, Training Loss: 0.5373%\n",
      "Epoch [44/300], Step [125/225], Training Accuracy: 76.0125%, Training Loss: 0.5372%\n",
      "Epoch [44/300], Step [126/225], Training Accuracy: 76.0417%, Training Loss: 0.5372%\n",
      "Epoch [44/300], Step [127/225], Training Accuracy: 76.0335%, Training Loss: 0.5380%\n",
      "Epoch [44/300], Step [128/225], Training Accuracy: 75.9399%, Training Loss: 0.5393%\n",
      "Epoch [44/300], Step [129/225], Training Accuracy: 75.9084%, Training Loss: 0.5394%\n",
      "Epoch [44/300], Step [130/225], Training Accuracy: 75.9375%, Training Loss: 0.5392%\n",
      "Epoch [44/300], Step [131/225], Training Accuracy: 75.9900%, Training Loss: 0.5386%\n",
      "Epoch [44/300], Step [132/225], Training Accuracy: 76.0535%, Training Loss: 0.5382%\n",
      "Epoch [44/300], Step [133/225], Training Accuracy: 75.9868%, Training Loss: 0.5385%\n",
      "Epoch [44/300], Step [134/225], Training Accuracy: 75.9445%, Training Loss: 0.5396%\n",
      "Epoch [44/300], Step [135/225], Training Accuracy: 75.9838%, Training Loss: 0.5396%\n",
      "Epoch [44/300], Step [136/225], Training Accuracy: 76.0225%, Training Loss: 0.5387%\n",
      "Epoch [44/300], Step [137/225], Training Accuracy: 76.0036%, Training Loss: 0.5388%\n",
      "Epoch [44/300], Step [138/225], Training Accuracy: 75.9964%, Training Loss: 0.5384%\n",
      "Epoch [44/300], Step [139/225], Training Accuracy: 75.9892%, Training Loss: 0.5387%\n",
      "Epoch [44/300], Step [140/225], Training Accuracy: 76.0938%, Training Loss: 0.5379%\n",
      "Epoch [44/300], Step [141/225], Training Accuracy: 76.0860%, Training Loss: 0.5378%\n",
      "Epoch [44/300], Step [142/225], Training Accuracy: 76.0233%, Training Loss: 0.5383%\n",
      "Epoch [44/300], Step [143/225], Training Accuracy: 76.0599%, Training Loss: 0.5381%\n",
      "Epoch [44/300], Step [144/225], Training Accuracy: 76.0525%, Training Loss: 0.5379%\n",
      "Epoch [44/300], Step [145/225], Training Accuracy: 76.0345%, Training Loss: 0.5375%\n",
      "Epoch [44/300], Step [146/225], Training Accuracy: 75.9953%, Training Loss: 0.5380%\n",
      "Epoch [44/300], Step [147/225], Training Accuracy: 75.9885%, Training Loss: 0.5384%\n",
      "Epoch [44/300], Step [148/225], Training Accuracy: 76.0452%, Training Loss: 0.5378%\n",
      "Epoch [44/300], Step [149/225], Training Accuracy: 76.0801%, Training Loss: 0.5376%\n",
      "Epoch [44/300], Step [150/225], Training Accuracy: 76.0938%, Training Loss: 0.5374%\n",
      "Epoch [44/300], Step [151/225], Training Accuracy: 76.0969%, Training Loss: 0.5372%\n",
      "Epoch [44/300], Step [152/225], Training Accuracy: 76.0794%, Training Loss: 0.5381%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Step [153/225], Training Accuracy: 76.0417%, Training Loss: 0.5382%\n",
      "Epoch [44/300], Step [154/225], Training Accuracy: 76.0653%, Training Loss: 0.5381%\n",
      "Epoch [44/300], Step [155/225], Training Accuracy: 76.0383%, Training Loss: 0.5384%\n",
      "Epoch [44/300], Step [156/225], Training Accuracy: 75.9816%, Training Loss: 0.5391%\n",
      "Epoch [44/300], Step [157/225], Training Accuracy: 75.9952%, Training Loss: 0.5393%\n",
      "Epoch [44/300], Step [158/225], Training Accuracy: 75.9494%, Training Loss: 0.5400%\n",
      "Epoch [44/300], Step [159/225], Training Accuracy: 75.9041%, Training Loss: 0.5411%\n",
      "Epoch [44/300], Step [160/225], Training Accuracy: 75.8789%, Training Loss: 0.5406%\n",
      "Epoch [44/300], Step [161/225], Training Accuracy: 75.8929%, Training Loss: 0.5404%\n",
      "Epoch [44/300], Step [162/225], Training Accuracy: 75.9452%, Training Loss: 0.5397%\n",
      "Epoch [44/300], Step [163/225], Training Accuracy: 75.9778%, Training Loss: 0.5396%\n",
      "Epoch [44/300], Step [164/225], Training Accuracy: 76.0480%, Training Loss: 0.5383%\n",
      "Epoch [44/300], Step [165/225], Training Accuracy: 76.0985%, Training Loss: 0.5382%\n",
      "Epoch [44/300], Step [166/225], Training Accuracy: 76.0730%, Training Loss: 0.5383%\n",
      "Epoch [44/300], Step [167/225], Training Accuracy: 76.0011%, Training Loss: 0.5388%\n",
      "Epoch [44/300], Step [168/225], Training Accuracy: 75.9859%, Training Loss: 0.5390%\n",
      "Epoch [44/300], Step [169/225], Training Accuracy: 75.9061%, Training Loss: 0.5393%\n",
      "Epoch [44/300], Step [170/225], Training Accuracy: 75.9007%, Training Loss: 0.5391%\n",
      "Epoch [44/300], Step [171/225], Training Accuracy: 75.8681%, Training Loss: 0.5391%\n",
      "Epoch [44/300], Step [172/225], Training Accuracy: 75.8812%, Training Loss: 0.5388%\n",
      "Epoch [44/300], Step [173/225], Training Accuracy: 75.8761%, Training Loss: 0.5386%\n",
      "Epoch [44/300], Step [174/225], Training Accuracy: 75.8710%, Training Loss: 0.5389%\n",
      "Epoch [44/300], Step [175/225], Training Accuracy: 75.8929%, Training Loss: 0.5389%\n",
      "Epoch [44/300], Step [176/225], Training Accuracy: 75.8967%, Training Loss: 0.5387%\n",
      "Epoch [44/300], Step [177/225], Training Accuracy: 75.8739%, Training Loss: 0.5389%\n",
      "Epoch [44/300], Step [178/225], Training Accuracy: 75.8778%, Training Loss: 0.5388%\n",
      "Epoch [44/300], Step [179/225], Training Accuracy: 75.9515%, Training Loss: 0.5381%\n",
      "Epoch [44/300], Step [180/225], Training Accuracy: 75.9635%, Training Loss: 0.5380%\n",
      "Epoch [44/300], Step [181/225], Training Accuracy: 75.9927%, Training Loss: 0.5377%\n",
      "Epoch [44/300], Step [182/225], Training Accuracy: 75.9615%, Training Loss: 0.5382%\n",
      "Epoch [44/300], Step [183/225], Training Accuracy: 75.9392%, Training Loss: 0.5385%\n",
      "Epoch [44/300], Step [184/225], Training Accuracy: 75.9426%, Training Loss: 0.5379%\n",
      "Epoch [44/300], Step [185/225], Training Accuracy: 75.9459%, Training Loss: 0.5381%\n",
      "Epoch [44/300], Step [186/225], Training Accuracy: 76.0165%, Training Loss: 0.5370%\n",
      "Epoch [44/300], Step [187/225], Training Accuracy: 76.0277%, Training Loss: 0.5369%\n",
      "Epoch [44/300], Step [188/225], Training Accuracy: 76.0638%, Training Loss: 0.5362%\n",
      "Epoch [44/300], Step [189/225], Training Accuracy: 76.0747%, Training Loss: 0.5363%\n",
      "Epoch [44/300], Step [190/225], Training Accuracy: 76.0938%, Training Loss: 0.5365%\n",
      "Epoch [44/300], Step [191/225], Training Accuracy: 76.1044%, Training Loss: 0.5365%\n",
      "Epoch [44/300], Step [192/225], Training Accuracy: 76.1312%, Training Loss: 0.5358%\n",
      "Epoch [44/300], Step [193/225], Training Accuracy: 76.1253%, Training Loss: 0.5359%\n",
      "Epoch [44/300], Step [194/225], Training Accuracy: 76.1115%, Training Loss: 0.5360%\n",
      "Epoch [44/300], Step [195/225], Training Accuracy: 76.1298%, Training Loss: 0.5354%\n",
      "Epoch [44/300], Step [196/225], Training Accuracy: 76.0603%, Training Loss: 0.5360%\n",
      "Epoch [44/300], Step [197/225], Training Accuracy: 76.0470%, Training Loss: 0.5364%\n",
      "Epoch [44/300], Step [198/225], Training Accuracy: 76.1127%, Training Loss: 0.5355%\n",
      "Epoch [44/300], Step [199/225], Training Accuracy: 76.0992%, Training Loss: 0.5355%\n",
      "Epoch [44/300], Step [200/225], Training Accuracy: 76.1016%, Training Loss: 0.5354%\n",
      "Epoch [44/300], Step [201/225], Training Accuracy: 76.1116%, Training Loss: 0.5354%\n",
      "Epoch [44/300], Step [202/225], Training Accuracy: 76.1525%, Training Loss: 0.5350%\n",
      "Epoch [44/300], Step [203/225], Training Accuracy: 76.1776%, Training Loss: 0.5348%\n",
      "Epoch [44/300], Step [204/225], Training Accuracy: 76.1489%, Training Loss: 0.5349%\n",
      "Epoch [44/300], Step [205/225], Training Accuracy: 76.1890%, Training Loss: 0.5340%\n",
      "Epoch [44/300], Step [206/225], Training Accuracy: 76.1833%, Training Loss: 0.5346%\n",
      "Epoch [44/300], Step [207/225], Training Accuracy: 76.2077%, Training Loss: 0.5348%\n",
      "Epoch [44/300], Step [208/225], Training Accuracy: 76.2395%, Training Loss: 0.5343%\n",
      "Epoch [44/300], Step [209/225], Training Accuracy: 76.2111%, Training Loss: 0.5345%\n",
      "Epoch [44/300], Step [210/225], Training Accuracy: 76.1756%, Training Loss: 0.5351%\n",
      "Epoch [44/300], Step [211/225], Training Accuracy: 76.2145%, Training Loss: 0.5344%\n",
      "Epoch [44/300], Step [212/225], Training Accuracy: 76.2014%, Training Loss: 0.5350%\n",
      "Epoch [44/300], Step [213/225], Training Accuracy: 76.1884%, Training Loss: 0.5349%\n",
      "Epoch [44/300], Step [214/225], Training Accuracy: 76.2047%, Training Loss: 0.5347%\n",
      "Epoch [44/300], Step [215/225], Training Accuracy: 76.1846%, Training Loss: 0.5344%\n",
      "Epoch [44/300], Step [216/225], Training Accuracy: 76.1791%, Training Loss: 0.5348%\n",
      "Epoch [44/300], Step [217/225], Training Accuracy: 76.2241%, Training Loss: 0.5346%\n",
      "Epoch [44/300], Step [218/225], Training Accuracy: 76.1970%, Training Loss: 0.5348%\n",
      "Epoch [44/300], Step [219/225], Training Accuracy: 76.1772%, Training Loss: 0.5348%\n",
      "Epoch [44/300], Step [220/225], Training Accuracy: 76.1648%, Training Loss: 0.5349%\n",
      "Epoch [44/300], Step [221/225], Training Accuracy: 76.1242%, Training Loss: 0.5356%\n",
      "Epoch [44/300], Step [222/225], Training Accuracy: 76.1120%, Training Loss: 0.5353%\n",
      "Epoch [44/300], Step [223/225], Training Accuracy: 76.0580%, Training Loss: 0.5362%\n",
      "Epoch [44/300], Step [224/225], Training Accuracy: 76.0812%, Training Loss: 0.5359%\n",
      "Epoch [44/300], Step [225/225], Training Accuracy: 76.0839%, Training Loss: 0.5358%\n",
      "Epoch [45/300], Step [1/225], Training Accuracy: 68.7500%, Training Loss: 0.6313%\n",
      "Epoch [45/300], Step [2/225], Training Accuracy: 71.8750%, Training Loss: 0.6116%\n",
      "Epoch [45/300], Step [3/225], Training Accuracy: 75.5208%, Training Loss: 0.5800%\n",
      "Epoch [45/300], Step [4/225], Training Accuracy: 75.7812%, Training Loss: 0.5785%\n",
      "Epoch [45/300], Step [5/225], Training Accuracy: 75.3125%, Training Loss: 0.5602%\n",
      "Epoch [45/300], Step [6/225], Training Accuracy: 75.7812%, Training Loss: 0.5555%\n",
      "Epoch [45/300], Step [7/225], Training Accuracy: 77.0089%, Training Loss: 0.5364%\n",
      "Epoch [45/300], Step [8/225], Training Accuracy: 76.3672%, Training Loss: 0.5422%\n",
      "Epoch [45/300], Step [9/225], Training Accuracy: 76.3889%, Training Loss: 0.5443%\n",
      "Epoch [45/300], Step [10/225], Training Accuracy: 76.7188%, Training Loss: 0.5471%\n",
      "Epoch [45/300], Step [11/225], Training Accuracy: 76.5625%, Training Loss: 0.5455%\n",
      "Epoch [45/300], Step [12/225], Training Accuracy: 76.3021%, Training Loss: 0.5440%\n",
      "Epoch [45/300], Step [13/225], Training Accuracy: 76.3221%, Training Loss: 0.5385%\n",
      "Epoch [45/300], Step [14/225], Training Accuracy: 76.5625%, Training Loss: 0.5290%\n",
      "Epoch [45/300], Step [15/225], Training Accuracy: 76.7708%, Training Loss: 0.5263%\n",
      "Epoch [45/300], Step [16/225], Training Accuracy: 76.8555%, Training Loss: 0.5216%\n",
      "Epoch [45/300], Step [17/225], Training Accuracy: 77.2059%, Training Loss: 0.5122%\n",
      "Epoch [45/300], Step [18/225], Training Accuracy: 76.9097%, Training Loss: 0.5172%\n",
      "Epoch [45/300], Step [19/225], Training Accuracy: 76.9737%, Training Loss: 0.5153%\n",
      "Epoch [45/300], Step [20/225], Training Accuracy: 77.1094%, Training Loss: 0.5146%\n",
      "Epoch [45/300], Step [21/225], Training Accuracy: 77.0833%, Training Loss: 0.5124%\n",
      "Epoch [45/300], Step [22/225], Training Accuracy: 76.9176%, Training Loss: 0.5164%\n",
      "Epoch [45/300], Step [23/225], Training Accuracy: 77.1060%, Training Loss: 0.5144%\n",
      "Epoch [45/300], Step [24/225], Training Accuracy: 76.8880%, Training Loss: 0.5167%\n",
      "Epoch [45/300], Step [25/225], Training Accuracy: 76.8125%, Training Loss: 0.5153%\n",
      "Epoch [45/300], Step [26/225], Training Accuracy: 76.9231%, Training Loss: 0.5129%\n",
      "Epoch [45/300], Step [27/225], Training Accuracy: 76.8519%, Training Loss: 0.5103%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Step [28/225], Training Accuracy: 77.1205%, Training Loss: 0.5049%\n",
      "Epoch [45/300], Step [29/225], Training Accuracy: 76.9397%, Training Loss: 0.5071%\n",
      "Epoch [45/300], Step [30/225], Training Accuracy: 76.9792%, Training Loss: 0.5064%\n",
      "Epoch [45/300], Step [31/225], Training Accuracy: 76.7137%, Training Loss: 0.5162%\n",
      "Epoch [45/300], Step [32/225], Training Accuracy: 76.5137%, Training Loss: 0.5196%\n",
      "Epoch [45/300], Step [33/225], Training Accuracy: 76.5152%, Training Loss: 0.5182%\n",
      "Epoch [45/300], Step [34/225], Training Accuracy: 76.3787%, Training Loss: 0.5205%\n",
      "Epoch [45/300], Step [35/225], Training Accuracy: 76.3393%, Training Loss: 0.5195%\n",
      "Epoch [45/300], Step [36/225], Training Accuracy: 76.3021%, Training Loss: 0.5186%\n",
      "Epoch [45/300], Step [37/225], Training Accuracy: 76.1824%, Training Loss: 0.5202%\n",
      "Epoch [45/300], Step [38/225], Training Accuracy: 76.2747%, Training Loss: 0.5201%\n",
      "Epoch [45/300], Step [39/225], Training Accuracy: 76.2420%, Training Loss: 0.5209%\n",
      "Epoch [45/300], Step [40/225], Training Accuracy: 76.2500%, Training Loss: 0.5216%\n",
      "Epoch [45/300], Step [41/225], Training Accuracy: 76.2195%, Training Loss: 0.5224%\n",
      "Epoch [45/300], Step [42/225], Training Accuracy: 76.2277%, Training Loss: 0.5216%\n",
      "Epoch [45/300], Step [43/225], Training Accuracy: 76.2718%, Training Loss: 0.5227%\n",
      "Epoch [45/300], Step [44/225], Training Accuracy: 76.4560%, Training Loss: 0.5205%\n",
      "Epoch [45/300], Step [45/225], Training Accuracy: 76.4931%, Training Loss: 0.5207%\n",
      "Epoch [45/300], Step [46/225], Training Accuracy: 76.4946%, Training Loss: 0.5197%\n",
      "Epoch [45/300], Step [47/225], Training Accuracy: 76.2633%, Training Loss: 0.5199%\n",
      "Epoch [45/300], Step [48/225], Training Accuracy: 76.1719%, Training Loss: 0.5213%\n",
      "Epoch [45/300], Step [49/225], Training Accuracy: 76.2755%, Training Loss: 0.5195%\n",
      "Epoch [45/300], Step [50/225], Training Accuracy: 76.1875%, Training Loss: 0.5204%\n",
      "Epoch [45/300], Step [51/225], Training Accuracy: 76.0417%, Training Loss: 0.5214%\n",
      "Epoch [45/300], Step [52/225], Training Accuracy: 75.9315%, Training Loss: 0.5215%\n",
      "Epoch [45/300], Step [53/225], Training Accuracy: 76.0024%, Training Loss: 0.5218%\n",
      "Epoch [45/300], Step [54/225], Training Accuracy: 75.8391%, Training Loss: 0.5251%\n",
      "Epoch [45/300], Step [55/225], Training Accuracy: 75.7670%, Training Loss: 0.5256%\n",
      "Epoch [45/300], Step [56/225], Training Accuracy: 75.6138%, Training Loss: 0.5275%\n",
      "Epoch [45/300], Step [57/225], Training Accuracy: 75.5208%, Training Loss: 0.5276%\n",
      "Epoch [45/300], Step [58/225], Training Accuracy: 75.4580%, Training Loss: 0.5286%\n",
      "Epoch [45/300], Step [59/225], Training Accuracy: 75.4767%, Training Loss: 0.5282%\n",
      "Epoch [45/300], Step [60/225], Training Accuracy: 75.5990%, Training Loss: 0.5272%\n",
      "Epoch [45/300], Step [61/225], Training Accuracy: 75.3842%, Training Loss: 0.5298%\n",
      "Epoch [45/300], Step [62/225], Training Accuracy: 75.4032%, Training Loss: 0.5300%\n",
      "Epoch [45/300], Step [63/225], Training Accuracy: 75.2728%, Training Loss: 0.5315%\n",
      "Epoch [45/300], Step [64/225], Training Accuracy: 75.3662%, Training Loss: 0.5303%\n",
      "Epoch [45/300], Step [65/225], Training Accuracy: 75.4327%, Training Loss: 0.5306%\n",
      "Epoch [45/300], Step [66/225], Training Accuracy: 75.5445%, Training Loss: 0.5293%\n",
      "Epoch [45/300], Step [67/225], Training Accuracy: 75.5597%, Training Loss: 0.5285%\n",
      "Epoch [45/300], Step [68/225], Training Accuracy: 75.5055%, Training Loss: 0.5295%\n",
      "Epoch [45/300], Step [69/225], Training Accuracy: 75.4982%, Training Loss: 0.5293%\n",
      "Epoch [45/300], Step [70/225], Training Accuracy: 75.4464%, Training Loss: 0.5306%\n",
      "Epoch [45/300], Step [71/225], Training Accuracy: 75.4621%, Training Loss: 0.5302%\n",
      "Epoch [45/300], Step [72/225], Training Accuracy: 75.4774%, Training Loss: 0.5299%\n",
      "Epoch [45/300], Step [73/225], Training Accuracy: 75.5565%, Training Loss: 0.5288%\n",
      "Epoch [45/300], Step [74/225], Training Accuracy: 75.4856%, Training Loss: 0.5282%\n",
      "Epoch [45/300], Step [75/225], Training Accuracy: 75.5000%, Training Loss: 0.5281%\n",
      "Epoch [45/300], Step [76/225], Training Accuracy: 75.3289%, Training Loss: 0.5305%\n",
      "Epoch [45/300], Step [77/225], Training Accuracy: 75.3450%, Training Loss: 0.5300%\n",
      "Epoch [45/300], Step [78/225], Training Accuracy: 75.3005%, Training Loss: 0.5297%\n",
      "Epoch [45/300], Step [79/225], Training Accuracy: 75.2769%, Training Loss: 0.5303%\n",
      "Epoch [45/300], Step [80/225], Training Accuracy: 75.3516%, Training Loss: 0.5297%\n",
      "Epoch [45/300], Step [81/225], Training Accuracy: 75.3472%, Training Loss: 0.5299%\n",
      "Epoch [45/300], Step [82/225], Training Accuracy: 75.4383%, Training Loss: 0.5285%\n",
      "Epoch [45/300], Step [83/225], Training Accuracy: 75.4142%, Training Loss: 0.5285%\n",
      "Epoch [45/300], Step [84/225], Training Accuracy: 75.4836%, Training Loss: 0.5283%\n",
      "Epoch [45/300], Step [85/225], Training Accuracy: 75.4596%, Training Loss: 0.5280%\n",
      "Epoch [45/300], Step [86/225], Training Accuracy: 75.5632%, Training Loss: 0.5267%\n",
      "Epoch [45/300], Step [87/225], Training Accuracy: 75.6106%, Training Loss: 0.5267%\n",
      "Epoch [45/300], Step [88/225], Training Accuracy: 75.5859%, Training Loss: 0.5277%\n",
      "Epoch [45/300], Step [89/225], Training Accuracy: 75.5794%, Training Loss: 0.5280%\n",
      "Epoch [45/300], Step [90/225], Training Accuracy: 75.5382%, Training Loss: 0.5293%\n",
      "Epoch [45/300], Step [91/225], Training Accuracy: 75.5666%, Training Loss: 0.5294%\n",
      "Epoch [45/300], Step [92/225], Training Accuracy: 75.4925%, Training Loss: 0.5304%\n",
      "Epoch [45/300], Step [93/225], Training Accuracy: 75.5544%, Training Loss: 0.5296%\n",
      "Epoch [45/300], Step [94/225], Training Accuracy: 75.5319%, Training Loss: 0.5299%\n",
      "Epoch [45/300], Step [95/225], Training Accuracy: 75.5263%, Training Loss: 0.5313%\n",
      "Epoch [45/300], Step [96/225], Training Accuracy: 75.6022%, Training Loss: 0.5304%\n",
      "Epoch [45/300], Step [97/225], Training Accuracy: 75.5799%, Training Loss: 0.5298%\n",
      "Epoch [45/300], Step [98/225], Training Accuracy: 75.5261%, Training Loss: 0.5310%\n",
      "Epoch [45/300], Step [99/225], Training Accuracy: 75.4261%, Training Loss: 0.5325%\n",
      "Epoch [45/300], Step [100/225], Training Accuracy: 75.3594%, Training Loss: 0.5341%\n",
      "Epoch [45/300], Step [101/225], Training Accuracy: 75.4177%, Training Loss: 0.5334%\n",
      "Epoch [45/300], Step [102/225], Training Accuracy: 75.3676%, Training Loss: 0.5341%\n",
      "Epoch [45/300], Step [103/225], Training Accuracy: 75.3792%, Training Loss: 0.5339%\n",
      "Epoch [45/300], Step [104/225], Training Accuracy: 75.2404%, Training Loss: 0.5360%\n",
      "Epoch [45/300], Step [105/225], Training Accuracy: 75.2381%, Training Loss: 0.5356%\n",
      "Epoch [45/300], Step [106/225], Training Accuracy: 75.2653%, Training Loss: 0.5354%\n",
      "Epoch [45/300], Step [107/225], Training Accuracy: 75.2629%, Training Loss: 0.5360%\n",
      "Epoch [45/300], Step [108/225], Training Accuracy: 75.2459%, Training Loss: 0.5360%\n",
      "Epoch [45/300], Step [109/225], Training Accuracy: 75.2007%, Training Loss: 0.5360%\n",
      "Epoch [45/300], Step [110/225], Training Accuracy: 75.1278%, Training Loss: 0.5364%\n",
      "Epoch [45/300], Step [111/225], Training Accuracy: 75.1267%, Training Loss: 0.5368%\n",
      "Epoch [45/300], Step [112/225], Training Accuracy: 75.1814%, Training Loss: 0.5365%\n",
      "Epoch [45/300], Step [113/225], Training Accuracy: 75.2074%, Training Loss: 0.5358%\n",
      "Epoch [45/300], Step [114/225], Training Accuracy: 75.1782%, Training Loss: 0.5360%\n",
      "Epoch [45/300], Step [115/225], Training Accuracy: 75.2310%, Training Loss: 0.5353%\n",
      "Epoch [45/300], Step [116/225], Training Accuracy: 75.2694%, Training Loss: 0.5344%\n",
      "Epoch [45/300], Step [117/225], Training Accuracy: 75.2671%, Training Loss: 0.5349%\n",
      "Epoch [45/300], Step [118/225], Training Accuracy: 75.2119%, Training Loss: 0.5368%\n",
      "Epoch [45/300], Step [119/225], Training Accuracy: 75.2232%, Training Loss: 0.5363%\n",
      "Epoch [45/300], Step [120/225], Training Accuracy: 75.2083%, Training Loss: 0.5362%\n",
      "Epoch [45/300], Step [121/225], Training Accuracy: 75.1420%, Training Loss: 0.5374%\n",
      "Epoch [45/300], Step [122/225], Training Accuracy: 75.1409%, Training Loss: 0.5377%\n",
      "Epoch [45/300], Step [123/225], Training Accuracy: 75.1651%, Training Loss: 0.5378%\n",
      "Epoch [45/300], Step [124/225], Training Accuracy: 75.1890%, Training Loss: 0.5374%\n",
      "Epoch [45/300], Step [125/225], Training Accuracy: 75.2250%, Training Loss: 0.5370%\n",
      "Epoch [45/300], Step [126/225], Training Accuracy: 75.2604%, Training Loss: 0.5372%\n",
      "Epoch [45/300], Step [127/225], Training Accuracy: 75.2953%, Training Loss: 0.5372%\n",
      "Epoch [45/300], Step [128/225], Training Accuracy: 75.2808%, Training Loss: 0.5381%\n",
      "Epoch [45/300], Step [129/225], Training Accuracy: 75.2786%, Training Loss: 0.5383%\n",
      "Epoch [45/300], Step [130/225], Training Accuracy: 75.3365%, Training Loss: 0.5384%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Step [131/225], Training Accuracy: 75.4055%, Training Loss: 0.5372%\n",
      "Epoch [45/300], Step [132/225], Training Accuracy: 75.4025%, Training Loss: 0.5373%\n",
      "Epoch [45/300], Step [133/225], Training Accuracy: 75.3642%, Training Loss: 0.5374%\n",
      "Epoch [45/300], Step [134/225], Training Accuracy: 75.2915%, Training Loss: 0.5385%\n",
      "Epoch [45/300], Step [135/225], Training Accuracy: 75.3819%, Training Loss: 0.5382%\n",
      "Epoch [45/300], Step [136/225], Training Accuracy: 75.4021%, Training Loss: 0.5379%\n",
      "Epoch [45/300], Step [137/225], Training Accuracy: 75.3650%, Training Loss: 0.5375%\n",
      "Epoch [45/300], Step [138/225], Training Accuracy: 75.4189%, Training Loss: 0.5364%\n",
      "Epoch [45/300], Step [139/225], Training Accuracy: 75.4159%, Training Loss: 0.5367%\n",
      "Epoch [45/300], Step [140/225], Training Accuracy: 75.4353%, Training Loss: 0.5366%\n",
      "Epoch [45/300], Step [141/225], Training Accuracy: 75.4543%, Training Loss: 0.5362%\n",
      "Epoch [45/300], Step [142/225], Training Accuracy: 75.4291%, Training Loss: 0.5361%\n",
      "Epoch [45/300], Step [143/225], Training Accuracy: 75.4917%, Training Loss: 0.5357%\n",
      "Epoch [45/300], Step [144/225], Training Accuracy: 75.4991%, Training Loss: 0.5353%\n",
      "Epoch [45/300], Step [145/225], Training Accuracy: 75.5496%, Training Loss: 0.5346%\n",
      "Epoch [45/300], Step [146/225], Training Accuracy: 75.5244%, Training Loss: 0.5348%\n",
      "Epoch [45/300], Step [147/225], Training Accuracy: 75.4677%, Training Loss: 0.5357%\n",
      "Epoch [45/300], Step [148/225], Training Accuracy: 75.4962%, Training Loss: 0.5351%\n",
      "Epoch [45/300], Step [149/225], Training Accuracy: 75.5138%, Training Loss: 0.5346%\n",
      "Epoch [45/300], Step [150/225], Training Accuracy: 75.4896%, Training Loss: 0.5345%\n",
      "Epoch [45/300], Step [151/225], Training Accuracy: 75.5484%, Training Loss: 0.5334%\n",
      "Epoch [45/300], Step [152/225], Training Accuracy: 75.5140%, Training Loss: 0.5335%\n",
      "Epoch [45/300], Step [153/225], Training Accuracy: 75.4800%, Training Loss: 0.5333%\n",
      "Epoch [45/300], Step [154/225], Training Accuracy: 75.5175%, Training Loss: 0.5329%\n",
      "Epoch [45/300], Step [155/225], Training Accuracy: 75.4738%, Training Loss: 0.5336%\n",
      "Epoch [45/300], Step [156/225], Training Accuracy: 75.3806%, Training Loss: 0.5348%\n",
      "Epoch [45/300], Step [157/225], Training Accuracy: 75.3881%, Training Loss: 0.5349%\n",
      "Epoch [45/300], Step [158/225], Training Accuracy: 75.3659%, Training Loss: 0.5353%\n",
      "Epoch [45/300], Step [159/225], Training Accuracy: 75.3439%, Training Loss: 0.5361%\n",
      "Epoch [45/300], Step [160/225], Training Accuracy: 75.4004%, Training Loss: 0.5353%\n",
      "Epoch [45/300], Step [161/225], Training Accuracy: 75.4367%, Training Loss: 0.5348%\n",
      "Epoch [45/300], Step [162/225], Training Accuracy: 75.5305%, Training Loss: 0.5337%\n",
      "Epoch [45/300], Step [163/225], Training Accuracy: 75.5656%, Training Loss: 0.5334%\n",
      "Epoch [45/300], Step [164/225], Training Accuracy: 75.6002%, Training Loss: 0.5327%\n",
      "Epoch [45/300], Step [165/225], Training Accuracy: 75.6723%, Training Loss: 0.5322%\n",
      "Epoch [45/300], Step [166/225], Training Accuracy: 75.6683%, Training Loss: 0.5323%\n",
      "Epoch [45/300], Step [167/225], Training Accuracy: 75.6643%, Training Loss: 0.5323%\n",
      "Epoch [45/300], Step [168/225], Training Accuracy: 75.6882%, Training Loss: 0.5326%\n",
      "Epoch [45/300], Step [169/225], Training Accuracy: 75.6749%, Training Loss: 0.5329%\n",
      "Epoch [45/300], Step [170/225], Training Accuracy: 75.6985%, Training Loss: 0.5327%\n",
      "Epoch [45/300], Step [171/225], Training Accuracy: 75.7401%, Training Loss: 0.5325%\n",
      "Epoch [45/300], Step [172/225], Training Accuracy: 75.7722%, Training Loss: 0.5322%\n",
      "Epoch [45/300], Step [173/225], Training Accuracy: 75.8129%, Training Loss: 0.5318%\n",
      "Epoch [45/300], Step [174/225], Training Accuracy: 75.8351%, Training Loss: 0.5319%\n",
      "Epoch [45/300], Step [175/225], Training Accuracy: 75.8571%, Training Loss: 0.5316%\n",
      "Epoch [45/300], Step [176/225], Training Accuracy: 75.8878%, Training Loss: 0.5312%\n",
      "Epoch [45/300], Step [177/225], Training Accuracy: 75.9093%, Training Loss: 0.5309%\n",
      "Epoch [45/300], Step [178/225], Training Accuracy: 75.8866%, Training Loss: 0.5309%\n",
      "Epoch [45/300], Step [179/225], Training Accuracy: 75.9078%, Training Loss: 0.5301%\n",
      "Epoch [45/300], Step [180/225], Training Accuracy: 75.9635%, Training Loss: 0.5295%\n",
      "Epoch [45/300], Step [181/225], Training Accuracy: 75.9755%, Training Loss: 0.5296%\n",
      "Epoch [45/300], Step [182/225], Training Accuracy: 75.9787%, Training Loss: 0.5296%\n",
      "Epoch [45/300], Step [183/225], Training Accuracy: 75.9136%, Training Loss: 0.5302%\n",
      "Epoch [45/300], Step [184/225], Training Accuracy: 75.9341%, Training Loss: 0.5299%\n",
      "Epoch [45/300], Step [185/225], Training Accuracy: 75.9797%, Training Loss: 0.5292%\n",
      "Epoch [45/300], Step [186/225], Training Accuracy: 76.0081%, Training Loss: 0.5286%\n",
      "Epoch [45/300], Step [187/225], Training Accuracy: 76.0361%, Training Loss: 0.5284%\n",
      "Epoch [45/300], Step [188/225], Training Accuracy: 76.0888%, Training Loss: 0.5277%\n",
      "Epoch [45/300], Step [189/225], Training Accuracy: 76.1243%, Training Loss: 0.5273%\n",
      "Epoch [45/300], Step [190/225], Training Accuracy: 76.1184%, Training Loss: 0.5274%\n",
      "Epoch [45/300], Step [191/225], Training Accuracy: 76.1207%, Training Loss: 0.5272%\n",
      "Epoch [45/300], Step [192/225], Training Accuracy: 76.1475%, Training Loss: 0.5267%\n",
      "Epoch [45/300], Step [193/225], Training Accuracy: 76.1415%, Training Loss: 0.5268%\n",
      "Epoch [45/300], Step [194/225], Training Accuracy: 76.1195%, Training Loss: 0.5272%\n",
      "Epoch [45/300], Step [195/225], Training Accuracy: 76.1138%, Training Loss: 0.5272%\n",
      "Epoch [45/300], Step [196/225], Training Accuracy: 76.0603%, Training Loss: 0.5275%\n",
      "Epoch [45/300], Step [197/225], Training Accuracy: 76.0787%, Training Loss: 0.5277%\n",
      "Epoch [45/300], Step [198/225], Training Accuracy: 76.1364%, Training Loss: 0.5269%\n",
      "Epoch [45/300], Step [199/225], Training Accuracy: 76.1385%, Training Loss: 0.5270%\n",
      "Epoch [45/300], Step [200/225], Training Accuracy: 76.1172%, Training Loss: 0.5271%\n",
      "Epoch [45/300], Step [201/225], Training Accuracy: 76.0961%, Training Loss: 0.5276%\n",
      "Epoch [45/300], Step [202/225], Training Accuracy: 76.1293%, Training Loss: 0.5270%\n",
      "Epoch [45/300], Step [203/225], Training Accuracy: 76.1700%, Training Loss: 0.5265%\n",
      "Epoch [45/300], Step [204/225], Training Accuracy: 76.1795%, Training Loss: 0.5263%\n",
      "Epoch [45/300], Step [205/225], Training Accuracy: 76.2195%, Training Loss: 0.5259%\n",
      "Epoch [45/300], Step [206/225], Training Accuracy: 76.2060%, Training Loss: 0.5268%\n",
      "Epoch [45/300], Step [207/225], Training Accuracy: 76.2002%, Training Loss: 0.5267%\n",
      "Epoch [45/300], Step [208/225], Training Accuracy: 76.2245%, Training Loss: 0.5261%\n",
      "Epoch [45/300], Step [209/225], Training Accuracy: 76.2410%, Training Loss: 0.5259%\n",
      "Epoch [45/300], Step [210/225], Training Accuracy: 76.2500%, Training Loss: 0.5257%\n",
      "Epoch [45/300], Step [211/225], Training Accuracy: 76.2515%, Training Loss: 0.5254%\n",
      "Epoch [45/300], Step [212/225], Training Accuracy: 76.2382%, Training Loss: 0.5258%\n",
      "Epoch [45/300], Step [213/225], Training Accuracy: 76.2397%, Training Loss: 0.5256%\n",
      "Epoch [45/300], Step [214/225], Training Accuracy: 76.2777%, Training Loss: 0.5251%\n",
      "Epoch [45/300], Step [215/225], Training Accuracy: 76.2573%, Training Loss: 0.5254%\n",
      "Epoch [45/300], Step [216/225], Training Accuracy: 76.2514%, Training Loss: 0.5257%\n",
      "Epoch [45/300], Step [217/225], Training Accuracy: 76.2601%, Training Loss: 0.5256%\n",
      "Epoch [45/300], Step [218/225], Training Accuracy: 76.2543%, Training Loss: 0.5258%\n",
      "Epoch [45/300], Step [219/225], Training Accuracy: 76.2557%, Training Loss: 0.5261%\n",
      "Epoch [45/300], Step [220/225], Training Accuracy: 76.2429%, Training Loss: 0.5263%\n",
      "Epoch [45/300], Step [221/225], Training Accuracy: 76.2090%, Training Loss: 0.5267%\n",
      "Epoch [45/300], Step [222/225], Training Accuracy: 76.2176%, Training Loss: 0.5265%\n",
      "Epoch [45/300], Step [223/225], Training Accuracy: 76.1771%, Training Loss: 0.5271%\n",
      "Epoch [45/300], Step [224/225], Training Accuracy: 76.1928%, Training Loss: 0.5274%\n",
      "Epoch [45/300], Step [225/225], Training Accuracy: 76.1812%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [1/225], Training Accuracy: 70.3125%, Training Loss: 0.5527%\n",
      "Epoch [46/300], Step [2/225], Training Accuracy: 76.5625%, Training Loss: 0.5284%\n",
      "Epoch [46/300], Step [3/225], Training Accuracy: 78.1250%, Training Loss: 0.5301%\n",
      "Epoch [46/300], Step [4/225], Training Accuracy: 77.7344%, Training Loss: 0.5339%\n",
      "Epoch [46/300], Step [5/225], Training Accuracy: 76.5625%, Training Loss: 0.5321%\n",
      "Epoch [46/300], Step [6/225], Training Accuracy: 77.8646%, Training Loss: 0.5204%\n",
      "Epoch [46/300], Step [7/225], Training Accuracy: 79.4643%, Training Loss: 0.5120%\n",
      "Epoch [46/300], Step [8/225], Training Accuracy: 80.2734%, Training Loss: 0.5028%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Step [9/225], Training Accuracy: 78.6458%, Training Loss: 0.5210%\n",
      "Epoch [46/300], Step [10/225], Training Accuracy: 78.1250%, Training Loss: 0.5223%\n",
      "Epoch [46/300], Step [11/225], Training Accuracy: 78.5511%, Training Loss: 0.5112%\n",
      "Epoch [46/300], Step [12/225], Training Accuracy: 77.8646%, Training Loss: 0.5130%\n",
      "Epoch [46/300], Step [13/225], Training Accuracy: 78.2452%, Training Loss: 0.5038%\n",
      "Epoch [46/300], Step [14/225], Training Accuracy: 78.5714%, Training Loss: 0.5010%\n",
      "Epoch [46/300], Step [15/225], Training Accuracy: 78.7500%, Training Loss: 0.4986%\n",
      "Epoch [46/300], Step [16/225], Training Accuracy: 78.8086%, Training Loss: 0.4953%\n",
      "Epoch [46/300], Step [17/225], Training Accuracy: 79.0441%, Training Loss: 0.4933%\n",
      "Epoch [46/300], Step [18/225], Training Accuracy: 79.0799%, Training Loss: 0.4937%\n",
      "Epoch [46/300], Step [19/225], Training Accuracy: 78.5362%, Training Loss: 0.4965%\n",
      "Epoch [46/300], Step [20/225], Training Accuracy: 78.5156%, Training Loss: 0.4979%\n",
      "Epoch [46/300], Step [21/225], Training Accuracy: 78.5714%, Training Loss: 0.4963%\n",
      "Epoch [46/300], Step [22/225], Training Accuracy: 78.4801%, Training Loss: 0.4982%\n",
      "Epoch [46/300], Step [23/225], Training Accuracy: 78.5326%, Training Loss: 0.4940%\n",
      "Epoch [46/300], Step [24/225], Training Accuracy: 78.3854%, Training Loss: 0.4970%\n",
      "Epoch [46/300], Step [25/225], Training Accuracy: 78.7500%, Training Loss: 0.4935%\n",
      "Epoch [46/300], Step [26/225], Training Accuracy: 78.1250%, Training Loss: 0.5018%\n",
      "Epoch [46/300], Step [27/225], Training Accuracy: 78.4144%, Training Loss: 0.5014%\n",
      "Epoch [46/300], Step [28/225], Training Accuracy: 78.6830%, Training Loss: 0.4974%\n",
      "Epoch [46/300], Step [29/225], Training Accuracy: 78.4483%, Training Loss: 0.5016%\n",
      "Epoch [46/300], Step [30/225], Training Accuracy: 78.5417%, Training Loss: 0.4993%\n",
      "Epoch [46/300], Step [31/225], Training Accuracy: 78.0242%, Training Loss: 0.5108%\n",
      "Epoch [46/300], Step [32/225], Training Accuracy: 78.0762%, Training Loss: 0.5095%\n",
      "Epoch [46/300], Step [33/225], Training Accuracy: 77.9830%, Training Loss: 0.5090%\n",
      "Epoch [46/300], Step [34/225], Training Accuracy: 77.7114%, Training Loss: 0.5097%\n",
      "Epoch [46/300], Step [35/225], Training Accuracy: 77.7679%, Training Loss: 0.5081%\n",
      "Epoch [46/300], Step [36/225], Training Accuracy: 77.7344%, Training Loss: 0.5070%\n",
      "Epoch [46/300], Step [37/225], Training Accuracy: 77.8294%, Training Loss: 0.5048%\n",
      "Epoch [46/300], Step [38/225], Training Accuracy: 77.7549%, Training Loss: 0.5069%\n",
      "Epoch [46/300], Step [39/225], Training Accuracy: 77.6843%, Training Loss: 0.5100%\n",
      "Epoch [46/300], Step [40/225], Training Accuracy: 77.5781%, Training Loss: 0.5128%\n",
      "Epoch [46/300], Step [41/225], Training Accuracy: 77.4009%, Training Loss: 0.5169%\n",
      "Epoch [46/300], Step [42/225], Training Accuracy: 77.4554%, Training Loss: 0.5152%\n",
      "Epoch [46/300], Step [43/225], Training Accuracy: 77.4346%, Training Loss: 0.5174%\n",
      "Epoch [46/300], Step [44/225], Training Accuracy: 77.4858%, Training Loss: 0.5164%\n",
      "Epoch [46/300], Step [45/225], Training Accuracy: 77.3611%, Training Loss: 0.5149%\n",
      "Epoch [46/300], Step [46/225], Training Accuracy: 77.4117%, Training Loss: 0.5146%\n",
      "Epoch [46/300], Step [47/225], Training Accuracy: 77.2606%, Training Loss: 0.5183%\n",
      "Epoch [46/300], Step [48/225], Training Accuracy: 77.1484%, Training Loss: 0.5190%\n",
      "Epoch [46/300], Step [49/225], Training Accuracy: 77.1365%, Training Loss: 0.5189%\n",
      "Epoch [46/300], Step [50/225], Training Accuracy: 77.1875%, Training Loss: 0.5180%\n",
      "Epoch [46/300], Step [51/225], Training Accuracy: 77.2365%, Training Loss: 0.5179%\n",
      "Epoch [46/300], Step [52/225], Training Accuracy: 77.1935%, Training Loss: 0.5185%\n",
      "Epoch [46/300], Step [53/225], Training Accuracy: 77.1816%, Training Loss: 0.5190%\n",
      "Epoch [46/300], Step [54/225], Training Accuracy: 77.0833%, Training Loss: 0.5198%\n",
      "Epoch [46/300], Step [55/225], Training Accuracy: 76.9034%, Training Loss: 0.5227%\n",
      "Epoch [46/300], Step [56/225], Training Accuracy: 76.9252%, Training Loss: 0.5227%\n",
      "Epoch [46/300], Step [57/225], Training Accuracy: 76.9463%, Training Loss: 0.5221%\n",
      "Epoch [46/300], Step [58/225], Training Accuracy: 76.8050%, Training Loss: 0.5246%\n",
      "Epoch [46/300], Step [59/225], Training Accuracy: 76.8538%, Training Loss: 0.5241%\n",
      "Epoch [46/300], Step [60/225], Training Accuracy: 76.7969%, Training Loss: 0.5240%\n",
      "Epoch [46/300], Step [61/225], Training Accuracy: 76.7162%, Training Loss: 0.5242%\n",
      "Epoch [46/300], Step [62/225], Training Accuracy: 76.7137%, Training Loss: 0.5248%\n",
      "Epoch [46/300], Step [63/225], Training Accuracy: 76.7361%, Training Loss: 0.5246%\n",
      "Epoch [46/300], Step [64/225], Training Accuracy: 76.8555%, Training Loss: 0.5236%\n",
      "Epoch [46/300], Step [65/225], Training Accuracy: 76.8510%, Training Loss: 0.5242%\n",
      "Epoch [46/300], Step [66/225], Training Accuracy: 76.8703%, Training Loss: 0.5232%\n",
      "Epoch [46/300], Step [67/225], Training Accuracy: 76.8657%, Training Loss: 0.5237%\n",
      "Epoch [46/300], Step [68/225], Training Accuracy: 76.7233%, Training Loss: 0.5254%\n",
      "Epoch [46/300], Step [69/225], Training Accuracy: 76.5851%, Training Loss: 0.5273%\n",
      "Epoch [46/300], Step [70/225], Training Accuracy: 76.5625%, Training Loss: 0.5276%\n",
      "Epoch [46/300], Step [71/225], Training Accuracy: 76.6945%, Training Loss: 0.5260%\n",
      "Epoch [46/300], Step [72/225], Training Accuracy: 76.8012%, Training Loss: 0.5251%\n",
      "Epoch [46/300], Step [73/225], Training Accuracy: 76.8193%, Training Loss: 0.5251%\n",
      "Epoch [46/300], Step [74/225], Training Accuracy: 76.8581%, Training Loss: 0.5238%\n",
      "Epoch [46/300], Step [75/225], Training Accuracy: 76.8125%, Training Loss: 0.5241%\n",
      "Epoch [46/300], Step [76/225], Training Accuracy: 76.7681%, Training Loss: 0.5250%\n",
      "Epoch [46/300], Step [77/225], Training Accuracy: 76.8466%, Training Loss: 0.5239%\n",
      "Epoch [46/300], Step [78/225], Training Accuracy: 76.8830%, Training Loss: 0.5243%\n",
      "Epoch [46/300], Step [79/225], Training Accuracy: 77.0174%, Training Loss: 0.5223%\n",
      "Epoch [46/300], Step [80/225], Training Accuracy: 77.0312%, Training Loss: 0.5226%\n",
      "Epoch [46/300], Step [81/225], Training Accuracy: 77.0062%, Training Loss: 0.5222%\n",
      "Epoch [46/300], Step [82/225], Training Accuracy: 77.0389%, Training Loss: 0.5218%\n",
      "Epoch [46/300], Step [83/225], Training Accuracy: 76.9578%, Training Loss: 0.5221%\n",
      "Epoch [46/300], Step [84/225], Training Accuracy: 77.0089%, Training Loss: 0.5208%\n",
      "Epoch [46/300], Step [85/225], Training Accuracy: 77.0772%, Training Loss: 0.5193%\n",
      "Epoch [46/300], Step [86/225], Training Accuracy: 77.0531%, Training Loss: 0.5191%\n",
      "Epoch [46/300], Step [87/225], Training Accuracy: 76.9756%, Training Loss: 0.5203%\n",
      "Epoch [46/300], Step [88/225], Training Accuracy: 76.9709%, Training Loss: 0.5206%\n",
      "Epoch [46/300], Step [89/225], Training Accuracy: 76.9312%, Training Loss: 0.5203%\n",
      "Epoch [46/300], Step [90/225], Training Accuracy: 76.8924%, Training Loss: 0.5222%\n",
      "Epoch [46/300], Step [91/225], Training Accuracy: 76.7514%, Training Loss: 0.5235%\n",
      "Epoch [46/300], Step [92/225], Training Accuracy: 76.7663%, Training Loss: 0.5234%\n",
      "Epoch [46/300], Step [93/225], Training Accuracy: 76.7977%, Training Loss: 0.5226%\n",
      "Epoch [46/300], Step [94/225], Training Accuracy: 76.8451%, Training Loss: 0.5218%\n",
      "Epoch [46/300], Step [95/225], Training Accuracy: 76.8586%, Training Loss: 0.5222%\n",
      "Epoch [46/300], Step [96/225], Training Accuracy: 76.9043%, Training Loss: 0.5215%\n",
      "Epoch [46/300], Step [97/225], Training Accuracy: 76.8686%, Training Loss: 0.5211%\n",
      "Epoch [46/300], Step [98/225], Training Accuracy: 76.8814%, Training Loss: 0.5225%\n",
      "Epoch [46/300], Step [99/225], Training Accuracy: 76.8466%, Training Loss: 0.5243%\n",
      "Epoch [46/300], Step [100/225], Training Accuracy: 76.7656%, Training Loss: 0.5262%\n",
      "Epoch [46/300], Step [101/225], Training Accuracy: 76.7481%, Training Loss: 0.5267%\n",
      "Epoch [46/300], Step [102/225], Training Accuracy: 76.7463%, Training Loss: 0.5271%\n",
      "Epoch [46/300], Step [103/225], Training Accuracy: 76.7900%, Training Loss: 0.5264%\n",
      "Epoch [46/300], Step [104/225], Training Accuracy: 76.7728%, Training Loss: 0.5267%\n",
      "Epoch [46/300], Step [105/225], Training Accuracy: 76.7411%, Training Loss: 0.5260%\n",
      "Epoch [46/300], Step [106/225], Training Accuracy: 76.7541%, Training Loss: 0.5254%\n",
      "Epoch [46/300], Step [107/225], Training Accuracy: 76.7085%, Training Loss: 0.5267%\n",
      "Epoch [46/300], Step [108/225], Training Accuracy: 76.6493%, Training Loss: 0.5284%\n",
      "Epoch [46/300], Step [109/225], Training Accuracy: 76.6198%, Training Loss: 0.5283%\n",
      "Epoch [46/300], Step [110/225], Training Accuracy: 76.6193%, Training Loss: 0.5281%\n",
      "Epoch [46/300], Step [111/225], Training Accuracy: 76.5907%, Training Loss: 0.5283%\n",
      "Epoch [46/300], Step [112/225], Training Accuracy: 76.6183%, Training Loss: 0.5281%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Step [113/225], Training Accuracy: 76.6455%, Training Loss: 0.5279%\n",
      "Epoch [46/300], Step [114/225], Training Accuracy: 76.6584%, Training Loss: 0.5278%\n",
      "Epoch [46/300], Step [115/225], Training Accuracy: 76.7120%, Training Loss: 0.5268%\n",
      "Epoch [46/300], Step [116/225], Training Accuracy: 76.7107%, Training Loss: 0.5264%\n",
      "Epoch [46/300], Step [117/225], Training Accuracy: 76.6159%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [118/225], Training Accuracy: 76.6287%, Training Loss: 0.5271%\n",
      "Epoch [46/300], Step [119/225], Training Accuracy: 76.6807%, Training Loss: 0.5267%\n",
      "Epoch [46/300], Step [120/225], Training Accuracy: 76.6536%, Training Loss: 0.5264%\n",
      "Epoch [46/300], Step [121/225], Training Accuracy: 76.5496%, Training Loss: 0.5280%\n",
      "Epoch [46/300], Step [122/225], Training Accuracy: 76.5753%, Training Loss: 0.5279%\n",
      "Epoch [46/300], Step [123/225], Training Accuracy: 76.5244%, Training Loss: 0.5284%\n",
      "Epoch [46/300], Step [124/225], Training Accuracy: 76.5247%, Training Loss: 0.5280%\n",
      "Epoch [46/300], Step [125/225], Training Accuracy: 76.5250%, Training Loss: 0.5277%\n",
      "Epoch [46/300], Step [126/225], Training Accuracy: 76.4757%, Training Loss: 0.5279%\n",
      "Epoch [46/300], Step [127/225], Training Accuracy: 76.4395%, Training Loss: 0.5281%\n",
      "Epoch [46/300], Step [128/225], Training Accuracy: 76.4160%, Training Loss: 0.5284%\n",
      "Epoch [46/300], Step [129/225], Training Accuracy: 76.4172%, Training Loss: 0.5287%\n",
      "Epoch [46/300], Step [130/225], Training Accuracy: 76.3942%, Training Loss: 0.5289%\n",
      "Epoch [46/300], Step [131/225], Training Accuracy: 76.4313%, Training Loss: 0.5280%\n",
      "Epoch [46/300], Step [132/225], Training Accuracy: 76.3849%, Training Loss: 0.5287%\n",
      "Epoch [46/300], Step [133/225], Training Accuracy: 76.3980%, Training Loss: 0.5283%\n",
      "Epoch [46/300], Step [134/225], Training Accuracy: 76.3993%, Training Loss: 0.5287%\n",
      "Epoch [46/300], Step [135/225], Training Accuracy: 76.3773%, Training Loss: 0.5291%\n",
      "Epoch [46/300], Step [136/225], Training Accuracy: 76.3902%, Training Loss: 0.5284%\n",
      "Epoch [46/300], Step [137/225], Training Accuracy: 76.3686%, Training Loss: 0.5282%\n",
      "Epoch [46/300], Step [138/225], Training Accuracy: 76.4266%, Training Loss: 0.5280%\n",
      "Epoch [46/300], Step [139/225], Training Accuracy: 76.4501%, Training Loss: 0.5281%\n",
      "Epoch [46/300], Step [140/225], Training Accuracy: 76.5067%, Training Loss: 0.5272%\n",
      "Epoch [46/300], Step [141/225], Training Accuracy: 76.5182%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [142/225], Training Accuracy: 76.5515%, Training Loss: 0.5271%\n",
      "Epoch [46/300], Step [143/225], Training Accuracy: 76.6062%, Training Loss: 0.5271%\n",
      "Epoch [46/300], Step [144/225], Training Accuracy: 76.6276%, Training Loss: 0.5267%\n",
      "Epoch [46/300], Step [145/225], Training Accuracy: 76.6379%, Training Loss: 0.5264%\n",
      "Epoch [46/300], Step [146/225], Training Accuracy: 76.5946%, Training Loss: 0.5268%\n",
      "Epoch [46/300], Step [147/225], Training Accuracy: 76.5519%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [148/225], Training Accuracy: 76.5097%, Training Loss: 0.5277%\n",
      "Epoch [46/300], Step [149/225], Training Accuracy: 76.5206%, Training Loss: 0.5272%\n",
      "Epoch [46/300], Step [150/225], Training Accuracy: 76.4688%, Training Loss: 0.5272%\n",
      "Epoch [46/300], Step [151/225], Training Accuracy: 76.5108%, Training Loss: 0.5262%\n",
      "Epoch [46/300], Step [152/225], Training Accuracy: 76.5008%, Training Loss: 0.5268%\n",
      "Epoch [46/300], Step [153/225], Training Accuracy: 76.4910%, Training Loss: 0.5270%\n",
      "Epoch [46/300], Step [154/225], Training Accuracy: 76.5219%, Training Loss: 0.5264%\n",
      "Epoch [46/300], Step [155/225], Training Accuracy: 76.4919%, Training Loss: 0.5267%\n",
      "Epoch [46/300], Step [156/225], Training Accuracy: 76.4523%, Training Loss: 0.5276%\n",
      "Epoch [46/300], Step [157/225], Training Accuracy: 76.4431%, Training Loss: 0.5281%\n",
      "Epoch [46/300], Step [158/225], Training Accuracy: 76.3746%, Training Loss: 0.5286%\n",
      "Epoch [46/300], Step [159/225], Training Accuracy: 76.3365%, Training Loss: 0.5290%\n",
      "Epoch [46/300], Step [160/225], Training Accuracy: 76.3379%, Training Loss: 0.5286%\n",
      "Epoch [46/300], Step [161/225], Training Accuracy: 76.3102%, Training Loss: 0.5283%\n",
      "Epoch [46/300], Step [162/225], Training Accuracy: 76.3600%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [163/225], Training Accuracy: 76.3804%, Training Loss: 0.5273%\n",
      "Epoch [46/300], Step [164/225], Training Accuracy: 76.3910%, Training Loss: 0.5264%\n",
      "Epoch [46/300], Step [165/225], Training Accuracy: 76.4489%, Training Loss: 0.5260%\n",
      "Epoch [46/300], Step [166/225], Training Accuracy: 76.4684%, Training Loss: 0.5257%\n",
      "Epoch [46/300], Step [167/225], Training Accuracy: 76.4502%, Training Loss: 0.5263%\n",
      "Epoch [46/300], Step [168/225], Training Accuracy: 76.4602%, Training Loss: 0.5263%\n",
      "Epoch [46/300], Step [169/225], Training Accuracy: 76.4053%, Training Loss: 0.5268%\n",
      "Epoch [46/300], Step [170/225], Training Accuracy: 76.3879%, Training Loss: 0.5270%\n",
      "Epoch [46/300], Step [171/225], Training Accuracy: 76.4529%, Training Loss: 0.5265%\n",
      "Epoch [46/300], Step [172/225], Training Accuracy: 76.4898%, Training Loss: 0.5261%\n",
      "Epoch [46/300], Step [173/225], Training Accuracy: 76.5083%, Training Loss: 0.5259%\n",
      "Epoch [46/300], Step [174/225], Training Accuracy: 76.5176%, Training Loss: 0.5258%\n",
      "Epoch [46/300], Step [175/225], Training Accuracy: 76.5357%, Training Loss: 0.5254%\n",
      "Epoch [46/300], Step [176/225], Training Accuracy: 76.5270%, Training Loss: 0.5252%\n",
      "Epoch [46/300], Step [177/225], Training Accuracy: 76.5448%, Training Loss: 0.5250%\n",
      "Epoch [46/300], Step [178/225], Training Accuracy: 76.5274%, Training Loss: 0.5250%\n",
      "Epoch [46/300], Step [179/225], Training Accuracy: 76.5363%, Training Loss: 0.5244%\n",
      "Epoch [46/300], Step [180/225], Training Accuracy: 76.5451%, Training Loss: 0.5244%\n",
      "Epoch [46/300], Step [181/225], Training Accuracy: 76.5798%, Training Loss: 0.5245%\n",
      "Epoch [46/300], Step [182/225], Training Accuracy: 76.5968%, Training Loss: 0.5245%\n",
      "Epoch [46/300], Step [183/225], Training Accuracy: 76.5967%, Training Loss: 0.5248%\n",
      "Epoch [46/300], Step [184/225], Training Accuracy: 76.6219%, Training Loss: 0.5242%\n",
      "Epoch [46/300], Step [185/225], Training Accuracy: 76.6132%, Training Loss: 0.5238%\n",
      "Epoch [46/300], Step [186/225], Training Accuracy: 76.6297%, Training Loss: 0.5235%\n",
      "Epoch [46/300], Step [187/225], Training Accuracy: 76.6293%, Training Loss: 0.5237%\n",
      "Epoch [46/300], Step [188/225], Training Accuracy: 76.6456%, Training Loss: 0.5231%\n",
      "Epoch [46/300], Step [189/225], Training Accuracy: 76.6452%, Training Loss: 0.5232%\n",
      "Epoch [46/300], Step [190/225], Training Accuracy: 76.6365%, Training Loss: 0.5234%\n",
      "Epoch [46/300], Step [191/225], Training Accuracy: 76.6116%, Training Loss: 0.5238%\n",
      "Epoch [46/300], Step [192/225], Training Accuracy: 76.6357%, Training Loss: 0.5235%\n",
      "Epoch [46/300], Step [193/225], Training Accuracy: 76.6192%, Training Loss: 0.5241%\n",
      "Epoch [46/300], Step [194/225], Training Accuracy: 76.5625%, Training Loss: 0.5253%\n",
      "Epoch [46/300], Step [195/225], Training Accuracy: 76.5865%, Training Loss: 0.5251%\n",
      "Epoch [46/300], Step [196/225], Training Accuracy: 76.5386%, Training Loss: 0.5250%\n",
      "Epoch [46/300], Step [197/225], Training Accuracy: 76.5228%, Training Loss: 0.5253%\n",
      "Epoch [46/300], Step [198/225], Training Accuracy: 76.5309%, Training Loss: 0.5251%\n",
      "Epoch [46/300], Step [199/225], Training Accuracy: 76.5154%, Training Loss: 0.5255%\n",
      "Epoch [46/300], Step [200/225], Training Accuracy: 76.5078%, Training Loss: 0.5254%\n",
      "Epoch [46/300], Step [201/225], Training Accuracy: 76.5547%, Training Loss: 0.5250%\n",
      "Epoch [46/300], Step [202/225], Training Accuracy: 76.5934%, Training Loss: 0.5245%\n",
      "Epoch [46/300], Step [203/225], Training Accuracy: 76.6087%, Training Loss: 0.5248%\n",
      "Epoch [46/300], Step [204/225], Training Accuracy: 76.6008%, Training Loss: 0.5248%\n",
      "Epoch [46/300], Step [205/225], Training Accuracy: 76.6082%, Training Loss: 0.5244%\n",
      "Epoch [46/300], Step [206/225], Training Accuracy: 76.6004%, Training Loss: 0.5250%\n",
      "Epoch [46/300], Step [207/225], Training Accuracy: 76.6078%, Training Loss: 0.5251%\n",
      "Epoch [46/300], Step [208/225], Training Accuracy: 76.6226%, Training Loss: 0.5249%\n",
      "Epoch [46/300], Step [209/225], Training Accuracy: 76.5999%, Training Loss: 0.5252%\n",
      "Epoch [46/300], Step [210/225], Training Accuracy: 76.6146%, Training Loss: 0.5256%\n",
      "Epoch [46/300], Step [211/225], Training Accuracy: 76.6143%, Training Loss: 0.5258%\n",
      "Epoch [46/300], Step [212/225], Training Accuracy: 76.5625%, Training Loss: 0.5263%\n",
      "Epoch [46/300], Step [213/225], Training Accuracy: 76.5698%, Training Loss: 0.5266%\n",
      "Epoch [46/300], Step [214/225], Training Accuracy: 76.5771%, Training Loss: 0.5263%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Step [215/225], Training Accuracy: 76.5480%, Training Loss: 0.5271%\n",
      "Epoch [46/300], Step [216/225], Training Accuracy: 76.5625%, Training Loss: 0.5269%\n",
      "Epoch [46/300], Step [217/225], Training Accuracy: 76.5841%, Training Loss: 0.5271%\n",
      "Epoch [46/300], Step [218/225], Training Accuracy: 76.5482%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [219/225], Training Accuracy: 76.5411%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [220/225], Training Accuracy: 76.5341%, Training Loss: 0.5275%\n",
      "Epoch [46/300], Step [221/225], Training Accuracy: 76.4706%, Training Loss: 0.5284%\n",
      "Epoch [46/300], Step [222/225], Training Accuracy: 76.4992%, Training Loss: 0.5282%\n",
      "Epoch [46/300], Step [223/225], Training Accuracy: 76.4504%, Training Loss: 0.5288%\n",
      "Epoch [46/300], Step [224/225], Training Accuracy: 76.4509%, Training Loss: 0.5289%\n",
      "Epoch [46/300], Step [225/225], Training Accuracy: 76.4175%, Training Loss: 0.5293%\n",
      "Epoch [47/300], Step [1/225], Training Accuracy: 73.4375%, Training Loss: 0.6525%\n",
      "Epoch [47/300], Step [2/225], Training Accuracy: 73.4375%, Training Loss: 0.6214%\n",
      "Epoch [47/300], Step [3/225], Training Accuracy: 71.8750%, Training Loss: 0.6074%\n",
      "Epoch [47/300], Step [4/225], Training Accuracy: 73.4375%, Training Loss: 0.5817%\n",
      "Epoch [47/300], Step [5/225], Training Accuracy: 73.4375%, Training Loss: 0.5702%\n",
      "Epoch [47/300], Step [6/225], Training Accuracy: 74.2188%, Training Loss: 0.5786%\n",
      "Epoch [47/300], Step [7/225], Training Accuracy: 74.1071%, Training Loss: 0.5745%\n",
      "Epoch [47/300], Step [8/225], Training Accuracy: 74.4141%, Training Loss: 0.5672%\n",
      "Epoch [47/300], Step [9/225], Training Accuracy: 73.9583%, Training Loss: 0.5918%\n",
      "Epoch [47/300], Step [10/225], Training Accuracy: 74.2188%, Training Loss: 0.5927%\n",
      "Epoch [47/300], Step [11/225], Training Accuracy: 74.7159%, Training Loss: 0.5843%\n",
      "Epoch [47/300], Step [12/225], Training Accuracy: 74.4792%, Training Loss: 0.5857%\n",
      "Epoch [47/300], Step [13/225], Training Accuracy: 75.3606%, Training Loss: 0.5663%\n",
      "Epoch [47/300], Step [14/225], Training Accuracy: 75.2232%, Training Loss: 0.5641%\n",
      "Epoch [47/300], Step [15/225], Training Accuracy: 75.4167%, Training Loss: 0.5620%\n",
      "Epoch [47/300], Step [16/225], Training Accuracy: 75.9766%, Training Loss: 0.5544%\n",
      "Epoch [47/300], Step [17/225], Training Accuracy: 76.2868%, Training Loss: 0.5511%\n",
      "Epoch [47/300], Step [18/225], Training Accuracy: 76.2153%, Training Loss: 0.5513%\n",
      "Epoch [47/300], Step [19/225], Training Accuracy: 76.5625%, Training Loss: 0.5484%\n",
      "Epoch [47/300], Step [20/225], Training Accuracy: 76.6406%, Training Loss: 0.5440%\n",
      "Epoch [47/300], Step [21/225], Training Accuracy: 77.1577%, Training Loss: 0.5382%\n",
      "Epoch [47/300], Step [22/225], Training Accuracy: 76.4915%, Training Loss: 0.5422%\n",
      "Epoch [47/300], Step [23/225], Training Accuracy: 76.6984%, Training Loss: 0.5374%\n",
      "Epoch [47/300], Step [24/225], Training Accuracy: 76.5625%, Training Loss: 0.5417%\n",
      "Epoch [47/300], Step [25/225], Training Accuracy: 76.7500%, Training Loss: 0.5388%\n",
      "Epoch [47/300], Step [26/225], Training Accuracy: 76.3822%, Training Loss: 0.5416%\n",
      "Epoch [47/300], Step [27/225], Training Accuracy: 76.6204%, Training Loss: 0.5374%\n",
      "Epoch [47/300], Step [28/225], Training Accuracy: 77.0647%, Training Loss: 0.5307%\n",
      "Epoch [47/300], Step [29/225], Training Accuracy: 76.9397%, Training Loss: 0.5323%\n",
      "Epoch [47/300], Step [30/225], Training Accuracy: 77.0312%, Training Loss: 0.5307%\n",
      "Epoch [47/300], Step [31/225], Training Accuracy: 76.6633%, Training Loss: 0.5363%\n",
      "Epoch [47/300], Step [32/225], Training Accuracy: 76.7090%, Training Loss: 0.5346%\n",
      "Epoch [47/300], Step [33/225], Training Accuracy: 76.7992%, Training Loss: 0.5313%\n",
      "Epoch [47/300], Step [34/225], Training Accuracy: 76.8382%, Training Loss: 0.5303%\n",
      "Epoch [47/300], Step [35/225], Training Accuracy: 76.7411%, Training Loss: 0.5284%\n",
      "Epoch [47/300], Step [36/225], Training Accuracy: 76.7361%, Training Loss: 0.5279%\n",
      "Epoch [47/300], Step [37/225], Training Accuracy: 76.7736%, Training Loss: 0.5273%\n",
      "Epoch [47/300], Step [38/225], Training Accuracy: 76.6859%, Training Loss: 0.5270%\n",
      "Epoch [47/300], Step [39/225], Training Accuracy: 76.6827%, Training Loss: 0.5277%\n",
      "Epoch [47/300], Step [40/225], Training Accuracy: 76.6797%, Training Loss: 0.5299%\n",
      "Epoch [47/300], Step [41/225], Training Accuracy: 76.8293%, Training Loss: 0.5290%\n",
      "Epoch [47/300], Step [42/225], Training Accuracy: 76.8973%, Training Loss: 0.5270%\n",
      "Epoch [47/300], Step [43/225], Training Accuracy: 76.8169%, Training Loss: 0.5285%\n",
      "Epoch [47/300], Step [44/225], Training Accuracy: 76.9886%, Training Loss: 0.5255%\n",
      "Epoch [47/300], Step [45/225], Training Accuracy: 77.0486%, Training Loss: 0.5244%\n",
      "Epoch [47/300], Step [46/225], Training Accuracy: 77.1060%, Training Loss: 0.5232%\n",
      "Epoch [47/300], Step [47/225], Training Accuracy: 77.0279%, Training Loss: 0.5224%\n",
      "Epoch [47/300], Step [48/225], Training Accuracy: 76.9857%, Training Loss: 0.5249%\n",
      "Epoch [47/300], Step [49/225], Training Accuracy: 77.0727%, Training Loss: 0.5244%\n",
      "Epoch [47/300], Step [50/225], Training Accuracy: 77.0000%, Training Loss: 0.5253%\n",
      "Epoch [47/300], Step [51/225], Training Accuracy: 77.0833%, Training Loss: 0.5246%\n",
      "Epoch [47/300], Step [52/225], Training Accuracy: 77.1334%, Training Loss: 0.5226%\n",
      "Epoch [47/300], Step [53/225], Training Accuracy: 77.1521%, Training Loss: 0.5223%\n",
      "Epoch [47/300], Step [54/225], Training Accuracy: 77.1123%, Training Loss: 0.5226%\n",
      "Epoch [47/300], Step [55/225], Training Accuracy: 76.9886%, Training Loss: 0.5251%\n",
      "Epoch [47/300], Step [56/225], Training Accuracy: 76.8694%, Training Loss: 0.5272%\n",
      "Epoch [47/300], Step [57/225], Training Accuracy: 76.7544%, Training Loss: 0.5271%\n",
      "Epoch [47/300], Step [58/225], Training Accuracy: 76.7511%, Training Loss: 0.5276%\n",
      "Epoch [47/300], Step [59/225], Training Accuracy: 76.8538%, Training Loss: 0.5268%\n",
      "Epoch [47/300], Step [60/225], Training Accuracy: 76.9010%, Training Loss: 0.5260%\n",
      "Epoch [47/300], Step [61/225], Training Accuracy: 76.6906%, Training Loss: 0.5278%\n",
      "Epoch [47/300], Step [62/225], Training Accuracy: 76.6885%, Training Loss: 0.5275%\n",
      "Epoch [47/300], Step [63/225], Training Accuracy: 76.6369%, Training Loss: 0.5296%\n",
      "Epoch [47/300], Step [64/225], Training Accuracy: 76.6846%, Training Loss: 0.5286%\n",
      "Epoch [47/300], Step [65/225], Training Accuracy: 76.7067%, Training Loss: 0.5279%\n",
      "Epoch [47/300], Step [66/225], Training Accuracy: 76.7756%, Training Loss: 0.5264%\n",
      "Epoch [47/300], Step [67/225], Training Accuracy: 76.7957%, Training Loss: 0.5255%\n",
      "Epoch [47/300], Step [68/225], Training Accuracy: 76.7004%, Training Loss: 0.5264%\n",
      "Epoch [47/300], Step [69/225], Training Accuracy: 76.5851%, Training Loss: 0.5269%\n",
      "Epoch [47/300], Step [70/225], Training Accuracy: 76.5179%, Training Loss: 0.5268%\n",
      "Epoch [47/300], Step [71/225], Training Accuracy: 76.6065%, Training Loss: 0.5254%\n",
      "Epoch [47/300], Step [72/225], Training Accuracy: 76.6927%, Training Loss: 0.5247%\n",
      "Epoch [47/300], Step [73/225], Training Accuracy: 76.6267%, Training Loss: 0.5244%\n",
      "Epoch [47/300], Step [74/225], Training Accuracy: 76.6470%, Training Loss: 0.5236%\n",
      "Epoch [47/300], Step [75/225], Training Accuracy: 76.5833%, Training Loss: 0.5231%\n",
      "Epoch [47/300], Step [76/225], Training Accuracy: 76.4803%, Training Loss: 0.5244%\n",
      "Epoch [47/300], Step [77/225], Training Accuracy: 76.6031%, Training Loss: 0.5234%\n",
      "Epoch [47/300], Step [78/225], Training Accuracy: 76.5825%, Training Loss: 0.5242%\n",
      "Epoch [47/300], Step [79/225], Training Accuracy: 76.6812%, Training Loss: 0.5230%\n",
      "Epoch [47/300], Step [80/225], Training Accuracy: 76.6797%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [81/225], Training Accuracy: 76.6975%, Training Loss: 0.5224%\n",
      "Epoch [47/300], Step [82/225], Training Accuracy: 76.6387%, Training Loss: 0.5217%\n",
      "Epoch [47/300], Step [83/225], Training Accuracy: 76.6002%, Training Loss: 0.5214%\n",
      "Epoch [47/300], Step [84/225], Training Accuracy: 76.5997%, Training Loss: 0.5205%\n",
      "Epoch [47/300], Step [85/225], Training Accuracy: 76.7279%, Training Loss: 0.5188%\n",
      "Epoch [47/300], Step [86/225], Training Accuracy: 76.7078%, Training Loss: 0.5181%\n",
      "Epoch [47/300], Step [87/225], Training Accuracy: 76.7421%, Training Loss: 0.5179%\n",
      "Epoch [47/300], Step [88/225], Training Accuracy: 76.7223%, Training Loss: 0.5180%\n",
      "Epoch [47/300], Step [89/225], Training Accuracy: 76.6503%, Training Loss: 0.5189%\n",
      "Epoch [47/300], Step [90/225], Training Accuracy: 76.5799%, Training Loss: 0.5203%\n",
      "Epoch [47/300], Step [91/225], Training Accuracy: 76.4938%, Training Loss: 0.5211%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Step [92/225], Training Accuracy: 76.4096%, Training Loss: 0.5223%\n",
      "Epoch [47/300], Step [93/225], Training Accuracy: 76.4785%, Training Loss: 0.5211%\n",
      "Epoch [47/300], Step [94/225], Training Accuracy: 76.4960%, Training Loss: 0.5208%\n",
      "Epoch [47/300], Step [95/225], Training Accuracy: 76.6118%, Training Loss: 0.5202%\n",
      "Epoch [47/300], Step [96/225], Training Accuracy: 76.6276%, Training Loss: 0.5194%\n",
      "Epoch [47/300], Step [97/225], Training Accuracy: 76.6591%, Training Loss: 0.5193%\n",
      "Epoch [47/300], Step [98/225], Training Accuracy: 76.5625%, Training Loss: 0.5207%\n",
      "Epoch [47/300], Step [99/225], Training Accuracy: 76.6098%, Training Loss: 0.5204%\n",
      "Epoch [47/300], Step [100/225], Training Accuracy: 76.6250%, Training Loss: 0.5205%\n",
      "Epoch [47/300], Step [101/225], Training Accuracy: 76.5316%, Training Loss: 0.5213%\n",
      "Epoch [47/300], Step [102/225], Training Accuracy: 76.5778%, Training Loss: 0.5214%\n",
      "Epoch [47/300], Step [103/225], Training Accuracy: 76.5625%, Training Loss: 0.5213%\n",
      "Epoch [47/300], Step [104/225], Training Accuracy: 76.4724%, Training Loss: 0.5231%\n",
      "Epoch [47/300], Step [105/225], Training Accuracy: 76.5030%, Training Loss: 0.5221%\n",
      "Epoch [47/300], Step [106/225], Training Accuracy: 76.5920%, Training Loss: 0.5219%\n",
      "Epoch [47/300], Step [107/225], Training Accuracy: 76.5625%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [108/225], Training Accuracy: 76.5336%, Training Loss: 0.5227%\n",
      "Epoch [47/300], Step [109/225], Training Accuracy: 76.5195%, Training Loss: 0.5222%\n",
      "Epoch [47/300], Step [110/225], Training Accuracy: 76.4915%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [111/225], Training Accuracy: 76.5203%, Training Loss: 0.5221%\n",
      "Epoch [47/300], Step [112/225], Training Accuracy: 76.5206%, Training Loss: 0.5223%\n",
      "Epoch [47/300], Step [113/225], Training Accuracy: 76.5072%, Training Loss: 0.5222%\n",
      "Epoch [47/300], Step [114/225], Training Accuracy: 76.4529%, Training Loss: 0.5221%\n",
      "Epoch [47/300], Step [115/225], Training Accuracy: 76.5353%, Training Loss: 0.5212%\n",
      "Epoch [47/300], Step [116/225], Training Accuracy: 76.5490%, Training Loss: 0.5201%\n",
      "Epoch [47/300], Step [117/225], Training Accuracy: 76.5759%, Training Loss: 0.5206%\n",
      "Epoch [47/300], Step [118/225], Training Accuracy: 76.5493%, Training Loss: 0.5210%\n",
      "Epoch [47/300], Step [119/225], Training Accuracy: 76.5625%, Training Loss: 0.5208%\n",
      "Epoch [47/300], Step [120/225], Training Accuracy: 76.5885%, Training Loss: 0.5206%\n",
      "Epoch [47/300], Step [121/225], Training Accuracy: 76.4334%, Training Loss: 0.5224%\n",
      "Epoch [47/300], Step [122/225], Training Accuracy: 76.3448%, Training Loss: 0.5236%\n",
      "Epoch [47/300], Step [123/225], Training Accuracy: 76.3465%, Training Loss: 0.5239%\n",
      "Epoch [47/300], Step [124/225], Training Accuracy: 76.3231%, Training Loss: 0.5237%\n",
      "Epoch [47/300], Step [125/225], Training Accuracy: 76.2875%, Training Loss: 0.5241%\n",
      "Epoch [47/300], Step [126/225], Training Accuracy: 76.2897%, Training Loss: 0.5246%\n",
      "Epoch [47/300], Step [127/225], Training Accuracy: 76.2426%, Training Loss: 0.5259%\n",
      "Epoch [47/300], Step [128/225], Training Accuracy: 76.1841%, Training Loss: 0.5270%\n",
      "Epoch [47/300], Step [129/225], Training Accuracy: 76.1265%, Training Loss: 0.5280%\n",
      "Epoch [47/300], Step [130/225], Training Accuracy: 76.1899%, Training Loss: 0.5274%\n",
      "Epoch [47/300], Step [131/225], Training Accuracy: 76.2643%, Training Loss: 0.5264%\n",
      "Epoch [47/300], Step [132/225], Training Accuracy: 76.2429%, Training Loss: 0.5267%\n",
      "Epoch [47/300], Step [133/225], Training Accuracy: 76.2336%, Training Loss: 0.5264%\n",
      "Epoch [47/300], Step [134/225], Training Accuracy: 76.1894%, Training Loss: 0.5273%\n",
      "Epoch [47/300], Step [135/225], Training Accuracy: 76.2269%, Training Loss: 0.5272%\n",
      "Epoch [47/300], Step [136/225], Training Accuracy: 76.3097%, Training Loss: 0.5260%\n",
      "Epoch [47/300], Step [137/225], Training Accuracy: 76.3344%, Training Loss: 0.5259%\n",
      "Epoch [47/300], Step [138/225], Training Accuracy: 76.3587%, Training Loss: 0.5250%\n",
      "Epoch [47/300], Step [139/225], Training Accuracy: 76.3377%, Training Loss: 0.5257%\n",
      "Epoch [47/300], Step [140/225], Training Accuracy: 76.3504%, Training Loss: 0.5253%\n",
      "Epoch [47/300], Step [141/225], Training Accuracy: 76.3298%, Training Loss: 0.5259%\n",
      "Epoch [47/300], Step [142/225], Training Accuracy: 76.3424%, Training Loss: 0.5253%\n",
      "Epoch [47/300], Step [143/225], Training Accuracy: 76.3767%, Training Loss: 0.5249%\n",
      "Epoch [47/300], Step [144/225], Training Accuracy: 76.3346%, Training Loss: 0.5250%\n",
      "Epoch [47/300], Step [145/225], Training Accuracy: 76.3793%, Training Loss: 0.5246%\n",
      "Epoch [47/300], Step [146/225], Training Accuracy: 76.3806%, Training Loss: 0.5248%\n",
      "Epoch [47/300], Step [147/225], Training Accuracy: 76.2968%, Training Loss: 0.5253%\n",
      "Epoch [47/300], Step [148/225], Training Accuracy: 76.3514%, Training Loss: 0.5244%\n",
      "Epoch [47/300], Step [149/225], Training Accuracy: 76.3108%, Training Loss: 0.5244%\n",
      "Epoch [47/300], Step [150/225], Training Accuracy: 76.3021%, Training Loss: 0.5241%\n",
      "Epoch [47/300], Step [151/225], Training Accuracy: 76.3349%, Training Loss: 0.5233%\n",
      "Epoch [47/300], Step [152/225], Training Accuracy: 76.3466%, Training Loss: 0.5232%\n",
      "Epoch [47/300], Step [153/225], Training Accuracy: 76.3480%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [154/225], Training Accuracy: 76.3799%, Training Loss: 0.5224%\n",
      "Epoch [47/300], Step [155/225], Training Accuracy: 76.3206%, Training Loss: 0.5232%\n",
      "Epoch [47/300], Step [156/225], Training Accuracy: 76.2921%, Training Loss: 0.5233%\n",
      "Epoch [47/300], Step [157/225], Training Accuracy: 76.3236%, Training Loss: 0.5232%\n",
      "Epoch [47/300], Step [158/225], Training Accuracy: 76.3054%, Training Loss: 0.5235%\n",
      "Epoch [47/300], Step [159/225], Training Accuracy: 76.2677%, Training Loss: 0.5240%\n",
      "Epoch [47/300], Step [160/225], Training Accuracy: 76.2891%, Training Loss: 0.5235%\n",
      "Epoch [47/300], Step [161/225], Training Accuracy: 76.2908%, Training Loss: 0.5238%\n",
      "Epoch [47/300], Step [162/225], Training Accuracy: 76.3214%, Training Loss: 0.5234%\n",
      "Epoch [47/300], Step [163/225], Training Accuracy: 76.3708%, Training Loss: 0.5231%\n",
      "Epoch [47/300], Step [164/225], Training Accuracy: 76.3815%, Training Loss: 0.5229%\n",
      "Epoch [47/300], Step [165/225], Training Accuracy: 76.3731%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [166/225], Training Accuracy: 76.3931%, Training Loss: 0.5225%\n",
      "Epoch [47/300], Step [167/225], Training Accuracy: 76.3660%, Training Loss: 0.5230%\n",
      "Epoch [47/300], Step [168/225], Training Accuracy: 76.3579%, Training Loss: 0.5232%\n",
      "Epoch [47/300], Step [169/225], Training Accuracy: 76.3591%, Training Loss: 0.5232%\n",
      "Epoch [47/300], Step [170/225], Training Accuracy: 76.3419%, Training Loss: 0.5235%\n",
      "Epoch [47/300], Step [171/225], Training Accuracy: 76.3615%, Training Loss: 0.5231%\n",
      "Epoch [47/300], Step [172/225], Training Accuracy: 76.3717%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [173/225], Training Accuracy: 76.3277%, Training Loss: 0.5229%\n",
      "Epoch [47/300], Step [174/225], Training Accuracy: 76.3200%, Training Loss: 0.5229%\n",
      "Epoch [47/300], Step [175/225], Training Accuracy: 76.3036%, Training Loss: 0.5229%\n",
      "Epoch [47/300], Step [176/225], Training Accuracy: 76.2962%, Training Loss: 0.5229%\n",
      "Epoch [47/300], Step [177/225], Training Accuracy: 76.2888%, Training Loss: 0.5231%\n",
      "Epoch [47/300], Step [178/225], Training Accuracy: 76.2465%, Training Loss: 0.5236%\n",
      "Epoch [47/300], Step [179/225], Training Accuracy: 76.2919%, Training Loss: 0.5230%\n",
      "Epoch [47/300], Step [180/225], Training Accuracy: 76.2847%, Training Loss: 0.5231%\n",
      "Epoch [47/300], Step [181/225], Training Accuracy: 76.2604%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [182/225], Training Accuracy: 76.2792%, Training Loss: 0.5227%\n",
      "Epoch [47/300], Step [183/225], Training Accuracy: 76.2551%, Training Loss: 0.5228%\n",
      "Epoch [47/300], Step [184/225], Training Accuracy: 76.2738%, Training Loss: 0.5225%\n",
      "Epoch [47/300], Step [185/225], Training Accuracy: 76.3176%, Training Loss: 0.5217%\n",
      "Epoch [47/300], Step [186/225], Training Accuracy: 76.3609%, Training Loss: 0.5211%\n",
      "Epoch [47/300], Step [187/225], Training Accuracy: 76.3285%, Training Loss: 0.5215%\n",
      "Epoch [47/300], Step [188/225], Training Accuracy: 76.3381%, Training Loss: 0.5215%\n",
      "Epoch [47/300], Step [189/225], Training Accuracy: 76.3972%, Training Loss: 0.5208%\n",
      "Epoch [47/300], Step [190/225], Training Accuracy: 76.3898%, Training Loss: 0.5205%\n",
      "Epoch [47/300], Step [191/225], Training Accuracy: 76.3662%, Training Loss: 0.5205%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Step [192/225], Training Accuracy: 76.4242%, Training Loss: 0.5197%\n",
      "Epoch [47/300], Step [193/225], Training Accuracy: 76.4249%, Training Loss: 0.5198%\n",
      "Epoch [47/300], Step [194/225], Training Accuracy: 76.4578%, Training Loss: 0.5194%\n",
      "Epoch [47/300], Step [195/225], Training Accuracy: 76.4824%, Training Loss: 0.5190%\n",
      "Epoch [47/300], Step [196/225], Training Accuracy: 76.4589%, Training Loss: 0.5191%\n",
      "Epoch [47/300], Step [197/225], Training Accuracy: 76.4435%, Training Loss: 0.5190%\n",
      "Epoch [47/300], Step [198/225], Training Accuracy: 76.4915%, Training Loss: 0.5182%\n",
      "Epoch [47/300], Step [199/225], Training Accuracy: 76.5075%, Training Loss: 0.5178%\n",
      "Epoch [47/300], Step [200/225], Training Accuracy: 76.4844%, Training Loss: 0.5182%\n",
      "Epoch [47/300], Step [201/225], Training Accuracy: 76.5003%, Training Loss: 0.5179%\n",
      "Epoch [47/300], Step [202/225], Training Accuracy: 76.5625%, Training Loss: 0.5171%\n",
      "Epoch [47/300], Step [203/225], Training Accuracy: 76.5933%, Training Loss: 0.5167%\n",
      "Epoch [47/300], Step [204/225], Training Accuracy: 76.6085%, Training Loss: 0.5164%\n",
      "Epoch [47/300], Step [205/225], Training Accuracy: 76.5854%, Training Loss: 0.5163%\n",
      "Epoch [47/300], Step [206/225], Training Accuracy: 76.6080%, Training Loss: 0.5162%\n",
      "Epoch [47/300], Step [207/225], Training Accuracy: 76.5927%, Training Loss: 0.5166%\n",
      "Epoch [47/300], Step [208/225], Training Accuracy: 76.6001%, Training Loss: 0.5162%\n",
      "Epoch [47/300], Step [209/225], Training Accuracy: 76.5849%, Training Loss: 0.5161%\n",
      "Epoch [47/300], Step [210/225], Training Accuracy: 76.5923%, Training Loss: 0.5162%\n",
      "Epoch [47/300], Step [211/225], Training Accuracy: 76.5995%, Training Loss: 0.5158%\n",
      "Epoch [47/300], Step [212/225], Training Accuracy: 76.5994%, Training Loss: 0.5162%\n",
      "Epoch [47/300], Step [213/225], Training Accuracy: 76.6212%, Training Loss: 0.5161%\n",
      "Epoch [47/300], Step [214/225], Training Accuracy: 76.6355%, Training Loss: 0.5159%\n",
      "Epoch [47/300], Step [215/225], Training Accuracy: 76.6424%, Training Loss: 0.5159%\n",
      "Epoch [47/300], Step [216/225], Training Accuracy: 76.6276%, Training Loss: 0.5164%\n",
      "Epoch [47/300], Step [217/225], Training Accuracy: 76.6417%, Training Loss: 0.5164%\n",
      "Epoch [47/300], Step [218/225], Training Accuracy: 76.6772%, Training Loss: 0.5163%\n",
      "Epoch [47/300], Step [219/225], Training Accuracy: 76.6624%, Training Loss: 0.5164%\n",
      "Epoch [47/300], Step [220/225], Training Accuracy: 76.6335%, Training Loss: 0.5168%\n",
      "Epoch [47/300], Step [221/225], Training Accuracy: 76.6261%, Training Loss: 0.5171%\n",
      "Epoch [47/300], Step [222/225], Training Accuracy: 76.6751%, Training Loss: 0.5166%\n",
      "Epoch [47/300], Step [223/225], Training Accuracy: 76.6466%, Training Loss: 0.5169%\n",
      "Epoch [47/300], Step [224/225], Training Accuracy: 76.6183%, Training Loss: 0.5174%\n",
      "Epoch [47/300], Step [225/225], Training Accuracy: 76.5981%, Training Loss: 0.5180%\n",
      "Epoch [48/300], Step [1/225], Training Accuracy: 67.1875%, Training Loss: 0.6044%\n",
      "Epoch [48/300], Step [2/225], Training Accuracy: 64.8438%, Training Loss: 0.6606%\n",
      "Epoch [48/300], Step [3/225], Training Accuracy: 67.1875%, Training Loss: 0.6329%\n",
      "Epoch [48/300], Step [4/225], Training Accuracy: 69.5312%, Training Loss: 0.6139%\n",
      "Epoch [48/300], Step [5/225], Training Accuracy: 70.6250%, Training Loss: 0.6087%\n",
      "Epoch [48/300], Step [6/225], Training Accuracy: 72.9167%, Training Loss: 0.5827%\n",
      "Epoch [48/300], Step [7/225], Training Accuracy: 72.9911%, Training Loss: 0.5629%\n",
      "Epoch [48/300], Step [8/225], Training Accuracy: 73.2422%, Training Loss: 0.5651%\n",
      "Epoch [48/300], Step [9/225], Training Accuracy: 73.6111%, Training Loss: 0.5568%\n",
      "Epoch [48/300], Step [10/225], Training Accuracy: 74.0625%, Training Loss: 0.5585%\n",
      "Epoch [48/300], Step [11/225], Training Accuracy: 73.5795%, Training Loss: 0.5577%\n",
      "Epoch [48/300], Step [12/225], Training Accuracy: 73.3073%, Training Loss: 0.5610%\n",
      "Epoch [48/300], Step [13/225], Training Accuracy: 73.5577%, Training Loss: 0.5551%\n",
      "Epoch [48/300], Step [14/225], Training Accuracy: 73.7723%, Training Loss: 0.5512%\n",
      "Epoch [48/300], Step [15/225], Training Accuracy: 73.7500%, Training Loss: 0.5483%\n",
      "Epoch [48/300], Step [16/225], Training Accuracy: 73.9258%, Training Loss: 0.5429%\n",
      "Epoch [48/300], Step [17/225], Training Accuracy: 73.9890%, Training Loss: 0.5397%\n",
      "Epoch [48/300], Step [18/225], Training Accuracy: 74.2188%, Training Loss: 0.5374%\n",
      "Epoch [48/300], Step [19/225], Training Accuracy: 74.3421%, Training Loss: 0.5318%\n",
      "Epoch [48/300], Step [20/225], Training Accuracy: 74.6094%, Training Loss: 0.5296%\n",
      "Epoch [48/300], Step [21/225], Training Accuracy: 74.7768%, Training Loss: 0.5292%\n",
      "Epoch [48/300], Step [22/225], Training Accuracy: 74.6449%, Training Loss: 0.5309%\n",
      "Epoch [48/300], Step [23/225], Training Accuracy: 74.7962%, Training Loss: 0.5280%\n",
      "Epoch [48/300], Step [24/225], Training Accuracy: 74.5443%, Training Loss: 0.5328%\n",
      "Epoch [48/300], Step [25/225], Training Accuracy: 74.4375%, Training Loss: 0.5328%\n",
      "Epoch [48/300], Step [26/225], Training Accuracy: 74.4591%, Training Loss: 0.5335%\n",
      "Epoch [48/300], Step [27/225], Training Accuracy: 74.3056%, Training Loss: 0.5358%\n",
      "Epoch [48/300], Step [28/225], Training Accuracy: 74.5536%, Training Loss: 0.5343%\n",
      "Epoch [48/300], Step [29/225], Training Accuracy: 74.5690%, Training Loss: 0.5352%\n",
      "Epoch [48/300], Step [30/225], Training Accuracy: 74.8438%, Training Loss: 0.5331%\n",
      "Epoch [48/300], Step [31/225], Training Accuracy: 74.6472%, Training Loss: 0.5409%\n",
      "Epoch [48/300], Step [32/225], Training Accuracy: 74.5117%, Training Loss: 0.5411%\n",
      "Epoch [48/300], Step [33/225], Training Accuracy: 74.5739%, Training Loss: 0.5396%\n",
      "Epoch [48/300], Step [34/225], Training Accuracy: 74.5864%, Training Loss: 0.5424%\n",
      "Epoch [48/300], Step [35/225], Training Accuracy: 74.5982%, Training Loss: 0.5427%\n",
      "Epoch [48/300], Step [36/225], Training Accuracy: 74.9132%, Training Loss: 0.5389%\n",
      "Epoch [48/300], Step [37/225], Training Accuracy: 75.0422%, Training Loss: 0.5384%\n",
      "Epoch [48/300], Step [38/225], Training Accuracy: 74.9589%, Training Loss: 0.5387%\n",
      "Epoch [48/300], Step [39/225], Training Accuracy: 75.0401%, Training Loss: 0.5376%\n",
      "Epoch [48/300], Step [40/225], Training Accuracy: 75.1172%, Training Loss: 0.5358%\n",
      "Epoch [48/300], Step [41/225], Training Accuracy: 75.0000%, Training Loss: 0.5381%\n",
      "Epoch [48/300], Step [42/225], Training Accuracy: 75.1116%, Training Loss: 0.5365%\n",
      "Epoch [48/300], Step [43/225], Training Accuracy: 75.1090%, Training Loss: 0.5365%\n",
      "Epoch [48/300], Step [44/225], Training Accuracy: 75.1776%, Training Loss: 0.5373%\n",
      "Epoch [48/300], Step [45/225], Training Accuracy: 75.3125%, Training Loss: 0.5358%\n",
      "Epoch [48/300], Step [46/225], Training Accuracy: 75.2378%, Training Loss: 0.5344%\n",
      "Epoch [48/300], Step [47/225], Training Accuracy: 74.9668%, Training Loss: 0.5386%\n",
      "Epoch [48/300], Step [48/225], Training Accuracy: 74.9349%, Training Loss: 0.5398%\n",
      "Epoch [48/300], Step [49/225], Training Accuracy: 74.9362%, Training Loss: 0.5409%\n",
      "Epoch [48/300], Step [50/225], Training Accuracy: 75.1562%, Training Loss: 0.5382%\n",
      "Epoch [48/300], Step [51/225], Training Accuracy: 75.2145%, Training Loss: 0.5375%\n",
      "Epoch [48/300], Step [52/225], Training Accuracy: 75.3906%, Training Loss: 0.5356%\n",
      "Epoch [48/300], Step [53/225], Training Accuracy: 75.4422%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [54/225], Training Accuracy: 75.4051%, Training Loss: 0.5372%\n",
      "Epoch [48/300], Step [55/225], Training Accuracy: 75.3125%, Training Loss: 0.5378%\n",
      "Epoch [48/300], Step [56/225], Training Accuracy: 75.2790%, Training Loss: 0.5377%\n",
      "Epoch [48/300], Step [57/225], Training Accuracy: 75.1919%, Training Loss: 0.5384%\n",
      "Epoch [48/300], Step [58/225], Training Accuracy: 75.3502%, Training Loss: 0.5379%\n",
      "Epoch [48/300], Step [59/225], Training Accuracy: 75.5032%, Training Loss: 0.5365%\n",
      "Epoch [48/300], Step [60/225], Training Accuracy: 75.6250%, Training Loss: 0.5353%\n",
      "Epoch [48/300], Step [61/225], Training Accuracy: 75.4611%, Training Loss: 0.5360%\n",
      "Epoch [48/300], Step [62/225], Training Accuracy: 75.4032%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [63/225], Training Accuracy: 75.3472%, Training Loss: 0.5370%\n",
      "Epoch [48/300], Step [64/225], Training Accuracy: 75.3906%, Training Loss: 0.5355%\n",
      "Epoch [48/300], Step [65/225], Training Accuracy: 75.4567%, Training Loss: 0.5343%\n",
      "Epoch [48/300], Step [66/225], Training Accuracy: 75.4261%, Training Loss: 0.5348%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Step [67/225], Training Accuracy: 75.5131%, Training Loss: 0.5339%\n",
      "Epoch [48/300], Step [68/225], Training Accuracy: 75.4136%, Training Loss: 0.5352%\n",
      "Epoch [48/300], Step [69/225], Training Accuracy: 75.3170%, Training Loss: 0.5371%\n",
      "Epoch [48/300], Step [70/225], Training Accuracy: 75.3348%, Training Loss: 0.5364%\n",
      "Epoch [48/300], Step [71/225], Training Accuracy: 75.4181%, Training Loss: 0.5350%\n",
      "Epoch [48/300], Step [72/225], Training Accuracy: 75.5208%, Training Loss: 0.5342%\n",
      "Epoch [48/300], Step [73/225], Training Accuracy: 75.5137%, Training Loss: 0.5348%\n",
      "Epoch [48/300], Step [74/225], Training Accuracy: 75.4645%, Training Loss: 0.5352%\n",
      "Epoch [48/300], Step [75/225], Training Accuracy: 75.4375%, Training Loss: 0.5352%\n",
      "Epoch [48/300], Step [76/225], Training Accuracy: 75.3495%, Training Loss: 0.5366%\n",
      "Epoch [48/300], Step [77/225], Training Accuracy: 75.3450%, Training Loss: 0.5359%\n",
      "Epoch [48/300], Step [78/225], Training Accuracy: 75.2204%, Training Loss: 0.5366%\n",
      "Epoch [48/300], Step [79/225], Training Accuracy: 75.2571%, Training Loss: 0.5360%\n",
      "Epoch [48/300], Step [80/225], Training Accuracy: 75.2539%, Training Loss: 0.5372%\n",
      "Epoch [48/300], Step [81/225], Training Accuracy: 75.3472%, Training Loss: 0.5355%\n",
      "Epoch [48/300], Step [82/225], Training Accuracy: 75.3239%, Training Loss: 0.5353%\n",
      "Epoch [48/300], Step [83/225], Training Accuracy: 75.3765%, Training Loss: 0.5344%\n",
      "Epoch [48/300], Step [84/225], Training Accuracy: 75.3720%, Training Loss: 0.5339%\n",
      "Epoch [48/300], Step [85/225], Training Accuracy: 75.4228%, Training Loss: 0.5325%\n",
      "Epoch [48/300], Step [86/225], Training Accuracy: 75.3452%, Training Loss: 0.5333%\n",
      "Epoch [48/300], Step [87/225], Training Accuracy: 75.3592%, Training Loss: 0.5329%\n",
      "Epoch [48/300], Step [88/225], Training Accuracy: 75.2841%, Training Loss: 0.5350%\n",
      "Epoch [48/300], Step [89/225], Training Accuracy: 75.1931%, Training Loss: 0.5361%\n",
      "Epoch [48/300], Step [90/225], Training Accuracy: 75.0347%, Training Loss: 0.5378%\n",
      "Epoch [48/300], Step [91/225], Training Accuracy: 75.0172%, Training Loss: 0.5373%\n",
      "Epoch [48/300], Step [92/225], Training Accuracy: 74.8981%, Training Loss: 0.5388%\n",
      "Epoch [48/300], Step [93/225], Training Accuracy: 74.9328%, Training Loss: 0.5378%\n",
      "Epoch [48/300], Step [94/225], Training Accuracy: 74.9501%, Training Loss: 0.5369%\n",
      "Epoch [48/300], Step [95/225], Training Accuracy: 75.0329%, Training Loss: 0.5358%\n",
      "Epoch [48/300], Step [96/225], Training Accuracy: 75.0977%, Training Loss: 0.5345%\n",
      "Epoch [48/300], Step [97/225], Training Accuracy: 75.1289%, Training Loss: 0.5340%\n",
      "Epoch [48/300], Step [98/225], Training Accuracy: 75.0000%, Training Loss: 0.5355%\n",
      "Epoch [48/300], Step [99/225], Training Accuracy: 75.0158%, Training Loss: 0.5357%\n",
      "Epoch [48/300], Step [100/225], Training Accuracy: 75.0156%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [101/225], Training Accuracy: 75.0309%, Training Loss: 0.5355%\n",
      "Epoch [48/300], Step [102/225], Training Accuracy: 74.9694%, Training Loss: 0.5362%\n",
      "Epoch [48/300], Step [103/225], Training Accuracy: 75.0758%, Training Loss: 0.5348%\n",
      "Epoch [48/300], Step [104/225], Training Accuracy: 75.0451%, Training Loss: 0.5355%\n",
      "Epoch [48/300], Step [105/225], Training Accuracy: 75.0595%, Training Loss: 0.5350%\n",
      "Epoch [48/300], Step [106/225], Training Accuracy: 75.1327%, Training Loss: 0.5349%\n",
      "Epoch [48/300], Step [107/225], Training Accuracy: 75.1022%, Training Loss: 0.5360%\n",
      "Epoch [48/300], Step [108/225], Training Accuracy: 75.1013%, Training Loss: 0.5361%\n",
      "Epoch [48/300], Step [109/225], Training Accuracy: 75.0860%, Training Loss: 0.5363%\n",
      "Epoch [48/300], Step [110/225], Training Accuracy: 75.1136%, Training Loss: 0.5359%\n",
      "Epoch [48/300], Step [111/225], Training Accuracy: 75.0422%, Training Loss: 0.5365%\n",
      "Epoch [48/300], Step [112/225], Training Accuracy: 75.0419%, Training Loss: 0.5368%\n",
      "Epoch [48/300], Step [113/225], Training Accuracy: 75.1106%, Training Loss: 0.5360%\n",
      "Epoch [48/300], Step [114/225], Training Accuracy: 75.1096%, Training Loss: 0.5359%\n",
      "Epoch [48/300], Step [115/225], Training Accuracy: 75.2310%, Training Loss: 0.5347%\n",
      "Epoch [48/300], Step [116/225], Training Accuracy: 75.2559%, Training Loss: 0.5348%\n",
      "Epoch [48/300], Step [117/225], Training Accuracy: 75.2671%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [118/225], Training Accuracy: 75.2516%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [119/225], Training Accuracy: 75.3020%, Training Loss: 0.5344%\n",
      "Epoch [48/300], Step [120/225], Training Accuracy: 75.3255%, Training Loss: 0.5340%\n",
      "Epoch [48/300], Step [121/225], Training Accuracy: 75.2195%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [122/225], Training Accuracy: 75.2049%, Training Loss: 0.5357%\n",
      "Epoch [48/300], Step [123/225], Training Accuracy: 75.2287%, Training Loss: 0.5352%\n",
      "Epoch [48/300], Step [124/225], Training Accuracy: 75.2016%, Training Loss: 0.5353%\n",
      "Epoch [48/300], Step [125/225], Training Accuracy: 75.2500%, Training Loss: 0.5350%\n",
      "Epoch [48/300], Step [126/225], Training Accuracy: 75.2852%, Training Loss: 0.5345%\n",
      "Epoch [48/300], Step [127/225], Training Accuracy: 75.2707%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [128/225], Training Accuracy: 75.1709%, Training Loss: 0.5365%\n",
      "Epoch [48/300], Step [129/225], Training Accuracy: 75.1575%, Training Loss: 0.5367%\n",
      "Epoch [48/300], Step [130/225], Training Accuracy: 75.1923%, Training Loss: 0.5360%\n",
      "Epoch [48/300], Step [131/225], Training Accuracy: 75.2385%, Training Loss: 0.5351%\n",
      "Epoch [48/300], Step [132/225], Training Accuracy: 75.2249%, Training Loss: 0.5356%\n",
      "Epoch [48/300], Step [133/225], Training Accuracy: 75.2232%, Training Loss: 0.5355%\n",
      "Epoch [48/300], Step [134/225], Training Accuracy: 75.2099%, Training Loss: 0.5364%\n",
      "Epoch [48/300], Step [135/225], Training Accuracy: 75.2083%, Training Loss: 0.5363%\n",
      "Epoch [48/300], Step [136/225], Training Accuracy: 75.2757%, Training Loss: 0.5359%\n",
      "Epoch [48/300], Step [137/225], Training Accuracy: 75.2281%, Training Loss: 0.5365%\n",
      "Epoch [48/300], Step [138/225], Training Accuracy: 75.2831%, Training Loss: 0.5354%\n",
      "Epoch [48/300], Step [139/225], Training Accuracy: 75.3035%, Training Loss: 0.5349%\n",
      "Epoch [48/300], Step [140/225], Training Accuracy: 75.3683%, Training Loss: 0.5345%\n",
      "Epoch [48/300], Step [141/225], Training Accuracy: 75.4322%, Training Loss: 0.5337%\n",
      "Epoch [48/300], Step [142/225], Training Accuracy: 75.4511%, Training Loss: 0.5333%\n",
      "Epoch [48/300], Step [143/225], Training Accuracy: 75.3824%, Training Loss: 0.5335%\n",
      "Epoch [48/300], Step [144/225], Training Accuracy: 75.4123%, Training Loss: 0.5329%\n",
      "Epoch [48/300], Step [145/225], Training Accuracy: 75.4741%, Training Loss: 0.5329%\n",
      "Epoch [48/300], Step [146/225], Training Accuracy: 75.4709%, Training Loss: 0.5328%\n",
      "Epoch [48/300], Step [147/225], Training Accuracy: 75.4571%, Training Loss: 0.5332%\n",
      "Epoch [48/300], Step [148/225], Training Accuracy: 75.4962%, Training Loss: 0.5325%\n",
      "Epoch [48/300], Step [149/225], Training Accuracy: 75.5138%, Training Loss: 0.5322%\n",
      "Epoch [48/300], Step [150/225], Training Accuracy: 75.5417%, Training Loss: 0.5318%\n",
      "Epoch [48/300], Step [151/225], Training Accuracy: 75.5795%, Training Loss: 0.5308%\n",
      "Epoch [48/300], Step [152/225], Training Accuracy: 75.5448%, Training Loss: 0.5315%\n",
      "Epoch [48/300], Step [153/225], Training Accuracy: 75.5821%, Training Loss: 0.5310%\n",
      "Epoch [48/300], Step [154/225], Training Accuracy: 75.5986%, Training Loss: 0.5302%\n",
      "Epoch [48/300], Step [155/225], Training Accuracy: 75.5847%, Training Loss: 0.5302%\n",
      "Epoch [48/300], Step [156/225], Training Accuracy: 75.5709%, Training Loss: 0.5305%\n",
      "Epoch [48/300], Step [157/225], Training Accuracy: 75.5673%, Training Loss: 0.5306%\n",
      "Epoch [48/300], Step [158/225], Training Accuracy: 75.5538%, Training Loss: 0.5312%\n",
      "Epoch [48/300], Step [159/225], Training Accuracy: 75.5307%, Training Loss: 0.5320%\n",
      "Epoch [48/300], Step [160/225], Training Accuracy: 75.6055%, Training Loss: 0.5308%\n",
      "Epoch [48/300], Step [161/225], Training Accuracy: 75.6017%, Training Loss: 0.5306%\n",
      "Epoch [48/300], Step [162/225], Training Accuracy: 75.6848%, Training Loss: 0.5296%\n",
      "Epoch [48/300], Step [163/225], Training Accuracy: 75.6614%, Training Loss: 0.5296%\n",
      "Epoch [48/300], Step [164/225], Training Accuracy: 75.7241%, Training Loss: 0.5295%\n",
      "Epoch [48/300], Step [165/225], Training Accuracy: 75.7292%, Training Loss: 0.5295%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Step [166/225], Training Accuracy: 75.7624%, Training Loss: 0.5294%\n",
      "Epoch [48/300], Step [167/225], Training Accuracy: 75.7579%, Training Loss: 0.5298%\n",
      "Epoch [48/300], Step [168/225], Training Accuracy: 75.7812%, Training Loss: 0.5298%\n",
      "Epoch [48/300], Step [169/225], Training Accuracy: 75.7859%, Training Loss: 0.5294%\n",
      "Epoch [48/300], Step [170/225], Training Accuracy: 75.7904%, Training Loss: 0.5293%\n",
      "Epoch [48/300], Step [171/225], Training Accuracy: 75.8041%, Training Loss: 0.5288%\n",
      "Epoch [48/300], Step [172/225], Training Accuracy: 75.8085%, Training Loss: 0.5287%\n",
      "Epoch [48/300], Step [173/225], Training Accuracy: 75.8129%, Training Loss: 0.5283%\n",
      "Epoch [48/300], Step [174/225], Training Accuracy: 75.8082%, Training Loss: 0.5290%\n",
      "Epoch [48/300], Step [175/225], Training Accuracy: 75.8304%, Training Loss: 0.5286%\n",
      "Epoch [48/300], Step [176/225], Training Accuracy: 75.8256%, Training Loss: 0.5286%\n",
      "Epoch [48/300], Step [177/225], Training Accuracy: 75.8563%, Training Loss: 0.5283%\n",
      "Epoch [48/300], Step [178/225], Training Accuracy: 75.8690%, Training Loss: 0.5282%\n",
      "Epoch [48/300], Step [179/225], Training Accuracy: 75.9340%, Training Loss: 0.5273%\n",
      "Epoch [48/300], Step [180/225], Training Accuracy: 75.9375%, Training Loss: 0.5273%\n",
      "Epoch [48/300], Step [181/225], Training Accuracy: 75.9151%, Training Loss: 0.5274%\n",
      "Epoch [48/300], Step [182/225], Training Accuracy: 75.9272%, Training Loss: 0.5274%\n",
      "Epoch [48/300], Step [183/225], Training Accuracy: 75.9307%, Training Loss: 0.5274%\n",
      "Epoch [48/300], Step [184/225], Training Accuracy: 75.9681%, Training Loss: 0.5267%\n",
      "Epoch [48/300], Step [185/225], Training Accuracy: 75.9966%, Training Loss: 0.5265%\n",
      "Epoch [48/300], Step [186/225], Training Accuracy: 76.0333%, Training Loss: 0.5261%\n",
      "Epoch [48/300], Step [187/225], Training Accuracy: 76.0110%, Training Loss: 0.5269%\n",
      "Epoch [48/300], Step [188/225], Training Accuracy: 76.0389%, Training Loss: 0.5267%\n",
      "Epoch [48/300], Step [189/225], Training Accuracy: 76.0334%, Training Loss: 0.5266%\n",
      "Epoch [48/300], Step [190/225], Training Accuracy: 76.0444%, Training Loss: 0.5266%\n",
      "Epoch [48/300], Step [191/225], Training Accuracy: 76.0389%, Training Loss: 0.5262%\n",
      "Epoch [48/300], Step [192/225], Training Accuracy: 76.0905%, Training Loss: 0.5255%\n",
      "Epoch [48/300], Step [193/225], Training Accuracy: 76.1172%, Training Loss: 0.5253%\n",
      "Epoch [48/300], Step [194/225], Training Accuracy: 76.1034%, Training Loss: 0.5253%\n",
      "Epoch [48/300], Step [195/225], Training Accuracy: 76.1058%, Training Loss: 0.5257%\n",
      "Epoch [48/300], Step [196/225], Training Accuracy: 76.0124%, Training Loss: 0.5263%\n",
      "Epoch [48/300], Step [197/225], Training Accuracy: 76.0470%, Training Loss: 0.5261%\n",
      "Epoch [48/300], Step [198/225], Training Accuracy: 76.1048%, Training Loss: 0.5251%\n",
      "Epoch [48/300], Step [199/225], Training Accuracy: 76.1149%, Training Loss: 0.5246%\n",
      "Epoch [48/300], Step [200/225], Training Accuracy: 76.1094%, Training Loss: 0.5246%\n",
      "Epoch [48/300], Step [201/225], Training Accuracy: 76.1116%, Training Loss: 0.5246%\n",
      "Epoch [48/300], Step [202/225], Training Accuracy: 76.1448%, Training Loss: 0.5244%\n",
      "Epoch [48/300], Step [203/225], Training Accuracy: 76.1546%, Training Loss: 0.5242%\n",
      "Epoch [48/300], Step [204/225], Training Accuracy: 76.1642%, Training Loss: 0.5241%\n",
      "Epoch [48/300], Step [205/225], Training Accuracy: 76.2043%, Training Loss: 0.5236%\n",
      "Epoch [48/300], Step [206/225], Training Accuracy: 76.2060%, Training Loss: 0.5237%\n",
      "Epoch [48/300], Step [207/225], Training Accuracy: 76.2002%, Training Loss: 0.5240%\n",
      "Epoch [48/300], Step [208/225], Training Accuracy: 76.2320%, Training Loss: 0.5239%\n",
      "Epoch [48/300], Step [209/225], Training Accuracy: 76.2336%, Training Loss: 0.5236%\n",
      "Epoch [48/300], Step [210/225], Training Accuracy: 76.2426%, Training Loss: 0.5236%\n",
      "Epoch [48/300], Step [211/225], Training Accuracy: 76.3033%, Training Loss: 0.5231%\n",
      "Epoch [48/300], Step [212/225], Training Accuracy: 76.2972%, Training Loss: 0.5235%\n",
      "Epoch [48/300], Step [213/225], Training Accuracy: 76.3204%, Training Loss: 0.5235%\n",
      "Epoch [48/300], Step [214/225], Training Accuracy: 76.3435%, Training Loss: 0.5232%\n",
      "Epoch [48/300], Step [215/225], Training Accuracy: 76.3299%, Training Loss: 0.5233%\n",
      "Epoch [48/300], Step [216/225], Training Accuracy: 76.3238%, Training Loss: 0.5235%\n",
      "Epoch [48/300], Step [217/225], Training Accuracy: 76.3249%, Training Loss: 0.5233%\n",
      "Epoch [48/300], Step [218/225], Training Accuracy: 76.3260%, Training Loss: 0.5234%\n",
      "Epoch [48/300], Step [219/225], Training Accuracy: 76.3271%, Training Loss: 0.5233%\n",
      "Epoch [48/300], Step [220/225], Training Accuracy: 76.3423%, Training Loss: 0.5230%\n",
      "Epoch [48/300], Step [221/225], Training Accuracy: 76.3009%, Training Loss: 0.5237%\n",
      "Epoch [48/300], Step [222/225], Training Accuracy: 76.3443%, Training Loss: 0.5233%\n",
      "Epoch [48/300], Step [223/225], Training Accuracy: 76.3033%, Training Loss: 0.5241%\n",
      "Epoch [48/300], Step [224/225], Training Accuracy: 76.3044%, Training Loss: 0.5241%\n",
      "Epoch [48/300], Step [225/225], Training Accuracy: 76.3063%, Training Loss: 0.5238%\n",
      "Epoch [49/300], Step [1/225], Training Accuracy: 76.5625%, Training Loss: 0.5049%\n",
      "Epoch [49/300], Step [2/225], Training Accuracy: 78.9062%, Training Loss: 0.5180%\n",
      "Epoch [49/300], Step [3/225], Training Accuracy: 78.1250%, Training Loss: 0.5168%\n",
      "Epoch [49/300], Step [4/225], Training Accuracy: 77.3438%, Training Loss: 0.5233%\n",
      "Epoch [49/300], Step [5/225], Training Accuracy: 76.8750%, Training Loss: 0.5254%\n",
      "Epoch [49/300], Step [6/225], Training Accuracy: 77.3438%, Training Loss: 0.5157%\n",
      "Epoch [49/300], Step [7/225], Training Accuracy: 77.4554%, Training Loss: 0.5098%\n",
      "Epoch [49/300], Step [8/225], Training Accuracy: 77.7344%, Training Loss: 0.5095%\n",
      "Epoch [49/300], Step [9/225], Training Accuracy: 76.9097%, Training Loss: 0.5202%\n",
      "Epoch [49/300], Step [10/225], Training Accuracy: 76.4062%, Training Loss: 0.5259%\n",
      "Epoch [49/300], Step [11/225], Training Accuracy: 77.4148%, Training Loss: 0.5138%\n",
      "Epoch [49/300], Step [12/225], Training Accuracy: 76.9531%, Training Loss: 0.5150%\n",
      "Epoch [49/300], Step [13/225], Training Accuracy: 77.7644%, Training Loss: 0.5045%\n",
      "Epoch [49/300], Step [14/225], Training Accuracy: 78.3482%, Training Loss: 0.4978%\n",
      "Epoch [49/300], Step [15/225], Training Accuracy: 78.1250%, Training Loss: 0.4987%\n",
      "Epoch [49/300], Step [16/225], Training Accuracy: 78.1250%, Training Loss: 0.4992%\n",
      "Epoch [49/300], Step [17/225], Training Accuracy: 78.5846%, Training Loss: 0.4917%\n",
      "Epoch [49/300], Step [18/225], Training Accuracy: 78.6458%, Training Loss: 0.4969%\n",
      "Epoch [49/300], Step [19/225], Training Accuracy: 78.5362%, Training Loss: 0.4970%\n",
      "Epoch [49/300], Step [20/225], Training Accuracy: 78.7500%, Training Loss: 0.4947%\n",
      "Epoch [49/300], Step [21/225], Training Accuracy: 79.1667%, Training Loss: 0.4897%\n",
      "Epoch [49/300], Step [22/225], Training Accuracy: 78.6932%, Training Loss: 0.5017%\n",
      "Epoch [49/300], Step [23/225], Training Accuracy: 78.9402%, Training Loss: 0.4980%\n",
      "Epoch [49/300], Step [24/225], Training Accuracy: 78.7760%, Training Loss: 0.5018%\n",
      "Epoch [49/300], Step [25/225], Training Accuracy: 78.8750%, Training Loss: 0.5015%\n",
      "Epoch [49/300], Step [26/225], Training Accuracy: 78.4856%, Training Loss: 0.5072%\n",
      "Epoch [49/300], Step [27/225], Training Accuracy: 78.5301%, Training Loss: 0.5040%\n",
      "Epoch [49/300], Step [28/225], Training Accuracy: 78.7946%, Training Loss: 0.4981%\n",
      "Epoch [49/300], Step [29/225], Training Accuracy: 78.6638%, Training Loss: 0.5002%\n",
      "Epoch [49/300], Step [30/225], Training Accuracy: 78.6979%, Training Loss: 0.4984%\n",
      "Epoch [49/300], Step [31/225], Training Accuracy: 78.4274%, Training Loss: 0.5080%\n",
      "Epoch [49/300], Step [32/225], Training Accuracy: 78.3203%, Training Loss: 0.5082%\n",
      "Epoch [49/300], Step [33/225], Training Accuracy: 78.1723%, Training Loss: 0.5076%\n",
      "Epoch [49/300], Step [34/225], Training Accuracy: 77.9412%, Training Loss: 0.5114%\n",
      "Epoch [49/300], Step [35/225], Training Accuracy: 77.9018%, Training Loss: 0.5102%\n",
      "Epoch [49/300], Step [36/225], Training Accuracy: 77.9080%, Training Loss: 0.5088%\n",
      "Epoch [49/300], Step [37/225], Training Accuracy: 77.8294%, Training Loss: 0.5119%\n",
      "Epoch [49/300], Step [38/225], Training Accuracy: 77.6316%, Training Loss: 0.5124%\n",
      "Epoch [49/300], Step [39/225], Training Accuracy: 77.7644%, Training Loss: 0.5107%\n",
      "Epoch [49/300], Step [40/225], Training Accuracy: 77.8125%, Training Loss: 0.5094%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Step [41/225], Training Accuracy: 77.5915%, Training Loss: 0.5108%\n",
      "Epoch [49/300], Step [42/225], Training Accuracy: 77.6414%, Training Loss: 0.5097%\n",
      "Epoch [49/300], Step [43/225], Training Accuracy: 77.5436%, Training Loss: 0.5117%\n",
      "Epoch [49/300], Step [44/225], Training Accuracy: 77.6278%, Training Loss: 0.5093%\n",
      "Epoch [49/300], Step [45/225], Training Accuracy: 77.5000%, Training Loss: 0.5102%\n",
      "Epoch [49/300], Step [46/225], Training Accuracy: 77.4117%, Training Loss: 0.5106%\n",
      "Epoch [49/300], Step [47/225], Training Accuracy: 77.3271%, Training Loss: 0.5115%\n",
      "Epoch [49/300], Step [48/225], Training Accuracy: 77.3112%, Training Loss: 0.5109%\n",
      "Epoch [49/300], Step [49/225], Training Accuracy: 77.2640%, Training Loss: 0.5123%\n",
      "Epoch [49/300], Step [50/225], Training Accuracy: 77.1875%, Training Loss: 0.5134%\n",
      "Epoch [49/300], Step [51/225], Training Accuracy: 77.1446%, Training Loss: 0.5132%\n",
      "Epoch [49/300], Step [52/225], Training Accuracy: 77.1935%, Training Loss: 0.5117%\n",
      "Epoch [49/300], Step [53/225], Training Accuracy: 77.2111%, Training Loss: 0.5129%\n",
      "Epoch [49/300], Step [54/225], Training Accuracy: 77.0255%, Training Loss: 0.5139%\n",
      "Epoch [49/300], Step [55/225], Training Accuracy: 76.7898%, Training Loss: 0.5159%\n",
      "Epoch [49/300], Step [56/225], Training Accuracy: 76.7857%, Training Loss: 0.5160%\n",
      "Epoch [49/300], Step [57/225], Training Accuracy: 76.7818%, Training Loss: 0.5154%\n",
      "Epoch [49/300], Step [58/225], Training Accuracy: 76.8319%, Training Loss: 0.5158%\n",
      "Epoch [49/300], Step [59/225], Training Accuracy: 76.9597%, Training Loss: 0.5144%\n",
      "Epoch [49/300], Step [60/225], Training Accuracy: 77.0052%, Training Loss: 0.5141%\n",
      "Epoch [49/300], Step [61/225], Training Accuracy: 76.9980%, Training Loss: 0.5146%\n",
      "Epoch [49/300], Step [62/225], Training Accuracy: 76.9909%, Training Loss: 0.5161%\n",
      "Epoch [49/300], Step [63/225], Training Accuracy: 76.8849%, Training Loss: 0.5168%\n",
      "Epoch [49/300], Step [64/225], Training Accuracy: 76.9775%, Training Loss: 0.5160%\n",
      "Epoch [49/300], Step [65/225], Training Accuracy: 77.0192%, Training Loss: 0.5160%\n",
      "Epoch [49/300], Step [66/225], Training Accuracy: 76.9886%, Training Loss: 0.5160%\n",
      "Epoch [49/300], Step [67/225], Training Accuracy: 76.9823%, Training Loss: 0.5162%\n",
      "Epoch [49/300], Step [68/225], Training Accuracy: 76.8382%, Training Loss: 0.5182%\n",
      "Epoch [49/300], Step [69/225], Training Accuracy: 76.8342%, Training Loss: 0.5185%\n",
      "Epoch [49/300], Step [70/225], Training Accuracy: 76.7188%, Training Loss: 0.5198%\n",
      "Epoch [49/300], Step [71/225], Training Accuracy: 76.6945%, Training Loss: 0.5197%\n",
      "Epoch [49/300], Step [72/225], Training Accuracy: 76.7361%, Training Loss: 0.5186%\n",
      "Epoch [49/300], Step [73/225], Training Accuracy: 76.7123%, Training Loss: 0.5195%\n",
      "Epoch [49/300], Step [74/225], Training Accuracy: 76.7525%, Training Loss: 0.5194%\n",
      "Epoch [49/300], Step [75/225], Training Accuracy: 76.7292%, Training Loss: 0.5204%\n",
      "Epoch [49/300], Step [76/225], Training Accuracy: 76.6447%, Training Loss: 0.5220%\n",
      "Epoch [49/300], Step [77/225], Training Accuracy: 76.6640%, Training Loss: 0.5217%\n",
      "Epoch [49/300], Step [78/225], Training Accuracy: 76.6226%, Training Loss: 0.5209%\n",
      "Epoch [49/300], Step [79/225], Training Accuracy: 76.6218%, Training Loss: 0.5203%\n",
      "Epoch [49/300], Step [80/225], Training Accuracy: 76.6211%, Training Loss: 0.5192%\n",
      "Epoch [49/300], Step [81/225], Training Accuracy: 76.6397%, Training Loss: 0.5187%\n",
      "Epoch [49/300], Step [82/225], Training Accuracy: 76.6578%, Training Loss: 0.5188%\n",
      "Epoch [49/300], Step [83/225], Training Accuracy: 76.6378%, Training Loss: 0.5196%\n",
      "Epoch [49/300], Step [84/225], Training Accuracy: 76.5997%, Training Loss: 0.5194%\n",
      "Epoch [49/300], Step [85/225], Training Accuracy: 76.7096%, Training Loss: 0.5181%\n",
      "Epoch [49/300], Step [86/225], Training Accuracy: 76.7260%, Training Loss: 0.5174%\n",
      "Epoch [49/300], Step [87/225], Training Accuracy: 76.7062%, Training Loss: 0.5175%\n",
      "Epoch [49/300], Step [88/225], Training Accuracy: 76.6513%, Training Loss: 0.5185%\n",
      "Epoch [49/300], Step [89/225], Training Accuracy: 76.5801%, Training Loss: 0.5198%\n",
      "Epoch [49/300], Step [90/225], Training Accuracy: 76.5799%, Training Loss: 0.5205%\n",
      "Epoch [49/300], Step [91/225], Training Accuracy: 76.5282%, Training Loss: 0.5209%\n",
      "Epoch [49/300], Step [92/225], Training Accuracy: 76.4436%, Training Loss: 0.5211%\n",
      "Epoch [49/300], Step [93/225], Training Accuracy: 76.5121%, Training Loss: 0.5210%\n",
      "Epoch [49/300], Step [94/225], Training Accuracy: 76.5957%, Training Loss: 0.5196%\n",
      "Epoch [49/300], Step [95/225], Training Accuracy: 76.5789%, Training Loss: 0.5206%\n",
      "Epoch [49/300], Step [96/225], Training Accuracy: 76.5299%, Training Loss: 0.5210%\n",
      "Epoch [49/300], Step [97/225], Training Accuracy: 76.5625%, Training Loss: 0.5203%\n",
      "Epoch [49/300], Step [98/225], Training Accuracy: 76.5306%, Training Loss: 0.5213%\n",
      "Epoch [49/300], Step [99/225], Training Accuracy: 76.5467%, Training Loss: 0.5210%\n",
      "Epoch [49/300], Step [100/225], Training Accuracy: 76.4531%, Training Loss: 0.5227%\n",
      "Epoch [49/300], Step [101/225], Training Accuracy: 76.3459%, Training Loss: 0.5246%\n",
      "Epoch [49/300], Step [102/225], Training Accuracy: 76.2408%, Training Loss: 0.5258%\n",
      "Epoch [49/300], Step [103/225], Training Accuracy: 76.2743%, Training Loss: 0.5256%\n",
      "Epoch [49/300], Step [104/225], Training Accuracy: 76.2169%, Training Loss: 0.5266%\n",
      "Epoch [49/300], Step [105/225], Training Accuracy: 76.2351%, Training Loss: 0.5258%\n",
      "Epoch [49/300], Step [106/225], Training Accuracy: 76.1645%, Training Loss: 0.5269%\n",
      "Epoch [49/300], Step [107/225], Training Accuracy: 76.1536%, Training Loss: 0.5277%\n",
      "Epoch [49/300], Step [108/225], Training Accuracy: 76.1574%, Training Loss: 0.5277%\n",
      "Epoch [49/300], Step [109/225], Training Accuracy: 76.1181%, Training Loss: 0.5279%\n",
      "Epoch [49/300], Step [110/225], Training Accuracy: 76.1080%, Training Loss: 0.5272%\n",
      "Epoch [49/300], Step [111/225], Training Accuracy: 76.1120%, Training Loss: 0.5267%\n",
      "Epoch [49/300], Step [112/225], Training Accuracy: 76.1300%, Training Loss: 0.5264%\n",
      "Epoch [49/300], Step [113/225], Training Accuracy: 76.1200%, Training Loss: 0.5260%\n",
      "Epoch [49/300], Step [114/225], Training Accuracy: 76.1650%, Training Loss: 0.5256%\n",
      "Epoch [49/300], Step [115/225], Training Accuracy: 76.2364%, Training Loss: 0.5247%\n",
      "Epoch [49/300], Step [116/225], Training Accuracy: 76.2527%, Training Loss: 0.5238%\n",
      "Epoch [49/300], Step [117/225], Training Accuracy: 76.1084%, Training Loss: 0.5248%\n",
      "Epoch [49/300], Step [118/225], Training Accuracy: 76.1255%, Training Loss: 0.5247%\n",
      "Epoch [49/300], Step [119/225], Training Accuracy: 76.1029%, Training Loss: 0.5243%\n",
      "Epoch [49/300], Step [120/225], Training Accuracy: 76.1328%, Training Loss: 0.5238%\n",
      "Epoch [49/300], Step [121/225], Training Accuracy: 76.0976%, Training Loss: 0.5246%\n",
      "Epoch [49/300], Step [122/225], Training Accuracy: 76.1399%, Training Loss: 0.5249%\n",
      "Epoch [49/300], Step [123/225], Training Accuracy: 76.1433%, Training Loss: 0.5249%\n",
      "Epoch [49/300], Step [124/225], Training Accuracy: 76.0963%, Training Loss: 0.5251%\n",
      "Epoch [49/300], Step [125/225], Training Accuracy: 76.1125%, Training Loss: 0.5252%\n",
      "Epoch [49/300], Step [126/225], Training Accuracy: 76.1409%, Training Loss: 0.5251%\n",
      "Epoch [49/300], Step [127/225], Training Accuracy: 76.1688%, Training Loss: 0.5251%\n",
      "Epoch [49/300], Step [128/225], Training Accuracy: 76.1353%, Training Loss: 0.5264%\n",
      "Epoch [49/300], Step [129/225], Training Accuracy: 76.1628%, Training Loss: 0.5263%\n",
      "Epoch [49/300], Step [130/225], Training Accuracy: 76.1659%, Training Loss: 0.5262%\n",
      "Epoch [49/300], Step [131/225], Training Accuracy: 76.2285%, Training Loss: 0.5255%\n",
      "Epoch [49/300], Step [132/225], Training Accuracy: 76.2311%, Training Loss: 0.5249%\n",
      "Epoch [49/300], Step [133/225], Training Accuracy: 76.2218%, Training Loss: 0.5250%\n",
      "Epoch [49/300], Step [134/225], Training Accuracy: 76.2477%, Training Loss: 0.5247%\n",
      "Epoch [49/300], Step [135/225], Training Accuracy: 76.2847%, Training Loss: 0.5245%\n",
      "Epoch [49/300], Step [136/225], Training Accuracy: 76.3327%, Training Loss: 0.5238%\n",
      "Epoch [49/300], Step [137/225], Training Accuracy: 76.3116%, Training Loss: 0.5241%\n",
      "Epoch [49/300], Step [138/225], Training Accuracy: 76.3361%, Training Loss: 0.5235%\n",
      "Epoch [49/300], Step [139/225], Training Accuracy: 76.2927%, Training Loss: 0.5246%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Step [140/225], Training Accuracy: 76.3504%, Training Loss: 0.5240%\n",
      "Epoch [49/300], Step [141/225], Training Accuracy: 76.3741%, Training Loss: 0.5235%\n",
      "Epoch [49/300], Step [142/225], Training Accuracy: 76.3644%, Training Loss: 0.5236%\n",
      "Epoch [49/300], Step [143/225], Training Accuracy: 76.3658%, Training Loss: 0.5231%\n",
      "Epoch [49/300], Step [144/225], Training Accuracy: 76.3563%, Training Loss: 0.5228%\n",
      "Epoch [49/300], Step [145/225], Training Accuracy: 76.3470%, Training Loss: 0.5228%\n",
      "Epoch [49/300], Step [146/225], Training Accuracy: 76.3592%, Training Loss: 0.5227%\n",
      "Epoch [49/300], Step [147/225], Training Accuracy: 76.3499%, Training Loss: 0.5226%\n",
      "Epoch [49/300], Step [148/225], Training Accuracy: 76.4358%, Training Loss: 0.5215%\n",
      "Epoch [49/300], Step [149/225], Training Accuracy: 76.4367%, Training Loss: 0.5211%\n",
      "Epoch [49/300], Step [150/225], Training Accuracy: 76.4167%, Training Loss: 0.5213%\n",
      "Epoch [49/300], Step [151/225], Training Accuracy: 76.4797%, Training Loss: 0.5204%\n",
      "Epoch [49/300], Step [152/225], Training Accuracy: 76.4391%, Training Loss: 0.5208%\n",
      "Epoch [49/300], Step [153/225], Training Accuracy: 76.4502%, Training Loss: 0.5206%\n",
      "Epoch [49/300], Step [154/225], Training Accuracy: 76.4509%, Training Loss: 0.5202%\n",
      "Epoch [49/300], Step [155/225], Training Accuracy: 76.4617%, Training Loss: 0.5200%\n",
      "Epoch [49/300], Step [156/225], Training Accuracy: 76.4724%, Training Loss: 0.5199%\n",
      "Epoch [49/300], Step [157/225], Training Accuracy: 76.5127%, Training Loss: 0.5197%\n",
      "Epoch [49/300], Step [158/225], Training Accuracy: 76.5032%, Training Loss: 0.5198%\n",
      "Epoch [49/300], Step [159/225], Training Accuracy: 76.4347%, Training Loss: 0.5207%\n",
      "Epoch [49/300], Step [160/225], Training Accuracy: 76.4453%, Training Loss: 0.5205%\n",
      "Epoch [49/300], Step [161/225], Training Accuracy: 76.4849%, Training Loss: 0.5199%\n",
      "Epoch [49/300], Step [162/225], Training Accuracy: 76.5336%, Training Loss: 0.5191%\n",
      "Epoch [49/300], Step [163/225], Training Accuracy: 76.5050%, Training Loss: 0.5191%\n",
      "Epoch [49/300], Step [164/225], Training Accuracy: 76.5244%, Training Loss: 0.5185%\n",
      "Epoch [49/300], Step [165/225], Training Accuracy: 76.5814%, Training Loss: 0.5179%\n",
      "Epoch [49/300], Step [166/225], Training Accuracy: 76.5625%, Training Loss: 0.5181%\n",
      "Epoch [49/300], Step [167/225], Training Accuracy: 76.5999%, Training Loss: 0.5178%\n",
      "Epoch [49/300], Step [168/225], Training Accuracy: 76.5532%, Training Loss: 0.5182%\n",
      "Epoch [49/300], Step [169/225], Training Accuracy: 76.5440%, Training Loss: 0.5186%\n",
      "Epoch [49/300], Step [170/225], Training Accuracy: 76.5625%, Training Loss: 0.5189%\n",
      "Epoch [49/300], Step [171/225], Training Accuracy: 76.5899%, Training Loss: 0.5195%\n",
      "Epoch [49/300], Step [172/225], Training Accuracy: 76.6261%, Training Loss: 0.5191%\n",
      "Epoch [49/300], Step [173/225], Training Accuracy: 76.6257%, Training Loss: 0.5190%\n",
      "Epoch [49/300], Step [174/225], Training Accuracy: 76.6523%, Training Loss: 0.5188%\n",
      "Epoch [49/300], Step [175/225], Training Accuracy: 76.6964%, Training Loss: 0.5186%\n",
      "Epoch [49/300], Step [176/225], Training Accuracy: 76.7223%, Training Loss: 0.5181%\n",
      "Epoch [49/300], Step [177/225], Training Accuracy: 76.7567%, Training Loss: 0.5180%\n",
      "Epoch [49/300], Step [178/225], Training Accuracy: 76.7029%, Training Loss: 0.5188%\n",
      "Epoch [49/300], Step [179/225], Training Accuracy: 76.7022%, Training Loss: 0.5185%\n",
      "Epoch [49/300], Step [180/225], Training Accuracy: 76.7014%, Training Loss: 0.5184%\n",
      "Epoch [49/300], Step [181/225], Training Accuracy: 76.7179%, Training Loss: 0.5186%\n",
      "Epoch [49/300], Step [182/225], Training Accuracy: 76.7084%, Training Loss: 0.5186%\n",
      "Epoch [49/300], Step [183/225], Training Accuracy: 76.6735%, Training Loss: 0.5188%\n",
      "Epoch [49/300], Step [184/225], Training Accuracy: 76.7069%, Training Loss: 0.5180%\n",
      "Epoch [49/300], Step [185/225], Training Accuracy: 76.7314%, Training Loss: 0.5173%\n",
      "Epoch [49/300], Step [186/225], Training Accuracy: 76.7389%, Training Loss: 0.5170%\n",
      "Epoch [49/300], Step [187/225], Training Accuracy: 76.7129%, Training Loss: 0.5170%\n",
      "Epoch [49/300], Step [188/225], Training Accuracy: 76.7537%, Training Loss: 0.5166%\n",
      "Epoch [49/300], Step [189/225], Training Accuracy: 76.7940%, Training Loss: 0.5161%\n",
      "Epoch [49/300], Step [190/225], Training Accuracy: 76.7681%, Training Loss: 0.5160%\n",
      "Epoch [49/300], Step [191/225], Training Accuracy: 76.7425%, Training Loss: 0.5164%\n",
      "Epoch [49/300], Step [192/225], Training Accuracy: 76.7822%, Training Loss: 0.5160%\n",
      "Epoch [49/300], Step [193/225], Training Accuracy: 76.7811%, Training Loss: 0.5161%\n",
      "Epoch [49/300], Step [194/225], Training Accuracy: 76.7639%, Training Loss: 0.5162%\n",
      "Epoch [49/300], Step [195/225], Training Accuracy: 76.7708%, Training Loss: 0.5162%\n",
      "Epoch [49/300], Step [196/225], Training Accuracy: 76.7777%, Training Loss: 0.5158%\n",
      "Epoch [49/300], Step [197/225], Training Accuracy: 76.7687%, Training Loss: 0.5161%\n",
      "Epoch [49/300], Step [198/225], Training Accuracy: 76.8229%, Training Loss: 0.5149%\n",
      "Epoch [49/300], Step [199/225], Training Accuracy: 76.7981%, Training Loss: 0.5150%\n",
      "Epoch [49/300], Step [200/225], Training Accuracy: 76.7656%, Training Loss: 0.5156%\n",
      "Epoch [49/300], Step [201/225], Training Accuracy: 76.8035%, Training Loss: 0.5151%\n",
      "Epoch [49/300], Step [202/225], Training Accuracy: 76.8178%, Training Loss: 0.5145%\n",
      "Epoch [49/300], Step [203/225], Training Accuracy: 76.8473%, Training Loss: 0.5142%\n",
      "Epoch [49/300], Step [204/225], Training Accuracy: 76.8612%, Training Loss: 0.5137%\n",
      "Epoch [49/300], Step [205/225], Training Accuracy: 76.8674%, Training Loss: 0.5133%\n",
      "Epoch [49/300], Step [206/225], Training Accuracy: 76.8659%, Training Loss: 0.5137%\n",
      "Epoch [49/300], Step [207/225], Training Accuracy: 76.9022%, Training Loss: 0.5135%\n",
      "Epoch [49/300], Step [208/225], Training Accuracy: 76.9531%, Training Loss: 0.5127%\n",
      "Epoch [49/300], Step [209/225], Training Accuracy: 76.9961%, Training Loss: 0.5121%\n",
      "Epoch [49/300], Step [210/225], Training Accuracy: 76.9717%, Training Loss: 0.5122%\n",
      "Epoch [49/300], Step [211/225], Training Accuracy: 77.0216%, Training Loss: 0.5114%\n",
      "Epoch [49/300], Step [212/225], Training Accuracy: 77.0047%, Training Loss: 0.5119%\n",
      "Epoch [49/300], Step [213/225], Training Accuracy: 77.0467%, Training Loss: 0.5116%\n",
      "Epoch [49/300], Step [214/225], Training Accuracy: 77.0444%, Training Loss: 0.5119%\n",
      "Epoch [49/300], Step [215/225], Training Accuracy: 76.9985%, Training Loss: 0.5126%\n",
      "Epoch [49/300], Step [216/225], Training Accuracy: 76.9893%, Training Loss: 0.5127%\n",
      "Epoch [49/300], Step [217/225], Training Accuracy: 76.9945%, Training Loss: 0.5125%\n",
      "Epoch [49/300], Step [218/225], Training Accuracy: 77.0069%, Training Loss: 0.5127%\n",
      "Epoch [49/300], Step [219/225], Training Accuracy: 76.9834%, Training Loss: 0.5128%\n",
      "Epoch [49/300], Step [220/225], Training Accuracy: 76.9886%, Training Loss: 0.5129%\n",
      "Epoch [49/300], Step [221/225], Training Accuracy: 76.9443%, Training Loss: 0.5133%\n",
      "Epoch [49/300], Step [222/225], Training Accuracy: 76.9355%, Training Loss: 0.5132%\n",
      "Epoch [49/300], Step [223/225], Training Accuracy: 76.9339%, Training Loss: 0.5132%\n",
      "Epoch [49/300], Step [224/225], Training Accuracy: 76.9182%, Training Loss: 0.5133%\n",
      "Epoch [49/300], Step [225/225], Training Accuracy: 76.9108%, Training Loss: 0.5134%\n",
      "Epoch [50/300], Step [1/225], Training Accuracy: 75.0000%, Training Loss: 0.5296%\n",
      "Epoch [50/300], Step [2/225], Training Accuracy: 77.3438%, Training Loss: 0.5189%\n",
      "Epoch [50/300], Step [3/225], Training Accuracy: 77.0833%, Training Loss: 0.5103%\n",
      "Epoch [50/300], Step [4/225], Training Accuracy: 77.3438%, Training Loss: 0.5111%\n",
      "Epoch [50/300], Step [5/225], Training Accuracy: 76.2500%, Training Loss: 0.5378%\n",
      "Epoch [50/300], Step [6/225], Training Accuracy: 77.6042%, Training Loss: 0.5197%\n",
      "Epoch [50/300], Step [7/225], Training Accuracy: 78.1250%, Training Loss: 0.5101%\n",
      "Epoch [50/300], Step [8/225], Training Accuracy: 78.3203%, Training Loss: 0.5085%\n",
      "Epoch [50/300], Step [9/225], Training Accuracy: 78.2986%, Training Loss: 0.5104%\n",
      "Epoch [50/300], Step [10/225], Training Accuracy: 77.9688%, Training Loss: 0.5122%\n",
      "Epoch [50/300], Step [11/225], Training Accuracy: 78.5511%, Training Loss: 0.5006%\n",
      "Epoch [50/300], Step [12/225], Training Accuracy: 78.2552%, Training Loss: 0.5006%\n",
      "Epoch [50/300], Step [13/225], Training Accuracy: 78.4856%, Training Loss: 0.4935%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Step [14/225], Training Accuracy: 78.5714%, Training Loss: 0.4891%\n",
      "Epoch [50/300], Step [15/225], Training Accuracy: 78.8542%, Training Loss: 0.4829%\n",
      "Epoch [50/300], Step [16/225], Training Accuracy: 79.1016%, Training Loss: 0.4777%\n",
      "Epoch [50/300], Step [17/225], Training Accuracy: 78.7684%, Training Loss: 0.4798%\n",
      "Epoch [50/300], Step [18/225], Training Accuracy: 78.7326%, Training Loss: 0.4849%\n",
      "Epoch [50/300], Step [19/225], Training Accuracy: 78.2072%, Training Loss: 0.4885%\n",
      "Epoch [50/300], Step [20/225], Training Accuracy: 78.2031%, Training Loss: 0.4879%\n",
      "Epoch [50/300], Step [21/225], Training Accuracy: 78.4226%, Training Loss: 0.4831%\n",
      "Epoch [50/300], Step [22/225], Training Accuracy: 78.0540%, Training Loss: 0.4883%\n",
      "Epoch [50/300], Step [23/225], Training Accuracy: 78.0571%, Training Loss: 0.4881%\n",
      "Epoch [50/300], Step [24/225], Training Accuracy: 77.9297%, Training Loss: 0.4937%\n",
      "Epoch [50/300], Step [25/225], Training Accuracy: 78.0000%, Training Loss: 0.4917%\n",
      "Epoch [50/300], Step [26/225], Training Accuracy: 77.8846%, Training Loss: 0.4941%\n",
      "Epoch [50/300], Step [27/225], Training Accuracy: 78.0671%, Training Loss: 0.4901%\n",
      "Epoch [50/300], Step [28/225], Training Accuracy: 78.1250%, Training Loss: 0.4861%\n",
      "Epoch [50/300], Step [29/225], Training Accuracy: 77.9095%, Training Loss: 0.4919%\n",
      "Epoch [50/300], Step [30/225], Training Accuracy: 77.9167%, Training Loss: 0.4903%\n",
      "Epoch [50/300], Step [31/225], Training Accuracy: 77.3690%, Training Loss: 0.4996%\n",
      "Epoch [50/300], Step [32/225], Training Accuracy: 77.0508%, Training Loss: 0.5022%\n",
      "Epoch [50/300], Step [33/225], Training Accuracy: 77.0833%, Training Loss: 0.4997%\n",
      "Epoch [50/300], Step [34/225], Training Accuracy: 77.2059%, Training Loss: 0.4987%\n",
      "Epoch [50/300], Step [35/225], Training Accuracy: 77.2768%, Training Loss: 0.4977%\n",
      "Epoch [50/300], Step [36/225], Training Accuracy: 77.2569%, Training Loss: 0.4978%\n",
      "Epoch [50/300], Step [37/225], Training Accuracy: 77.3649%, Training Loss: 0.4976%\n",
      "Epoch [50/300], Step [38/225], Training Accuracy: 77.3849%, Training Loss: 0.4978%\n",
      "Epoch [50/300], Step [39/225], Training Accuracy: 77.5641%, Training Loss: 0.4959%\n",
      "Epoch [50/300], Step [40/225], Training Accuracy: 77.4219%, Training Loss: 0.4973%\n",
      "Epoch [50/300], Step [41/225], Training Accuracy: 77.2866%, Training Loss: 0.4992%\n",
      "Epoch [50/300], Step [42/225], Training Accuracy: 77.1949%, Training Loss: 0.5009%\n",
      "Epoch [50/300], Step [43/225], Training Accuracy: 77.2892%, Training Loss: 0.5008%\n",
      "Epoch [50/300], Step [44/225], Training Accuracy: 77.2727%, Training Loss: 0.5006%\n",
      "Epoch [50/300], Step [45/225], Training Accuracy: 77.3611%, Training Loss: 0.4995%\n",
      "Epoch [50/300], Step [46/225], Training Accuracy: 77.4117%, Training Loss: 0.4976%\n",
      "Epoch [50/300], Step [47/225], Training Accuracy: 77.3936%, Training Loss: 0.4973%\n",
      "Epoch [50/300], Step [48/225], Training Accuracy: 77.3763%, Training Loss: 0.4996%\n",
      "Epoch [50/300], Step [49/225], Training Accuracy: 77.6148%, Training Loss: 0.4976%\n",
      "Epoch [50/300], Step [50/225], Training Accuracy: 77.7188%, Training Loss: 0.4960%\n",
      "Epoch [50/300], Step [51/225], Training Accuracy: 77.7574%, Training Loss: 0.4959%\n",
      "Epoch [50/300], Step [52/225], Training Accuracy: 77.8846%, Training Loss: 0.4952%\n",
      "Epoch [50/300], Step [53/225], Training Accuracy: 77.8892%, Training Loss: 0.4958%\n",
      "Epoch [50/300], Step [54/225], Training Accuracy: 77.8646%, Training Loss: 0.4957%\n",
      "Epoch [50/300], Step [55/225], Training Accuracy: 77.8409%, Training Loss: 0.4959%\n",
      "Epoch [50/300], Step [56/225], Training Accuracy: 77.8739%, Training Loss: 0.4963%\n",
      "Epoch [50/300], Step [57/225], Training Accuracy: 77.8509%, Training Loss: 0.4960%\n",
      "Epoch [50/300], Step [58/225], Training Accuracy: 77.9095%, Training Loss: 0.4963%\n",
      "Epoch [50/300], Step [59/225], Training Accuracy: 77.9131%, Training Loss: 0.4972%\n",
      "Epoch [50/300], Step [60/225], Training Accuracy: 78.1250%, Training Loss: 0.4953%\n",
      "Epoch [50/300], Step [61/225], Training Accuracy: 77.9969%, Training Loss: 0.4971%\n",
      "Epoch [50/300], Step [62/225], Training Accuracy: 77.9990%, Training Loss: 0.4981%\n",
      "Epoch [50/300], Step [63/225], Training Accuracy: 77.7530%, Training Loss: 0.5009%\n",
      "Epoch [50/300], Step [64/225], Training Accuracy: 77.8076%, Training Loss: 0.5005%\n",
      "Epoch [50/300], Step [65/225], Training Accuracy: 77.7404%, Training Loss: 0.5012%\n",
      "Epoch [50/300], Step [66/225], Training Accuracy: 77.7225%, Training Loss: 0.5004%\n",
      "Epoch [50/300], Step [67/225], Training Accuracy: 77.8218%, Training Loss: 0.4993%\n",
      "Epoch [50/300], Step [68/225], Training Accuracy: 77.8033%, Training Loss: 0.4999%\n",
      "Epoch [50/300], Step [69/225], Training Accuracy: 77.6495%, Training Loss: 0.5017%\n",
      "Epoch [50/300], Step [70/225], Training Accuracy: 77.5000%, Training Loss: 0.5030%\n",
      "Epoch [50/300], Step [71/225], Training Accuracy: 77.5308%, Training Loss: 0.5027%\n",
      "Epoch [50/300], Step [72/225], Training Accuracy: 77.6259%, Training Loss: 0.5017%\n",
      "Epoch [50/300], Step [73/225], Training Accuracy: 77.7183%, Training Loss: 0.5009%\n",
      "Epoch [50/300], Step [74/225], Training Accuracy: 77.8294%, Training Loss: 0.5003%\n",
      "Epoch [50/300], Step [75/225], Training Accuracy: 77.8333%, Training Loss: 0.4997%\n",
      "Epoch [50/300], Step [76/225], Training Accuracy: 77.6521%, Training Loss: 0.5024%\n",
      "Epoch [50/300], Step [77/225], Training Accuracy: 77.6583%, Training Loss: 0.5021%\n",
      "Epoch [50/300], Step [78/225], Training Accuracy: 77.6843%, Training Loss: 0.5017%\n",
      "Epoch [50/300], Step [79/225], Training Accuracy: 77.6701%, Training Loss: 0.5021%\n",
      "Epoch [50/300], Step [80/225], Training Accuracy: 77.6562%, Training Loss: 0.5022%\n",
      "Epoch [50/300], Step [81/225], Training Accuracy: 77.7392%, Training Loss: 0.5005%\n",
      "Epoch [50/300], Step [82/225], Training Accuracy: 77.7248%, Training Loss: 0.5006%\n",
      "Epoch [50/300], Step [83/225], Training Accuracy: 77.6732%, Training Loss: 0.5010%\n",
      "Epoch [50/300], Step [84/225], Training Accuracy: 77.6786%, Training Loss: 0.5014%\n",
      "Epoch [50/300], Step [85/225], Training Accuracy: 77.7757%, Training Loss: 0.5002%\n",
      "Epoch [50/300], Step [86/225], Training Accuracy: 77.7616%, Training Loss: 0.4998%\n",
      "Epoch [50/300], Step [87/225], Training Accuracy: 77.7478%, Training Loss: 0.5005%\n",
      "Epoch [50/300], Step [88/225], Training Accuracy: 77.6811%, Training Loss: 0.5012%\n",
      "Epoch [50/300], Step [89/225], Training Accuracy: 77.6159%, Training Loss: 0.5021%\n",
      "Epoch [50/300], Step [90/225], Training Accuracy: 77.6042%, Training Loss: 0.5031%\n",
      "Epoch [50/300], Step [91/225], Training Accuracy: 77.5584%, Training Loss: 0.5045%\n",
      "Epoch [50/300], Step [92/225], Training Accuracy: 77.4796%, Training Loss: 0.5047%\n",
      "Epoch [50/300], Step [93/225], Training Accuracy: 77.5538%, Training Loss: 0.5036%\n",
      "Epoch [50/300], Step [94/225], Training Accuracy: 77.6097%, Training Loss: 0.5027%\n",
      "Epoch [50/300], Step [95/225], Training Accuracy: 77.5822%, Training Loss: 0.5030%\n",
      "Epoch [50/300], Step [96/225], Training Accuracy: 77.5391%, Training Loss: 0.5027%\n",
      "Epoch [50/300], Step [97/225], Training Accuracy: 77.5934%, Training Loss: 0.5018%\n",
      "Epoch [50/300], Step [98/225], Training Accuracy: 77.5191%, Training Loss: 0.5030%\n",
      "Epoch [50/300], Step [99/225], Training Accuracy: 77.5410%, Training Loss: 0.5032%\n",
      "Epoch [50/300], Step [100/225], Training Accuracy: 77.4375%, Training Loss: 0.5046%\n",
      "Epoch [50/300], Step [101/225], Training Accuracy: 77.4134%, Training Loss: 0.5052%\n",
      "Epoch [50/300], Step [102/225], Training Accuracy: 77.3591%, Training Loss: 0.5051%\n",
      "Epoch [50/300], Step [103/225], Training Accuracy: 77.3513%, Training Loss: 0.5051%\n",
      "Epoch [50/300], Step [104/225], Training Accuracy: 77.2837%, Training Loss: 0.5065%\n",
      "Epoch [50/300], Step [105/225], Training Accuracy: 77.2173%, Training Loss: 0.5066%\n",
      "Epoch [50/300], Step [106/225], Training Accuracy: 77.2553%, Training Loss: 0.5061%\n",
      "Epoch [50/300], Step [107/225], Training Accuracy: 77.2488%, Training Loss: 0.5074%\n",
      "Epoch [50/300], Step [108/225], Training Accuracy: 77.1991%, Training Loss: 0.5081%\n",
      "Epoch [50/300], Step [109/225], Training Accuracy: 77.2506%, Training Loss: 0.5084%\n",
      "Epoch [50/300], Step [110/225], Training Accuracy: 77.3011%, Training Loss: 0.5082%\n",
      "Epoch [50/300], Step [111/225], Training Accuracy: 77.2382%, Training Loss: 0.5089%\n",
      "Epoch [50/300], Step [112/225], Training Accuracy: 77.2600%, Training Loss: 0.5087%\n",
      "Epoch [50/300], Step [113/225], Training Accuracy: 77.2539%, Training Loss: 0.5084%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Step [114/225], Training Accuracy: 77.2341%, Training Loss: 0.5081%\n",
      "Epoch [50/300], Step [115/225], Training Accuracy: 77.3370%, Training Loss: 0.5072%\n",
      "Epoch [50/300], Step [116/225], Training Accuracy: 77.3303%, Training Loss: 0.5069%\n",
      "Epoch [50/300], Step [117/225], Training Accuracy: 77.3104%, Training Loss: 0.5074%\n",
      "Epoch [50/300], Step [118/225], Training Accuracy: 77.2643%, Training Loss: 0.5078%\n",
      "Epoch [50/300], Step [119/225], Training Accuracy: 77.2059%, Training Loss: 0.5085%\n",
      "Epoch [50/300], Step [120/225], Training Accuracy: 77.2266%, Training Loss: 0.5080%\n",
      "Epoch [50/300], Step [121/225], Training Accuracy: 77.1694%, Training Loss: 0.5088%\n",
      "Epoch [50/300], Step [122/225], Training Accuracy: 77.1004%, Training Loss: 0.5097%\n",
      "Epoch [50/300], Step [123/225], Training Accuracy: 77.0960%, Training Loss: 0.5099%\n",
      "Epoch [50/300], Step [124/225], Training Accuracy: 77.0413%, Training Loss: 0.5102%\n",
      "Epoch [50/300], Step [125/225], Training Accuracy: 77.0875%, Training Loss: 0.5098%\n",
      "Epoch [50/300], Step [126/225], Training Accuracy: 77.0461%, Training Loss: 0.5104%\n",
      "Epoch [50/300], Step [127/225], Training Accuracy: 77.0423%, Training Loss: 0.5106%\n",
      "Epoch [50/300], Step [128/225], Training Accuracy: 76.9897%, Training Loss: 0.5120%\n",
      "Epoch [50/300], Step [129/225], Training Accuracy: 76.9864%, Training Loss: 0.5121%\n",
      "Epoch [50/300], Step [130/225], Training Accuracy: 76.9952%, Training Loss: 0.5122%\n",
      "Epoch [50/300], Step [131/225], Training Accuracy: 77.0038%, Training Loss: 0.5118%\n",
      "Epoch [50/300], Step [132/225], Training Accuracy: 77.0005%, Training Loss: 0.5120%\n",
      "Epoch [50/300], Step [133/225], Training Accuracy: 76.9972%, Training Loss: 0.5126%\n",
      "Epoch [50/300], Step [134/225], Training Accuracy: 76.9473%, Training Loss: 0.5136%\n",
      "Epoch [50/300], Step [135/225], Training Accuracy: 76.9792%, Training Loss: 0.5136%\n",
      "Epoch [50/300], Step [136/225], Training Accuracy: 77.0221%, Training Loss: 0.5129%\n",
      "Epoch [50/300], Step [137/225], Training Accuracy: 77.0073%, Training Loss: 0.5130%\n",
      "Epoch [50/300], Step [138/225], Training Accuracy: 77.0041%, Training Loss: 0.5128%\n",
      "Epoch [50/300], Step [139/225], Training Accuracy: 76.9897%, Training Loss: 0.5127%\n",
      "Epoch [50/300], Step [140/225], Training Accuracy: 77.0312%, Training Loss: 0.5118%\n",
      "Epoch [50/300], Step [141/225], Training Accuracy: 77.0723%, Training Loss: 0.5110%\n",
      "Epoch [50/300], Step [142/225], Training Accuracy: 77.0357%, Training Loss: 0.5117%\n",
      "Epoch [50/300], Step [143/225], Training Accuracy: 77.0323%, Training Loss: 0.5113%\n",
      "Epoch [50/300], Step [144/225], Training Accuracy: 77.0616%, Training Loss: 0.5114%\n",
      "Epoch [50/300], Step [145/225], Training Accuracy: 77.1013%, Training Loss: 0.5112%\n",
      "Epoch [50/300], Step [146/225], Training Accuracy: 77.0869%, Training Loss: 0.5115%\n",
      "Epoch [50/300], Step [147/225], Training Accuracy: 77.0514%, Training Loss: 0.5119%\n",
      "Epoch [50/300], Step [148/225], Training Accuracy: 77.0587%, Training Loss: 0.5117%\n",
      "Epoch [50/300], Step [149/225], Training Accuracy: 77.0763%, Training Loss: 0.5114%\n",
      "Epoch [50/300], Step [150/225], Training Accuracy: 77.1042%, Training Loss: 0.5111%\n",
      "Epoch [50/300], Step [151/225], Training Accuracy: 77.1523%, Training Loss: 0.5104%\n",
      "Epoch [50/300], Step [152/225], Training Accuracy: 77.1279%, Training Loss: 0.5106%\n",
      "Epoch [50/300], Step [153/225], Training Accuracy: 77.1548%, Training Loss: 0.5099%\n",
      "Epoch [50/300], Step [154/225], Training Accuracy: 77.1916%, Training Loss: 0.5098%\n",
      "Epoch [50/300], Step [155/225], Training Accuracy: 77.1472%, Training Loss: 0.5104%\n",
      "Epoch [50/300], Step [156/225], Training Accuracy: 77.1835%, Training Loss: 0.5102%\n",
      "Epoch [50/300], Step [157/225], Training Accuracy: 77.1895%, Training Loss: 0.5102%\n",
      "Epoch [50/300], Step [158/225], Training Accuracy: 77.1460%, Training Loss: 0.5106%\n",
      "Epoch [50/300], Step [159/225], Training Accuracy: 77.1030%, Training Loss: 0.5114%\n",
      "Epoch [50/300], Step [160/225], Training Accuracy: 77.1191%, Training Loss: 0.5112%\n",
      "Epoch [50/300], Step [161/225], Training Accuracy: 77.1254%, Training Loss: 0.5109%\n",
      "Epoch [50/300], Step [162/225], Training Accuracy: 77.2087%, Training Loss: 0.5099%\n",
      "Epoch [50/300], Step [163/225], Training Accuracy: 77.1952%, Training Loss: 0.5103%\n",
      "Epoch [50/300], Step [164/225], Training Accuracy: 77.2294%, Training Loss: 0.5099%\n",
      "Epoch [50/300], Step [165/225], Training Accuracy: 77.2822%, Training Loss: 0.5096%\n",
      "Epoch [50/300], Step [166/225], Training Accuracy: 77.3061%, Training Loss: 0.5096%\n",
      "Epoch [50/300], Step [167/225], Training Accuracy: 77.3204%, Training Loss: 0.5100%\n",
      "Epoch [50/300], Step [168/225], Training Accuracy: 77.2693%, Training Loss: 0.5107%\n",
      "Epoch [50/300], Step [169/225], Training Accuracy: 77.2282%, Training Loss: 0.5107%\n",
      "Epoch [50/300], Step [170/225], Training Accuracy: 77.2335%, Training Loss: 0.5108%\n",
      "Epoch [50/300], Step [171/225], Training Accuracy: 77.2661%, Training Loss: 0.5103%\n",
      "Epoch [50/300], Step [172/225], Training Accuracy: 77.2802%, Training Loss: 0.5099%\n",
      "Epoch [50/300], Step [173/225], Training Accuracy: 77.2850%, Training Loss: 0.5103%\n",
      "Epoch [50/300], Step [174/225], Training Accuracy: 77.2719%, Training Loss: 0.5104%\n",
      "Epoch [50/300], Step [175/225], Training Accuracy: 77.2857%, Training Loss: 0.5098%\n",
      "Epoch [50/300], Step [176/225], Training Accuracy: 77.3171%, Training Loss: 0.5092%\n",
      "Epoch [50/300], Step [177/225], Training Accuracy: 77.3658%, Training Loss: 0.5090%\n",
      "Epoch [50/300], Step [178/225], Training Accuracy: 77.3613%, Training Loss: 0.5088%\n",
      "Epoch [50/300], Step [179/225], Training Accuracy: 77.3830%, Training Loss: 0.5086%\n",
      "Epoch [50/300], Step [180/225], Training Accuracy: 77.3872%, Training Loss: 0.5085%\n",
      "Epoch [50/300], Step [181/225], Training Accuracy: 77.4085%, Training Loss: 0.5083%\n",
      "Epoch [50/300], Step [182/225], Training Accuracy: 77.4382%, Training Loss: 0.5083%\n",
      "Epoch [50/300], Step [183/225], Training Accuracy: 77.4249%, Training Loss: 0.5086%\n",
      "Epoch [50/300], Step [184/225], Training Accuracy: 77.4541%, Training Loss: 0.5077%\n",
      "Epoch [50/300], Step [185/225], Training Accuracy: 77.5169%, Training Loss: 0.5069%\n",
      "Epoch [50/300], Step [186/225], Training Accuracy: 77.5370%, Training Loss: 0.5064%\n",
      "Epoch [50/300], Step [187/225], Training Accuracy: 77.5318%, Training Loss: 0.5068%\n",
      "Epoch [50/300], Step [188/225], Training Accuracy: 77.5765%, Training Loss: 0.5062%\n",
      "Epoch [50/300], Step [189/225], Training Accuracy: 77.5876%, Training Loss: 0.5059%\n",
      "Epoch [50/300], Step [190/225], Training Accuracy: 77.6151%, Training Loss: 0.5061%\n",
      "Epoch [50/300], Step [191/225], Training Accuracy: 77.6178%, Training Loss: 0.5065%\n",
      "Epoch [50/300], Step [192/225], Training Accuracy: 77.6611%, Training Loss: 0.5058%\n",
      "Epoch [50/300], Step [193/225], Training Accuracy: 77.6959%, Training Loss: 0.5057%\n",
      "Epoch [50/300], Step [194/225], Training Accuracy: 77.6901%, Training Loss: 0.5054%\n",
      "Epoch [50/300], Step [195/225], Training Accuracy: 77.7163%, Training Loss: 0.5048%\n",
      "Epoch [50/300], Step [196/225], Training Accuracy: 77.6706%, Training Loss: 0.5052%\n",
      "Epoch [50/300], Step [197/225], Training Accuracy: 77.6729%, Training Loss: 0.5050%\n",
      "Epoch [50/300], Step [198/225], Training Accuracy: 77.6989%, Training Loss: 0.5043%\n",
      "Epoch [50/300], Step [199/225], Training Accuracy: 77.7167%, Training Loss: 0.5040%\n",
      "Epoch [50/300], Step [200/225], Training Accuracy: 77.7344%, Training Loss: 0.5036%\n",
      "Epoch [50/300], Step [201/225], Training Accuracy: 77.7363%, Training Loss: 0.5036%\n",
      "Epoch [50/300], Step [202/225], Training Accuracy: 77.7614%, Training Loss: 0.5028%\n",
      "Epoch [50/300], Step [203/225], Training Accuracy: 77.7863%, Training Loss: 0.5027%\n",
      "Epoch [50/300], Step [204/225], Training Accuracy: 77.7956%, Training Loss: 0.5024%\n",
      "Epoch [50/300], Step [205/225], Training Accuracy: 77.8125%, Training Loss: 0.5017%\n",
      "Epoch [50/300], Step [206/225], Training Accuracy: 77.8368%, Training Loss: 0.5019%\n",
      "Epoch [50/300], Step [207/225], Training Accuracy: 77.8306%, Training Loss: 0.5024%\n",
      "Epoch [50/300], Step [208/225], Training Accuracy: 77.8546%, Training Loss: 0.5021%\n",
      "Epoch [50/300], Step [209/225], Training Accuracy: 77.8783%, Training Loss: 0.5016%\n",
      "Epoch [50/300], Step [210/225], Training Accuracy: 77.8646%, Training Loss: 0.5013%\n",
      "Epoch [50/300], Step [211/225], Training Accuracy: 77.9102%, Training Loss: 0.5008%\n",
      "Epoch [50/300], Step [212/225], Training Accuracy: 77.8965%, Training Loss: 0.5012%\n",
      "Epoch [50/300], Step [213/225], Training Accuracy: 77.8829%, Training Loss: 0.5015%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Step [214/225], Training Accuracy: 77.8768%, Training Loss: 0.5013%\n",
      "Epoch [50/300], Step [215/225], Training Accuracy: 77.8561%, Training Loss: 0.5018%\n",
      "Epoch [50/300], Step [216/225], Training Accuracy: 77.8573%, Training Loss: 0.5018%\n",
      "Epoch [50/300], Step [217/225], Training Accuracy: 77.8226%, Training Loss: 0.5025%\n",
      "Epoch [50/300], Step [218/225], Training Accuracy: 77.7881%, Training Loss: 0.5026%\n",
      "Epoch [50/300], Step [219/225], Training Accuracy: 77.7897%, Training Loss: 0.5028%\n",
      "Epoch [50/300], Step [220/225], Training Accuracy: 77.8054%, Training Loss: 0.5028%\n",
      "Epoch [50/300], Step [221/225], Training Accuracy: 77.7856%, Training Loss: 0.5031%\n",
      "Epoch [50/300], Step [222/225], Training Accuracy: 77.7872%, Training Loss: 0.5028%\n",
      "Epoch [50/300], Step [223/225], Training Accuracy: 77.8027%, Training Loss: 0.5026%\n",
      "Epoch [50/300], Step [224/225], Training Accuracy: 77.8181%, Training Loss: 0.5024%\n",
      "Epoch [50/300], Step [225/225], Training Accuracy: 77.8141%, Training Loss: 0.5024%\n",
      "Epoch [51/300], Step [1/225], Training Accuracy: 68.7500%, Training Loss: 0.6112%\n",
      "Epoch [51/300], Step [2/225], Training Accuracy: 75.0000%, Training Loss: 0.5790%\n",
      "Epoch [51/300], Step [3/225], Training Accuracy: 75.5208%, Training Loss: 0.5619%\n",
      "Epoch [51/300], Step [4/225], Training Accuracy: 74.6094%, Training Loss: 0.5562%\n",
      "Epoch [51/300], Step [5/225], Training Accuracy: 74.6875%, Training Loss: 0.5405%\n",
      "Epoch [51/300], Step [6/225], Training Accuracy: 75.0000%, Training Loss: 0.5212%\n",
      "Epoch [51/300], Step [7/225], Training Accuracy: 77.4554%, Training Loss: 0.4958%\n",
      "Epoch [51/300], Step [8/225], Training Accuracy: 77.3438%, Training Loss: 0.4981%\n",
      "Epoch [51/300], Step [9/225], Training Accuracy: 76.9097%, Training Loss: 0.4928%\n",
      "Epoch [51/300], Step [10/225], Training Accuracy: 77.0312%, Training Loss: 0.4964%\n",
      "Epoch [51/300], Step [11/225], Training Accuracy: 77.5568%, Training Loss: 0.4851%\n",
      "Epoch [51/300], Step [12/225], Training Accuracy: 77.4740%, Training Loss: 0.4860%\n",
      "Epoch [51/300], Step [13/225], Training Accuracy: 78.2452%, Training Loss: 0.4713%\n",
      "Epoch [51/300], Step [14/225], Training Accuracy: 78.4598%, Training Loss: 0.4644%\n",
      "Epoch [51/300], Step [15/225], Training Accuracy: 78.9583%, Training Loss: 0.4613%\n",
      "Epoch [51/300], Step [16/225], Training Accuracy: 78.7109%, Training Loss: 0.4630%\n",
      "Epoch [51/300], Step [17/225], Training Accuracy: 78.8603%, Training Loss: 0.4604%\n",
      "Epoch [51/300], Step [18/225], Training Accuracy: 78.8194%, Training Loss: 0.4588%\n",
      "Epoch [51/300], Step [19/225], Training Accuracy: 79.0296%, Training Loss: 0.4633%\n",
      "Epoch [51/300], Step [20/225], Training Accuracy: 79.2969%, Training Loss: 0.4607%\n",
      "Epoch [51/300], Step [21/225], Training Accuracy: 79.4643%, Training Loss: 0.4566%\n",
      "Epoch [51/300], Step [22/225], Training Accuracy: 79.3324%, Training Loss: 0.4581%\n",
      "Epoch [51/300], Step [23/225], Training Accuracy: 79.4837%, Training Loss: 0.4535%\n",
      "Epoch [51/300], Step [24/225], Training Accuracy: 79.3620%, Training Loss: 0.4559%\n",
      "Epoch [51/300], Step [25/225], Training Accuracy: 79.5625%, Training Loss: 0.4559%\n",
      "Epoch [51/300], Step [26/225], Training Accuracy: 79.3269%, Training Loss: 0.4608%\n",
      "Epoch [51/300], Step [27/225], Training Accuracy: 78.9931%, Training Loss: 0.4634%\n",
      "Epoch [51/300], Step [28/225], Training Accuracy: 79.1853%, Training Loss: 0.4620%\n",
      "Epoch [51/300], Step [29/225], Training Accuracy: 79.0948%, Training Loss: 0.4627%\n",
      "Epoch [51/300], Step [30/225], Training Accuracy: 79.3229%, Training Loss: 0.4598%\n",
      "Epoch [51/300], Step [31/225], Training Accuracy: 79.0323%, Training Loss: 0.4655%\n",
      "Epoch [51/300], Step [32/225], Training Accuracy: 79.0527%, Training Loss: 0.4657%\n",
      "Epoch [51/300], Step [33/225], Training Accuracy: 79.2140%, Training Loss: 0.4630%\n",
      "Epoch [51/300], Step [34/225], Training Accuracy: 78.9982%, Training Loss: 0.4650%\n",
      "Epoch [51/300], Step [35/225], Training Accuracy: 79.1071%, Training Loss: 0.4634%\n",
      "Epoch [51/300], Step [36/225], Training Accuracy: 79.0365%, Training Loss: 0.4635%\n",
      "Epoch [51/300], Step [37/225], Training Accuracy: 79.0963%, Training Loss: 0.4634%\n",
      "Epoch [51/300], Step [38/225], Training Accuracy: 78.9474%, Training Loss: 0.4650%\n",
      "Epoch [51/300], Step [39/225], Training Accuracy: 78.9663%, Training Loss: 0.4663%\n",
      "Epoch [51/300], Step [40/225], Training Accuracy: 79.1016%, Training Loss: 0.4648%\n",
      "Epoch [51/300], Step [41/225], Training Accuracy: 79.3064%, Training Loss: 0.4629%\n",
      "Epoch [51/300], Step [42/225], Training Accuracy: 79.1667%, Training Loss: 0.4636%\n",
      "Epoch [51/300], Step [43/225], Training Accuracy: 79.0698%, Training Loss: 0.4648%\n",
      "Epoch [51/300], Step [44/225], Training Accuracy: 79.1548%, Training Loss: 0.4635%\n",
      "Epoch [51/300], Step [45/225], Training Accuracy: 79.0972%, Training Loss: 0.4636%\n",
      "Epoch [51/300], Step [46/225], Training Accuracy: 79.1440%, Training Loss: 0.4619%\n",
      "Epoch [51/300], Step [47/225], Training Accuracy: 79.1556%, Training Loss: 0.4622%\n",
      "Epoch [51/300], Step [48/225], Training Accuracy: 79.1667%, Training Loss: 0.4634%\n",
      "Epoch [51/300], Step [49/225], Training Accuracy: 79.2092%, Training Loss: 0.4622%\n",
      "Epoch [51/300], Step [50/225], Training Accuracy: 79.1562%, Training Loss: 0.4631%\n",
      "Epoch [51/300], Step [51/225], Training Accuracy: 79.1054%, Training Loss: 0.4625%\n",
      "Epoch [51/300], Step [52/225], Training Accuracy: 79.2067%, Training Loss: 0.4620%\n",
      "Epoch [51/300], Step [53/225], Training Accuracy: 79.3042%, Training Loss: 0.4617%\n",
      "Epoch [51/300], Step [54/225], Training Accuracy: 79.1956%, Training Loss: 0.4632%\n",
      "Epoch [51/300], Step [55/225], Training Accuracy: 79.1477%, Training Loss: 0.4638%\n",
      "Epoch [51/300], Step [56/225], Training Accuracy: 79.1295%, Training Loss: 0.4637%\n",
      "Epoch [51/300], Step [57/225], Training Accuracy: 79.1667%, Training Loss: 0.4638%\n",
      "Epoch [51/300], Step [58/225], Training Accuracy: 79.1756%, Training Loss: 0.4640%\n",
      "Epoch [51/300], Step [59/225], Training Accuracy: 79.1843%, Training Loss: 0.4642%\n",
      "Epoch [51/300], Step [60/225], Training Accuracy: 79.1927%, Training Loss: 0.4632%\n",
      "Epoch [51/300], Step [61/225], Training Accuracy: 79.1240%, Training Loss: 0.4657%\n",
      "Epoch [51/300], Step [62/225], Training Accuracy: 79.2087%, Training Loss: 0.4653%\n",
      "Epoch [51/300], Step [63/225], Training Accuracy: 79.0675%, Training Loss: 0.4675%\n",
      "Epoch [51/300], Step [64/225], Training Accuracy: 79.1016%, Training Loss: 0.4670%\n",
      "Epoch [51/300], Step [65/225], Training Accuracy: 79.2067%, Training Loss: 0.4668%\n",
      "Epoch [51/300], Step [66/225], Training Accuracy: 79.2377%, Training Loss: 0.4657%\n",
      "Epoch [51/300], Step [67/225], Training Accuracy: 79.1978%, Training Loss: 0.4658%\n",
      "Epoch [51/300], Step [68/225], Training Accuracy: 79.1131%, Training Loss: 0.4679%\n",
      "Epoch [51/300], Step [69/225], Training Accuracy: 79.0082%, Training Loss: 0.4690%\n",
      "Epoch [51/300], Step [70/225], Training Accuracy: 78.9732%, Training Loss: 0.4687%\n",
      "Epoch [51/300], Step [71/225], Training Accuracy: 79.0493%, Training Loss: 0.4673%\n",
      "Epoch [51/300], Step [72/225], Training Accuracy: 79.1016%, Training Loss: 0.4668%\n",
      "Epoch [51/300], Step [73/225], Training Accuracy: 79.1524%, Training Loss: 0.4669%\n",
      "Epoch [51/300], Step [74/225], Training Accuracy: 79.1385%, Training Loss: 0.4671%\n",
      "Epoch [51/300], Step [75/225], Training Accuracy: 79.1250%, Training Loss: 0.4669%\n",
      "Epoch [51/300], Step [76/225], Training Accuracy: 79.0296%, Training Loss: 0.4689%\n",
      "Epoch [51/300], Step [77/225], Training Accuracy: 79.1193%, Training Loss: 0.4679%\n",
      "Epoch [51/300], Step [78/225], Training Accuracy: 79.1066%, Training Loss: 0.4681%\n",
      "Epoch [51/300], Step [79/225], Training Accuracy: 79.0941%, Training Loss: 0.4682%\n",
      "Epoch [51/300], Step [80/225], Training Accuracy: 79.0625%, Training Loss: 0.4699%\n",
      "Epoch [51/300], Step [81/225], Training Accuracy: 79.1474%, Training Loss: 0.4685%\n",
      "Epoch [51/300], Step [82/225], Training Accuracy: 79.1159%, Training Loss: 0.4679%\n",
      "Epoch [51/300], Step [83/225], Training Accuracy: 79.0474%, Training Loss: 0.4688%\n",
      "Epoch [51/300], Step [84/225], Training Accuracy: 79.1109%, Training Loss: 0.4684%\n",
      "Epoch [51/300], Step [85/225], Training Accuracy: 79.1544%, Training Loss: 0.4679%\n",
      "Epoch [51/300], Step [86/225], Training Accuracy: 79.1969%, Training Loss: 0.4675%\n",
      "Epoch [51/300], Step [87/225], Training Accuracy: 79.2385%, Training Loss: 0.4673%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Step [88/225], Training Accuracy: 79.1016%, Training Loss: 0.4690%\n",
      "Epoch [51/300], Step [89/225], Training Accuracy: 79.1959%, Training Loss: 0.4681%\n",
      "Epoch [51/300], Step [90/225], Training Accuracy: 79.1319%, Training Loss: 0.4689%\n",
      "Epoch [51/300], Step [91/225], Training Accuracy: 79.1380%, Training Loss: 0.4689%\n",
      "Epoch [51/300], Step [92/225], Training Accuracy: 79.0761%, Training Loss: 0.4699%\n",
      "Epoch [51/300], Step [93/225], Training Accuracy: 79.2003%, Training Loss: 0.4684%\n",
      "Epoch [51/300], Step [94/225], Training Accuracy: 79.1888%, Training Loss: 0.4682%\n",
      "Epoch [51/300], Step [95/225], Training Accuracy: 79.1776%, Training Loss: 0.4688%\n",
      "Epoch [51/300], Step [96/225], Training Accuracy: 79.2480%, Training Loss: 0.4675%\n",
      "Epoch [51/300], Step [97/225], Training Accuracy: 79.2848%, Training Loss: 0.4673%\n",
      "Epoch [51/300], Step [98/225], Training Accuracy: 79.2411%, Training Loss: 0.4680%\n",
      "Epoch [51/300], Step [99/225], Training Accuracy: 79.2929%, Training Loss: 0.4676%\n",
      "Epoch [51/300], Step [100/225], Training Accuracy: 79.2969%, Training Loss: 0.4685%\n",
      "Epoch [51/300], Step [101/225], Training Accuracy: 79.2389%, Training Loss: 0.4701%\n",
      "Epoch [51/300], Step [102/225], Training Accuracy: 79.2126%, Training Loss: 0.4705%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss = 0\n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #scheduler.step() \n",
    "        #print(scheduler.get_last_lr()[0])\n",
    "      \n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        #print(optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += Y.size(0)\n",
    "        correct += predicted.eq(Y).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        accu=100.*correct/total\n",
    "        train_loss = running_loss/(i+1)\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.4f}%, Training Loss: {:.4f}%'.format(epoch+1, num_epochs, i+1, total_step, accu, train_loss))\n",
    "    \n",
    "   \n",
    "        #writer.add_scalar(f'train/accuracy', accu, epoch)\n",
    "        #writer.add_scalar(f'train/loss', train_loss, epoch)\n",
    "        writer.add_scalars(f'train/accuracy_loss', {\n",
    "            'accuracy': accu,\n",
    "            'loss': train_loss,\n",
    "        }, epoch)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, Y in test_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += Y.size(0)\n",
    "        correct += (predicted == Y).sum().item()\n",
    "\n",
    "    print('Test Accuracy : {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce71586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066157a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
