{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce5e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58eded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a3116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/final_format/train_set.csv\",header=None).to_numpy()\n",
    "train_label = pd.read_csv(\"data/final_format/train_label.csv\",header=None).to_numpy()\n",
    "test_set = pd.read_csv(\"data/final_format/test_set.csv\",header=None).to_numpy()\n",
    "test_label = pd.read_csv(\"data/final_format/test_label.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f64e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14393, 4096) (14393, 1) (3599, 4096) (3599, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d522cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 4096) (14392, 1) (3598, 4096) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "#delet first row data\n",
    "train_set = train_set[1:]\n",
    "train_label = train_label[1:]\n",
    "test_set = test_set[1:]\n",
    "test_label = test_label[1:]\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a84a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 1, 64, 64) (14392, 1) (3598, 1, 64, 64) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.reshape((-1,1,64,64))\n",
    "test_set = test_set.reshape((-1,1,64,64))\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e23a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 1, 64, 64) (14392,) (3598, 1, 64, 64) (3598,)\n"
     ]
    }
   ],
   "source": [
    "train_label = train_label.reshape(-1)\n",
    "test_label = test_label.reshape(-1)\n",
    "\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f62253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 300\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d66b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tensor = Tensor(train_set) \n",
    "train_label_tensor = Tensor(train_label).type(torch.LongTensor)\n",
    "\n",
    "train_dataset = TensorDataset(train_set_tensor,train_label_tensor) \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size) \n",
    "\n",
    "test_set_tensor = Tensor(test_set) \n",
    "test_label_tensor = Tensor(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_dataset = TensorDataset(test_set_tensor,test_label_tensor) \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33820b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6f6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ThinResCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ThinResCNN, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 48, kernel_size=1, padding='same')\n",
    "        self.c2 = nn.Conv2d(48, 48, kernel_size=3,padding='same')\n",
    "        self.c3 = nn.Conv2d(48, 96, kernel_size=1,padding='same') \n",
    "        self.c4 = nn.Conv2d(97, 96, kernel_size=1, padding='same')\n",
    "        self.c5 = nn.Conv2d(96, 96, kernel_size=3,padding='same')\n",
    "        self.c6 = nn.Conv2d(96, 128, kernel_size=1, padding='same')\n",
    "        self.c7 = nn.Conv2d(225, 128, kernel_size=3, padding='same')\n",
    "        self.c8 = nn.Conv2d(128, 128, kernel_size=3,padding='same')\n",
    "        self.c9 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
    "        \n",
    "        self.d1 = nn.Dropout(p=0.25)\n",
    "        self.d2 = nn.Dropout(p=0.25)\n",
    "        self.d3 = nn.Dropout(p=0.25)\n",
    "        self.d4 = nn.Dropout(p=0.25)\n",
    "        self.d5 = nn.Dropout(p=0.25)\n",
    "        self.d6 = nn.Dropout(p=0.25) \n",
    "        self.d7 = nn.Dropout(p=0.25)\n",
    "        self.d8 = nn.Dropout(p=0.25)\n",
    "        self.d9 = nn.Dropout(p=0.25) \n",
    "        self.d10 = nn.Dropout(p=0.25) \n",
    "        \n",
    "        self.bn1 =  nn.BatchNorm2d(1)\n",
    "        self.bn2 =  nn.BatchNorm2d(48)\n",
    "        self.bn3 =  nn.BatchNorm2d(48)\n",
    "        self.bn4 =  nn.BatchNorm2d(97)\n",
    "        self.bn5 =  nn.BatchNorm2d(96)\n",
    "        self.bn6 =  nn.BatchNorm2d(96)\n",
    "        self.bn7 =  nn.BatchNorm2d(225)\n",
    "        self.bn8 =  nn.BatchNorm2d(128)\n",
    "        self.bn9 =  nn.BatchNorm2d(128)\n",
    "        self.bn10 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(481*8*8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        C1 = self.c1(self.bn1(x))\n",
    "        C1 = F.leaky_relu(C1,0.2)\n",
    "        C1 = self.d1(C1)\n",
    "        C2 = self.c2(self.bn2(C1))\n",
    "        C2 = F.leaky_relu(C2,0.2)\n",
    "        C2 = self.d2(C2)\n",
    "        C3 = self.c3(self.bn3(C2))\n",
    "        sum1 = torch.cat((x, C3), dim=1)\n",
    "        sum1 = F.leaky_relu(sum1,0.2)\n",
    "        sum1 = self.d3(sum1)\n",
    "        M1 = F.max_pool2d(sum1, kernel_size=2, stride=2)\n",
    "         \n",
    "        C4 = self.c4(self.bn4(M1))\n",
    "        C4 = F.leaky_relu(C4,0.2)\n",
    "        C4 = self.d4(C4)\n",
    "        C5 = self.c5(self.bn5(C4))\n",
    "        C5 = F.leaky_relu(C5,0.2)\n",
    "        C5 = self.d5(C5)\n",
    "        C6 = self.c6(self.bn6(C5))\n",
    "        sum2 = torch.cat((M1, C6), dim=1)\n",
    "        sum2 = F.leaky_relu(sum2,0.2)\n",
    "        sum2 = self.d6(sum2)\n",
    "        M2 = F.max_pool2d(sum2, kernel_size=2, stride=2)\n",
    "        \n",
    "        C7 = self.c7(self.bn7(M2))\n",
    "        C7 = F.leaky_relu(C7,0.2)\n",
    "        C7 = self.d7(C7)\n",
    "        C8 = self.c8(self.bn8(C7))\n",
    "        C8 = F.leaky_relu(C8,0.2)\n",
    "        C8 = self.d8(C8)\n",
    "        C9 = self.c9(self.bn9(C8))\n",
    "        sum3 = torch.cat((M2, C9), dim=1)\n",
    "        \n",
    "        \n",
    "        sum3 = F.leaky_relu(sum3,0.2)\n",
    "        sum3 = self.d9(sum3)\n",
    "        M3 = F.max_pool2d(sum3, kernel_size=2, stride=2)\n",
    "        \n",
    "\n",
    "        F1 = M3.reshape(M3.size(0), -1)\n",
    "        Fc1 = self.fc1(F1)\n",
    "        Fc1 = self.bn10(Fc1)\n",
    "        Fc1 = F.leaky_relu(Fc1,0.2)\n",
    "        Fc1 = self.d10(Fc1)\n",
    "        Fc2 = self.fc2(Fc1)\n",
    "       \n",
    "        return Fc2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901366a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThinResCNN(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ae0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3) \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "milestones = [50,100,150,200,250]\n",
    "milestones = [a * len(train_loader) for a in milestones]\n",
    "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b03775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [1/225], Training Accuracy: 26.5625%, Training Loss: 1.4292%\n",
      "Epoch [1/300], Step [2/225], Training Accuracy: 23.4375%, Training Loss: 1.9829%\n",
      "Epoch [1/300], Step [3/225], Training Accuracy: 28.1250%, Training Loss: 1.8179%\n",
      "Epoch [1/300], Step [4/225], Training Accuracy: 28.1250%, Training Loss: 1.7856%\n",
      "Epoch [1/300], Step [5/225], Training Accuracy: 27.8125%, Training Loss: 1.7718%\n",
      "Epoch [1/300], Step [6/225], Training Accuracy: 27.3438%, Training Loss: 1.7672%\n",
      "Epoch [1/300], Step [7/225], Training Accuracy: 27.0089%, Training Loss: 1.7211%\n",
      "Epoch [1/300], Step [8/225], Training Accuracy: 26.7578%, Training Loss: 1.6988%\n",
      "Epoch [1/300], Step [9/225], Training Accuracy: 27.4306%, Training Loss: 1.6657%\n",
      "Epoch [1/300], Step [10/225], Training Accuracy: 27.3438%, Training Loss: 1.6430%\n",
      "Epoch [1/300], Step [11/225], Training Accuracy: 27.1307%, Training Loss: 1.6326%\n",
      "Epoch [1/300], Step [12/225], Training Accuracy: 27.7344%, Training Loss: 1.6104%\n",
      "Epoch [1/300], Step [13/225], Training Accuracy: 28.0048%, Training Loss: 1.5874%\n",
      "Epoch [1/300], Step [14/225], Training Accuracy: 27.1205%, Training Loss: 1.5777%\n",
      "Epoch [1/300], Step [15/225], Training Accuracy: 27.3958%, Training Loss: 1.5683%\n",
      "Epoch [1/300], Step [16/225], Training Accuracy: 27.1484%, Training Loss: 1.5600%\n",
      "Epoch [1/300], Step [17/225], Training Accuracy: 26.8382%, Training Loss: 1.5548%\n",
      "Epoch [1/300], Step [18/225], Training Accuracy: 26.3889%, Training Loss: 1.5474%\n",
      "Epoch [1/300], Step [19/225], Training Accuracy: 26.6447%, Training Loss: 1.5405%\n",
      "Epoch [1/300], Step [20/225], Training Accuracy: 26.9531%, Training Loss: 1.5357%\n",
      "Epoch [1/300], Step [21/225], Training Accuracy: 27.3065%, Training Loss: 1.5284%\n",
      "Epoch [1/300], Step [22/225], Training Accuracy: 27.0597%, Training Loss: 1.5221%\n",
      "Epoch [1/300], Step [23/225], Training Accuracy: 26.9701%, Training Loss: 1.5149%\n",
      "Epoch [1/300], Step [24/225], Training Accuracy: 26.8880%, Training Loss: 1.5105%\n",
      "Epoch [1/300], Step [25/225], Training Accuracy: 27.2500%, Training Loss: 1.5077%\n",
      "Epoch [1/300], Step [26/225], Training Accuracy: 27.2236%, Training Loss: 1.5038%\n",
      "Epoch [1/300], Step [27/225], Training Accuracy: 27.3148%, Training Loss: 1.5010%\n",
      "Epoch [1/300], Step [28/225], Training Accuracy: 27.2879%, Training Loss: 1.4981%\n",
      "Epoch [1/300], Step [29/225], Training Accuracy: 27.6401%, Training Loss: 1.4957%\n",
      "Epoch [1/300], Step [30/225], Training Accuracy: 27.6562%, Training Loss: 1.4906%\n",
      "Epoch [1/300], Step [31/225], Training Accuracy: 27.3690%, Training Loss: 1.4900%\n",
      "Epoch [1/300], Step [32/225], Training Accuracy: 27.6367%, Training Loss: 1.4871%\n",
      "Epoch [1/300], Step [33/225], Training Accuracy: 27.6989%, Training Loss: 1.4848%\n",
      "Epoch [1/300], Step [34/225], Training Accuracy: 27.7574%, Training Loss: 1.4810%\n",
      "Epoch [1/300], Step [35/225], Training Accuracy: 27.9464%, Training Loss: 1.4779%\n",
      "Epoch [1/300], Step [36/225], Training Accuracy: 27.9514%, Training Loss: 1.4767%\n",
      "Epoch [1/300], Step [37/225], Training Accuracy: 28.1672%, Training Loss: 1.4748%\n",
      "Epoch [1/300], Step [38/225], Training Accuracy: 28.2072%, Training Loss: 1.4704%\n",
      "Epoch [1/300], Step [39/225], Training Accuracy: 28.2051%, Training Loss: 1.4661%\n",
      "Epoch [1/300], Step [40/225], Training Accuracy: 28.2422%, Training Loss: 1.4623%\n",
      "Epoch [1/300], Step [41/225], Training Accuracy: 28.3155%, Training Loss: 1.4596%\n",
      "Epoch [1/300], Step [42/225], Training Accuracy: 28.4970%, Training Loss: 1.4580%\n",
      "Epoch [1/300], Step [43/225], Training Accuracy: 28.6701%, Training Loss: 1.4567%\n",
      "Epoch [1/300], Step [44/225], Training Accuracy: 28.7642%, Training Loss: 1.4548%\n",
      "Epoch [1/300], Step [45/225], Training Accuracy: 29.0625%, Training Loss: 1.4526%\n",
      "Epoch [1/300], Step [46/225], Training Accuracy: 29.4837%, Training Loss: 1.4490%\n",
      "Epoch [1/300], Step [47/225], Training Accuracy: 29.5545%, Training Loss: 1.4473%\n",
      "Epoch [1/300], Step [48/225], Training Accuracy: 29.6224%, Training Loss: 1.4462%\n",
      "Epoch [1/300], Step [49/225], Training Accuracy: 29.5918%, Training Loss: 1.4475%\n",
      "Epoch [1/300], Step [50/225], Training Accuracy: 29.4688%, Training Loss: 1.4469%\n",
      "Epoch [1/300], Step [51/225], Training Accuracy: 29.5650%, Training Loss: 1.4447%\n",
      "Epoch [1/300], Step [52/225], Training Accuracy: 29.5974%, Training Loss: 1.4447%\n",
      "Epoch [1/300], Step [53/225], Training Accuracy: 29.6285%, Training Loss: 1.4424%\n",
      "Epoch [1/300], Step [54/225], Training Accuracy: 29.7164%, Training Loss: 1.4406%\n",
      "Epoch [1/300], Step [55/225], Training Accuracy: 29.8011%, Training Loss: 1.4388%\n",
      "Epoch [1/300], Step [56/225], Training Accuracy: 29.9386%, Training Loss: 1.4367%\n",
      "Epoch [1/300], Step [57/225], Training Accuracy: 29.9616%, Training Loss: 1.4345%\n",
      "Epoch [1/300], Step [58/225], Training Accuracy: 29.9569%, Training Loss: 1.4324%\n",
      "Epoch [1/300], Step [59/225], Training Accuracy: 30.0053%, Training Loss: 1.4294%\n",
      "Epoch [1/300], Step [60/225], Training Accuracy: 29.9740%, Training Loss: 1.4280%\n",
      "Epoch [1/300], Step [61/225], Training Accuracy: 29.9436%, Training Loss: 1.4266%\n",
      "Epoch [1/300], Step [62/225], Training Accuracy: 29.8891%, Training Loss: 1.4271%\n",
      "Epoch [1/300], Step [63/225], Training Accuracy: 29.7867%, Training Loss: 1.4265%\n",
      "Epoch [1/300], Step [64/225], Training Accuracy: 29.7363%, Training Loss: 1.4265%\n",
      "Epoch [1/300], Step [65/225], Training Accuracy: 29.7837%, Training Loss: 1.4253%\n",
      "Epoch [1/300], Step [66/225], Training Accuracy: 29.8059%, Training Loss: 1.4245%\n",
      "Epoch [1/300], Step [67/225], Training Accuracy: 29.8974%, Training Loss: 1.4228%\n",
      "Epoch [1/300], Step [68/225], Training Accuracy: 29.7794%, Training Loss: 1.4220%\n",
      "Epoch [1/300], Step [69/225], Training Accuracy: 29.7328%, Training Loss: 1.4215%\n",
      "Epoch [1/300], Step [70/225], Training Accuracy: 29.8661%, Training Loss: 1.4216%\n",
      "Epoch [1/300], Step [71/225], Training Accuracy: 29.8195%, Training Loss: 1.4215%\n",
      "Epoch [1/300], Step [72/225], Training Accuracy: 29.8394%, Training Loss: 1.4219%\n",
      "Epoch [1/300], Step [73/225], Training Accuracy: 29.7731%, Training Loss: 1.4218%\n",
      "Epoch [1/300], Step [74/225], Training Accuracy: 29.9831%, Training Loss: 1.4211%\n",
      "Epoch [1/300], Step [75/225], Training Accuracy: 30.0417%, Training Loss: 1.4198%\n",
      "Epoch [1/300], Step [76/225], Training Accuracy: 30.1398%, Training Loss: 1.4179%\n",
      "Epoch [1/300], Step [77/225], Training Accuracy: 30.1136%, Training Loss: 1.4183%\n",
      "Epoch [1/300], Step [78/225], Training Accuracy: 30.0881%, Training Loss: 1.4171%\n",
      "Epoch [1/300], Step [79/225], Training Accuracy: 30.2809%, Training Loss: 1.4163%\n",
      "Epoch [1/300], Step [80/225], Training Accuracy: 30.3516%, Training Loss: 1.4160%\n",
      "Epoch [1/300], Step [81/225], Training Accuracy: 30.4205%, Training Loss: 1.4153%\n",
      "Epoch [1/300], Step [82/225], Training Accuracy: 30.4688%, Training Loss: 1.4146%\n",
      "Epoch [1/300], Step [83/225], Training Accuracy: 30.5911%, Training Loss: 1.4134%\n",
      "Epoch [1/300], Step [84/225], Training Accuracy: 30.6176%, Training Loss: 1.4135%\n",
      "Epoch [1/300], Step [85/225], Training Accuracy: 30.6434%, Training Loss: 1.4135%\n",
      "Epoch [1/300], Step [86/225], Training Accuracy: 30.7413%, Training Loss: 1.4122%\n",
      "Epoch [1/300], Step [87/225], Training Accuracy: 30.7112%, Training Loss: 1.4116%\n",
      "Epoch [1/300], Step [88/225], Training Accuracy: 30.7351%, Training Loss: 1.4110%\n",
      "Epoch [1/300], Step [89/225], Training Accuracy: 30.7584%, Training Loss: 1.4114%\n",
      "Epoch [1/300], Step [90/225], Training Accuracy: 30.7118%, Training Loss: 1.4129%\n",
      "Epoch [1/300], Step [91/225], Training Accuracy: 30.6834%, Training Loss: 1.4126%\n",
      "Epoch [1/300], Step [92/225], Training Accuracy: 30.6386%, Training Loss: 1.4120%\n",
      "Epoch [1/300], Step [93/225], Training Accuracy: 30.5780%, Training Loss: 1.4125%\n",
      "Epoch [1/300], Step [94/225], Training Accuracy: 30.6848%, Training Loss: 1.4106%\n",
      "Epoch [1/300], Step [95/225], Training Accuracy: 30.6250%, Training Loss: 1.4109%\n",
      "Epoch [1/300], Step [96/225], Training Accuracy: 30.5990%, Training Loss: 1.4099%\n",
      "Epoch [1/300], Step [97/225], Training Accuracy: 30.6701%, Training Loss: 1.4089%\n",
      "Epoch [1/300], Step [98/225], Training Accuracy: 30.6282%, Training Loss: 1.4084%\n",
      "Epoch [1/300], Step [99/225], Training Accuracy: 30.6503%, Training Loss: 1.4077%\n",
      "Epoch [1/300], Step [100/225], Training Accuracy: 30.7656%, Training Loss: 1.4067%\n",
      "Epoch [1/300], Step [101/225], Training Accuracy: 30.8323%, Training Loss: 1.4060%\n",
      "Epoch [1/300], Step [102/225], Training Accuracy: 30.8517%, Training Loss: 1.4056%\n",
      "Epoch [1/300], Step [103/225], Training Accuracy: 30.8404%, Training Loss: 1.4058%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [104/225], Training Accuracy: 30.9195%, Training Loss: 1.4046%\n",
      "Epoch [1/300], Step [105/225], Training Accuracy: 30.9077%, Training Loss: 1.4046%\n",
      "Epoch [1/300], Step [106/225], Training Accuracy: 30.9552%, Training Loss: 1.4040%\n",
      "Epoch [1/300], Step [107/225], Training Accuracy: 31.0164%, Training Loss: 1.4036%\n",
      "Epoch [1/300], Step [108/225], Training Accuracy: 31.0619%, Training Loss: 1.4035%\n",
      "Epoch [1/300], Step [109/225], Training Accuracy: 31.0206%, Training Loss: 1.4032%\n",
      "Epoch [1/300], Step [110/225], Training Accuracy: 31.0227%, Training Loss: 1.4025%\n",
      "Epoch [1/300], Step [111/225], Training Accuracy: 30.9685%, Training Loss: 1.4022%\n",
      "Epoch [1/300], Step [112/225], Training Accuracy: 31.0268%, Training Loss: 1.4022%\n",
      "Epoch [1/300], Step [113/225], Training Accuracy: 30.9735%, Training Loss: 1.4022%\n",
      "Epoch [1/300], Step [114/225], Training Accuracy: 31.0444%, Training Loss: 1.4008%\n",
      "Epoch [1/300], Step [115/225], Training Accuracy: 31.1141%, Training Loss: 1.4005%\n",
      "Epoch [1/300], Step [116/225], Training Accuracy: 31.1961%, Training Loss: 1.4001%\n",
      "Epoch [1/300], Step [117/225], Training Accuracy: 31.1165%, Training Loss: 1.4010%\n",
      "Epoch [1/300], Step [118/225], Training Accuracy: 31.1838%, Training Loss: 1.3999%\n",
      "Epoch [1/300], Step [119/225], Training Accuracy: 31.2369%, Training Loss: 1.3994%\n",
      "Epoch [1/300], Step [120/225], Training Accuracy: 31.2630%, Training Loss: 1.3985%\n",
      "Epoch [1/300], Step [121/225], Training Accuracy: 31.2758%, Training Loss: 1.3984%\n",
      "Epoch [1/300], Step [122/225], Training Accuracy: 31.2884%, Training Loss: 1.3977%\n",
      "Epoch [1/300], Step [123/225], Training Accuracy: 31.2627%, Training Loss: 1.3978%\n",
      "Epoch [1/300], Step [124/225], Training Accuracy: 31.3634%, Training Loss: 1.3968%\n",
      "Epoch [1/300], Step [125/225], Training Accuracy: 31.3500%, Training Loss: 1.3980%\n",
      "Epoch [1/300], Step [126/225], Training Accuracy: 31.3988%, Training Loss: 1.3974%\n",
      "Epoch [1/300], Step [127/225], Training Accuracy: 31.3853%, Training Loss: 1.3968%\n",
      "Epoch [1/300], Step [128/225], Training Accuracy: 31.3965%, Training Loss: 1.3965%\n",
      "Epoch [1/300], Step [129/225], Training Accuracy: 31.4438%, Training Loss: 1.3961%\n",
      "Epoch [1/300], Step [130/225], Training Accuracy: 31.4423%, Training Loss: 1.3960%\n",
      "Epoch [1/300], Step [131/225], Training Accuracy: 31.3931%, Training Loss: 1.3954%\n",
      "Epoch [1/300], Step [132/225], Training Accuracy: 31.3684%, Training Loss: 1.3951%\n",
      "Epoch [1/300], Step [133/225], Training Accuracy: 31.4262%, Training Loss: 1.3945%\n",
      "Epoch [1/300], Step [134/225], Training Accuracy: 31.4249%, Training Loss: 1.3945%\n",
      "Epoch [1/300], Step [135/225], Training Accuracy: 31.3773%, Training Loss: 1.3946%\n",
      "Epoch [1/300], Step [136/225], Training Accuracy: 31.3304%, Training Loss: 1.3945%\n",
      "Epoch [1/300], Step [137/225], Training Accuracy: 31.3641%, Training Loss: 1.3943%\n",
      "Epoch [1/300], Step [138/225], Training Accuracy: 31.3632%, Training Loss: 1.3942%\n",
      "Epoch [1/300], Step [139/225], Training Accuracy: 31.3512%, Training Loss: 1.3941%\n",
      "Epoch [1/300], Step [140/225], Training Accuracy: 31.3393%, Training Loss: 1.3942%\n",
      "Epoch [1/300], Step [141/225], Training Accuracy: 31.3387%, Training Loss: 1.3938%\n",
      "Epoch [1/300], Step [142/225], Training Accuracy: 31.3930%, Training Loss: 1.3930%\n",
      "Epoch [1/300], Step [143/225], Training Accuracy: 31.4795%, Training Loss: 1.3922%\n",
      "Epoch [1/300], Step [144/225], Training Accuracy: 31.5213%, Training Loss: 1.3921%\n",
      "Epoch [1/300], Step [145/225], Training Accuracy: 31.5625%, Training Loss: 1.3921%\n",
      "Epoch [1/300], Step [146/225], Training Accuracy: 31.5818%, Training Loss: 1.3917%\n",
      "Epoch [1/300], Step [147/225], Training Accuracy: 31.6539%, Training Loss: 1.3909%\n",
      "Epoch [1/300], Step [148/225], Training Accuracy: 31.6934%, Training Loss: 1.3903%\n",
      "Epoch [1/300], Step [149/225], Training Accuracy: 31.7429%, Training Loss: 1.3903%\n",
      "Epoch [1/300], Step [150/225], Training Accuracy: 31.7396%, Training Loss: 1.3903%\n",
      "Epoch [1/300], Step [151/225], Training Accuracy: 31.7570%, Training Loss: 1.3902%\n",
      "Epoch [1/300], Step [152/225], Training Accuracy: 31.7537%, Training Loss: 1.3901%\n",
      "Epoch [1/300], Step [153/225], Training Accuracy: 31.7504%, Training Loss: 1.3898%\n",
      "Epoch [1/300], Step [154/225], Training Accuracy: 31.7269%, Training Loss: 1.3895%\n",
      "Epoch [1/300], Step [155/225], Training Accuracy: 31.7540%, Training Loss: 1.3893%\n",
      "Epoch [1/300], Step [156/225], Training Accuracy: 31.7909%, Training Loss: 1.3889%\n",
      "Epoch [1/300], Step [157/225], Training Accuracy: 31.7974%, Training Loss: 1.3886%\n",
      "Epoch [1/300], Step [158/225], Training Accuracy: 31.7939%, Training Loss: 1.3883%\n",
      "Epoch [1/300], Step [159/225], Training Accuracy: 31.8691%, Training Loss: 1.3876%\n",
      "Epoch [1/300], Step [160/225], Training Accuracy: 31.8945%, Training Loss: 1.3871%\n",
      "Epoch [1/300], Step [161/225], Training Accuracy: 31.9585%, Training Loss: 1.3863%\n",
      "Epoch [1/300], Step [162/225], Training Accuracy: 32.0216%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [163/225], Training Accuracy: 32.0073%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [164/225], Training Accuracy: 32.0694%, Training Loss: 1.3846%\n",
      "Epoch [1/300], Step [165/225], Training Accuracy: 32.0549%, Training Loss: 1.3845%\n",
      "Epoch [1/300], Step [166/225], Training Accuracy: 32.1160%, Training Loss: 1.3837%\n",
      "Epoch [1/300], Step [167/225], Training Accuracy: 32.1201%, Training Loss: 1.3834%\n",
      "Epoch [1/300], Step [168/225], Training Accuracy: 32.1522%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [169/225], Training Accuracy: 32.1561%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [170/225], Training Accuracy: 32.1415%, Training Loss: 1.3835%\n",
      "Epoch [1/300], Step [171/225], Training Accuracy: 32.2094%, Training Loss: 1.3828%\n",
      "Epoch [1/300], Step [172/225], Training Accuracy: 32.2856%, Training Loss: 1.3821%\n",
      "Epoch [1/300], Step [173/225], Training Accuracy: 32.3519%, Training Loss: 1.3815%\n",
      "Epoch [1/300], Step [174/225], Training Accuracy: 32.3635%, Training Loss: 1.3809%\n",
      "Epoch [1/300], Step [175/225], Training Accuracy: 32.3304%, Training Loss: 1.3815%\n",
      "Epoch [1/300], Step [176/225], Training Accuracy: 32.3775%, Training Loss: 1.3814%\n",
      "Epoch [1/300], Step [177/225], Training Accuracy: 32.4682%, Training Loss: 1.3810%\n",
      "Epoch [1/300], Step [178/225], Training Accuracy: 32.4614%, Training Loss: 1.3804%\n",
      "Epoch [1/300], Step [179/225], Training Accuracy: 32.4633%, Training Loss: 1.3800%\n",
      "Epoch [1/300], Step [180/225], Training Accuracy: 32.4479%, Training Loss: 1.3796%\n",
      "Epoch [1/300], Step [181/225], Training Accuracy: 32.4586%, Training Loss: 1.3790%\n",
      "Epoch [1/300], Step [182/225], Training Accuracy: 32.4948%, Training Loss: 1.3785%\n",
      "Epoch [1/300], Step [183/225], Training Accuracy: 32.5564%, Training Loss: 1.3780%\n",
      "Epoch [1/300], Step [184/225], Training Accuracy: 32.5662%, Training Loss: 1.3777%\n",
      "Epoch [1/300], Step [185/225], Training Accuracy: 32.5929%, Training Loss: 1.3770%\n",
      "Epoch [1/300], Step [186/225], Training Accuracy: 32.6277%, Training Loss: 1.3772%\n",
      "Epoch [1/300], Step [187/225], Training Accuracy: 32.6621%, Training Loss: 1.3768%\n",
      "Epoch [1/300], Step [188/225], Training Accuracy: 32.6712%, Training Loss: 1.3767%\n",
      "Epoch [1/300], Step [189/225], Training Accuracy: 32.6802%, Training Loss: 1.3763%\n",
      "Epoch [1/300], Step [190/225], Training Accuracy: 32.6727%, Training Loss: 1.3765%\n",
      "Epoch [1/300], Step [191/225], Training Accuracy: 32.7062%, Training Loss: 1.3761%\n",
      "Epoch [1/300], Step [192/225], Training Accuracy: 32.7067%, Training Loss: 1.3759%\n",
      "Epoch [1/300], Step [193/225], Training Accuracy: 32.7153%, Training Loss: 1.3756%\n",
      "Epoch [1/300], Step [194/225], Training Accuracy: 32.7320%, Training Loss: 1.3751%\n",
      "Epoch [1/300], Step [195/225], Training Accuracy: 32.7724%, Training Loss: 1.3747%\n",
      "Epoch [1/300], Step [196/225], Training Accuracy: 32.7408%, Training Loss: 1.3752%\n",
      "Epoch [1/300], Step [197/225], Training Accuracy: 32.7253%, Training Loss: 1.3753%\n",
      "Epoch [1/300], Step [198/225], Training Accuracy: 32.7652%, Training Loss: 1.3746%\n",
      "Epoch [1/300], Step [199/225], Training Accuracy: 32.7654%, Training Loss: 1.3747%\n",
      "Epoch [1/300], Step [200/225], Training Accuracy: 32.7656%, Training Loss: 1.3744%\n",
      "Epoch [1/300], Step [201/225], Training Accuracy: 32.7659%, Training Loss: 1.3739%\n",
      "Epoch [1/300], Step [202/225], Training Accuracy: 32.7661%, Training Loss: 1.3737%\n",
      "Epoch [1/300], Step [203/225], Training Accuracy: 32.7509%, Training Loss: 1.3739%\n",
      "Epoch [1/300], Step [204/225], Training Accuracy: 32.7512%, Training Loss: 1.3738%\n",
      "Epoch [1/300], Step [205/225], Training Accuracy: 32.7820%, Training Loss: 1.3735%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [206/225], Training Accuracy: 32.7518%, Training Loss: 1.3735%\n",
      "Epoch [1/300], Step [207/225], Training Accuracy: 32.7446%, Training Loss: 1.3736%\n",
      "Epoch [1/300], Step [208/225], Training Accuracy: 32.8050%, Training Loss: 1.3732%\n",
      "Epoch [1/300], Step [209/225], Training Accuracy: 32.7975%, Training Loss: 1.3731%\n",
      "Epoch [1/300], Step [210/225], Training Accuracy: 32.7604%, Training Loss: 1.3732%\n",
      "Epoch [1/300], Step [211/225], Training Accuracy: 32.7829%, Training Loss: 1.3729%\n",
      "Epoch [1/300], Step [212/225], Training Accuracy: 32.7904%, Training Loss: 1.3725%\n",
      "Epoch [1/300], Step [213/225], Training Accuracy: 32.7905%, Training Loss: 1.3723%\n",
      "Epoch [1/300], Step [214/225], Training Accuracy: 32.8052%, Training Loss: 1.3721%\n",
      "Epoch [1/300], Step [215/225], Training Accuracy: 32.8343%, Training Loss: 1.3719%\n",
      "Epoch [1/300], Step [216/225], Training Accuracy: 32.8125%, Training Loss: 1.3717%\n",
      "Epoch [1/300], Step [217/225], Training Accuracy: 32.8485%, Training Loss: 1.3710%\n",
      "Epoch [1/300], Step [218/225], Training Accuracy: 32.8197%, Training Loss: 1.3712%\n",
      "Epoch [1/300], Step [219/225], Training Accuracy: 32.8482%, Training Loss: 1.3709%\n",
      "Epoch [1/300], Step [220/225], Training Accuracy: 32.8693%, Training Loss: 1.3708%\n",
      "Epoch [1/300], Step [221/225], Training Accuracy: 32.8479%, Training Loss: 1.3711%\n",
      "Epoch [1/300], Step [222/225], Training Accuracy: 32.8407%, Training Loss: 1.3714%\n",
      "Epoch [1/300], Step [223/225], Training Accuracy: 32.8756%, Training Loss: 1.3710%\n",
      "Epoch [1/300], Step [224/225], Training Accuracy: 32.8683%, Training Loss: 1.3709%\n",
      "Epoch [1/300], Step [225/225], Training Accuracy: 32.8724%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 1.4181%\n",
      "Epoch [2/300], Step [2/225], Training Accuracy: 35.1562%, Training Loss: 1.3389%\n",
      "Epoch [2/300], Step [3/225], Training Accuracy: 38.5417%, Training Loss: 1.3405%\n",
      "Epoch [2/300], Step [4/225], Training Accuracy: 39.0625%, Training Loss: 1.3750%\n",
      "Epoch [2/300], Step [5/225], Training Accuracy: 38.7500%, Training Loss: 1.3861%\n",
      "Epoch [2/300], Step [6/225], Training Accuracy: 38.2812%, Training Loss: 1.3833%\n",
      "Epoch [2/300], Step [7/225], Training Accuracy: 36.1607%, Training Loss: 1.3904%\n",
      "Epoch [2/300], Step [8/225], Training Accuracy: 35.3516%, Training Loss: 1.3939%\n",
      "Epoch [2/300], Step [9/225], Training Accuracy: 34.8958%, Training Loss: 1.3826%\n",
      "Epoch [2/300], Step [10/225], Training Accuracy: 34.6875%, Training Loss: 1.3791%\n",
      "Epoch [2/300], Step [11/225], Training Accuracy: 34.0909%, Training Loss: 1.3771%\n",
      "Epoch [2/300], Step [12/225], Training Accuracy: 33.5938%, Training Loss: 1.3708%\n",
      "Epoch [2/300], Step [13/225], Training Accuracy: 34.1346%, Training Loss: 1.3727%\n",
      "Epoch [2/300], Step [14/225], Training Accuracy: 34.2634%, Training Loss: 1.3724%\n",
      "Epoch [2/300], Step [15/225], Training Accuracy: 34.0625%, Training Loss: 1.3712%\n",
      "Epoch [2/300], Step [16/225], Training Accuracy: 34.0820%, Training Loss: 1.3641%\n",
      "Epoch [2/300], Step [17/225], Training Accuracy: 34.4669%, Training Loss: 1.3590%\n",
      "Epoch [2/300], Step [18/225], Training Accuracy: 34.1146%, Training Loss: 1.3594%\n",
      "Epoch [2/300], Step [19/225], Training Accuracy: 34.1283%, Training Loss: 1.3588%\n",
      "Epoch [2/300], Step [20/225], Training Accuracy: 34.3750%, Training Loss: 1.3539%\n",
      "Epoch [2/300], Step [21/225], Training Accuracy: 34.4494%, Training Loss: 1.3526%\n",
      "Epoch [2/300], Step [22/225], Training Accuracy: 34.0199%, Training Loss: 1.3525%\n",
      "Epoch [2/300], Step [23/225], Training Accuracy: 34.6467%, Training Loss: 1.3482%\n",
      "Epoch [2/300], Step [24/225], Training Accuracy: 34.7005%, Training Loss: 1.3457%\n",
      "Epoch [2/300], Step [25/225], Training Accuracy: 34.7500%, Training Loss: 1.3450%\n",
      "Epoch [2/300], Step [26/225], Training Accuracy: 34.5553%, Training Loss: 1.3474%\n",
      "Epoch [2/300], Step [27/225], Training Accuracy: 34.7222%, Training Loss: 1.3468%\n",
      "Epoch [2/300], Step [28/225], Training Accuracy: 34.4308%, Training Loss: 1.3457%\n",
      "Epoch [2/300], Step [29/225], Training Accuracy: 34.6444%, Training Loss: 1.3444%\n",
      "Epoch [2/300], Step [30/225], Training Accuracy: 34.7396%, Training Loss: 1.3447%\n",
      "Epoch [2/300], Step [31/225], Training Accuracy: 34.7782%, Training Loss: 1.3455%\n",
      "Epoch [2/300], Step [32/225], Training Accuracy: 35.3516%, Training Loss: 1.3420%\n",
      "Epoch [2/300], Step [33/225], Training Accuracy: 35.4167%, Training Loss: 1.3399%\n",
      "Epoch [2/300], Step [34/225], Training Accuracy: 35.3860%, Training Loss: 1.3384%\n",
      "Epoch [2/300], Step [35/225], Training Accuracy: 35.3125%, Training Loss: 1.3365%\n",
      "Epoch [2/300], Step [36/225], Training Accuracy: 35.3733%, Training Loss: 1.3365%\n",
      "Epoch [2/300], Step [37/225], Training Accuracy: 35.3885%, Training Loss: 1.3363%\n",
      "Epoch [2/300], Step [38/225], Training Accuracy: 35.4441%, Training Loss: 1.3353%\n",
      "Epoch [2/300], Step [39/225], Training Accuracy: 35.2564%, Training Loss: 1.3345%\n",
      "Epoch [2/300], Step [40/225], Training Accuracy: 35.0000%, Training Loss: 1.3344%\n",
      "Epoch [2/300], Step [41/225], Training Accuracy: 34.9085%, Training Loss: 1.3348%\n",
      "Epoch [2/300], Step [42/225], Training Accuracy: 34.8214%, Training Loss: 1.3345%\n",
      "Epoch [2/300], Step [43/225], Training Accuracy: 34.6657%, Training Loss: 1.3357%\n",
      "Epoch [2/300], Step [44/225], Training Accuracy: 34.9787%, Training Loss: 1.3326%\n",
      "Epoch [2/300], Step [45/225], Training Accuracy: 35.0000%, Training Loss: 1.3309%\n",
      "Epoch [2/300], Step [46/225], Training Accuracy: 35.1562%, Training Loss: 1.3300%\n",
      "Epoch [2/300], Step [47/225], Training Accuracy: 35.2726%, Training Loss: 1.3287%\n",
      "Epoch [2/300], Step [48/225], Training Accuracy: 35.1562%, Training Loss: 1.3282%\n",
      "Epoch [2/300], Step [49/225], Training Accuracy: 35.0446%, Training Loss: 1.3291%\n",
      "Epoch [2/300], Step [50/225], Training Accuracy: 35.0938%, Training Loss: 1.3295%\n",
      "Epoch [2/300], Step [51/225], Training Accuracy: 35.2635%, Training Loss: 1.3289%\n",
      "Epoch [2/300], Step [52/225], Training Accuracy: 35.1262%, Training Loss: 1.3295%\n",
      "Epoch [2/300], Step [53/225], Training Accuracy: 35.1120%, Training Loss: 1.3289%\n",
      "Epoch [2/300], Step [54/225], Training Accuracy: 35.1562%, Training Loss: 1.3276%\n",
      "Epoch [2/300], Step [55/225], Training Accuracy: 35.3409%, Training Loss: 1.3266%\n",
      "Epoch [2/300], Step [56/225], Training Accuracy: 35.3516%, Training Loss: 1.3260%\n",
      "Epoch [2/300], Step [57/225], Training Accuracy: 35.4441%, Training Loss: 1.3240%\n",
      "Epoch [2/300], Step [58/225], Training Accuracy: 35.2640%, Training Loss: 1.3249%\n",
      "Epoch [2/300], Step [59/225], Training Accuracy: 35.4608%, Training Loss: 1.3217%\n",
      "Epoch [2/300], Step [60/225], Training Accuracy: 35.5208%, Training Loss: 1.3210%\n",
      "Epoch [2/300], Step [61/225], Training Accuracy: 35.4252%, Training Loss: 1.3207%\n",
      "Epoch [2/300], Step [62/225], Training Accuracy: 35.3831%, Training Loss: 1.3200%\n",
      "Epoch [2/300], Step [63/225], Training Accuracy: 35.2927%, Training Loss: 1.3193%\n",
      "Epoch [2/300], Step [64/225], Training Accuracy: 35.2783%, Training Loss: 1.3195%\n",
      "Epoch [2/300], Step [65/225], Training Accuracy: 35.3606%, Training Loss: 1.3179%\n",
      "Epoch [2/300], Step [66/225], Training Accuracy: 35.5350%, Training Loss: 1.3173%\n",
      "Epoch [2/300], Step [67/225], Training Accuracy: 35.7043%, Training Loss: 1.3160%\n",
      "Epoch [2/300], Step [68/225], Training Accuracy: 35.6388%, Training Loss: 1.3148%\n",
      "Epoch [2/300], Step [69/225], Training Accuracy: 35.6431%, Training Loss: 1.3136%\n",
      "Epoch [2/300], Step [70/225], Training Accuracy: 35.6920%, Training Loss: 1.3138%\n",
      "Epoch [2/300], Step [71/225], Training Accuracy: 35.7835%, Training Loss: 1.3131%\n",
      "Epoch [2/300], Step [72/225], Training Accuracy: 35.6337%, Training Loss: 1.3138%\n",
      "Epoch [2/300], Step [73/225], Training Accuracy: 35.5308%, Training Loss: 1.3147%\n",
      "Epoch [2/300], Step [74/225], Training Accuracy: 35.5997%, Training Loss: 1.3141%\n",
      "Epoch [2/300], Step [75/225], Training Accuracy: 35.6458%, Training Loss: 1.3127%\n",
      "Epoch [2/300], Step [76/225], Training Accuracy: 35.5880%, Training Loss: 1.3128%\n",
      "Epoch [2/300], Step [77/225], Training Accuracy: 35.5317%, Training Loss: 1.3136%\n",
      "Epoch [2/300], Step [78/225], Training Accuracy: 35.6571%, Training Loss: 1.3126%\n",
      "Epoch [2/300], Step [79/225], Training Accuracy: 35.7199%, Training Loss: 1.3123%\n",
      "Epoch [2/300], Step [80/225], Training Accuracy: 35.7812%, Training Loss: 1.3115%\n",
      "Epoch [2/300], Step [81/225], Training Accuracy: 35.8410%, Training Loss: 1.3117%\n",
      "Epoch [2/300], Step [82/225], Training Accuracy: 35.9566%, Training Loss: 1.3112%\n",
      "Epoch [2/300], Step [83/225], Training Accuracy: 36.0505%, Training Loss: 1.3106%\n",
      "Epoch [2/300], Step [84/225], Training Accuracy: 36.1049%, Training Loss: 1.3099%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [85/225], Training Accuracy: 36.1581%, Training Loss: 1.3103%\n",
      "Epoch [2/300], Step [86/225], Training Accuracy: 36.1010%, Training Loss: 1.3096%\n",
      "Epoch [2/300], Step [87/225], Training Accuracy: 36.0812%, Training Loss: 1.3104%\n",
      "Epoch [2/300], Step [88/225], Training Accuracy: 36.1328%, Training Loss: 1.3100%\n",
      "Epoch [2/300], Step [89/225], Training Accuracy: 36.0604%, Training Loss: 1.3116%\n",
      "Epoch [2/300], Step [90/225], Training Accuracy: 36.0590%, Training Loss: 1.3121%\n",
      "Epoch [2/300], Step [91/225], Training Accuracy: 36.0405%, Training Loss: 1.3121%\n",
      "Epoch [2/300], Step [92/225], Training Accuracy: 36.0054%, Training Loss: 1.3110%\n",
      "Epoch [2/300], Step [93/225], Training Accuracy: 35.9375%, Training Loss: 1.3119%\n",
      "Epoch [2/300], Step [94/225], Training Accuracy: 35.9874%, Training Loss: 1.3107%\n",
      "Epoch [2/300], Step [95/225], Training Accuracy: 35.9211%, Training Loss: 1.3118%\n",
      "Epoch [2/300], Step [96/225], Training Accuracy: 35.9701%, Training Loss: 1.3111%\n",
      "Epoch [2/300], Step [97/225], Training Accuracy: 36.0503%, Training Loss: 1.3104%\n",
      "Epoch [2/300], Step [98/225], Training Accuracy: 36.0332%, Training Loss: 1.3108%\n",
      "Epoch [2/300], Step [99/225], Training Accuracy: 36.0795%, Training Loss: 1.3101%\n",
      "Epoch [2/300], Step [100/225], Training Accuracy: 36.0781%, Training Loss: 1.3100%\n",
      "Epoch [2/300], Step [101/225], Training Accuracy: 36.1386%, Training Loss: 1.3095%\n",
      "Epoch [2/300], Step [102/225], Training Accuracy: 36.2132%, Training Loss: 1.3088%\n",
      "Epoch [2/300], Step [103/225], Training Accuracy: 36.2257%, Training Loss: 1.3083%\n",
      "Epoch [2/300], Step [104/225], Training Accuracy: 36.1629%, Training Loss: 1.3081%\n",
      "Epoch [2/300], Step [105/225], Training Accuracy: 36.2054%, Training Loss: 1.3078%\n",
      "Epoch [2/300], Step [106/225], Training Accuracy: 36.2618%, Training Loss: 1.3067%\n",
      "Epoch [2/300], Step [107/225], Training Accuracy: 36.3610%, Training Loss: 1.3062%\n",
      "Epoch [2/300], Step [108/225], Training Accuracy: 36.3426%, Training Loss: 1.3056%\n",
      "Epoch [2/300], Step [109/225], Training Accuracy: 36.2672%, Training Loss: 1.3062%\n",
      "Epoch [2/300], Step [110/225], Training Accuracy: 36.2784%, Training Loss: 1.3054%\n",
      "Epoch [2/300], Step [111/225], Training Accuracy: 36.2613%, Training Loss: 1.3051%\n",
      "Epoch [2/300], Step [112/225], Training Accuracy: 36.3142%, Training Loss: 1.3043%\n",
      "Epoch [2/300], Step [113/225], Training Accuracy: 36.3247%, Training Loss: 1.3043%\n",
      "Epoch [2/300], Step [114/225], Training Accuracy: 36.3350%, Training Loss: 1.3033%\n",
      "Epoch [2/300], Step [115/225], Training Accuracy: 36.3179%, Training Loss: 1.3034%\n",
      "Epoch [2/300], Step [116/225], Training Accuracy: 36.3147%, Training Loss: 1.3032%\n",
      "Epoch [2/300], Step [117/225], Training Accuracy: 36.1912%, Training Loss: 1.3044%\n",
      "Epoch [2/300], Step [118/225], Training Accuracy: 36.2421%, Training Loss: 1.3038%\n",
      "Epoch [2/300], Step [119/225], Training Accuracy: 36.2395%, Training Loss: 1.3035%\n",
      "Epoch [2/300], Step [120/225], Training Accuracy: 36.2109%, Training Loss: 1.3034%\n",
      "Epoch [2/300], Step [121/225], Training Accuracy: 36.2603%, Training Loss: 1.3032%\n",
      "Epoch [2/300], Step [122/225], Training Accuracy: 36.2449%, Training Loss: 1.3029%\n",
      "Epoch [2/300], Step [123/225], Training Accuracy: 36.1916%, Training Loss: 1.3034%\n",
      "Epoch [2/300], Step [124/225], Training Accuracy: 36.3281%, Training Loss: 1.3023%\n",
      "Epoch [2/300], Step [125/225], Training Accuracy: 36.2500%, Training Loss: 1.3033%\n",
      "Epoch [2/300], Step [126/225], Training Accuracy: 36.1731%, Training Loss: 1.3035%\n",
      "Epoch [2/300], Step [127/225], Training Accuracy: 36.1220%, Training Loss: 1.3032%\n",
      "Epoch [2/300], Step [128/225], Training Accuracy: 36.0352%, Training Loss: 1.3036%\n",
      "Epoch [2/300], Step [129/225], Training Accuracy: 36.0586%, Training Loss: 1.3032%\n",
      "Epoch [2/300], Step [130/225], Training Accuracy: 36.0817%, Training Loss: 1.3033%\n",
      "Epoch [2/300], Step [131/225], Training Accuracy: 36.1522%, Training Loss: 1.3029%\n",
      "Epoch [2/300], Step [132/225], Training Accuracy: 36.1269%, Training Loss: 1.3028%\n",
      "Epoch [2/300], Step [133/225], Training Accuracy: 36.1490%, Training Loss: 1.3028%\n",
      "Epoch [2/300], Step [134/225], Training Accuracy: 36.0308%, Training Loss: 1.3033%\n",
      "Epoch [2/300], Step [135/225], Training Accuracy: 35.9838%, Training Loss: 1.3034%\n",
      "Epoch [2/300], Step [136/225], Training Accuracy: 35.9835%, Training Loss: 1.3030%\n",
      "Epoch [2/300], Step [137/225], Training Accuracy: 35.9489%, Training Loss: 1.3030%\n",
      "Epoch [2/300], Step [138/225], Training Accuracy: 35.9941%, Training Loss: 1.3030%\n",
      "Epoch [2/300], Step [139/225], Training Accuracy: 35.9825%, Training Loss: 1.3030%\n",
      "Epoch [2/300], Step [140/225], Training Accuracy: 36.0714%, Training Loss: 1.3027%\n",
      "Epoch [2/300], Step [141/225], Training Accuracy: 36.0816%, Training Loss: 1.3023%\n",
      "Epoch [2/300], Step [142/225], Training Accuracy: 36.1906%, Training Loss: 1.3015%\n",
      "Epoch [2/300], Step [143/225], Training Accuracy: 36.2107%, Training Loss: 1.3010%\n",
      "Epoch [2/300], Step [144/225], Training Accuracy: 36.2088%, Training Loss: 1.3013%\n",
      "Epoch [2/300], Step [145/225], Training Accuracy: 36.2500%, Training Loss: 1.3016%\n",
      "Epoch [2/300], Step [146/225], Training Accuracy: 36.2693%, Training Loss: 1.3017%\n",
      "Epoch [2/300], Step [147/225], Training Accuracy: 36.3308%, Training Loss: 1.3010%\n",
      "Epoch [2/300], Step [148/225], Training Accuracy: 36.4020%, Training Loss: 1.3007%\n",
      "Epoch [2/300], Step [149/225], Training Accuracy: 36.4199%, Training Loss: 1.3004%\n",
      "Epoch [2/300], Step [150/225], Training Accuracy: 36.4062%, Training Loss: 1.3007%\n",
      "Epoch [2/300], Step [151/225], Training Accuracy: 36.4549%, Training Loss: 1.3000%\n",
      "Epoch [2/300], Step [152/225], Training Accuracy: 36.4206%, Training Loss: 1.3001%\n",
      "Epoch [2/300], Step [153/225], Training Accuracy: 36.3562%, Training Loss: 1.2999%\n",
      "Epoch [2/300], Step [154/225], Training Accuracy: 36.3535%, Training Loss: 1.3000%\n",
      "Epoch [2/300], Step [155/225], Training Accuracy: 36.3004%, Training Loss: 1.3000%\n",
      "Epoch [2/300], Step [156/225], Training Accuracy: 36.3081%, Training Loss: 1.3002%\n",
      "Epoch [2/300], Step [157/225], Training Accuracy: 36.3256%, Training Loss: 1.2999%\n",
      "Epoch [2/300], Step [158/225], Training Accuracy: 36.2935%, Training Loss: 1.3003%\n",
      "Epoch [2/300], Step [159/225], Training Accuracy: 36.3601%, Training Loss: 1.2997%\n",
      "Epoch [2/300], Step [160/225], Training Accuracy: 36.4062%, Training Loss: 1.2992%\n",
      "Epoch [2/300], Step [161/225], Training Accuracy: 36.5198%, Training Loss: 1.2986%\n",
      "Epoch [2/300], Step [162/225], Training Accuracy: 36.5934%, Training Loss: 1.2976%\n",
      "Epoch [2/300], Step [163/225], Training Accuracy: 36.6181%, Training Loss: 1.2973%\n",
      "Epoch [2/300], Step [164/225], Training Accuracy: 36.6425%, Training Loss: 1.2971%\n",
      "Epoch [2/300], Step [165/225], Training Accuracy: 36.5720%, Training Loss: 1.2981%\n",
      "Epoch [2/300], Step [166/225], Training Accuracy: 36.5964%, Training Loss: 1.2974%\n",
      "Epoch [2/300], Step [167/225], Training Accuracy: 36.6392%, Training Loss: 1.2972%\n",
      "Epoch [2/300], Step [168/225], Training Accuracy: 36.6164%, Training Loss: 1.2971%\n",
      "Epoch [2/300], Step [169/225], Training Accuracy: 36.6587%, Training Loss: 1.2968%\n",
      "Epoch [2/300], Step [170/225], Training Accuracy: 36.6544%, Training Loss: 1.2967%\n",
      "Epoch [2/300], Step [171/225], Training Accuracy: 36.7050%, Training Loss: 1.2963%\n",
      "Epoch [2/300], Step [172/225], Training Accuracy: 36.7278%, Training Loss: 1.2961%\n",
      "Epoch [2/300], Step [173/225], Training Accuracy: 36.8046%, Training Loss: 1.2955%\n",
      "Epoch [2/300], Step [174/225], Training Accuracy: 36.8355%, Training Loss: 1.2949%\n",
      "Epoch [2/300], Step [175/225], Training Accuracy: 36.8304%, Training Loss: 1.2955%\n",
      "Epoch [2/300], Step [176/225], Training Accuracy: 36.8075%, Training Loss: 1.2959%\n",
      "Epoch [2/300], Step [177/225], Training Accuracy: 36.8026%, Training Loss: 1.2962%\n",
      "Epoch [2/300], Step [178/225], Training Accuracy: 36.7802%, Training Loss: 1.2958%\n",
      "Epoch [2/300], Step [179/225], Training Accuracy: 36.8715%, Training Loss: 1.2952%\n",
      "Epoch [2/300], Step [180/225], Training Accuracy: 36.8750%, Training Loss: 1.2949%\n",
      "Epoch [2/300], Step [181/225], Training Accuracy: 36.8871%, Training Loss: 1.2946%\n",
      "Epoch [2/300], Step [182/225], Training Accuracy: 36.8990%, Training Loss: 1.2942%\n",
      "Epoch [2/300], Step [183/225], Training Accuracy: 36.9194%, Training Loss: 1.2939%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [184/225], Training Accuracy: 36.9650%, Training Loss: 1.2934%\n",
      "Epoch [2/300], Step [185/225], Training Accuracy: 37.0186%, Training Loss: 1.2926%\n",
      "Epoch [2/300], Step [186/225], Training Accuracy: 37.0548%, Training Loss: 1.2924%\n",
      "Epoch [2/300], Step [187/225], Training Accuracy: 37.0822%, Training Loss: 1.2922%\n",
      "Epoch [2/300], Step [188/225], Training Accuracy: 37.1094%, Training Loss: 1.2919%\n",
      "Epoch [2/300], Step [189/225], Training Accuracy: 37.1114%, Training Loss: 1.2914%\n",
      "Epoch [2/300], Step [190/225], Training Accuracy: 37.1217%, Training Loss: 1.2917%\n",
      "Epoch [2/300], Step [191/225], Training Accuracy: 37.1237%, Training Loss: 1.2913%\n",
      "Epoch [2/300], Step [192/225], Training Accuracy: 37.1826%, Training Loss: 1.2910%\n",
      "Epoch [2/300], Step [193/225], Training Accuracy: 37.1924%, Training Loss: 1.2909%\n",
      "Epoch [2/300], Step [194/225], Training Accuracy: 37.2020%, Training Loss: 1.2905%\n",
      "Epoch [2/300], Step [195/225], Training Accuracy: 37.2196%, Training Loss: 1.2904%\n",
      "Epoch [2/300], Step [196/225], Training Accuracy: 37.2290%, Training Loss: 1.2907%\n",
      "Epoch [2/300], Step [197/225], Training Accuracy: 37.2938%, Training Loss: 1.2904%\n",
      "Epoch [2/300], Step [198/225], Training Accuracy: 37.3580%, Training Loss: 1.2899%\n",
      "Epoch [2/300], Step [199/225], Training Accuracy: 37.3665%, Training Loss: 1.2896%\n",
      "Epoch [2/300], Step [200/225], Training Accuracy: 37.3906%, Training Loss: 1.2898%\n",
      "Epoch [2/300], Step [201/225], Training Accuracy: 37.3834%, Training Loss: 1.2897%\n",
      "Epoch [2/300], Step [202/225], Training Accuracy: 37.4149%, Training Loss: 1.2894%\n",
      "Epoch [2/300], Step [203/225], Training Accuracy: 37.4076%, Training Loss: 1.2897%\n",
      "Epoch [2/300], Step [204/225], Training Accuracy: 37.4311%, Training Loss: 1.2894%\n",
      "Epoch [2/300], Step [205/225], Training Accuracy: 37.4390%, Training Loss: 1.2890%\n",
      "Epoch [2/300], Step [206/225], Training Accuracy: 37.4166%, Training Loss: 1.2889%\n",
      "Epoch [2/300], Step [207/225], Training Accuracy: 37.3868%, Training Loss: 1.2889%\n",
      "Epoch [2/300], Step [208/225], Training Accuracy: 37.4249%, Training Loss: 1.2881%\n",
      "Epoch [2/300], Step [209/225], Training Accuracy: 37.4551%, Training Loss: 1.2877%\n",
      "Epoch [2/300], Step [210/225], Training Accuracy: 37.4851%, Training Loss: 1.2871%\n",
      "Epoch [2/300], Step [211/225], Training Accuracy: 37.5000%, Training Loss: 1.2868%\n",
      "Epoch [2/300], Step [212/225], Training Accuracy: 37.5221%, Training Loss: 1.2867%\n",
      "Epoch [2/300], Step [213/225], Training Accuracy: 37.5220%, Training Loss: 1.2870%\n",
      "Epoch [2/300], Step [214/225], Training Accuracy: 37.5365%, Training Loss: 1.2869%\n",
      "Epoch [2/300], Step [215/225], Training Accuracy: 37.5291%, Training Loss: 1.2870%\n",
      "Epoch [2/300], Step [216/225], Training Accuracy: 37.5217%, Training Loss: 1.2870%\n",
      "Epoch [2/300], Step [217/225], Training Accuracy: 37.5576%, Training Loss: 1.2865%\n",
      "Epoch [2/300], Step [218/225], Training Accuracy: 37.5573%, Training Loss: 1.2864%\n",
      "Epoch [2/300], Step [219/225], Training Accuracy: 37.5928%, Training Loss: 1.2862%\n",
      "Epoch [2/300], Step [220/225], Training Accuracy: 37.6065%, Training Loss: 1.2861%\n",
      "Epoch [2/300], Step [221/225], Training Accuracy: 37.5636%, Training Loss: 1.2862%\n",
      "Epoch [2/300], Step [222/225], Training Accuracy: 37.5563%, Training Loss: 1.2861%\n",
      "Epoch [2/300], Step [223/225], Training Accuracy: 37.6051%, Training Loss: 1.2856%\n",
      "Epoch [2/300], Step [224/225], Training Accuracy: 37.5907%, Training Loss: 1.2854%\n",
      "Epoch [2/300], Step [225/225], Training Accuracy: 37.5973%, Training Loss: 1.2852%\n",
      "Epoch [3/300], Step [1/225], Training Accuracy: 40.6250%, Training Loss: 1.2517%\n",
      "Epoch [3/300], Step [2/225], Training Accuracy: 47.6562%, Training Loss: 1.2066%\n",
      "Epoch [3/300], Step [3/225], Training Accuracy: 45.3125%, Training Loss: 1.2249%\n",
      "Epoch [3/300], Step [4/225], Training Accuracy: 44.5312%, Training Loss: 1.2525%\n",
      "Epoch [3/300], Step [5/225], Training Accuracy: 44.6875%, Training Loss: 1.2783%\n",
      "Epoch [3/300], Step [6/225], Training Accuracy: 43.4896%, Training Loss: 1.2909%\n",
      "Epoch [3/300], Step [7/225], Training Accuracy: 41.9643%, Training Loss: 1.3031%\n",
      "Epoch [3/300], Step [8/225], Training Accuracy: 41.4062%, Training Loss: 1.2999%\n",
      "Epoch [3/300], Step [9/225], Training Accuracy: 40.7986%, Training Loss: 1.2964%\n",
      "Epoch [3/300], Step [10/225], Training Accuracy: 40.3125%, Training Loss: 1.2966%\n",
      "Epoch [3/300], Step [11/225], Training Accuracy: 40.1989%, Training Loss: 1.2988%\n",
      "Epoch [3/300], Step [12/225], Training Accuracy: 39.7135%, Training Loss: 1.2918%\n",
      "Epoch [3/300], Step [13/225], Training Accuracy: 39.4231%, Training Loss: 1.2907%\n",
      "Epoch [3/300], Step [14/225], Training Accuracy: 38.9509%, Training Loss: 1.2892%\n",
      "Epoch [3/300], Step [15/225], Training Accuracy: 38.4375%, Training Loss: 1.3014%\n",
      "Epoch [3/300], Step [16/225], Training Accuracy: 38.3789%, Training Loss: 1.2989%\n",
      "Epoch [3/300], Step [17/225], Training Accuracy: 38.4191%, Training Loss: 1.2961%\n",
      "Epoch [3/300], Step [18/225], Training Accuracy: 38.1944%, Training Loss: 1.2940%\n",
      "Epoch [3/300], Step [19/225], Training Accuracy: 38.2401%, Training Loss: 1.2893%\n",
      "Epoch [3/300], Step [20/225], Training Accuracy: 38.7500%, Training Loss: 1.2833%\n",
      "Epoch [3/300], Step [21/225], Training Accuracy: 38.7649%, Training Loss: 1.2806%\n",
      "Epoch [3/300], Step [22/225], Training Accuracy: 38.6364%, Training Loss: 1.2797%\n",
      "Epoch [3/300], Step [23/225], Training Accuracy: 38.8587%, Training Loss: 1.2745%\n",
      "Epoch [3/300], Step [24/225], Training Accuracy: 38.9323%, Training Loss: 1.2714%\n",
      "Epoch [3/300], Step [25/225], Training Accuracy: 39.0000%, Training Loss: 1.2699%\n",
      "Epoch [3/300], Step [26/225], Training Accuracy: 38.7620%, Training Loss: 1.2723%\n",
      "Epoch [3/300], Step [27/225], Training Accuracy: 39.1782%, Training Loss: 1.2735%\n",
      "Epoch [3/300], Step [28/225], Training Accuracy: 39.2299%, Training Loss: 1.2694%\n",
      "Epoch [3/300], Step [29/225], Training Accuracy: 39.4397%, Training Loss: 1.2646%\n",
      "Epoch [3/300], Step [30/225], Training Accuracy: 39.2708%, Training Loss: 1.2653%\n",
      "Epoch [3/300], Step [31/225], Training Accuracy: 39.1633%, Training Loss: 1.2656%\n",
      "Epoch [3/300], Step [32/225], Training Accuracy: 39.4531%, Training Loss: 1.2636%\n",
      "Epoch [3/300], Step [33/225], Training Accuracy: 39.4886%, Training Loss: 1.2608%\n",
      "Epoch [3/300], Step [34/225], Training Accuracy: 39.3842%, Training Loss: 1.2620%\n",
      "Epoch [3/300], Step [35/225], Training Accuracy: 39.5089%, Training Loss: 1.2599%\n",
      "Epoch [3/300], Step [36/225], Training Accuracy: 39.1927%, Training Loss: 1.2606%\n",
      "Epoch [3/300], Step [37/225], Training Accuracy: 39.3159%, Training Loss: 1.2599%\n",
      "Epoch [3/300], Step [38/225], Training Accuracy: 39.3914%, Training Loss: 1.2574%\n",
      "Epoch [3/300], Step [39/225], Training Accuracy: 39.4631%, Training Loss: 1.2564%\n",
      "Epoch [3/300], Step [40/225], Training Accuracy: 39.3750%, Training Loss: 1.2556%\n",
      "Epoch [3/300], Step [41/225], Training Accuracy: 39.4436%, Training Loss: 1.2547%\n",
      "Epoch [3/300], Step [42/225], Training Accuracy: 39.5089%, Training Loss: 1.2527%\n",
      "Epoch [3/300], Step [43/225], Training Accuracy: 39.7166%, Training Loss: 1.2520%\n",
      "Epoch [3/300], Step [44/225], Training Accuracy: 39.9148%, Training Loss: 1.2492%\n",
      "Epoch [3/300], Step [45/225], Training Accuracy: 40.1736%, Training Loss: 1.2456%\n",
      "Epoch [3/300], Step [46/225], Training Accuracy: 40.2514%, Training Loss: 1.2441%\n",
      "Epoch [3/300], Step [47/225], Training Accuracy: 40.2261%, Training Loss: 1.2425%\n",
      "Epoch [3/300], Step [48/225], Training Accuracy: 40.1367%, Training Loss: 1.2408%\n",
      "Epoch [3/300], Step [49/225], Training Accuracy: 40.1467%, Training Loss: 1.2433%\n",
      "Epoch [3/300], Step [50/225], Training Accuracy: 40.2812%, Training Loss: 1.2425%\n",
      "Epoch [3/300], Step [51/225], Training Accuracy: 40.3799%, Training Loss: 1.2420%\n",
      "Epoch [3/300], Step [52/225], Training Accuracy: 40.2945%, Training Loss: 1.2415%\n",
      "Epoch [3/300], Step [53/225], Training Accuracy: 40.5071%, Training Loss: 1.2405%\n",
      "Epoch [3/300], Step [54/225], Training Accuracy: 40.4514%, Training Loss: 1.2397%\n",
      "Epoch [3/300], Step [55/225], Training Accuracy: 40.4545%, Training Loss: 1.2387%\n",
      "Epoch [3/300], Step [56/225], Training Accuracy: 40.5134%, Training Loss: 1.2382%\n",
      "Epoch [3/300], Step [57/225], Training Accuracy: 40.5428%, Training Loss: 1.2367%\n",
      "Epoch [3/300], Step [58/225], Training Accuracy: 40.5442%, Training Loss: 1.2370%\n",
      "Epoch [3/300], Step [59/225], Training Accuracy: 40.5456%, Training Loss: 1.2358%\n",
      "Epoch [3/300], Step [60/225], Training Accuracy: 40.7031%, Training Loss: 1.2342%\n",
      "Epoch [3/300], Step [61/225], Training Accuracy: 40.7787%, Training Loss: 1.2335%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [62/225], Training Accuracy: 40.8266%, Training Loss: 1.2320%\n",
      "Epoch [3/300], Step [63/225], Training Accuracy: 40.7242%, Training Loss: 1.2325%\n",
      "Epoch [3/300], Step [64/225], Training Accuracy: 40.5518%, Training Loss: 1.2333%\n",
      "Epoch [3/300], Step [65/225], Training Accuracy: 40.4808%, Training Loss: 1.2331%\n",
      "Epoch [3/300], Step [66/225], Training Accuracy: 40.5540%, Training Loss: 1.2314%\n",
      "Epoch [3/300], Step [67/225], Training Accuracy: 40.5784%, Training Loss: 1.2305%\n",
      "Epoch [3/300], Step [68/225], Training Accuracy: 40.6020%, Training Loss: 1.2292%\n",
      "Epoch [3/300], Step [69/225], Training Accuracy: 40.6024%, Training Loss: 1.2284%\n",
      "Epoch [3/300], Step [70/225], Training Accuracy: 40.5357%, Training Loss: 1.2279%\n",
      "Epoch [3/300], Step [71/225], Training Accuracy: 40.4269%, Training Loss: 1.2277%\n",
      "Epoch [3/300], Step [72/225], Training Accuracy: 40.3863%, Training Loss: 1.2285%\n",
      "Epoch [3/300], Step [73/225], Training Accuracy: 40.3682%, Training Loss: 1.2286%\n",
      "Epoch [3/300], Step [74/225], Training Accuracy: 40.4561%, Training Loss: 1.2283%\n",
      "Epoch [3/300], Step [75/225], Training Accuracy: 40.5833%, Training Loss: 1.2272%\n",
      "Epoch [3/300], Step [76/225], Training Accuracy: 40.5016%, Training Loss: 1.2262%\n",
      "Epoch [3/300], Step [77/225], Training Accuracy: 40.4018%, Training Loss: 1.2281%\n",
      "Epoch [3/300], Step [78/225], Training Accuracy: 40.5248%, Training Loss: 1.2270%\n",
      "Epoch [3/300], Step [79/225], Training Accuracy: 40.4470%, Training Loss: 1.2270%\n",
      "Epoch [3/300], Step [80/225], Training Accuracy: 40.5273%, Training Loss: 1.2270%\n",
      "Epoch [3/300], Step [81/225], Training Accuracy: 40.6443%, Training Loss: 1.2267%\n",
      "Epoch [3/300], Step [82/225], Training Accuracy: 40.7393%, Training Loss: 1.2268%\n",
      "Epoch [3/300], Step [83/225], Training Accuracy: 40.8509%, Training Loss: 1.2257%\n",
      "Epoch [3/300], Step [84/225], Training Accuracy: 40.8854%, Training Loss: 1.2244%\n",
      "Epoch [3/300], Step [85/225], Training Accuracy: 40.8640%, Training Loss: 1.2247%\n",
      "Epoch [3/300], Step [86/225], Training Accuracy: 40.8975%, Training Loss: 1.2237%\n",
      "Epoch [3/300], Step [87/225], Training Accuracy: 40.8764%, Training Loss: 1.2239%\n",
      "Epoch [3/300], Step [88/225], Training Accuracy: 40.8203%, Training Loss: 1.2242%\n",
      "Epoch [3/300], Step [89/225], Training Accuracy: 40.7128%, Training Loss: 1.2247%\n",
      "Epoch [3/300], Step [90/225], Training Accuracy: 40.7292%, Training Loss: 1.2244%\n",
      "Epoch [3/300], Step [91/225], Training Accuracy: 40.7109%, Training Loss: 1.2238%\n",
      "Epoch [3/300], Step [92/225], Training Accuracy: 40.7099%, Training Loss: 1.2229%\n",
      "Epoch [3/300], Step [93/225], Training Accuracy: 40.7258%, Training Loss: 1.2229%\n",
      "Epoch [3/300], Step [94/225], Training Accuracy: 40.8411%, Training Loss: 1.2208%\n",
      "Epoch [3/300], Step [95/225], Training Accuracy: 40.7237%, Training Loss: 1.2227%\n",
      "Epoch [3/300], Step [96/225], Training Accuracy: 40.8040%, Training Loss: 1.2223%\n",
      "Epoch [3/300], Step [97/225], Training Accuracy: 40.8666%, Training Loss: 1.2218%\n",
      "Epoch [3/300], Step [98/225], Training Accuracy: 40.9120%, Training Loss: 1.2215%\n",
      "Epoch [3/300], Step [99/225], Training Accuracy: 41.0038%, Training Loss: 1.2215%\n",
      "Epoch [3/300], Step [100/225], Training Accuracy: 40.9219%, Training Loss: 1.2213%\n",
      "Epoch [3/300], Step [101/225], Training Accuracy: 40.9035%, Training Loss: 1.2204%\n",
      "Epoch [3/300], Step [102/225], Training Accuracy: 40.9314%, Training Loss: 1.2199%\n",
      "Epoch [3/300], Step [103/225], Training Accuracy: 40.9132%, Training Loss: 1.2199%\n",
      "Epoch [3/300], Step [104/225], Training Accuracy: 40.8203%, Training Loss: 1.2200%\n",
      "Epoch [3/300], Step [105/225], Training Accuracy: 40.8036%, Training Loss: 1.2203%\n",
      "Epoch [3/300], Step [106/225], Training Accuracy: 40.9198%, Training Loss: 1.2195%\n",
      "Epoch [3/300], Step [107/225], Training Accuracy: 40.9609%, Training Loss: 1.2192%\n",
      "Epoch [3/300], Step [108/225], Training Accuracy: 41.0012%, Training Loss: 1.2188%\n",
      "Epoch [3/300], Step [109/225], Training Accuracy: 40.9547%, Training Loss: 1.2194%\n",
      "Epoch [3/300], Step [110/225], Training Accuracy: 40.9659%, Training Loss: 1.2192%\n",
      "Epoch [3/300], Step [111/225], Training Accuracy: 40.9347%, Training Loss: 1.2198%\n",
      "Epoch [3/300], Step [112/225], Training Accuracy: 40.9877%, Training Loss: 1.2191%\n",
      "Epoch [3/300], Step [113/225], Training Accuracy: 41.0122%, Training Loss: 1.2189%\n",
      "Epoch [3/300], Step [114/225], Training Accuracy: 40.9951%, Training Loss: 1.2184%\n",
      "Epoch [3/300], Step [115/225], Training Accuracy: 40.9511%, Training Loss: 1.2181%\n",
      "Epoch [3/300], Step [116/225], Training Accuracy: 40.9348%, Training Loss: 1.2178%\n",
      "Epoch [3/300], Step [117/225], Training Accuracy: 40.8520%, Training Loss: 1.2193%\n",
      "Epoch [3/300], Step [118/225], Training Accuracy: 40.9825%, Training Loss: 1.2182%\n",
      "Epoch [3/300], Step [119/225], Training Accuracy: 40.9795%, Training Loss: 1.2178%\n",
      "Epoch [3/300], Step [120/225], Training Accuracy: 40.9635%, Training Loss: 1.2181%\n",
      "Epoch [3/300], Step [121/225], Training Accuracy: 41.0124%, Training Loss: 1.2175%\n",
      "Epoch [3/300], Step [122/225], Training Accuracy: 40.9964%, Training Loss: 1.2172%\n",
      "Epoch [3/300], Step [123/225], Training Accuracy: 40.9680%, Training Loss: 1.2173%\n",
      "Epoch [3/300], Step [124/225], Training Accuracy: 40.9652%, Training Loss: 1.2164%\n",
      "Epoch [3/300], Step [125/225], Training Accuracy: 40.9875%, Training Loss: 1.2166%\n",
      "Epoch [3/300], Step [126/225], Training Accuracy: 41.0094%, Training Loss: 1.2169%\n",
      "Epoch [3/300], Step [127/225], Training Accuracy: 40.9326%, Training Loss: 1.2172%\n",
      "Epoch [3/300], Step [128/225], Training Accuracy: 40.8813%, Training Loss: 1.2174%\n",
      "Epoch [3/300], Step [129/225], Training Accuracy: 40.9157%, Training Loss: 1.2171%\n",
      "Epoch [3/300], Step [130/225], Training Accuracy: 40.9736%, Training Loss: 1.2171%\n",
      "Epoch [3/300], Step [131/225], Training Accuracy: 41.0425%, Training Loss: 1.2166%\n",
      "Epoch [3/300], Step [132/225], Training Accuracy: 41.0038%, Training Loss: 1.2165%\n",
      "Epoch [3/300], Step [133/225], Training Accuracy: 40.9774%, Training Loss: 1.2164%\n",
      "Epoch [3/300], Step [134/225], Training Accuracy: 40.8465%, Training Loss: 1.2173%\n",
      "Epoch [3/300], Step [135/225], Training Accuracy: 40.8681%, Training Loss: 1.2168%\n",
      "Epoch [3/300], Step [136/225], Training Accuracy: 40.9122%, Training Loss: 1.2168%\n",
      "Epoch [3/300], Step [137/225], Training Accuracy: 40.9101%, Training Loss: 1.2167%\n",
      "Epoch [3/300], Step [138/225], Training Accuracy: 40.9194%, Training Loss: 1.2165%\n",
      "Epoch [3/300], Step [139/225], Training Accuracy: 40.9173%, Training Loss: 1.2161%\n",
      "Epoch [3/300], Step [140/225], Training Accuracy: 40.9598%, Training Loss: 1.2154%\n",
      "Epoch [3/300], Step [141/225], Training Accuracy: 40.9685%, Training Loss: 1.2148%\n",
      "Epoch [3/300], Step [142/225], Training Accuracy: 41.0321%, Training Loss: 1.2141%\n",
      "Epoch [3/300], Step [143/225], Training Accuracy: 41.0402%, Training Loss: 1.2134%\n",
      "Epoch [3/300], Step [144/225], Training Accuracy: 41.0482%, Training Loss: 1.2137%\n",
      "Epoch [3/300], Step [145/225], Training Accuracy: 41.0237%, Training Loss: 1.2146%\n",
      "Epoch [3/300], Step [146/225], Training Accuracy: 41.0103%, Training Loss: 1.2157%\n",
      "Epoch [3/300], Step [147/225], Training Accuracy: 40.9439%, Training Loss: 1.2153%\n",
      "Epoch [3/300], Step [148/225], Training Accuracy: 40.9945%, Training Loss: 1.2155%\n",
      "Epoch [3/300], Step [149/225], Training Accuracy: 40.9291%, Training Loss: 1.2157%\n",
      "Epoch [3/300], Step [150/225], Training Accuracy: 40.8958%, Training Loss: 1.2155%\n",
      "Epoch [3/300], Step [151/225], Training Accuracy: 40.9251%, Training Loss: 1.2151%\n",
      "Epoch [3/300], Step [152/225], Training Accuracy: 40.9025%, Training Loss: 1.2151%\n",
      "Epoch [3/300], Step [153/225], Training Accuracy: 40.9212%, Training Loss: 1.2145%\n",
      "Epoch [3/300], Step [154/225], Training Accuracy: 40.8685%, Training Loss: 1.2149%\n",
      "Epoch [3/300], Step [155/225], Training Accuracy: 40.8367%, Training Loss: 1.2150%\n",
      "Epoch [3/300], Step [156/225], Training Accuracy: 40.8554%, Training Loss: 1.2152%\n",
      "Epoch [3/300], Step [157/225], Training Accuracy: 40.8539%, Training Loss: 1.2153%\n",
      "Epoch [3/300], Step [158/225], Training Accuracy: 40.7733%, Training Loss: 1.2162%\n",
      "Epoch [3/300], Step [159/225], Training Accuracy: 40.7921%, Training Loss: 1.2161%\n",
      "Epoch [3/300], Step [160/225], Training Accuracy: 40.8203%, Training Loss: 1.2153%\n",
      "Epoch [3/300], Step [161/225], Training Accuracy: 40.8579%, Training Loss: 1.2147%\n",
      "Epoch [3/300], Step [162/225], Training Accuracy: 40.9240%, Training Loss: 1.2136%\n",
      "Epoch [3/300], Step [163/225], Training Accuracy: 40.9126%, Training Loss: 1.2132%\n",
      "Epoch [3/300], Step [164/225], Training Accuracy: 40.9489%, Training Loss: 1.2129%\n",
      "Epoch [3/300], Step [165/225], Training Accuracy: 40.9375%, Training Loss: 1.2133%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [166/225], Training Accuracy: 40.9827%, Training Loss: 1.2128%\n",
      "Epoch [3/300], Step [167/225], Training Accuracy: 40.9805%, Training Loss: 1.2123%\n",
      "Epoch [3/300], Step [168/225], Training Accuracy: 40.9877%, Training Loss: 1.2124%\n",
      "Epoch [3/300], Step [169/225], Training Accuracy: 40.9763%, Training Loss: 1.2123%\n",
      "Epoch [3/300], Step [170/225], Training Accuracy: 40.9651%, Training Loss: 1.2124%\n",
      "Epoch [3/300], Step [171/225], Training Accuracy: 41.0362%, Training Loss: 1.2118%\n",
      "Epoch [3/300], Step [172/225], Training Accuracy: 41.0883%, Training Loss: 1.2112%\n",
      "Epoch [3/300], Step [173/225], Training Accuracy: 41.1850%, Training Loss: 1.2108%\n",
      "Epoch [3/300], Step [174/225], Training Accuracy: 41.2446%, Training Loss: 1.2103%\n",
      "Epoch [3/300], Step [175/225], Training Accuracy: 41.2679%, Training Loss: 1.2102%\n",
      "Epoch [3/300], Step [176/225], Training Accuracy: 41.2021%, Training Loss: 1.2105%\n",
      "Epoch [3/300], Step [177/225], Training Accuracy: 41.2871%, Training Loss: 1.2106%\n",
      "Epoch [3/300], Step [178/225], Training Accuracy: 41.3009%, Training Loss: 1.2102%\n",
      "Epoch [3/300], Step [179/225], Training Accuracy: 41.3321%, Training Loss: 1.2095%\n",
      "Epoch [3/300], Step [180/225], Training Accuracy: 41.3889%, Training Loss: 1.2088%\n",
      "Epoch [3/300], Step [181/225], Training Accuracy: 41.3847%, Training Loss: 1.2093%\n",
      "Epoch [3/300], Step [182/225], Training Accuracy: 41.3719%, Training Loss: 1.2092%\n",
      "Epoch [3/300], Step [183/225], Training Accuracy: 41.4105%, Training Loss: 1.2085%\n",
      "Epoch [3/300], Step [184/225], Training Accuracy: 41.4402%, Training Loss: 1.2077%\n",
      "Epoch [3/300], Step [185/225], Training Accuracy: 41.5203%, Training Loss: 1.2067%\n",
      "Epoch [3/300], Step [186/225], Training Accuracy: 41.5659%, Training Loss: 1.2061%\n",
      "Epoch [3/300], Step [187/225], Training Accuracy: 41.6360%, Training Loss: 1.2057%\n",
      "Epoch [3/300], Step [188/225], Training Accuracy: 41.6390%, Training Loss: 1.2052%\n",
      "Epoch [3/300], Step [189/225], Training Accuracy: 41.6749%, Training Loss: 1.2045%\n",
      "Epoch [3/300], Step [190/225], Training Accuracy: 41.7105%, Training Loss: 1.2044%\n",
      "Epoch [3/300], Step [191/225], Training Accuracy: 41.7457%, Training Loss: 1.2039%\n",
      "Epoch [3/300], Step [192/225], Training Accuracy: 41.7725%, Training Loss: 1.2038%\n",
      "Epoch [3/300], Step [193/225], Training Accuracy: 41.7503%, Training Loss: 1.2037%\n",
      "Epoch [3/300], Step [194/225], Training Accuracy: 41.7767%, Training Loss: 1.2034%\n",
      "Epoch [3/300], Step [195/225], Training Accuracy: 41.7869%, Training Loss: 1.2028%\n",
      "Epoch [3/300], Step [196/225], Training Accuracy: 41.7730%, Training Loss: 1.2032%\n",
      "Epoch [3/300], Step [197/225], Training Accuracy: 41.7830%, Training Loss: 1.2028%\n",
      "Epoch [3/300], Step [198/225], Training Accuracy: 41.8561%, Training Loss: 1.2016%\n",
      "Epoch [3/300], Step [199/225], Training Accuracy: 41.8420%, Training Loss: 1.2015%\n",
      "Epoch [3/300], Step [200/225], Training Accuracy: 41.8750%, Training Loss: 1.2017%\n",
      "Epoch [3/300], Step [201/225], Training Accuracy: 41.9076%, Training Loss: 1.2015%\n",
      "Epoch [3/300], Step [202/225], Training Accuracy: 41.9477%, Training Loss: 1.2012%\n",
      "Epoch [3/300], Step [203/225], Training Accuracy: 41.9181%, Training Loss: 1.2016%\n",
      "Epoch [3/300], Step [204/225], Training Accuracy: 41.9807%, Training Loss: 1.2011%\n",
      "Epoch [3/300], Step [205/225], Training Accuracy: 42.0427%, Training Loss: 1.2004%\n",
      "Epoch [3/300], Step [206/225], Training Accuracy: 42.0358%, Training Loss: 1.2004%\n",
      "Epoch [3/300], Step [207/225], Training Accuracy: 41.9912%, Training Loss: 1.2008%\n",
      "Epoch [3/300], Step [208/225], Training Accuracy: 42.0598%, Training Loss: 1.1996%\n",
      "Epoch [3/300], Step [209/225], Training Accuracy: 42.0828%, Training Loss: 1.1994%\n",
      "Epoch [3/300], Step [210/225], Training Accuracy: 42.0685%, Training Loss: 1.1990%\n",
      "Epoch [3/300], Step [211/225], Training Accuracy: 42.0764%, Training Loss: 1.1984%\n",
      "Epoch [3/300], Step [212/225], Training Accuracy: 42.0548%, Training Loss: 1.1986%\n",
      "Epoch [3/300], Step [213/225], Training Accuracy: 42.0261%, Training Loss: 1.1992%\n",
      "Epoch [3/300], Step [214/225], Training Accuracy: 42.0342%, Training Loss: 1.1990%\n",
      "Epoch [3/300], Step [215/225], Training Accuracy: 42.0349%, Training Loss: 1.1989%\n",
      "Epoch [3/300], Step [216/225], Training Accuracy: 42.0211%, Training Loss: 1.1989%\n",
      "Epoch [3/300], Step [217/225], Training Accuracy: 42.0363%, Training Loss: 1.1986%\n",
      "Epoch [3/300], Step [218/225], Training Accuracy: 42.0226%, Training Loss: 1.1986%\n",
      "Epoch [3/300], Step [219/225], Training Accuracy: 42.0234%, Training Loss: 1.1984%\n",
      "Epoch [3/300], Step [220/225], Training Accuracy: 42.0241%, Training Loss: 1.1982%\n",
      "Epoch [3/300], Step [221/225], Training Accuracy: 42.0178%, Training Loss: 1.1984%\n",
      "Epoch [3/300], Step [222/225], Training Accuracy: 42.0538%, Training Loss: 1.1980%\n",
      "Epoch [3/300], Step [223/225], Training Accuracy: 42.0474%, Training Loss: 1.1980%\n",
      "Epoch [3/300], Step [224/225], Training Accuracy: 42.0550%, Training Loss: 1.1975%\n",
      "Epoch [3/300], Step [225/225], Training Accuracy: 42.0581%, Training Loss: 1.1977%\n",
      "Epoch [4/300], Step [1/225], Training Accuracy: 46.8750%, Training Loss: 1.1116%\n",
      "Epoch [4/300], Step [2/225], Training Accuracy: 43.7500%, Training Loss: 1.1590%\n",
      "Epoch [4/300], Step [3/225], Training Accuracy: 43.7500%, Training Loss: 1.1658%\n",
      "Epoch [4/300], Step [4/225], Training Accuracy: 45.3125%, Training Loss: 1.1604%\n",
      "Epoch [4/300], Step [5/225], Training Accuracy: 45.9375%, Training Loss: 1.1639%\n",
      "Epoch [4/300], Step [6/225], Training Accuracy: 45.3125%, Training Loss: 1.1599%\n",
      "Epoch [4/300], Step [7/225], Training Accuracy: 45.9821%, Training Loss: 1.1673%\n",
      "Epoch [4/300], Step [8/225], Training Accuracy: 44.7266%, Training Loss: 1.1719%\n",
      "Epoch [4/300], Step [9/225], Training Accuracy: 45.8333%, Training Loss: 1.1579%\n",
      "Epoch [4/300], Step [10/225], Training Accuracy: 45.1562%, Training Loss: 1.1608%\n",
      "Epoch [4/300], Step [11/225], Training Accuracy: 44.7443%, Training Loss: 1.1630%\n",
      "Epoch [4/300], Step [12/225], Training Accuracy: 45.5729%, Training Loss: 1.1518%\n",
      "Epoch [4/300], Step [13/225], Training Accuracy: 46.2740%, Training Loss: 1.1513%\n",
      "Epoch [4/300], Step [14/225], Training Accuracy: 45.8705%, Training Loss: 1.1496%\n",
      "Epoch [4/300], Step [15/225], Training Accuracy: 44.8958%, Training Loss: 1.1588%\n",
      "Epoch [4/300], Step [16/225], Training Accuracy: 45.2148%, Training Loss: 1.1579%\n",
      "Epoch [4/300], Step [17/225], Training Accuracy: 45.4963%, Training Loss: 1.1549%\n",
      "Epoch [4/300], Step [18/225], Training Accuracy: 45.0521%, Training Loss: 1.1528%\n",
      "Epoch [4/300], Step [19/225], Training Accuracy: 44.8191%, Training Loss: 1.1520%\n",
      "Epoch [4/300], Step [20/225], Training Accuracy: 45.2344%, Training Loss: 1.1515%\n",
      "Epoch [4/300], Step [21/225], Training Accuracy: 45.4613%, Training Loss: 1.1486%\n",
      "Epoch [4/300], Step [22/225], Training Accuracy: 44.8153%, Training Loss: 1.1511%\n",
      "Epoch [4/300], Step [23/225], Training Accuracy: 45.0408%, Training Loss: 1.1455%\n",
      "Epoch [4/300], Step [24/225], Training Accuracy: 45.0521%, Training Loss: 1.1437%\n",
      "Epoch [4/300], Step [25/225], Training Accuracy: 45.5000%, Training Loss: 1.1389%\n",
      "Epoch [4/300], Step [26/225], Training Accuracy: 45.1923%, Training Loss: 1.1388%\n",
      "Epoch [4/300], Step [27/225], Training Accuracy: 45.3125%, Training Loss: 1.1410%\n",
      "Epoch [4/300], Step [28/225], Training Accuracy: 45.1451%, Training Loss: 1.1381%\n",
      "Epoch [4/300], Step [29/225], Training Accuracy: 45.5280%, Training Loss: 1.1355%\n",
      "Epoch [4/300], Step [30/225], Training Accuracy: 45.2604%, Training Loss: 1.1361%\n",
      "Epoch [4/300], Step [31/225], Training Accuracy: 45.2117%, Training Loss: 1.1391%\n",
      "Epoch [4/300], Step [32/225], Training Accuracy: 45.3125%, Training Loss: 1.1370%\n",
      "Epoch [4/300], Step [33/225], Training Accuracy: 45.1705%, Training Loss: 1.1355%\n",
      "Epoch [4/300], Step [34/225], Training Accuracy: 44.8989%, Training Loss: 1.1376%\n",
      "Epoch [4/300], Step [35/225], Training Accuracy: 44.9554%, Training Loss: 1.1391%\n",
      "Epoch [4/300], Step [36/225], Training Accuracy: 44.7483%, Training Loss: 1.1399%\n",
      "Epoch [4/300], Step [37/225], Training Accuracy: 44.8902%, Training Loss: 1.1378%\n",
      "Epoch [4/300], Step [38/225], Training Accuracy: 44.8602%, Training Loss: 1.1368%\n",
      "Epoch [4/300], Step [39/225], Training Accuracy: 45.0721%, Training Loss: 1.1358%\n",
      "Epoch [4/300], Step [40/225], Training Accuracy: 44.9609%, Training Loss: 1.1350%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [41/225], Training Accuracy: 44.5884%, Training Loss: 1.1384%\n",
      "Epoch [4/300], Step [42/225], Training Accuracy: 44.8289%, Training Loss: 1.1370%\n",
      "Epoch [4/300], Step [43/225], Training Accuracy: 44.6221%, Training Loss: 1.1366%\n",
      "Epoch [4/300], Step [44/225], Training Accuracy: 44.8509%, Training Loss: 1.1342%\n",
      "Epoch [4/300], Step [45/225], Training Accuracy: 45.0347%, Training Loss: 1.1315%\n",
      "Epoch [4/300], Step [46/225], Training Accuracy: 45.2446%, Training Loss: 1.1307%\n",
      "Epoch [4/300], Step [47/225], Training Accuracy: 45.1463%, Training Loss: 1.1306%\n",
      "Epoch [4/300], Step [48/225], Training Accuracy: 45.1172%, Training Loss: 1.1295%\n",
      "Epoch [4/300], Step [49/225], Training Accuracy: 45.2806%, Training Loss: 1.1299%\n",
      "Epoch [4/300], Step [50/225], Training Accuracy: 45.2188%, Training Loss: 1.1302%\n",
      "Epoch [4/300], Step [51/225], Training Accuracy: 45.4044%, Training Loss: 1.1278%\n",
      "Epoch [4/300], Step [52/225], Training Accuracy: 45.5228%, Training Loss: 1.1274%\n",
      "Epoch [4/300], Step [53/225], Training Accuracy: 45.7547%, Training Loss: 1.1249%\n",
      "Epoch [4/300], Step [54/225], Training Accuracy: 45.6019%, Training Loss: 1.1250%\n",
      "Epoch [4/300], Step [55/225], Training Accuracy: 45.5682%, Training Loss: 1.1241%\n",
      "Epoch [4/300], Step [56/225], Training Accuracy: 45.7031%, Training Loss: 1.1231%\n",
      "Epoch [4/300], Step [57/225], Training Accuracy: 45.9978%, Training Loss: 1.1202%\n",
      "Epoch [4/300], Step [58/225], Training Accuracy: 45.9591%, Training Loss: 1.1211%\n",
      "Epoch [4/300], Step [59/225], Training Accuracy: 46.0275%, Training Loss: 1.1202%\n",
      "Epoch [4/300], Step [60/225], Training Accuracy: 46.0677%, Training Loss: 1.1201%\n",
      "Epoch [4/300], Step [61/225], Training Accuracy: 46.0297%, Training Loss: 1.1200%\n",
      "Epoch [4/300], Step [62/225], Training Accuracy: 46.1694%, Training Loss: 1.1193%\n",
      "Epoch [4/300], Step [63/225], Training Accuracy: 46.1310%, Training Loss: 1.1194%\n",
      "Epoch [4/300], Step [64/225], Training Accuracy: 46.0449%, Training Loss: 1.1189%\n",
      "Epoch [4/300], Step [65/225], Training Accuracy: 46.0337%, Training Loss: 1.1194%\n",
      "Epoch [4/300], Step [66/225], Training Accuracy: 46.1411%, Training Loss: 1.1176%\n",
      "Epoch [4/300], Step [67/225], Training Accuracy: 46.0588%, Training Loss: 1.1178%\n",
      "Epoch [4/300], Step [68/225], Training Accuracy: 46.0938%, Training Loss: 1.1176%\n",
      "Epoch [4/300], Step [69/225], Training Accuracy: 46.0371%, Training Loss: 1.1169%\n",
      "Epoch [4/300], Step [70/225], Training Accuracy: 46.0938%, Training Loss: 1.1169%\n",
      "Epoch [4/300], Step [71/225], Training Accuracy: 46.1928%, Training Loss: 1.1164%\n",
      "Epoch [4/300], Step [72/225], Training Accuracy: 46.1589%, Training Loss: 1.1174%\n",
      "Epoch [4/300], Step [73/225], Training Accuracy: 46.1473%, Training Loss: 1.1186%\n",
      "Epoch [4/300], Step [74/225], Training Accuracy: 46.1993%, Training Loss: 1.1179%\n",
      "Epoch [4/300], Step [75/225], Training Accuracy: 46.1667%, Training Loss: 1.1166%\n",
      "Epoch [4/300], Step [76/225], Training Accuracy: 46.0321%, Training Loss: 1.1165%\n",
      "Epoch [4/300], Step [77/225], Training Accuracy: 46.1039%, Training Loss: 1.1161%\n",
      "Epoch [4/300], Step [78/225], Training Accuracy: 46.1338%, Training Loss: 1.1150%\n",
      "Epoch [4/300], Step [79/225], Training Accuracy: 46.0443%, Training Loss: 1.1161%\n",
      "Epoch [4/300], Step [80/225], Training Accuracy: 46.0156%, Training Loss: 1.1164%\n",
      "Epoch [4/300], Step [81/225], Training Accuracy: 46.0262%, Training Loss: 1.1161%\n",
      "Epoch [4/300], Step [82/225], Training Accuracy: 46.0747%, Training Loss: 1.1160%\n",
      "Epoch [4/300], Step [83/225], Training Accuracy: 46.1973%, Training Loss: 1.1149%\n",
      "Epoch [4/300], Step [84/225], Training Accuracy: 46.2240%, Training Loss: 1.1142%\n",
      "Epoch [4/300], Step [85/225], Training Accuracy: 46.2316%, Training Loss: 1.1136%\n",
      "Epoch [4/300], Step [86/225], Training Accuracy: 46.1846%, Training Loss: 1.1131%\n",
      "Epoch [4/300], Step [87/225], Training Accuracy: 46.1925%, Training Loss: 1.1125%\n",
      "Epoch [4/300], Step [88/225], Training Accuracy: 46.0582%, Training Loss: 1.1141%\n",
      "Epoch [4/300], Step [89/225], Training Accuracy: 46.0674%, Training Loss: 1.1145%\n",
      "Epoch [4/300], Step [90/225], Training Accuracy: 46.1111%, Training Loss: 1.1142%\n",
      "Epoch [4/300], Step [91/225], Training Accuracy: 46.1195%, Training Loss: 1.1132%\n",
      "Epoch [4/300], Step [92/225], Training Accuracy: 46.0258%, Training Loss: 1.1126%\n",
      "Epoch [4/300], Step [93/225], Training Accuracy: 46.0685%, Training Loss: 1.1125%\n",
      "Epoch [4/300], Step [94/225], Training Accuracy: 46.1270%, Training Loss: 1.1107%\n",
      "Epoch [4/300], Step [95/225], Training Accuracy: 46.0526%, Training Loss: 1.1122%\n",
      "Epoch [4/300], Step [96/225], Training Accuracy: 46.1100%, Training Loss: 1.1116%\n",
      "Epoch [4/300], Step [97/225], Training Accuracy: 46.0857%, Training Loss: 1.1116%\n",
      "Epoch [4/300], Step [98/225], Training Accuracy: 46.0459%, Training Loss: 1.1113%\n",
      "Epoch [4/300], Step [99/225], Training Accuracy: 46.0227%, Training Loss: 1.1121%\n",
      "Epoch [4/300], Step [100/225], Training Accuracy: 45.8906%, Training Loss: 1.1132%\n",
      "Epoch [4/300], Step [101/225], Training Accuracy: 45.8694%, Training Loss: 1.1137%\n",
      "Epoch [4/300], Step [102/225], Training Accuracy: 45.8793%, Training Loss: 1.1141%\n",
      "Epoch [4/300], Step [103/225], Training Accuracy: 45.9041%, Training Loss: 1.1138%\n",
      "Epoch [4/300], Step [104/225], Training Accuracy: 45.7933%, Training Loss: 1.1141%\n",
      "Epoch [4/300], Step [105/225], Training Accuracy: 45.8185%, Training Loss: 1.1145%\n",
      "Epoch [4/300], Step [106/225], Training Accuracy: 45.8284%, Training Loss: 1.1147%\n",
      "Epoch [4/300], Step [107/225], Training Accuracy: 45.8382%, Training Loss: 1.1148%\n",
      "Epoch [4/300], Step [108/225], Training Accuracy: 45.7899%, Training Loss: 1.1152%\n",
      "Epoch [4/300], Step [109/225], Training Accuracy: 45.7569%, Training Loss: 1.1152%\n",
      "Epoch [4/300], Step [110/225], Training Accuracy: 45.7528%, Training Loss: 1.1149%\n",
      "Epoch [4/300], Step [111/225], Training Accuracy: 45.7348%, Training Loss: 1.1151%\n",
      "Epoch [4/300], Step [112/225], Training Accuracy: 45.7729%, Training Loss: 1.1146%\n",
      "Epoch [4/300], Step [113/225], Training Accuracy: 45.8103%, Training Loss: 1.1150%\n",
      "Epoch [4/300], Step [114/225], Training Accuracy: 45.7511%, Training Loss: 1.1147%\n",
      "Epoch [4/300], Step [115/225], Training Accuracy: 45.7880%, Training Loss: 1.1147%\n",
      "Epoch [4/300], Step [116/225], Training Accuracy: 45.8917%, Training Loss: 1.1143%\n",
      "Epoch [4/300], Step [117/225], Training Accuracy: 45.7799%, Training Loss: 1.1151%\n",
      "Epoch [4/300], Step [118/225], Training Accuracy: 45.8157%, Training Loss: 1.1141%\n",
      "Epoch [4/300], Step [119/225], Training Accuracy: 45.8114%, Training Loss: 1.1143%\n",
      "Epoch [4/300], Step [120/225], Training Accuracy: 45.8073%, Training Loss: 1.1154%\n",
      "Epoch [4/300], Step [121/225], Training Accuracy: 45.8032%, Training Loss: 1.1153%\n",
      "Epoch [4/300], Step [122/225], Training Accuracy: 45.8504%, Training Loss: 1.1146%\n",
      "Epoch [4/300], Step [123/225], Training Accuracy: 45.9096%, Training Loss: 1.1139%\n",
      "Epoch [4/300], Step [124/225], Training Accuracy: 45.9425%, Training Loss: 1.1137%\n",
      "Epoch [4/300], Step [125/225], Training Accuracy: 45.9125%, Training Loss: 1.1145%\n",
      "Epoch [4/300], Step [126/225], Training Accuracy: 45.9201%, Training Loss: 1.1147%\n",
      "Epoch [4/300], Step [127/225], Training Accuracy: 45.8907%, Training Loss: 1.1147%\n",
      "Epoch [4/300], Step [128/225], Training Accuracy: 45.8252%, Training Loss: 1.1156%\n",
      "Epoch [4/300], Step [129/225], Training Accuracy: 45.9302%, Training Loss: 1.1150%\n",
      "Epoch [4/300], Step [130/225], Training Accuracy: 45.9736%, Training Loss: 1.1150%\n",
      "Epoch [4/300], Step [131/225], Training Accuracy: 45.9447%, Training Loss: 1.1146%\n",
      "Epoch [4/300], Step [132/225], Training Accuracy: 45.9044%, Training Loss: 1.1144%\n",
      "Epoch [4/300], Step [133/225], Training Accuracy: 45.9117%, Training Loss: 1.1144%\n",
      "Epoch [4/300], Step [134/225], Training Accuracy: 45.8256%, Training Loss: 1.1151%\n",
      "Epoch [4/300], Step [135/225], Training Accuracy: 45.8218%, Training Loss: 1.1146%\n",
      "Epoch [4/300], Step [136/225], Training Accuracy: 45.8525%, Training Loss: 1.1139%\n",
      "Epoch [4/300], Step [137/225], Training Accuracy: 45.8143%, Training Loss: 1.1141%\n",
      "Epoch [4/300], Step [138/225], Training Accuracy: 45.8220%, Training Loss: 1.1132%\n",
      "Epoch [4/300], Step [139/225], Training Accuracy: 45.8183%, Training Loss: 1.1128%\n",
      "Epoch [4/300], Step [140/225], Training Accuracy: 45.8371%, Training Loss: 1.1118%\n",
      "Epoch [4/300], Step [141/225], Training Accuracy: 45.8333%, Training Loss: 1.1113%\n",
      "Epoch [4/300], Step [142/225], Training Accuracy: 45.8297%, Training Loss: 1.1110%\n",
      "Epoch [4/300], Step [143/225], Training Accuracy: 45.8588%, Training Loss: 1.1100%\n",
      "Epoch [4/300], Step [144/225], Training Accuracy: 45.8984%, Training Loss: 1.1098%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [145/225], Training Accuracy: 45.9052%, Training Loss: 1.1095%\n",
      "Epoch [4/300], Step [146/225], Training Accuracy: 45.8690%, Training Loss: 1.1104%\n",
      "Epoch [4/300], Step [147/225], Training Accuracy: 45.8546%, Training Loss: 1.1094%\n",
      "Epoch [4/300], Step [148/225], Training Accuracy: 45.9037%, Training Loss: 1.1090%\n",
      "Epoch [4/300], Step [149/225], Training Accuracy: 45.8788%, Training Loss: 1.1093%\n",
      "Epoch [4/300], Step [150/225], Training Accuracy: 45.8958%, Training Loss: 1.1087%\n",
      "Epoch [4/300], Step [151/225], Training Accuracy: 45.9644%, Training Loss: 1.1079%\n",
      "Epoch [4/300], Step [152/225], Training Accuracy: 45.9396%, Training Loss: 1.1077%\n",
      "Epoch [4/300], Step [153/225], Training Accuracy: 45.9150%, Training Loss: 1.1071%\n",
      "Epoch [4/300], Step [154/225], Training Accuracy: 45.9010%, Training Loss: 1.1069%\n",
      "Epoch [4/300], Step [155/225], Training Accuracy: 45.8972%, Training Loss: 1.1072%\n",
      "Epoch [4/300], Step [156/225], Training Accuracy: 45.9034%, Training Loss: 1.1074%\n",
      "Epoch [4/300], Step [157/225], Training Accuracy: 45.8499%, Training Loss: 1.1078%\n",
      "Epoch [4/300], Step [158/225], Training Accuracy: 45.7674%, Training Loss: 1.1085%\n",
      "Epoch [4/300], Step [159/225], Training Accuracy: 45.7449%, Training Loss: 1.1087%\n",
      "Epoch [4/300], Step [160/225], Training Accuracy: 45.7031%, Training Loss: 1.1086%\n",
      "Epoch [4/300], Step [161/225], Training Accuracy: 45.7492%, Training Loss: 1.1078%\n",
      "Epoch [4/300], Step [162/225], Training Accuracy: 45.7851%, Training Loss: 1.1072%\n",
      "Epoch [4/300], Step [163/225], Training Accuracy: 45.8206%, Training Loss: 1.1069%\n",
      "Epoch [4/300], Step [164/225], Training Accuracy: 45.9127%, Training Loss: 1.1063%\n",
      "Epoch [4/300], Step [165/225], Training Accuracy: 45.9375%, Training Loss: 1.1065%\n",
      "Epoch [4/300], Step [166/225], Training Accuracy: 45.9337%, Training Loss: 1.1062%\n",
      "Epoch [4/300], Step [167/225], Training Accuracy: 45.9862%, Training Loss: 1.1060%\n",
      "Epoch [4/300], Step [168/225], Training Accuracy: 45.9821%, Training Loss: 1.1060%\n",
      "Epoch [4/300], Step [169/225], Training Accuracy: 45.9874%, Training Loss: 1.1060%\n",
      "Epoch [4/300], Step [170/225], Training Accuracy: 45.9835%, Training Loss: 1.1063%\n",
      "Epoch [4/300], Step [171/225], Training Accuracy: 46.0252%, Training Loss: 1.1057%\n",
      "Epoch [4/300], Step [172/225], Training Accuracy: 46.0847%, Training Loss: 1.1052%\n",
      "Epoch [4/300], Step [173/225], Training Accuracy: 46.1344%, Training Loss: 1.1051%\n",
      "Epoch [4/300], Step [174/225], Training Accuracy: 46.1925%, Training Loss: 1.1045%\n",
      "Epoch [4/300], Step [175/225], Training Accuracy: 46.2411%, Training Loss: 1.1047%\n",
      "Epoch [4/300], Step [176/225], Training Accuracy: 46.2180%, Training Loss: 1.1050%\n",
      "Epoch [4/300], Step [177/225], Training Accuracy: 46.2835%, Training Loss: 1.1049%\n",
      "Epoch [4/300], Step [178/225], Training Accuracy: 46.2869%, Training Loss: 1.1046%\n",
      "Epoch [4/300], Step [179/225], Training Accuracy: 46.3163%, Training Loss: 1.1038%\n",
      "Epoch [4/300], Step [180/225], Training Accuracy: 46.3628%, Training Loss: 1.1028%\n",
      "Epoch [4/300], Step [181/225], Training Accuracy: 46.3139%, Training Loss: 1.1031%\n",
      "Epoch [4/300], Step [182/225], Training Accuracy: 46.3427%, Training Loss: 1.1031%\n",
      "Epoch [4/300], Step [183/225], Training Accuracy: 46.3456%, Training Loss: 1.1025%\n",
      "Epoch [4/300], Step [184/225], Training Accuracy: 46.3570%, Training Loss: 1.1020%\n",
      "Epoch [4/300], Step [185/225], Training Accuracy: 46.4189%, Training Loss: 1.1013%\n",
      "Epoch [4/300], Step [186/225], Training Accuracy: 46.5054%, Training Loss: 1.1005%\n",
      "Epoch [4/300], Step [187/225], Training Accuracy: 46.5241%, Training Loss: 1.1005%\n",
      "Epoch [4/300], Step [188/225], Training Accuracy: 46.5758%, Training Loss: 1.1000%\n",
      "Epoch [4/300], Step [189/225], Training Accuracy: 46.5939%, Training Loss: 1.0991%\n",
      "Epoch [4/300], Step [190/225], Training Accuracy: 46.6283%, Training Loss: 1.0989%\n",
      "Epoch [4/300], Step [191/225], Training Accuracy: 46.6623%, Training Loss: 1.0987%\n",
      "Epoch [4/300], Step [192/225], Training Accuracy: 46.7204%, Training Loss: 1.0977%\n",
      "Epoch [4/300], Step [193/225], Training Accuracy: 46.6645%, Training Loss: 1.0979%\n",
      "Epoch [4/300], Step [194/225], Training Accuracy: 46.6575%, Training Loss: 1.0973%\n",
      "Epoch [4/300], Step [195/225], Training Accuracy: 46.6827%, Training Loss: 1.0966%\n",
      "Epoch [4/300], Step [196/225], Training Accuracy: 46.6677%, Training Loss: 1.0972%\n",
      "Epoch [4/300], Step [197/225], Training Accuracy: 46.6767%, Training Loss: 1.0971%\n",
      "Epoch [4/300], Step [198/225], Training Accuracy: 46.7566%, Training Loss: 1.0960%\n",
      "Epoch [4/300], Step [199/225], Training Accuracy: 46.8043%, Training Loss: 1.0957%\n",
      "Epoch [4/300], Step [200/225], Training Accuracy: 46.8438%, Training Loss: 1.0956%\n",
      "Epoch [4/300], Step [201/225], Training Accuracy: 46.8361%, Training Loss: 1.0960%\n",
      "Epoch [4/300], Step [202/225], Training Accuracy: 46.8750%, Training Loss: 1.0960%\n",
      "Epoch [4/300], Step [203/225], Training Accuracy: 46.8519%, Training Loss: 1.0960%\n",
      "Epoch [4/300], Step [204/225], Training Accuracy: 46.8750%, Training Loss: 1.0957%\n",
      "Epoch [4/300], Step [205/225], Training Accuracy: 46.9284%, Training Loss: 1.0952%\n",
      "Epoch [4/300], Step [206/225], Training Accuracy: 46.9660%, Training Loss: 1.0953%\n",
      "Epoch [4/300], Step [207/225], Training Accuracy: 46.9429%, Training Loss: 1.0955%\n",
      "Epoch [4/300], Step [208/225], Training Accuracy: 46.9501%, Training Loss: 1.0949%\n",
      "Epoch [4/300], Step [209/225], Training Accuracy: 46.9124%, Training Loss: 1.0949%\n",
      "Epoch [4/300], Step [210/225], Training Accuracy: 46.8973%, Training Loss: 1.0944%\n",
      "Epoch [4/300], Step [211/225], Training Accuracy: 46.9046%, Training Loss: 1.0938%\n",
      "Epoch [4/300], Step [212/225], Training Accuracy: 46.9119%, Training Loss: 1.0940%\n",
      "Epoch [4/300], Step [213/225], Training Accuracy: 46.8897%, Training Loss: 1.0949%\n",
      "Epoch [4/300], Step [214/225], Training Accuracy: 46.8969%, Training Loss: 1.0950%\n",
      "Epoch [4/300], Step [215/225], Training Accuracy: 46.8459%, Training Loss: 1.0954%\n",
      "Epoch [4/300], Step [216/225], Training Accuracy: 46.8099%, Training Loss: 1.0959%\n",
      "Epoch [4/300], Step [217/225], Training Accuracy: 46.7886%, Training Loss: 1.0961%\n",
      "Epoch [4/300], Step [218/225], Training Accuracy: 46.7890%, Training Loss: 1.0964%\n",
      "Epoch [4/300], Step [219/225], Training Accuracy: 46.7965%, Training Loss: 1.0963%\n",
      "Epoch [4/300], Step [220/225], Training Accuracy: 46.8111%, Training Loss: 1.0961%\n",
      "Epoch [4/300], Step [221/225], Training Accuracy: 46.7972%, Training Loss: 1.0963%\n",
      "Epoch [4/300], Step [222/225], Training Accuracy: 46.7694%, Training Loss: 1.0962%\n",
      "Epoch [4/300], Step [223/225], Training Accuracy: 46.7699%, Training Loss: 1.0962%\n",
      "Epoch [4/300], Step [224/225], Training Accuracy: 46.7983%, Training Loss: 1.0956%\n",
      "Epoch [4/300], Step [225/225], Training Accuracy: 46.8038%, Training Loss: 1.0954%\n",
      "Epoch [5/300], Step [1/225], Training Accuracy: 62.5000%, Training Loss: 1.0080%\n",
      "Epoch [5/300], Step [2/225], Training Accuracy: 50.7812%, Training Loss: 1.0773%\n",
      "Epoch [5/300], Step [3/225], Training Accuracy: 46.3542%, Training Loss: 1.1450%\n",
      "Epoch [5/300], Step [4/225], Training Accuracy: 46.4844%, Training Loss: 1.1231%\n",
      "Epoch [5/300], Step [5/225], Training Accuracy: 48.1250%, Training Loss: 1.1034%\n",
      "Epoch [5/300], Step [6/225], Training Accuracy: 48.4375%, Training Loss: 1.1026%\n",
      "Epoch [5/300], Step [7/225], Training Accuracy: 48.2143%, Training Loss: 1.1059%\n",
      "Epoch [5/300], Step [8/225], Training Accuracy: 47.2656%, Training Loss: 1.1031%\n",
      "Epoch [5/300], Step [9/225], Training Accuracy: 47.5694%, Training Loss: 1.1048%\n",
      "Epoch [5/300], Step [10/225], Training Accuracy: 47.1875%, Training Loss: 1.1080%\n",
      "Epoch [5/300], Step [11/225], Training Accuracy: 47.5852%, Training Loss: 1.0966%\n",
      "Epoch [5/300], Step [12/225], Training Accuracy: 48.4375%, Training Loss: 1.0802%\n",
      "Epoch [5/300], Step [13/225], Training Accuracy: 48.9183%, Training Loss: 1.0750%\n",
      "Epoch [5/300], Step [14/225], Training Accuracy: 48.4375%, Training Loss: 1.0712%\n",
      "Epoch [5/300], Step [15/225], Training Accuracy: 48.0208%, Training Loss: 1.0797%\n",
      "Epoch [5/300], Step [16/225], Training Accuracy: 47.8516%, Training Loss: 1.0787%\n",
      "Epoch [5/300], Step [17/225], Training Accuracy: 48.0699%, Training Loss: 1.0746%\n",
      "Epoch [5/300], Step [18/225], Training Accuracy: 47.4826%, Training Loss: 1.0710%\n",
      "Epoch [5/300], Step [19/225], Training Accuracy: 47.2862%, Training Loss: 1.0731%\n",
      "Epoch [5/300], Step [20/225], Training Accuracy: 47.4219%, Training Loss: 1.0735%\n",
      "Epoch [5/300], Step [21/225], Training Accuracy: 47.9167%, Training Loss: 1.0716%\n",
      "Epoch [5/300], Step [22/225], Training Accuracy: 47.3011%, Training Loss: 1.0757%\n",
      "Epoch [5/300], Step [23/225], Training Accuracy: 47.4864%, Training Loss: 1.0699%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [24/225], Training Accuracy: 47.5260%, Training Loss: 1.0695%\n",
      "Epoch [5/300], Step [25/225], Training Accuracy: 47.7500%, Training Loss: 1.0651%\n",
      "Epoch [5/300], Step [26/225], Training Accuracy: 47.6562%, Training Loss: 1.0633%\n",
      "Epoch [5/300], Step [27/225], Training Accuracy: 47.3380%, Training Loss: 1.0655%\n",
      "Epoch [5/300], Step [28/225], Training Accuracy: 47.3772%, Training Loss: 1.0650%\n",
      "Epoch [5/300], Step [29/225], Training Accuracy: 47.4677%, Training Loss: 1.0618%\n",
      "Epoch [5/300], Step [30/225], Training Accuracy: 47.3958%, Training Loss: 1.0612%\n",
      "Epoch [5/300], Step [31/225], Training Accuracy: 47.3286%, Training Loss: 1.0630%\n",
      "Epoch [5/300], Step [32/225], Training Accuracy: 47.3633%, Training Loss: 1.0605%\n",
      "Epoch [5/300], Step [33/225], Training Accuracy: 47.4905%, Training Loss: 1.0576%\n",
      "Epoch [5/300], Step [34/225], Training Accuracy: 47.1967%, Training Loss: 1.0609%\n",
      "Epoch [5/300], Step [35/225], Training Accuracy: 47.3214%, Training Loss: 1.0645%\n",
      "Epoch [5/300], Step [36/225], Training Accuracy: 47.0052%, Training Loss: 1.0641%\n",
      "Epoch [5/300], Step [37/225], Training Accuracy: 47.1284%, Training Loss: 1.0602%\n",
      "Epoch [5/300], Step [38/225], Training Accuracy: 47.1628%, Training Loss: 1.0590%\n",
      "Epoch [5/300], Step [39/225], Training Accuracy: 47.2356%, Training Loss: 1.0583%\n",
      "Epoch [5/300], Step [40/225], Training Accuracy: 47.3047%, Training Loss: 1.0579%\n",
      "Epoch [5/300], Step [41/225], Training Accuracy: 46.9131%, Training Loss: 1.0616%\n",
      "Epoch [5/300], Step [42/225], Training Accuracy: 46.9866%, Training Loss: 1.0611%\n",
      "Epoch [5/300], Step [43/225], Training Accuracy: 46.8387%, Training Loss: 1.0621%\n",
      "Epoch [5/300], Step [44/225], Training Accuracy: 46.8750%, Training Loss: 1.0608%\n",
      "Epoch [5/300], Step [45/225], Training Accuracy: 47.0833%, Training Loss: 1.0578%\n",
      "Epoch [5/300], Step [46/225], Training Accuracy: 47.4185%, Training Loss: 1.0549%\n",
      "Epoch [5/300], Step [47/225], Training Accuracy: 47.4069%, Training Loss: 1.0545%\n",
      "Epoch [5/300], Step [48/225], Training Accuracy: 47.4935%, Training Loss: 1.0541%\n",
      "Epoch [5/300], Step [49/225], Training Accuracy: 47.5128%, Training Loss: 1.0561%\n",
      "Epoch [5/300], Step [50/225], Training Accuracy: 47.4688%, Training Loss: 1.0561%\n",
      "Epoch [5/300], Step [51/225], Training Accuracy: 47.6409%, Training Loss: 1.0548%\n",
      "Epoch [5/300], Step [52/225], Training Accuracy: 47.8966%, Training Loss: 1.0535%\n",
      "Epoch [5/300], Step [53/225], Training Accuracy: 48.0248%, Training Loss: 1.0524%\n",
      "Epoch [5/300], Step [54/225], Training Accuracy: 48.0324%, Training Loss: 1.0529%\n",
      "Epoch [5/300], Step [55/225], Training Accuracy: 47.9261%, Training Loss: 1.0527%\n",
      "Epoch [5/300], Step [56/225], Training Accuracy: 47.9074%, Training Loss: 1.0521%\n",
      "Epoch [5/300], Step [57/225], Training Accuracy: 48.0811%, Training Loss: 1.0494%\n",
      "Epoch [5/300], Step [58/225], Training Accuracy: 47.7909%, Training Loss: 1.0506%\n",
      "Epoch [5/300], Step [59/225], Training Accuracy: 47.7754%, Training Loss: 1.0499%\n",
      "Epoch [5/300], Step [60/225], Training Accuracy: 47.8125%, Training Loss: 1.0485%\n",
      "Epoch [5/300], Step [61/225], Training Accuracy: 47.9252%, Training Loss: 1.0482%\n",
      "Epoch [5/300], Step [62/225], Training Accuracy: 47.9587%, Training Loss: 1.0482%\n",
      "Epoch [5/300], Step [63/225], Training Accuracy: 47.7927%, Training Loss: 1.0496%\n",
      "Epoch [5/300], Step [64/225], Training Accuracy: 47.6807%, Training Loss: 1.0500%\n",
      "Epoch [5/300], Step [65/225], Training Accuracy: 47.6683%, Training Loss: 1.0504%\n",
      "Epoch [5/300], Step [66/225], Training Accuracy: 47.7746%, Training Loss: 1.0481%\n",
      "Epoch [5/300], Step [67/225], Training Accuracy: 47.7379%, Training Loss: 1.0487%\n",
      "Epoch [5/300], Step [68/225], Training Accuracy: 47.7252%, Training Loss: 1.0490%\n",
      "Epoch [5/300], Step [69/225], Training Accuracy: 47.6676%, Training Loss: 1.0484%\n",
      "Epoch [5/300], Step [70/225], Training Accuracy: 47.7679%, Training Loss: 1.0477%\n",
      "Epoch [5/300], Step [71/225], Training Accuracy: 47.8213%, Training Loss: 1.0472%\n",
      "Epoch [5/300], Step [72/225], Training Accuracy: 47.8082%, Training Loss: 1.0479%\n",
      "Epoch [5/300], Step [73/225], Training Accuracy: 47.7098%, Training Loss: 1.0488%\n",
      "Epoch [5/300], Step [74/225], Training Accuracy: 47.7196%, Training Loss: 1.0476%\n",
      "Epoch [5/300], Step [75/225], Training Accuracy: 47.7292%, Training Loss: 1.0464%\n",
      "Epoch [5/300], Step [76/225], Training Accuracy: 47.6562%, Training Loss: 1.0472%\n",
      "Epoch [5/300], Step [77/225], Training Accuracy: 47.6461%, Training Loss: 1.0475%\n",
      "Epoch [5/300], Step [78/225], Training Accuracy: 47.5761%, Training Loss: 1.0471%\n",
      "Epoch [5/300], Step [79/225], Training Accuracy: 47.4684%, Training Loss: 1.0483%\n",
      "Epoch [5/300], Step [80/225], Training Accuracy: 47.4609%, Training Loss: 1.0489%\n",
      "Epoch [5/300], Step [81/225], Training Accuracy: 47.5309%, Training Loss: 1.0478%\n",
      "Epoch [5/300], Step [82/225], Training Accuracy: 47.6181%, Training Loss: 1.0466%\n",
      "Epoch [5/300], Step [83/225], Training Accuracy: 47.6657%, Training Loss: 1.0460%\n",
      "Epoch [5/300], Step [84/225], Training Accuracy: 47.7493%, Training Loss: 1.0447%\n",
      "Epoch [5/300], Step [85/225], Training Accuracy: 47.7941%, Training Loss: 1.0445%\n",
      "Epoch [5/300], Step [86/225], Training Accuracy: 47.8016%, Training Loss: 1.0437%\n",
      "Epoch [5/300], Step [87/225], Training Accuracy: 47.8628%, Training Loss: 1.0434%\n",
      "Epoch [5/300], Step [88/225], Training Accuracy: 47.7628%, Training Loss: 1.0441%\n",
      "Epoch [5/300], Step [89/225], Training Accuracy: 47.7879%, Training Loss: 1.0444%\n",
      "Epoch [5/300], Step [90/225], Training Accuracy: 47.7778%, Training Loss: 1.0444%\n",
      "Epoch [5/300], Step [91/225], Training Accuracy: 47.7335%, Training Loss: 1.0438%\n",
      "Epoch [5/300], Step [92/225], Training Accuracy: 47.6902%, Training Loss: 1.0431%\n",
      "Epoch [5/300], Step [93/225], Training Accuracy: 47.7655%, Training Loss: 1.0425%\n",
      "Epoch [5/300], Step [94/225], Training Accuracy: 47.9388%, Training Loss: 1.0408%\n",
      "Epoch [5/300], Step [95/225], Training Accuracy: 47.9605%, Training Loss: 1.0414%\n",
      "Epoch [5/300], Step [96/225], Training Accuracy: 48.1120%, Training Loss: 1.0400%\n",
      "Epoch [5/300], Step [97/225], Training Accuracy: 48.1798%, Training Loss: 1.0391%\n",
      "Epoch [5/300], Step [98/225], Training Accuracy: 48.2143%, Training Loss: 1.0392%\n",
      "Epoch [5/300], Step [99/225], Training Accuracy: 48.2165%, Training Loss: 1.0397%\n",
      "Epoch [5/300], Step [100/225], Training Accuracy: 48.1406%, Training Loss: 1.0403%\n",
      "Epoch [5/300], Step [101/225], Training Accuracy: 48.1436%, Training Loss: 1.0395%\n",
      "Epoch [5/300], Step [102/225], Training Accuracy: 48.1158%, Training Loss: 1.0410%\n",
      "Epoch [5/300], Step [103/225], Training Accuracy: 48.0279%, Training Loss: 1.0410%\n",
      "Epoch [5/300], Step [104/225], Training Accuracy: 47.9417%, Training Loss: 1.0411%\n",
      "Epoch [5/300], Step [105/225], Training Accuracy: 48.0060%, Training Loss: 1.0406%\n",
      "Epoch [5/300], Step [106/225], Training Accuracy: 47.9805%, Training Loss: 1.0406%\n",
      "Epoch [5/300], Step [107/225], Training Accuracy: 47.9702%, Training Loss: 1.0407%\n",
      "Epoch [5/300], Step [108/225], Training Accuracy: 48.0035%, Training Loss: 1.0408%\n",
      "Epoch [5/300], Step [109/225], Training Accuracy: 47.9788%, Training Loss: 1.0411%\n",
      "Epoch [5/300], Step [110/225], Training Accuracy: 48.0114%, Training Loss: 1.0409%\n",
      "Epoch [5/300], Step [111/225], Training Accuracy: 48.0011%, Training Loss: 1.0414%\n",
      "Epoch [5/300], Step [112/225], Training Accuracy: 48.0748%, Training Loss: 1.0412%\n",
      "Epoch [5/300], Step [113/225], Training Accuracy: 48.1610%, Training Loss: 1.0407%\n",
      "Epoch [5/300], Step [114/225], Training Accuracy: 48.0674%, Training Loss: 1.0407%\n",
      "Epoch [5/300], Step [115/225], Training Accuracy: 48.0299%, Training Loss: 1.0411%\n",
      "Epoch [5/300], Step [116/225], Training Accuracy: 48.0738%, Training Loss: 1.0407%\n",
      "Epoch [5/300], Step [117/225], Training Accuracy: 47.9834%, Training Loss: 1.0413%\n",
      "Epoch [5/300], Step [118/225], Training Accuracy: 48.0403%, Training Loss: 1.0406%\n",
      "Epoch [5/300], Step [119/225], Training Accuracy: 48.0567%, Training Loss: 1.0408%\n",
      "Epoch [5/300], Step [120/225], Training Accuracy: 48.0990%, Training Loss: 1.0403%\n",
      "Epoch [5/300], Step [121/225], Training Accuracy: 48.0630%, Training Loss: 1.0410%\n",
      "Epoch [5/300], Step [122/225], Training Accuracy: 48.1045%, Training Loss: 1.0405%\n",
      "Epoch [5/300], Step [123/225], Training Accuracy: 48.1580%, Training Loss: 1.0395%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [124/225], Training Accuracy: 48.2485%, Training Loss: 1.0389%\n",
      "Epoch [5/300], Step [125/225], Training Accuracy: 48.3125%, Training Loss: 1.0399%\n",
      "Epoch [5/300], Step [126/225], Training Accuracy: 48.2887%, Training Loss: 1.0407%\n",
      "Epoch [5/300], Step [127/225], Training Accuracy: 48.2406%, Training Loss: 1.0408%\n",
      "Epoch [5/300], Step [128/225], Training Accuracy: 48.2544%, Training Loss: 1.0416%\n",
      "Epoch [5/300], Step [129/225], Training Accuracy: 48.3285%, Training Loss: 1.0413%\n",
      "Epoch [5/300], Step [130/225], Training Accuracy: 48.3053%, Training Loss: 1.0418%\n",
      "Epoch [5/300], Step [131/225], Training Accuracy: 48.2944%, Training Loss: 1.0417%\n",
      "Epoch [5/300], Step [132/225], Training Accuracy: 48.2363%, Training Loss: 1.0416%\n",
      "Epoch [5/300], Step [133/225], Training Accuracy: 48.1908%, Training Loss: 1.0425%\n",
      "Epoch [5/300], Step [134/225], Training Accuracy: 48.0644%, Training Loss: 1.0435%\n",
      "Epoch [5/300], Step [135/225], Training Accuracy: 48.1019%, Training Loss: 1.0429%\n",
      "Epoch [5/300], Step [136/225], Training Accuracy: 48.1618%, Training Loss: 1.0424%\n",
      "Epoch [5/300], Step [137/225], Training Accuracy: 48.1752%, Training Loss: 1.0423%\n",
      "Epoch [5/300], Step [138/225], Training Accuracy: 48.2677%, Training Loss: 1.0409%\n",
      "Epoch [5/300], Step [139/225], Training Accuracy: 48.2239%, Training Loss: 1.0411%\n",
      "Epoch [5/300], Step [140/225], Training Accuracy: 48.2366%, Training Loss: 1.0405%\n",
      "Epoch [5/300], Step [141/225], Training Accuracy: 48.2602%, Training Loss: 1.0397%\n",
      "Epoch [5/300], Step [142/225], Training Accuracy: 48.2945%, Training Loss: 1.0393%\n",
      "Epoch [5/300], Step [143/225], Training Accuracy: 48.3282%, Training Loss: 1.0386%\n",
      "Epoch [5/300], Step [144/225], Training Accuracy: 48.3941%, Training Loss: 1.0385%\n",
      "Epoch [5/300], Step [145/225], Training Accuracy: 48.4159%, Training Loss: 1.0387%\n",
      "Epoch [5/300], Step [146/225], Training Accuracy: 48.4375%, Training Loss: 1.0396%\n",
      "Epoch [5/300], Step [147/225], Training Accuracy: 48.4162%, Training Loss: 1.0390%\n",
      "Epoch [5/300], Step [148/225], Training Accuracy: 48.4903%, Training Loss: 1.0386%\n",
      "Epoch [5/300], Step [149/225], Training Accuracy: 48.4899%, Training Loss: 1.0393%\n",
      "Epoch [5/300], Step [150/225], Training Accuracy: 48.5104%, Training Loss: 1.0385%\n",
      "Epoch [5/300], Step [151/225], Training Accuracy: 48.5203%, Training Loss: 1.0384%\n",
      "Epoch [5/300], Step [152/225], Training Accuracy: 48.4786%, Training Loss: 1.0386%\n",
      "Epoch [5/300], Step [153/225], Training Accuracy: 48.4375%, Training Loss: 1.0386%\n",
      "Epoch [5/300], Step [154/225], Training Accuracy: 48.4578%, Training Loss: 1.0382%\n",
      "Epoch [5/300], Step [155/225], Training Accuracy: 48.4375%, Training Loss: 1.0384%\n",
      "Epoch [5/300], Step [156/225], Training Accuracy: 48.4075%, Training Loss: 1.0390%\n",
      "Epoch [5/300], Step [157/225], Training Accuracy: 48.3877%, Training Loss: 1.0388%\n",
      "Epoch [5/300], Step [158/225], Training Accuracy: 48.3089%, Training Loss: 1.0394%\n",
      "Epoch [5/300], Step [159/225], Training Accuracy: 48.2606%, Training Loss: 1.0402%\n",
      "Epoch [5/300], Step [160/225], Training Accuracy: 48.2227%, Training Loss: 1.0403%\n",
      "Epoch [5/300], Step [161/225], Training Accuracy: 48.2628%, Training Loss: 1.0400%\n",
      "Epoch [5/300], Step [162/225], Training Accuracy: 48.3796%, Training Loss: 1.0389%\n",
      "Epoch [5/300], Step [163/225], Training Accuracy: 48.3225%, Training Loss: 1.0389%\n",
      "Epoch [5/300], Step [164/225], Training Accuracy: 48.3708%, Training Loss: 1.0382%\n",
      "Epoch [5/300], Step [165/225], Training Accuracy: 48.4091%, Training Loss: 1.0383%\n",
      "Epoch [5/300], Step [166/225], Training Accuracy: 48.4187%, Training Loss: 1.0376%\n",
      "Epoch [5/300], Step [167/225], Training Accuracy: 48.4281%, Training Loss: 1.0372%\n",
      "Epoch [5/300], Step [168/225], Training Accuracy: 48.4375%, Training Loss: 1.0372%\n",
      "Epoch [5/300], Step [169/225], Training Accuracy: 48.4467%, Training Loss: 1.0373%\n",
      "Epoch [5/300], Step [170/225], Training Accuracy: 48.4375%, Training Loss: 1.0374%\n",
      "Epoch [5/300], Step [171/225], Training Accuracy: 48.4558%, Training Loss: 1.0369%\n",
      "Epoch [5/300], Step [172/225], Training Accuracy: 48.4829%, Training Loss: 1.0364%\n",
      "Epoch [5/300], Step [173/225], Training Accuracy: 48.5098%, Training Loss: 1.0368%\n",
      "Epoch [5/300], Step [174/225], Training Accuracy: 48.4914%, Training Loss: 1.0366%\n",
      "Epoch [5/300], Step [175/225], Training Accuracy: 48.5268%, Training Loss: 1.0365%\n",
      "Epoch [5/300], Step [176/225], Training Accuracy: 48.5352%, Training Loss: 1.0363%\n",
      "Epoch [5/300], Step [177/225], Training Accuracy: 48.5964%, Training Loss: 1.0360%\n",
      "Epoch [5/300], Step [178/225], Training Accuracy: 48.5692%, Training Loss: 1.0361%\n",
      "Epoch [5/300], Step [179/225], Training Accuracy: 48.6121%, Training Loss: 1.0353%\n",
      "Epoch [5/300], Step [180/225], Training Accuracy: 48.6372%, Training Loss: 1.0348%\n",
      "Epoch [5/300], Step [181/225], Training Accuracy: 48.6188%, Training Loss: 1.0350%\n",
      "Epoch [5/300], Step [182/225], Training Accuracy: 48.6092%, Training Loss: 1.0351%\n",
      "Epoch [5/300], Step [183/225], Training Accuracy: 48.6510%, Training Loss: 1.0350%\n",
      "Epoch [5/300], Step [184/225], Training Accuracy: 48.6243%, Training Loss: 1.0349%\n",
      "Epoch [5/300], Step [185/225], Training Accuracy: 48.6318%, Training Loss: 1.0348%\n",
      "Epoch [5/300], Step [186/225], Training Accuracy: 48.6727%, Training Loss: 1.0339%\n",
      "Epoch [5/300], Step [187/225], Training Accuracy: 48.6798%, Training Loss: 1.0334%\n",
      "Epoch [5/300], Step [188/225], Training Accuracy: 48.6785%, Training Loss: 1.0328%\n",
      "Epoch [5/300], Step [189/225], Training Accuracy: 48.7186%, Training Loss: 1.0318%\n",
      "Epoch [5/300], Step [190/225], Training Accuracy: 48.7336%, Training Loss: 1.0322%\n",
      "Epoch [5/300], Step [191/225], Training Accuracy: 48.7402%, Training Loss: 1.0322%\n",
      "Epoch [5/300], Step [192/225], Training Accuracy: 48.8037%, Training Loss: 1.0315%\n",
      "Epoch [5/300], Step [193/225], Training Accuracy: 48.7856%, Training Loss: 1.0314%\n",
      "Epoch [5/300], Step [194/225], Training Accuracy: 48.7597%, Training Loss: 1.0310%\n",
      "Epoch [5/300], Step [195/225], Training Accuracy: 48.8061%, Training Loss: 1.0305%\n",
      "Epoch [5/300], Step [196/225], Training Accuracy: 48.7643%, Training Loss: 1.0313%\n",
      "Epoch [5/300], Step [197/225], Training Accuracy: 48.7548%, Training Loss: 1.0315%\n",
      "Epoch [5/300], Step [198/225], Training Accuracy: 48.8005%, Training Loss: 1.0306%\n",
      "Epoch [5/300], Step [199/225], Training Accuracy: 48.8693%, Training Loss: 1.0300%\n",
      "Epoch [5/300], Step [200/225], Training Accuracy: 48.8750%, Training Loss: 1.0302%\n",
      "Epoch [5/300], Step [201/225], Training Accuracy: 48.8884%, Training Loss: 1.0306%\n",
      "Epoch [5/300], Step [202/225], Training Accuracy: 48.8861%, Training Loss: 1.0305%\n",
      "Epoch [5/300], Step [203/225], Training Accuracy: 48.8993%, Training Loss: 1.0304%\n",
      "Epoch [5/300], Step [204/225], Training Accuracy: 48.9354%, Training Loss: 1.0302%\n",
      "Epoch [5/300], Step [205/225], Training Accuracy: 49.0091%, Training Loss: 1.0295%\n",
      "Epoch [5/300], Step [206/225], Training Accuracy: 49.0064%, Training Loss: 1.0295%\n",
      "Epoch [5/300], Step [207/225], Training Accuracy: 48.9508%, Training Loss: 1.0301%\n",
      "Epoch [5/300], Step [208/225], Training Accuracy: 48.9859%, Training Loss: 1.0296%\n",
      "Epoch [5/300], Step [209/225], Training Accuracy: 48.9758%, Training Loss: 1.0296%\n",
      "Epoch [5/300], Step [210/225], Training Accuracy: 48.9807%, Training Loss: 1.0295%\n",
      "Epoch [5/300], Step [211/225], Training Accuracy: 48.9929%, Training Loss: 1.0290%\n",
      "Epoch [5/300], Step [212/225], Training Accuracy: 48.9976%, Training Loss: 1.0295%\n",
      "Epoch [5/300], Step [213/225], Training Accuracy: 48.9657%, Training Loss: 1.0304%\n",
      "Epoch [5/300], Step [214/225], Training Accuracy: 48.9705%, Training Loss: 1.0302%\n",
      "Epoch [5/300], Step [215/225], Training Accuracy: 48.9462%, Training Loss: 1.0300%\n",
      "Epoch [5/300], Step [216/225], Training Accuracy: 48.9511%, Training Loss: 1.0303%\n",
      "Epoch [5/300], Step [217/225], Training Accuracy: 48.9631%, Training Loss: 1.0305%\n",
      "Epoch [5/300], Step [218/225], Training Accuracy: 48.9464%, Training Loss: 1.0309%\n",
      "Epoch [5/300], Step [219/225], Training Accuracy: 48.9583%, Training Loss: 1.0306%\n",
      "Epoch [5/300], Step [220/225], Training Accuracy: 48.9489%, Training Loss: 1.0306%\n",
      "Epoch [5/300], Step [221/225], Training Accuracy: 48.9183%, Training Loss: 1.0308%\n",
      "Epoch [5/300], Step [222/225], Training Accuracy: 48.9231%, Training Loss: 1.0306%\n",
      "Epoch [5/300], Step [223/225], Training Accuracy: 48.9420%, Training Loss: 1.0303%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [224/225], Training Accuracy: 48.9258%, Training Loss: 1.0301%\n",
      "Epoch [5/300], Step [225/225], Training Accuracy: 48.9161%, Training Loss: 1.0299%\n",
      "Epoch [6/300], Step [1/225], Training Accuracy: 54.6875%, Training Loss: 0.9904%\n",
      "Epoch [6/300], Step [2/225], Training Accuracy: 47.6562%, Training Loss: 1.0223%\n",
      "Epoch [6/300], Step [3/225], Training Accuracy: 46.3542%, Training Loss: 1.0754%\n",
      "Epoch [6/300], Step [4/225], Training Accuracy: 44.9219%, Training Loss: 1.0544%\n",
      "Epoch [6/300], Step [5/225], Training Accuracy: 47.5000%, Training Loss: 1.0328%\n",
      "Epoch [6/300], Step [6/225], Training Accuracy: 47.6562%, Training Loss: 1.0355%\n",
      "Epoch [6/300], Step [7/225], Training Accuracy: 47.7679%, Training Loss: 1.0302%\n",
      "Epoch [6/300], Step [8/225], Training Accuracy: 48.2422%, Training Loss: 1.0210%\n",
      "Epoch [6/300], Step [9/225], Training Accuracy: 49.3056%, Training Loss: 1.0237%\n",
      "Epoch [6/300], Step [10/225], Training Accuracy: 49.2188%, Training Loss: 1.0276%\n",
      "Epoch [6/300], Step [11/225], Training Accuracy: 48.8636%, Training Loss: 1.0311%\n",
      "Epoch [6/300], Step [12/225], Training Accuracy: 49.7396%, Training Loss: 1.0208%\n",
      "Epoch [6/300], Step [13/225], Training Accuracy: 50.8413%, Training Loss: 1.0110%\n",
      "Epoch [6/300], Step [14/225], Training Accuracy: 50.6696%, Training Loss: 1.0089%\n",
      "Epoch [6/300], Step [15/225], Training Accuracy: 50.8333%, Training Loss: 1.0079%\n",
      "Epoch [6/300], Step [16/225], Training Accuracy: 50.6836%, Training Loss: 1.0143%\n",
      "Epoch [6/300], Step [17/225], Training Accuracy: 50.9191%, Training Loss: 1.0097%\n",
      "Epoch [6/300], Step [18/225], Training Accuracy: 50.6076%, Training Loss: 1.0082%\n",
      "Epoch [6/300], Step [19/225], Training Accuracy: 50.4112%, Training Loss: 1.0098%\n",
      "Epoch [6/300], Step [20/225], Training Accuracy: 50.1562%, Training Loss: 1.0129%\n",
      "Epoch [6/300], Step [21/225], Training Accuracy: 50.4464%, Training Loss: 1.0085%\n",
      "Epoch [6/300], Step [22/225], Training Accuracy: 49.8580%, Training Loss: 1.0137%\n",
      "Epoch [6/300], Step [23/225], Training Accuracy: 50.3397%, Training Loss: 1.0082%\n",
      "Epoch [6/300], Step [24/225], Training Accuracy: 50.3906%, Training Loss: 1.0062%\n",
      "Epoch [6/300], Step [25/225], Training Accuracy: 50.6250%, Training Loss: 1.0052%\n",
      "Epoch [6/300], Step [26/225], Training Accuracy: 50.3005%, Training Loss: 1.0055%\n",
      "Epoch [6/300], Step [27/225], Training Accuracy: 50.1736%, Training Loss: 1.0070%\n",
      "Epoch [6/300], Step [28/225], Training Accuracy: 50.3348%, Training Loss: 1.0039%\n",
      "Epoch [6/300], Step [29/225], Training Accuracy: 50.5388%, Training Loss: 1.0010%\n",
      "Epoch [6/300], Step [30/225], Training Accuracy: 50.4167%, Training Loss: 1.0007%\n",
      "Epoch [6/300], Step [31/225], Training Accuracy: 50.2016%, Training Loss: 1.0041%\n",
      "Epoch [6/300], Step [32/225], Training Accuracy: 50.0977%, Training Loss: 1.0022%\n",
      "Epoch [6/300], Step [33/225], Training Accuracy: 50.1894%, Training Loss: 0.9983%\n",
      "Epoch [6/300], Step [34/225], Training Accuracy: 49.9081%, Training Loss: 1.0026%\n",
      "Epoch [6/300], Step [35/225], Training Accuracy: 50.0446%, Training Loss: 1.0085%\n",
      "Epoch [6/300], Step [36/225], Training Accuracy: 49.7830%, Training Loss: 1.0086%\n",
      "Epoch [6/300], Step [37/225], Training Accuracy: 49.9155%, Training Loss: 1.0080%\n",
      "Epoch [6/300], Step [38/225], Training Accuracy: 49.9589%, Training Loss: 1.0056%\n",
      "Epoch [6/300], Step [39/225], Training Accuracy: 50.0801%, Training Loss: 1.0053%\n",
      "Epoch [6/300], Step [40/225], Training Accuracy: 50.0391%, Training Loss: 1.0068%\n",
      "Epoch [6/300], Step [41/225], Training Accuracy: 49.6951%, Training Loss: 1.0121%\n",
      "Epoch [6/300], Step [42/225], Training Accuracy: 49.5536%, Training Loss: 1.0126%\n",
      "Epoch [6/300], Step [43/225], Training Accuracy: 49.5640%, Training Loss: 1.0128%\n",
      "Epoch [6/300], Step [44/225], Training Accuracy: 49.7159%, Training Loss: 1.0110%\n",
      "Epoch [6/300], Step [45/225], Training Accuracy: 49.9306%, Training Loss: 1.0101%\n",
      "Epoch [6/300], Step [46/225], Training Accuracy: 50.1698%, Training Loss: 1.0079%\n",
      "Epoch [6/300], Step [47/225], Training Accuracy: 50.0665%, Training Loss: 1.0082%\n",
      "Epoch [6/300], Step [48/225], Training Accuracy: 50.1628%, Training Loss: 1.0079%\n",
      "Epoch [6/300], Step [49/225], Training Accuracy: 50.2870%, Training Loss: 1.0107%\n",
      "Epoch [6/300], Step [50/225], Training Accuracy: 50.2500%, Training Loss: 1.0098%\n",
      "Epoch [6/300], Step [51/225], Training Accuracy: 50.3983%, Training Loss: 1.0087%\n",
      "Epoch [6/300], Step [52/225], Training Accuracy: 50.5108%, Training Loss: 1.0080%\n",
      "Epoch [6/300], Step [53/225], Training Accuracy: 50.6486%, Training Loss: 1.0071%\n",
      "Epoch [6/300], Step [54/225], Training Accuracy: 50.6655%, Training Loss: 1.0081%\n",
      "Epoch [6/300], Step [55/225], Training Accuracy: 50.3977%, Training Loss: 1.0092%\n",
      "Epoch [6/300], Step [56/225], Training Accuracy: 50.4185%, Training Loss: 1.0087%\n",
      "Epoch [6/300], Step [57/225], Training Accuracy: 50.5482%, Training Loss: 1.0063%\n",
      "Epoch [6/300], Step [58/225], Training Accuracy: 50.5119%, Training Loss: 1.0082%\n",
      "Epoch [6/300], Step [59/225], Training Accuracy: 50.5561%, Training Loss: 1.0064%\n",
      "Epoch [6/300], Step [60/225], Training Accuracy: 50.5469%, Training Loss: 1.0053%\n",
      "Epoch [6/300], Step [61/225], Training Accuracy: 50.5379%, Training Loss: 1.0055%\n",
      "Epoch [6/300], Step [62/225], Training Accuracy: 50.5796%, Training Loss: 1.0040%\n",
      "Epoch [6/300], Step [63/225], Training Accuracy: 50.5704%, Training Loss: 1.0052%\n",
      "Epoch [6/300], Step [64/225], Training Accuracy: 50.6104%, Training Loss: 1.0053%\n",
      "Epoch [6/300], Step [65/225], Training Accuracy: 50.6250%, Training Loss: 1.0051%\n",
      "Epoch [6/300], Step [66/225], Training Accuracy: 50.6629%, Training Loss: 1.0031%\n",
      "Epoch [6/300], Step [67/225], Training Accuracy: 50.5131%, Training Loss: 1.0030%\n",
      "Epoch [6/300], Step [68/225], Training Accuracy: 50.4136%, Training Loss: 1.0033%\n",
      "Epoch [6/300], Step [69/225], Training Accuracy: 50.2944%, Training Loss: 1.0027%\n",
      "Epoch [6/300], Step [70/225], Training Accuracy: 50.2902%, Training Loss: 1.0038%\n",
      "Epoch [6/300], Step [71/225], Training Accuracy: 50.3301%, Training Loss: 1.0045%\n",
      "Epoch [6/300], Step [72/225], Training Accuracy: 50.3038%, Training Loss: 1.0061%\n",
      "Epoch [6/300], Step [73/225], Training Accuracy: 50.1926%, Training Loss: 1.0064%\n",
      "Epoch [6/300], Step [74/225], Training Accuracy: 50.1900%, Training Loss: 1.0053%\n",
      "Epoch [6/300], Step [75/225], Training Accuracy: 50.3125%, Training Loss: 1.0031%\n",
      "Epoch [6/300], Step [76/225], Training Accuracy: 50.2056%, Training Loss: 1.0039%\n",
      "Epoch [6/300], Step [77/225], Training Accuracy: 50.2638%, Training Loss: 1.0029%\n",
      "Epoch [6/300], Step [78/225], Training Accuracy: 50.2604%, Training Loss: 1.0016%\n",
      "Epoch [6/300], Step [79/225], Training Accuracy: 50.1384%, Training Loss: 1.0024%\n",
      "Epoch [6/300], Step [80/225], Training Accuracy: 50.0977%, Training Loss: 1.0029%\n",
      "Epoch [6/300], Step [81/225], Training Accuracy: 50.1543%, Training Loss: 1.0019%\n",
      "Epoch [6/300], Step [82/225], Training Accuracy: 50.2287%, Training Loss: 1.0006%\n",
      "Epoch [6/300], Step [83/225], Training Accuracy: 50.2259%, Training Loss: 0.9998%\n",
      "Epoch [6/300], Step [84/225], Training Accuracy: 50.3162%, Training Loss: 0.9988%\n",
      "Epoch [6/300], Step [85/225], Training Accuracy: 50.4412%, Training Loss: 0.9980%\n",
      "Epoch [6/300], Step [86/225], Training Accuracy: 50.5087%, Training Loss: 0.9967%\n",
      "Epoch [6/300], Step [87/225], Training Accuracy: 50.5568%, Training Loss: 0.9971%\n",
      "Epoch [6/300], Step [88/225], Training Accuracy: 50.4794%, Training Loss: 0.9980%\n",
      "Epoch [6/300], Step [89/225], Training Accuracy: 50.5267%, Training Loss: 0.9984%\n",
      "Epoch [6/300], Step [90/225], Training Accuracy: 50.5556%, Training Loss: 0.9987%\n",
      "Epoch [6/300], Step [91/225], Training Accuracy: 50.5495%, Training Loss: 0.9977%\n",
      "Epoch [6/300], Step [92/225], Training Accuracy: 50.5265%, Training Loss: 0.9980%\n",
      "Epoch [6/300], Step [93/225], Training Accuracy: 50.6384%, Training Loss: 0.9973%\n",
      "Epoch [6/300], Step [94/225], Training Accuracy: 50.7480%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [95/225], Training Accuracy: 50.7237%, Training Loss: 0.9963%\n",
      "Epoch [6/300], Step [96/225], Training Accuracy: 50.8301%, Training Loss: 0.9952%\n",
      "Epoch [6/300], Step [97/225], Training Accuracy: 50.9343%, Training Loss: 0.9941%\n",
      "Epoch [6/300], Step [98/225], Training Accuracy: 50.8929%, Training Loss: 0.9941%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [99/225], Training Accuracy: 50.8207%, Training Loss: 0.9948%\n",
      "Epoch [6/300], Step [100/225], Training Accuracy: 50.7969%, Training Loss: 0.9952%\n",
      "Epoch [6/300], Step [101/225], Training Accuracy: 50.8045%, Training Loss: 0.9954%\n",
      "Epoch [6/300], Step [102/225], Training Accuracy: 50.8119%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [103/225], Training Accuracy: 50.7585%, Training Loss: 0.9959%\n",
      "Epoch [6/300], Step [104/225], Training Accuracy: 50.6761%, Training Loss: 0.9957%\n",
      "Epoch [6/300], Step [105/225], Training Accuracy: 50.7143%, Training Loss: 0.9959%\n",
      "Epoch [6/300], Step [106/225], Training Accuracy: 50.7075%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [107/225], Training Accuracy: 50.6571%, Training Loss: 0.9962%\n",
      "Epoch [6/300], Step [108/225], Training Accuracy: 50.6221%, Training Loss: 0.9968%\n",
      "Epoch [6/300], Step [109/225], Training Accuracy: 50.6021%, Training Loss: 0.9972%\n",
      "Epoch [6/300], Step [110/225], Training Accuracy: 50.6534%, Training Loss: 0.9968%\n",
      "Epoch [6/300], Step [111/225], Training Accuracy: 50.6194%, Training Loss: 0.9963%\n",
      "Epoch [6/300], Step [112/225], Training Accuracy: 50.6417%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [113/225], Training Accuracy: 50.7052%, Training Loss: 0.9964%\n",
      "Epoch [6/300], Step [114/225], Training Accuracy: 50.6305%, Training Loss: 0.9962%\n",
      "Epoch [6/300], Step [115/225], Training Accuracy: 50.6114%, Training Loss: 0.9964%\n",
      "Epoch [6/300], Step [116/225], Training Accuracy: 50.6466%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [117/225], Training Accuracy: 50.5208%, Training Loss: 0.9971%\n",
      "Epoch [6/300], Step [118/225], Training Accuracy: 50.5694%, Training Loss: 0.9964%\n",
      "Epoch [6/300], Step [119/225], Training Accuracy: 50.6040%, Training Loss: 0.9962%\n",
      "Epoch [6/300], Step [120/225], Training Accuracy: 50.6380%, Training Loss: 0.9964%\n",
      "Epoch [6/300], Step [121/225], Training Accuracy: 50.6069%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [122/225], Training Accuracy: 50.6276%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [123/225], Training Accuracy: 50.6479%, Training Loss: 0.9950%\n",
      "Epoch [6/300], Step [124/225], Training Accuracy: 50.6552%, Training Loss: 0.9945%\n",
      "Epoch [6/300], Step [125/225], Training Accuracy: 50.6500%, Training Loss: 0.9959%\n",
      "Epoch [6/300], Step [126/225], Training Accuracy: 50.6324%, Training Loss: 0.9964%\n",
      "Epoch [6/300], Step [127/225], Training Accuracy: 50.5659%, Training Loss: 0.9967%\n",
      "Epoch [6/300], Step [128/225], Training Accuracy: 50.4395%, Training Loss: 0.9982%\n",
      "Epoch [6/300], Step [129/225], Training Accuracy: 50.5208%, Training Loss: 0.9986%\n",
      "Epoch [6/300], Step [130/225], Training Accuracy: 50.4327%, Training Loss: 0.9991%\n",
      "Epoch [6/300], Step [131/225], Training Accuracy: 50.4055%, Training Loss: 0.9990%\n",
      "Epoch [6/300], Step [132/225], Training Accuracy: 50.4025%, Training Loss: 0.9991%\n",
      "Epoch [6/300], Step [133/225], Training Accuracy: 50.4229%, Training Loss: 0.9998%\n",
      "Epoch [6/300], Step [134/225], Training Accuracy: 50.4198%, Training Loss: 1.0002%\n",
      "Epoch [6/300], Step [135/225], Training Accuracy: 50.4630%, Training Loss: 0.9996%\n",
      "Epoch [6/300], Step [136/225], Training Accuracy: 50.5400%, Training Loss: 0.9994%\n",
      "Epoch [6/300], Step [137/225], Training Accuracy: 50.5360%, Training Loss: 0.9998%\n",
      "Epoch [6/300], Step [138/225], Training Accuracy: 50.5435%, Training Loss: 0.9990%\n",
      "Epoch [6/300], Step [139/225], Training Accuracy: 50.5508%, Training Loss: 0.9986%\n",
      "Epoch [6/300], Step [140/225], Training Accuracy: 50.5469%, Training Loss: 0.9984%\n",
      "Epoch [6/300], Step [141/225], Training Accuracy: 50.5762%, Training Loss: 0.9975%\n",
      "Epoch [6/300], Step [142/225], Training Accuracy: 50.6272%, Training Loss: 0.9972%\n",
      "Epoch [6/300], Step [143/225], Training Accuracy: 50.6884%, Training Loss: 0.9967%\n",
      "Epoch [6/300], Step [144/225], Training Accuracy: 50.7270%, Training Loss: 0.9960%\n",
      "Epoch [6/300], Step [145/225], Training Accuracy: 50.7435%, Training Loss: 0.9959%\n",
      "Epoch [6/300], Step [146/225], Training Accuracy: 50.7063%, Training Loss: 0.9967%\n",
      "Epoch [6/300], Step [147/225], Training Accuracy: 50.6696%, Training Loss: 0.9961%\n",
      "Epoch [6/300], Step [148/225], Training Accuracy: 50.7285%, Training Loss: 0.9957%\n",
      "Epoch [6/300], Step [149/225], Training Accuracy: 50.6502%, Training Loss: 0.9965%\n",
      "Epoch [6/300], Step [150/225], Training Accuracy: 50.6979%, Training Loss: 0.9955%\n",
      "Epoch [6/300], Step [151/225], Training Accuracy: 50.7450%, Training Loss: 0.9946%\n",
      "Epoch [6/300], Step [152/225], Training Accuracy: 50.7299%, Training Loss: 0.9950%\n",
      "Epoch [6/300], Step [153/225], Training Accuracy: 50.7353%, Training Loss: 0.9949%\n",
      "Epoch [6/300], Step [154/225], Training Accuracy: 50.7305%, Training Loss: 0.9945%\n",
      "Epoch [6/300], Step [155/225], Training Accuracy: 50.7560%, Training Loss: 0.9946%\n",
      "Epoch [6/300], Step [156/225], Training Accuracy: 50.7512%, Training Loss: 0.9943%\n",
      "Epoch [6/300], Step [157/225], Training Accuracy: 50.7862%, Training Loss: 0.9937%\n",
      "Epoch [6/300], Step [158/225], Training Accuracy: 50.6922%, Training Loss: 0.9946%\n",
      "Epoch [6/300], Step [159/225], Training Accuracy: 50.6191%, Training Loss: 0.9953%\n",
      "Epoch [6/300], Step [160/225], Training Accuracy: 50.6055%, Training Loss: 0.9954%\n",
      "Epoch [6/300], Step [161/225], Training Accuracy: 50.5920%, Training Loss: 0.9950%\n",
      "Epoch [6/300], Step [162/225], Training Accuracy: 50.6462%, Training Loss: 0.9943%\n",
      "Epoch [6/300], Step [163/225], Training Accuracy: 50.5752%, Training Loss: 0.9944%\n",
      "Epoch [6/300], Step [164/225], Training Accuracy: 50.6479%, Training Loss: 0.9933%\n",
      "Epoch [6/300], Step [165/225], Training Accuracy: 50.6534%, Training Loss: 0.9937%\n",
      "Epoch [6/300], Step [166/225], Training Accuracy: 50.6871%, Training Loss: 0.9934%\n",
      "Epoch [6/300], Step [167/225], Training Accuracy: 50.7111%, Training Loss: 0.9928%\n",
      "Epoch [6/300], Step [168/225], Training Accuracy: 50.7254%, Training Loss: 0.9924%\n",
      "Epoch [6/300], Step [169/225], Training Accuracy: 50.7027%, Training Loss: 0.9924%\n",
      "Epoch [6/300], Step [170/225], Training Accuracy: 50.6985%, Training Loss: 0.9925%\n",
      "Epoch [6/300], Step [171/225], Training Accuracy: 50.7310%, Training Loss: 0.9923%\n",
      "Epoch [6/300], Step [172/225], Training Accuracy: 50.7540%, Training Loss: 0.9923%\n",
      "Epoch [6/300], Step [173/225], Training Accuracy: 50.7858%, Training Loss: 0.9920%\n",
      "Epoch [6/300], Step [174/225], Training Accuracy: 50.7902%, Training Loss: 0.9920%\n",
      "Epoch [6/300], Step [175/225], Training Accuracy: 50.8304%, Training Loss: 0.9921%\n",
      "Epoch [6/300], Step [176/225], Training Accuracy: 50.8345%, Training Loss: 0.9921%\n",
      "Epoch [6/300], Step [177/225], Training Accuracy: 50.9093%, Training Loss: 0.9918%\n",
      "Epoch [6/300], Step [178/225], Training Accuracy: 50.8866%, Training Loss: 0.9919%\n",
      "Epoch [6/300], Step [179/225], Training Accuracy: 50.9166%, Training Loss: 0.9913%\n",
      "Epoch [6/300], Step [180/225], Training Accuracy: 50.9549%, Training Loss: 0.9906%\n",
      "Epoch [6/300], Step [181/225], Training Accuracy: 50.9669%, Training Loss: 0.9911%\n",
      "Epoch [6/300], Step [182/225], Training Accuracy: 50.9787%, Training Loss: 0.9913%\n",
      "Epoch [6/300], Step [183/225], Training Accuracy: 50.9819%, Training Loss: 0.9907%\n",
      "Epoch [6/300], Step [184/225], Training Accuracy: 51.0105%, Training Loss: 0.9905%\n",
      "Epoch [6/300], Step [185/225], Training Accuracy: 51.0220%, Training Loss: 0.9902%\n",
      "Epoch [6/300], Step [186/225], Training Accuracy: 51.0669%, Training Loss: 0.9895%\n",
      "Epoch [6/300], Step [187/225], Training Accuracy: 51.0695%, Training Loss: 0.9894%\n",
      "Epoch [6/300], Step [188/225], Training Accuracy: 51.1220%, Training Loss: 0.9887%\n",
      "Epoch [6/300], Step [189/225], Training Accuracy: 51.1739%, Training Loss: 0.9879%\n",
      "Epoch [6/300], Step [190/225], Training Accuracy: 51.1349%, Training Loss: 0.9887%\n",
      "Epoch [6/300], Step [191/225], Training Accuracy: 51.1371%, Training Loss: 0.9887%\n",
      "Epoch [6/300], Step [192/225], Training Accuracy: 51.2370%, Training Loss: 0.9880%\n",
      "Epoch [6/300], Step [193/225], Training Accuracy: 51.1901%, Training Loss: 0.9879%\n",
      "Epoch [6/300], Step [194/225], Training Accuracy: 51.1598%, Training Loss: 0.9877%\n",
      "Epoch [6/300], Step [195/225], Training Accuracy: 51.2340%, Training Loss: 0.9870%\n",
      "Epoch [6/300], Step [196/225], Training Accuracy: 51.2197%, Training Loss: 0.9877%\n",
      "Epoch [6/300], Step [197/225], Training Accuracy: 51.2214%, Training Loss: 0.9874%\n",
      "Epoch [6/300], Step [198/225], Training Accuracy: 51.2626%, Training Loss: 0.9865%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [199/225], Training Accuracy: 51.3034%, Training Loss: 0.9861%\n",
      "Epoch [6/300], Step [200/225], Training Accuracy: 51.3281%, Training Loss: 0.9859%\n",
      "Epoch [6/300], Step [201/225], Training Accuracy: 51.3137%, Training Loss: 0.9863%\n",
      "Epoch [6/300], Step [202/225], Training Accuracy: 51.3150%, Training Loss: 0.9867%\n",
      "Epoch [6/300], Step [203/225], Training Accuracy: 51.3239%, Training Loss: 0.9865%\n",
      "Epoch [6/300], Step [204/225], Training Accuracy: 51.3251%, Training Loss: 0.9862%\n",
      "Epoch [6/300], Step [205/225], Training Accuracy: 51.3872%, Training Loss: 0.9856%\n",
      "Epoch [6/300], Step [206/225], Training Accuracy: 51.3274%, Training Loss: 0.9860%\n",
      "Epoch [6/300], Step [207/225], Training Accuracy: 51.3436%, Training Loss: 0.9862%\n",
      "Epoch [6/300], Step [208/225], Training Accuracy: 51.3522%, Training Loss: 0.9855%\n",
      "Epoch [6/300], Step [209/225], Training Accuracy: 51.3307%, Training Loss: 0.9858%\n",
      "Epoch [6/300], Step [210/225], Training Accuracy: 51.3021%, Training Loss: 0.9857%\n",
      "Epoch [6/300], Step [211/225], Training Accuracy: 51.3700%, Training Loss: 0.9851%\n",
      "Epoch [6/300], Step [212/225], Training Accuracy: 51.3561%, Training Loss: 0.9851%\n",
      "Epoch [6/300], Step [213/225], Training Accuracy: 51.3424%, Training Loss: 0.9863%\n",
      "Epoch [6/300], Step [214/225], Training Accuracy: 51.3362%, Training Loss: 0.9865%\n",
      "Epoch [6/300], Step [215/225], Training Accuracy: 51.3299%, Training Loss: 0.9865%\n",
      "Epoch [6/300], Step [216/225], Training Accuracy: 51.3455%, Training Loss: 0.9865%\n",
      "Epoch [6/300], Step [217/225], Training Accuracy: 51.3321%, Training Loss: 0.9867%\n",
      "Epoch [6/300], Step [218/225], Training Accuracy: 51.3188%, Training Loss: 0.9870%\n",
      "Epoch [6/300], Step [219/225], Training Accuracy: 51.3342%, Training Loss: 0.9869%\n",
      "Epoch [6/300], Step [220/225], Training Accuracy: 51.3281%, Training Loss: 0.9869%\n",
      "Epoch [6/300], Step [221/225], Training Accuracy: 51.2797%, Training Loss: 0.9875%\n",
      "Epoch [6/300], Step [222/225], Training Accuracy: 51.2739%, Training Loss: 0.9872%\n",
      "Epoch [6/300], Step [223/225], Training Accuracy: 51.2892%, Training Loss: 0.9871%\n",
      "Epoch [6/300], Step [224/225], Training Accuracy: 51.2835%, Training Loss: 0.9869%\n",
      "Epoch [6/300], Step [225/225], Training Accuracy: 51.2854%, Training Loss: 0.9868%\n",
      "Epoch [7/300], Step [1/225], Training Accuracy: 54.6875%, Training Loss: 0.9867%\n",
      "Epoch [7/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 0.9786%\n",
      "Epoch [7/300], Step [3/225], Training Accuracy: 52.6042%, Training Loss: 1.0072%\n",
      "Epoch [7/300], Step [4/225], Training Accuracy: 51.9531%, Training Loss: 0.9951%\n",
      "Epoch [7/300], Step [5/225], Training Accuracy: 54.6875%, Training Loss: 0.9718%\n",
      "Epoch [7/300], Step [6/225], Training Accuracy: 54.1667%, Training Loss: 0.9756%\n",
      "Epoch [7/300], Step [7/225], Training Accuracy: 53.3482%, Training Loss: 0.9706%\n",
      "Epoch [7/300], Step [8/225], Training Accuracy: 53.3203%, Training Loss: 0.9702%\n",
      "Epoch [7/300], Step [9/225], Training Accuracy: 53.1250%, Training Loss: 0.9722%\n",
      "Epoch [7/300], Step [10/225], Training Accuracy: 52.3438%, Training Loss: 0.9831%\n",
      "Epoch [7/300], Step [11/225], Training Accuracy: 52.6989%, Training Loss: 0.9845%\n",
      "Epoch [7/300], Step [12/225], Training Accuracy: 53.2552%, Training Loss: 0.9806%\n",
      "Epoch [7/300], Step [13/225], Training Accuracy: 53.8462%, Training Loss: 0.9688%\n",
      "Epoch [7/300], Step [14/225], Training Accuracy: 54.1295%, Training Loss: 0.9633%\n",
      "Epoch [7/300], Step [15/225], Training Accuracy: 54.1667%, Training Loss: 0.9602%\n",
      "Epoch [7/300], Step [16/225], Training Accuracy: 54.2969%, Training Loss: 0.9616%\n",
      "Epoch [7/300], Step [17/225], Training Accuracy: 54.5956%, Training Loss: 0.9557%\n",
      "Epoch [7/300], Step [18/225], Training Accuracy: 54.3403%, Training Loss: 0.9541%\n",
      "Epoch [7/300], Step [19/225], Training Accuracy: 54.1941%, Training Loss: 0.9566%\n",
      "Epoch [7/300], Step [20/225], Training Accuracy: 54.1406%, Training Loss: 0.9573%\n",
      "Epoch [7/300], Step [21/225], Training Accuracy: 54.4643%, Training Loss: 0.9566%\n",
      "Epoch [7/300], Step [22/225], Training Accuracy: 54.1193%, Training Loss: 0.9642%\n",
      "Epoch [7/300], Step [23/225], Training Accuracy: 54.2120%, Training Loss: 0.9570%\n",
      "Epoch [7/300], Step [24/225], Training Accuracy: 53.7760%, Training Loss: 0.9577%\n",
      "Epoch [7/300], Step [25/225], Training Accuracy: 53.6875%, Training Loss: 0.9564%\n",
      "Epoch [7/300], Step [26/225], Training Accuracy: 53.5457%, Training Loss: 0.9553%\n",
      "Epoch [7/300], Step [27/225], Training Accuracy: 53.0671%, Training Loss: 0.9576%\n",
      "Epoch [7/300], Step [28/225], Training Accuracy: 53.0134%, Training Loss: 0.9575%\n",
      "Epoch [7/300], Step [29/225], Training Accuracy: 53.1250%, Training Loss: 0.9555%\n",
      "Epoch [7/300], Step [30/225], Training Accuracy: 53.1250%, Training Loss: 0.9559%\n",
      "Epoch [7/300], Step [31/225], Training Accuracy: 52.7722%, Training Loss: 0.9592%\n",
      "Epoch [7/300], Step [32/225], Training Accuracy: 52.9297%, Training Loss: 0.9556%\n",
      "Epoch [7/300], Step [33/225], Training Accuracy: 52.9356%, Training Loss: 0.9519%\n",
      "Epoch [7/300], Step [34/225], Training Accuracy: 52.8493%, Training Loss: 0.9548%\n",
      "Epoch [7/300], Step [35/225], Training Accuracy: 52.9464%, Training Loss: 0.9564%\n",
      "Epoch [7/300], Step [36/225], Training Accuracy: 52.8212%, Training Loss: 0.9568%\n",
      "Epoch [7/300], Step [37/225], Training Accuracy: 52.9139%, Training Loss: 0.9556%\n",
      "Epoch [7/300], Step [38/225], Training Accuracy: 52.9605%, Training Loss: 0.9551%\n",
      "Epoch [7/300], Step [39/225], Training Accuracy: 53.0849%, Training Loss: 0.9549%\n",
      "Epoch [7/300], Step [40/225], Training Accuracy: 52.9688%, Training Loss: 0.9568%\n",
      "Epoch [7/300], Step [41/225], Training Accuracy: 52.6296%, Training Loss: 0.9606%\n",
      "Epoch [7/300], Step [42/225], Training Accuracy: 52.6414%, Training Loss: 0.9596%\n",
      "Epoch [7/300], Step [43/225], Training Accuracy: 52.5073%, Training Loss: 0.9586%\n",
      "Epoch [7/300], Step [44/225], Training Accuracy: 52.6634%, Training Loss: 0.9563%\n",
      "Epoch [7/300], Step [45/225], Training Accuracy: 52.8819%, Training Loss: 0.9536%\n",
      "Epoch [7/300], Step [46/225], Training Accuracy: 52.8533%, Training Loss: 0.9513%\n",
      "Epoch [7/300], Step [47/225], Training Accuracy: 52.9255%, Training Loss: 0.9498%\n",
      "Epoch [7/300], Step [48/225], Training Accuracy: 52.8320%, Training Loss: 0.9487%\n",
      "Epoch [7/300], Step [49/225], Training Accuracy: 52.7423%, Training Loss: 0.9500%\n",
      "Epoch [7/300], Step [50/225], Training Accuracy: 52.7812%, Training Loss: 0.9502%\n",
      "Epoch [7/300], Step [51/225], Training Accuracy: 53.0331%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [52/225], Training Accuracy: 53.0950%, Training Loss: 0.9452%\n",
      "Epoch [7/300], Step [53/225], Training Accuracy: 53.1545%, Training Loss: 0.9447%\n",
      "Epoch [7/300], Step [54/225], Training Accuracy: 52.9514%, Training Loss: 0.9466%\n",
      "Epoch [7/300], Step [55/225], Training Accuracy: 52.8693%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [56/225], Training Accuracy: 52.8460%, Training Loss: 0.9463%\n",
      "Epoch [7/300], Step [57/225], Training Accuracy: 52.9057%, Training Loss: 0.9451%\n",
      "Epoch [7/300], Step [58/225], Training Accuracy: 52.9364%, Training Loss: 0.9454%\n",
      "Epoch [7/300], Step [59/225], Training Accuracy: 52.9926%, Training Loss: 0.9438%\n",
      "Epoch [7/300], Step [60/225], Training Accuracy: 52.8646%, Training Loss: 0.9431%\n",
      "Epoch [7/300], Step [61/225], Training Accuracy: 52.9201%, Training Loss: 0.9430%\n",
      "Epoch [7/300], Step [62/225], Training Accuracy: 52.9990%, Training Loss: 0.9421%\n",
      "Epoch [7/300], Step [63/225], Training Accuracy: 53.0506%, Training Loss: 0.9434%\n",
      "Epoch [7/300], Step [64/225], Training Accuracy: 53.0762%, Training Loss: 0.9438%\n",
      "Epoch [7/300], Step [65/225], Training Accuracy: 52.9327%, Training Loss: 0.9444%\n",
      "Epoch [7/300], Step [66/225], Training Accuracy: 53.0066%, Training Loss: 0.9431%\n",
      "Epoch [7/300], Step [67/225], Training Accuracy: 53.0084%, Training Loss: 0.9434%\n",
      "Epoch [7/300], Step [68/225], Training Accuracy: 52.9871%, Training Loss: 0.9435%\n",
      "Epoch [7/300], Step [69/225], Training Accuracy: 52.8306%, Training Loss: 0.9440%\n",
      "Epoch [7/300], Step [70/225], Training Accuracy: 52.9018%, Training Loss: 0.9437%\n",
      "Epoch [7/300], Step [71/225], Training Accuracy: 52.8829%, Training Loss: 0.9452%\n",
      "Epoch [7/300], Step [72/225], Training Accuracy: 52.8212%, Training Loss: 0.9465%\n",
      "Epoch [7/300], Step [73/225], Training Accuracy: 52.6969%, Training Loss: 0.9481%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Step [74/225], Training Accuracy: 52.6394%, Training Loss: 0.9473%\n",
      "Epoch [7/300], Step [75/225], Training Accuracy: 52.6667%, Training Loss: 0.9464%\n",
      "Epoch [7/300], Step [76/225], Training Accuracy: 52.5699%, Training Loss: 0.9478%\n",
      "Epoch [7/300], Step [77/225], Training Accuracy: 52.7394%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [78/225], Training Accuracy: 52.7244%, Training Loss: 0.9470%\n",
      "Epoch [7/300], Step [79/225], Training Accuracy: 52.6701%, Training Loss: 0.9487%\n",
      "Epoch [7/300], Step [80/225], Training Accuracy: 52.7930%, Training Loss: 0.9484%\n",
      "Epoch [7/300], Step [81/225], Training Accuracy: 52.7778%, Training Loss: 0.9477%\n",
      "Epoch [7/300], Step [82/225], Training Accuracy: 52.7820%, Training Loss: 0.9468%\n",
      "Epoch [7/300], Step [83/225], Training Accuracy: 52.8238%, Training Loss: 0.9460%\n",
      "Epoch [7/300], Step [84/225], Training Accuracy: 53.0134%, Training Loss: 0.9455%\n",
      "Epoch [7/300], Step [85/225], Training Accuracy: 53.1434%, Training Loss: 0.9449%\n",
      "Epoch [7/300], Step [86/225], Training Accuracy: 53.1250%, Training Loss: 0.9445%\n",
      "Epoch [7/300], Step [87/225], Training Accuracy: 53.0711%, Training Loss: 0.9453%\n",
      "Epoch [7/300], Step [88/225], Training Accuracy: 53.0185%, Training Loss: 0.9457%\n",
      "Epoch [7/300], Step [89/225], Training Accuracy: 53.0021%, Training Loss: 0.9466%\n",
      "Epoch [7/300], Step [90/225], Training Accuracy: 53.0729%, Training Loss: 0.9470%\n",
      "Epoch [7/300], Step [91/225], Training Accuracy: 53.0563%, Training Loss: 0.9464%\n",
      "Epoch [7/300], Step [92/225], Training Accuracy: 53.0231%, Training Loss: 0.9469%\n",
      "Epoch [7/300], Step [93/225], Training Accuracy: 53.1586%, Training Loss: 0.9460%\n",
      "Epoch [7/300], Step [94/225], Training Accuracy: 53.2746%, Training Loss: 0.9448%\n",
      "Epoch [7/300], Step [95/225], Training Accuracy: 53.2072%, Training Loss: 0.9454%\n",
      "Epoch [7/300], Step [96/225], Training Accuracy: 53.3529%, Training Loss: 0.9449%\n",
      "Epoch [7/300], Step [97/225], Training Accuracy: 53.4311%, Training Loss: 0.9439%\n",
      "Epoch [7/300], Step [98/225], Training Accuracy: 53.3960%, Training Loss: 0.9435%\n",
      "Epoch [7/300], Step [99/225], Training Accuracy: 53.3775%, Training Loss: 0.9444%\n",
      "Epoch [7/300], Step [100/225], Training Accuracy: 53.3438%, Training Loss: 0.9450%\n",
      "Epoch [7/300], Step [101/225], Training Accuracy: 53.2797%, Training Loss: 0.9452%\n",
      "Epoch [7/300], Step [102/225], Training Accuracy: 53.2935%, Training Loss: 0.9472%\n",
      "Epoch [7/300], Step [103/225], Training Accuracy: 53.2615%, Training Loss: 0.9469%\n",
      "Epoch [7/300], Step [104/225], Training Accuracy: 53.2302%, Training Loss: 0.9474%\n",
      "Epoch [7/300], Step [105/225], Training Accuracy: 53.2589%, Training Loss: 0.9473%\n",
      "Epoch [7/300], Step [106/225], Training Accuracy: 53.2429%, Training Loss: 0.9473%\n",
      "Epoch [7/300], Step [107/225], Training Accuracy: 53.2272%, Training Loss: 0.9473%\n",
      "Epoch [7/300], Step [108/225], Training Accuracy: 53.2263%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [109/225], Training Accuracy: 53.1823%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [110/225], Training Accuracy: 53.1534%, Training Loss: 0.9473%\n",
      "Epoch [7/300], Step [111/225], Training Accuracy: 53.0968%, Training Loss: 0.9478%\n",
      "Epoch [7/300], Step [112/225], Training Accuracy: 53.1390%, Training Loss: 0.9473%\n",
      "Epoch [7/300], Step [113/225], Training Accuracy: 53.1941%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [114/225], Training Accuracy: 53.1387%, Training Loss: 0.9471%\n",
      "Epoch [7/300], Step [115/225], Training Accuracy: 53.1114%, Training Loss: 0.9469%\n",
      "Epoch [7/300], Step [116/225], Training Accuracy: 53.0846%, Training Loss: 0.9469%\n",
      "Epoch [7/300], Step [117/225], Training Accuracy: 53.0449%, Training Loss: 0.9481%\n",
      "Epoch [7/300], Step [118/225], Training Accuracy: 53.0853%, Training Loss: 0.9482%\n",
      "Epoch [7/300], Step [119/225], Training Accuracy: 53.0462%, Training Loss: 0.9486%\n",
      "Epoch [7/300], Step [120/225], Training Accuracy: 53.0208%, Training Loss: 0.9502%\n",
      "Epoch [7/300], Step [121/225], Training Accuracy: 52.9830%, Training Loss: 0.9506%\n",
      "Epoch [7/300], Step [122/225], Training Accuracy: 52.9585%, Training Loss: 0.9506%\n",
      "Epoch [7/300], Step [123/225], Training Accuracy: 52.9726%, Training Loss: 0.9501%\n",
      "Epoch [7/300], Step [124/225], Training Accuracy: 52.9864%, Training Loss: 0.9495%\n",
      "Epoch [7/300], Step [125/225], Training Accuracy: 52.9875%, Training Loss: 0.9508%\n",
      "Epoch [7/300], Step [126/225], Training Accuracy: 52.9142%, Training Loss: 0.9518%\n",
      "Epoch [7/300], Step [127/225], Training Accuracy: 52.7805%, Training Loss: 0.9531%\n",
      "Epoch [7/300], Step [128/225], Training Accuracy: 52.6489%, Training Loss: 0.9549%\n",
      "Epoch [7/300], Step [129/225], Training Accuracy: 52.6405%, Training Loss: 0.9555%\n",
      "Epoch [7/300], Step [130/225], Training Accuracy: 52.5962%, Training Loss: 0.9560%\n",
      "Epoch [7/300], Step [131/225], Training Accuracy: 52.6002%, Training Loss: 0.9560%\n",
      "Epoch [7/300], Step [132/225], Training Accuracy: 52.6042%, Training Loss: 0.9560%\n",
      "Epoch [7/300], Step [133/225], Training Accuracy: 52.5728%, Training Loss: 0.9566%\n",
      "Epoch [7/300], Step [134/225], Training Accuracy: 52.5420%, Training Loss: 0.9568%\n",
      "Epoch [7/300], Step [135/225], Training Accuracy: 52.6505%, Training Loss: 0.9558%\n",
      "Epoch [7/300], Step [136/225], Training Accuracy: 52.6769%, Training Loss: 0.9558%\n",
      "Epoch [7/300], Step [137/225], Training Accuracy: 52.7030%, Training Loss: 0.9561%\n",
      "Epoch [7/300], Step [138/225], Training Accuracy: 52.6721%, Training Loss: 0.9564%\n",
      "Epoch [7/300], Step [139/225], Training Accuracy: 52.5967%, Training Loss: 0.9567%\n",
      "Epoch [7/300], Step [140/225], Training Accuracy: 52.5893%, Training Loss: 0.9568%\n",
      "Epoch [7/300], Step [141/225], Training Accuracy: 52.6263%, Training Loss: 0.9565%\n",
      "Epoch [7/300], Step [142/225], Training Accuracy: 52.6078%, Training Loss: 0.9565%\n",
      "Epoch [7/300], Step [143/225], Training Accuracy: 52.6115%, Training Loss: 0.9562%\n",
      "Epoch [7/300], Step [144/225], Training Accuracy: 52.6042%, Training Loss: 0.9558%\n",
      "Epoch [7/300], Step [145/225], Training Accuracy: 52.6078%, Training Loss: 0.9559%\n",
      "Epoch [7/300], Step [146/225], Training Accuracy: 52.5364%, Training Loss: 0.9569%\n",
      "Epoch [7/300], Step [147/225], Training Accuracy: 52.5510%, Training Loss: 0.9564%\n",
      "Epoch [7/300], Step [148/225], Training Accuracy: 52.6077%, Training Loss: 0.9558%\n",
      "Epoch [7/300], Step [149/225], Training Accuracy: 52.6216%, Training Loss: 0.9563%\n",
      "Epoch [7/300], Step [150/225], Training Accuracy: 52.6562%, Training Loss: 0.9553%\n",
      "Epoch [7/300], Step [151/225], Training Accuracy: 52.7318%, Training Loss: 0.9545%\n",
      "Epoch [7/300], Step [152/225], Training Accuracy: 52.7447%, Training Loss: 0.9544%\n",
      "Epoch [7/300], Step [153/225], Training Accuracy: 52.7165%, Training Loss: 0.9542%\n",
      "Epoch [7/300], Step [154/225], Training Accuracy: 52.7394%, Training Loss: 0.9544%\n",
      "Epoch [7/300], Step [155/225], Training Accuracy: 52.7520%, Training Loss: 0.9543%\n",
      "Epoch [7/300], Step [156/225], Training Accuracy: 52.7444%, Training Loss: 0.9549%\n",
      "Epoch [7/300], Step [157/225], Training Accuracy: 52.7369%, Training Loss: 0.9546%\n",
      "Epoch [7/300], Step [158/225], Training Accuracy: 52.6701%, Training Loss: 0.9556%\n",
      "Epoch [7/300], Step [159/225], Training Accuracy: 52.6238%, Training Loss: 0.9564%\n",
      "Epoch [7/300], Step [160/225], Training Accuracy: 52.6172%, Training Loss: 0.9564%\n",
      "Epoch [7/300], Step [161/225], Training Accuracy: 52.6398%, Training Loss: 0.9565%\n",
      "Epoch [7/300], Step [162/225], Training Accuracy: 52.6910%, Training Loss: 0.9562%\n",
      "Epoch [7/300], Step [163/225], Training Accuracy: 52.7224%, Training Loss: 0.9560%\n",
      "Epoch [7/300], Step [164/225], Training Accuracy: 52.7534%, Training Loss: 0.9557%\n",
      "Epoch [7/300], Step [165/225], Training Accuracy: 52.7652%, Training Loss: 0.9557%\n",
      "Epoch [7/300], Step [166/225], Training Accuracy: 52.7861%, Training Loss: 0.9556%\n",
      "Epoch [7/300], Step [167/225], Training Accuracy: 52.8069%, Training Loss: 0.9555%\n",
      "Epoch [7/300], Step [168/225], Training Accuracy: 52.7995%, Training Loss: 0.9554%\n",
      "Epoch [7/300], Step [169/225], Training Accuracy: 52.7922%, Training Loss: 0.9553%\n",
      "Epoch [7/300], Step [170/225], Training Accuracy: 52.7757%, Training Loss: 0.9558%\n",
      "Epoch [7/300], Step [171/225], Training Accuracy: 52.8143%, Training Loss: 0.9556%\n",
      "Epoch [7/300], Step [172/225], Training Accuracy: 52.8252%, Training Loss: 0.9552%\n",
      "Epoch [7/300], Step [173/225], Training Accuracy: 52.8179%, Training Loss: 0.9552%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Step [174/225], Training Accuracy: 52.8017%, Training Loss: 0.9555%\n",
      "Epoch [7/300], Step [175/225], Training Accuracy: 52.8482%, Training Loss: 0.9556%\n",
      "Epoch [7/300], Step [176/225], Training Accuracy: 52.8587%, Training Loss: 0.9553%\n",
      "Epoch [7/300], Step [177/225], Training Accuracy: 52.8867%, Training Loss: 0.9552%\n",
      "Epoch [7/300], Step [178/225], Training Accuracy: 52.8617%, Training Loss: 0.9553%\n",
      "Epoch [7/300], Step [179/225], Training Accuracy: 52.8806%, Training Loss: 0.9551%\n",
      "Epoch [7/300], Step [180/225], Training Accuracy: 52.9601%, Training Loss: 0.9543%\n",
      "Epoch [7/300], Step [181/225], Training Accuracy: 52.9523%, Training Loss: 0.9548%\n",
      "Epoch [7/300], Step [182/225], Training Accuracy: 52.9619%, Training Loss: 0.9551%\n",
      "Epoch [7/300], Step [183/225], Training Accuracy: 52.9713%, Training Loss: 0.9543%\n",
      "Epoch [7/300], Step [184/225], Training Accuracy: 52.9891%, Training Loss: 0.9539%\n",
      "Epoch [7/300], Step [185/225], Training Accuracy: 52.9983%, Training Loss: 0.9535%\n",
      "Epoch [7/300], Step [186/225], Training Accuracy: 53.0326%, Training Loss: 0.9529%\n",
      "Epoch [7/300], Step [187/225], Training Accuracy: 52.9997%, Training Loss: 0.9529%\n",
      "Epoch [7/300], Step [188/225], Training Accuracy: 52.9837%, Training Loss: 0.9528%\n",
      "Epoch [7/300], Step [189/225], Training Accuracy: 53.0506%, Training Loss: 0.9518%\n",
      "Epoch [7/300], Step [190/225], Training Accuracy: 53.0099%, Training Loss: 0.9524%\n",
      "Epoch [7/300], Step [191/225], Training Accuracy: 53.0268%, Training Loss: 0.9523%\n",
      "Epoch [7/300], Step [192/225], Training Accuracy: 53.1250%, Training Loss: 0.9512%\n",
      "Epoch [7/300], Step [193/225], Training Accuracy: 53.0602%, Training Loss: 0.9514%\n",
      "Epoch [7/300], Step [194/225], Training Accuracy: 53.0445%, Training Loss: 0.9514%\n",
      "Epoch [7/300], Step [195/225], Training Accuracy: 53.0849%, Training Loss: 0.9509%\n",
      "Epoch [7/300], Step [196/225], Training Accuracy: 53.0453%, Training Loss: 0.9514%\n",
      "Epoch [7/300], Step [197/225], Training Accuracy: 53.0615%, Training Loss: 0.9516%\n",
      "Epoch [7/300], Step [198/225], Training Accuracy: 53.1092%, Training Loss: 0.9509%\n",
      "Epoch [7/300], Step [199/225], Training Accuracy: 53.1643%, Training Loss: 0.9501%\n",
      "Epoch [7/300], Step [200/225], Training Accuracy: 53.1953%, Training Loss: 0.9500%\n",
      "Epoch [7/300], Step [201/225], Training Accuracy: 53.2105%, Training Loss: 0.9501%\n",
      "Epoch [7/300], Step [202/225], Training Accuracy: 53.2024%, Training Loss: 0.9500%\n",
      "Epoch [7/300], Step [203/225], Training Accuracy: 53.2097%, Training Loss: 0.9499%\n",
      "Epoch [7/300], Step [204/225], Training Accuracy: 53.1863%, Training Loss: 0.9500%\n",
      "Epoch [7/300], Step [205/225], Training Accuracy: 53.2622%, Training Loss: 0.9492%\n",
      "Epoch [7/300], Step [206/225], Training Accuracy: 53.2388%, Training Loss: 0.9497%\n",
      "Epoch [7/300], Step [207/225], Training Accuracy: 53.2156%, Training Loss: 0.9497%\n",
      "Epoch [7/300], Step [208/225], Training Accuracy: 53.2302%, Training Loss: 0.9490%\n",
      "Epoch [7/300], Step [209/225], Training Accuracy: 53.2521%, Training Loss: 0.9494%\n",
      "Epoch [7/300], Step [210/225], Training Accuracy: 53.2143%, Training Loss: 0.9494%\n",
      "Epoch [7/300], Step [211/225], Training Accuracy: 53.2287%, Training Loss: 0.9490%\n",
      "Epoch [7/300], Step [212/225], Training Accuracy: 53.2650%, Training Loss: 0.9489%\n",
      "Epoch [7/300], Step [213/225], Training Accuracy: 53.2204%, Training Loss: 0.9503%\n",
      "Epoch [7/300], Step [214/225], Training Accuracy: 53.2710%, Training Loss: 0.9497%\n",
      "Epoch [7/300], Step [215/225], Training Accuracy: 53.2631%, Training Loss: 0.9493%\n",
      "Epoch [7/300], Step [216/225], Training Accuracy: 53.2624%, Training Loss: 0.9499%\n",
      "Epoch [7/300], Step [217/225], Training Accuracy: 53.2258%, Training Loss: 0.9504%\n",
      "Epoch [7/300], Step [218/225], Training Accuracy: 53.2038%, Training Loss: 0.9507%\n",
      "Epoch [7/300], Step [219/225], Training Accuracy: 53.2178%, Training Loss: 0.9504%\n",
      "Epoch [7/300], Step [220/225], Training Accuracy: 53.1960%, Training Loss: 0.9502%\n",
      "Epoch [7/300], Step [221/225], Training Accuracy: 53.1816%, Training Loss: 0.9508%\n",
      "Epoch [7/300], Step [222/225], Training Accuracy: 53.1883%, Training Loss: 0.9507%\n",
      "Epoch [7/300], Step [223/225], Training Accuracy: 53.2161%, Training Loss: 0.9507%\n",
      "Epoch [7/300], Step [224/225], Training Accuracy: 53.2296%, Training Loss: 0.9505%\n",
      "Epoch [7/300], Step [225/225], Training Accuracy: 53.2101%, Training Loss: 0.9506%\n",
      "Epoch [8/300], Step [1/225], Training Accuracy: 59.3750%, Training Loss: 0.9144%\n",
      "Epoch [8/300], Step [2/225], Training Accuracy: 53.1250%, Training Loss: 0.9711%\n",
      "Epoch [8/300], Step [3/225], Training Accuracy: 48.9583%, Training Loss: 1.0129%\n",
      "Epoch [8/300], Step [4/225], Training Accuracy: 47.2656%, Training Loss: 0.9898%\n",
      "Epoch [8/300], Step [5/225], Training Accuracy: 49.6875%, Training Loss: 0.9718%\n",
      "Epoch [8/300], Step [6/225], Training Accuracy: 50.0000%, Training Loss: 0.9680%\n",
      "Epoch [8/300], Step [7/225], Training Accuracy: 51.3393%, Training Loss: 0.9613%\n",
      "Epoch [8/300], Step [8/225], Training Accuracy: 51.7578%, Training Loss: 0.9620%\n",
      "Epoch [8/300], Step [9/225], Training Accuracy: 51.3889%, Training Loss: 0.9634%\n",
      "Epoch [8/300], Step [10/225], Training Accuracy: 50.6250%, Training Loss: 0.9791%\n",
      "Epoch [8/300], Step [11/225], Training Accuracy: 51.2784%, Training Loss: 0.9806%\n",
      "Epoch [8/300], Step [12/225], Training Accuracy: 51.6927%, Training Loss: 0.9806%\n",
      "Epoch [8/300], Step [13/225], Training Accuracy: 53.0048%, Training Loss: 0.9710%\n",
      "Epoch [8/300], Step [14/225], Training Accuracy: 53.2366%, Training Loss: 0.9642%\n",
      "Epoch [8/300], Step [15/225], Training Accuracy: 53.6458%, Training Loss: 0.9624%\n",
      "Epoch [8/300], Step [16/225], Training Accuracy: 54.1992%, Training Loss: 0.9573%\n",
      "Epoch [8/300], Step [17/225], Training Accuracy: 54.7794%, Training Loss: 0.9488%\n",
      "Epoch [8/300], Step [18/225], Training Accuracy: 54.6007%, Training Loss: 0.9465%\n",
      "Epoch [8/300], Step [19/225], Training Accuracy: 54.6053%, Training Loss: 0.9442%\n",
      "Epoch [8/300], Step [20/225], Training Accuracy: 54.6875%, Training Loss: 0.9429%\n",
      "Epoch [8/300], Step [21/225], Training Accuracy: 55.3571%, Training Loss: 0.9344%\n",
      "Epoch [8/300], Step [22/225], Training Accuracy: 54.9716%, Training Loss: 0.9388%\n",
      "Epoch [8/300], Step [23/225], Training Accuracy: 55.4348%, Training Loss: 0.9331%\n",
      "Epoch [8/300], Step [24/225], Training Accuracy: 55.0130%, Training Loss: 0.9373%\n",
      "Epoch [8/300], Step [25/225], Training Accuracy: 55.1250%, Training Loss: 0.9342%\n",
      "Epoch [8/300], Step [26/225], Training Accuracy: 54.8077%, Training Loss: 0.9350%\n",
      "Epoch [8/300], Step [27/225], Training Accuracy: 54.3403%, Training Loss: 0.9369%\n",
      "Epoch [8/300], Step [28/225], Training Accuracy: 54.5201%, Training Loss: 0.9345%\n",
      "Epoch [8/300], Step [29/225], Training Accuracy: 54.6875%, Training Loss: 0.9291%\n",
      "Epoch [8/300], Step [30/225], Training Accuracy: 54.4271%, Training Loss: 0.9291%\n",
      "Epoch [8/300], Step [31/225], Training Accuracy: 53.8810%, Training Loss: 0.9358%\n",
      "Epoch [8/300], Step [32/225], Training Accuracy: 53.9062%, Training Loss: 0.9333%\n",
      "Epoch [8/300], Step [33/225], Training Accuracy: 54.2140%, Training Loss: 0.9301%\n",
      "Epoch [8/300], Step [34/225], Training Accuracy: 53.7224%, Training Loss: 0.9331%\n",
      "Epoch [8/300], Step [35/225], Training Accuracy: 53.8393%, Training Loss: 0.9358%\n",
      "Epoch [8/300], Step [36/225], Training Accuracy: 53.7760%, Training Loss: 0.9357%\n",
      "Epoch [8/300], Step [37/225], Training Accuracy: 53.8429%, Training Loss: 0.9337%\n",
      "Epoch [8/300], Step [38/225], Training Accuracy: 53.7418%, Training Loss: 0.9345%\n",
      "Epoch [8/300], Step [39/225], Training Accuracy: 53.6859%, Training Loss: 0.9349%\n",
      "Epoch [8/300], Step [40/225], Training Accuracy: 53.7109%, Training Loss: 0.9372%\n",
      "Epoch [8/300], Step [41/225], Training Accuracy: 53.5061%, Training Loss: 0.9416%\n",
      "Epoch [8/300], Step [42/225], Training Accuracy: 53.3110%, Training Loss: 0.9407%\n",
      "Epoch [8/300], Step [43/225], Training Accuracy: 53.2340%, Training Loss: 0.9398%\n",
      "Epoch [8/300], Step [44/225], Training Accuracy: 53.3736%, Training Loss: 0.9374%\n",
      "Epoch [8/300], Step [45/225], Training Accuracy: 53.4375%, Training Loss: 0.9355%\n",
      "Epoch [8/300], Step [46/225], Training Accuracy: 53.8043%, Training Loss: 0.9317%\n",
      "Epoch [8/300], Step [47/225], Training Accuracy: 53.7566%, Training Loss: 0.9310%\n",
      "Epoch [8/300], Step [48/225], Training Accuracy: 53.8737%, Training Loss: 0.9298%\n",
      "Epoch [8/300], Step [49/225], Training Accuracy: 54.0497%, Training Loss: 0.9307%\n",
      "Epoch [8/300], Step [50/225], Training Accuracy: 54.0625%, Training Loss: 0.9309%\n",
      "Epoch [8/300], Step [51/225], Training Accuracy: 54.4118%, Training Loss: 0.9270%\n",
      "Epoch [8/300], Step [52/225], Training Accuracy: 54.4471%, Training Loss: 0.9264%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [53/225], Training Accuracy: 54.4517%, Training Loss: 0.9263%\n",
      "Epoch [8/300], Step [54/225], Training Accuracy: 54.4271%, Training Loss: 0.9274%\n",
      "Epoch [8/300], Step [55/225], Training Accuracy: 54.4886%, Training Loss: 0.9270%\n",
      "Epoch [8/300], Step [56/225], Training Accuracy: 54.5480%, Training Loss: 0.9257%\n",
      "Epoch [8/300], Step [57/225], Training Accuracy: 54.6875%, Training Loss: 0.9227%\n",
      "Epoch [8/300], Step [58/225], Training Accuracy: 54.7953%, Training Loss: 0.9229%\n",
      "Epoch [8/300], Step [59/225], Training Accuracy: 54.8464%, Training Loss: 0.9210%\n",
      "Epoch [8/300], Step [60/225], Training Accuracy: 54.8958%, Training Loss: 0.9198%\n",
      "Epoch [8/300], Step [61/225], Training Accuracy: 54.9180%, Training Loss: 0.9189%\n",
      "Epoch [8/300], Step [62/225], Training Accuracy: 54.9143%, Training Loss: 0.9181%\n",
      "Epoch [8/300], Step [63/225], Training Accuracy: 54.8363%, Training Loss: 0.9195%\n",
      "Epoch [8/300], Step [64/225], Training Accuracy: 54.8828%, Training Loss: 0.9207%\n",
      "Epoch [8/300], Step [65/225], Training Accuracy: 54.7356%, Training Loss: 0.9211%\n",
      "Epoch [8/300], Step [66/225], Training Accuracy: 54.7112%, Training Loss: 0.9205%\n",
      "Epoch [8/300], Step [67/225], Training Accuracy: 54.7341%, Training Loss: 0.9206%\n",
      "Epoch [8/300], Step [68/225], Training Accuracy: 54.7564%, Training Loss: 0.9210%\n",
      "Epoch [8/300], Step [69/225], Training Accuracy: 54.6422%, Training Loss: 0.9212%\n",
      "Epoch [8/300], Step [70/225], Training Accuracy: 54.5312%, Training Loss: 0.9232%\n",
      "Epoch [8/300], Step [71/225], Training Accuracy: 54.4894%, Training Loss: 0.9228%\n",
      "Epoch [8/300], Step [72/225], Training Accuracy: 54.4488%, Training Loss: 0.9240%\n",
      "Epoch [8/300], Step [73/225], Training Accuracy: 54.2380%, Training Loss: 0.9248%\n",
      "Epoch [8/300], Step [74/225], Training Accuracy: 54.3285%, Training Loss: 0.9228%\n",
      "Epoch [8/300], Step [75/225], Training Accuracy: 54.4792%, Training Loss: 0.9218%\n",
      "Epoch [8/300], Step [76/225], Training Accuracy: 54.3997%, Training Loss: 0.9227%\n",
      "Epoch [8/300], Step [77/225], Training Accuracy: 54.4440%, Training Loss: 0.9225%\n",
      "Epoch [8/300], Step [78/225], Training Accuracy: 54.4671%, Training Loss: 0.9226%\n",
      "Epoch [8/300], Step [79/225], Training Accuracy: 54.2919%, Training Loss: 0.9245%\n",
      "Epoch [8/300], Step [80/225], Training Accuracy: 54.2578%, Training Loss: 0.9251%\n",
      "Epoch [8/300], Step [81/225], Training Accuracy: 54.3789%, Training Loss: 0.9239%\n",
      "Epoch [8/300], Step [82/225], Training Accuracy: 54.4017%, Training Loss: 0.9234%\n",
      "Epoch [8/300], Step [83/225], Training Accuracy: 54.4428%, Training Loss: 0.9227%\n",
      "Epoch [8/300], Step [84/225], Training Accuracy: 54.4829%, Training Loss: 0.9223%\n",
      "Epoch [8/300], Step [85/225], Training Accuracy: 54.5221%, Training Loss: 0.9218%\n",
      "Epoch [8/300], Step [86/225], Training Accuracy: 54.4695%, Training Loss: 0.9217%\n",
      "Epoch [8/300], Step [87/225], Training Accuracy: 54.4361%, Training Loss: 0.9218%\n",
      "Epoch [8/300], Step [88/225], Training Accuracy: 54.3857%, Training Loss: 0.9235%\n",
      "Epoch [8/300], Step [89/225], Training Accuracy: 54.3715%, Training Loss: 0.9247%\n",
      "Epoch [8/300], Step [90/225], Training Accuracy: 54.4271%, Training Loss: 0.9258%\n",
      "Epoch [8/300], Step [91/225], Training Accuracy: 54.3098%, Training Loss: 0.9259%\n",
      "Epoch [8/300], Step [92/225], Training Accuracy: 54.2799%, Training Loss: 0.9262%\n",
      "Epoch [8/300], Step [93/225], Training Accuracy: 54.3515%, Training Loss: 0.9259%\n",
      "Epoch [8/300], Step [94/225], Training Accuracy: 54.4548%, Training Loss: 0.9249%\n",
      "Epoch [8/300], Step [95/225], Training Accuracy: 54.4079%, Training Loss: 0.9252%\n",
      "Epoch [8/300], Step [96/225], Training Accuracy: 54.4759%, Training Loss: 0.9243%\n",
      "Epoch [8/300], Step [97/225], Training Accuracy: 54.4942%, Training Loss: 0.9234%\n",
      "Epoch [8/300], Step [98/225], Training Accuracy: 54.5440%, Training Loss: 0.9230%\n",
      "Epoch [8/300], Step [99/225], Training Accuracy: 54.6086%, Training Loss: 0.9232%\n",
      "Epoch [8/300], Step [100/225], Training Accuracy: 54.5156%, Training Loss: 0.9248%\n",
      "Epoch [8/300], Step [101/225], Training Accuracy: 54.4709%, Training Loss: 0.9244%\n",
      "Epoch [8/300], Step [102/225], Training Accuracy: 54.3811%, Training Loss: 0.9256%\n",
      "Epoch [8/300], Step [103/225], Training Accuracy: 54.3234%, Training Loss: 0.9254%\n",
      "Epoch [8/300], Step [104/225], Training Accuracy: 54.2067%, Training Loss: 0.9258%\n",
      "Epoch [8/300], Step [105/225], Training Accuracy: 54.2560%, Training Loss: 0.9252%\n",
      "Epoch [8/300], Step [106/225], Training Accuracy: 54.2600%, Training Loss: 0.9249%\n",
      "Epoch [8/300], Step [107/225], Training Accuracy: 54.1910%, Training Loss: 0.9257%\n",
      "Epoch [8/300], Step [108/225], Training Accuracy: 54.2679%, Training Loss: 0.9256%\n",
      "Epoch [8/300], Step [109/225], Training Accuracy: 54.2718%, Training Loss: 0.9254%\n",
      "Epoch [8/300], Step [110/225], Training Accuracy: 54.2472%, Training Loss: 0.9252%\n",
      "Epoch [8/300], Step [111/225], Training Accuracy: 54.2370%, Training Loss: 0.9256%\n",
      "Epoch [8/300], Step [112/225], Training Accuracy: 54.3108%, Training Loss: 0.9251%\n",
      "Epoch [8/300], Step [113/225], Training Accuracy: 54.3280%, Training Loss: 0.9252%\n",
      "Epoch [8/300], Step [114/225], Training Accuracy: 54.2900%, Training Loss: 0.9253%\n",
      "Epoch [8/300], Step [115/225], Training Accuracy: 54.3478%, Training Loss: 0.9250%\n",
      "Epoch [8/300], Step [116/225], Training Accuracy: 54.3642%, Training Loss: 0.9251%\n",
      "Epoch [8/300], Step [117/225], Training Accuracy: 54.3002%, Training Loss: 0.9262%\n",
      "Epoch [8/300], Step [118/225], Training Accuracy: 54.3167%, Training Loss: 0.9259%\n",
      "Epoch [8/300], Step [119/225], Training Accuracy: 54.2805%, Training Loss: 0.9262%\n",
      "Epoch [8/300], Step [120/225], Training Accuracy: 54.2969%, Training Loss: 0.9266%\n",
      "Epoch [8/300], Step [121/225], Training Accuracy: 54.2614%, Training Loss: 0.9274%\n",
      "Epoch [8/300], Step [122/225], Training Accuracy: 54.3033%, Training Loss: 0.9266%\n",
      "Epoch [8/300], Step [123/225], Training Accuracy: 54.2810%, Training Loss: 0.9262%\n",
      "Epoch [8/300], Step [124/225], Training Accuracy: 54.2591%, Training Loss: 0.9266%\n",
      "Epoch [8/300], Step [125/225], Training Accuracy: 54.2250%, Training Loss: 0.9280%\n",
      "Epoch [8/300], Step [126/225], Training Accuracy: 54.2411%, Training Loss: 0.9284%\n",
      "Epoch [8/300], Step [127/225], Training Accuracy: 54.1954%, Training Loss: 0.9289%\n",
      "Epoch [8/300], Step [128/225], Training Accuracy: 54.0649%, Training Loss: 0.9307%\n",
      "Epoch [8/300], Step [129/225], Training Accuracy: 54.1303%, Training Loss: 0.9307%\n",
      "Epoch [8/300], Step [130/225], Training Accuracy: 54.0745%, Training Loss: 0.9316%\n",
      "Epoch [8/300], Step [131/225], Training Accuracy: 53.9957%, Training Loss: 0.9320%\n",
      "Epoch [8/300], Step [132/225], Training Accuracy: 53.9654%, Training Loss: 0.9325%\n",
      "Epoch [8/300], Step [133/225], Training Accuracy: 53.9591%, Training Loss: 0.9329%\n",
      "Epoch [8/300], Step [134/225], Training Accuracy: 53.8596%, Training Loss: 0.9335%\n",
      "Epoch [8/300], Step [135/225], Training Accuracy: 53.8889%, Training Loss: 0.9331%\n",
      "Epoch [8/300], Step [136/225], Training Accuracy: 53.9177%, Training Loss: 0.9328%\n",
      "Epoch [8/300], Step [137/225], Training Accuracy: 53.9120%, Training Loss: 0.9334%\n",
      "Epoch [8/300], Step [138/225], Training Accuracy: 53.9968%, Training Loss: 0.9326%\n",
      "Epoch [8/300], Step [139/225], Training Accuracy: 53.9568%, Training Loss: 0.9329%\n",
      "Epoch [8/300], Step [140/225], Training Accuracy: 54.0067%, Training Loss: 0.9325%\n",
      "Epoch [8/300], Step [141/225], Training Accuracy: 54.0559%, Training Loss: 0.9320%\n",
      "Epoch [8/300], Step [142/225], Training Accuracy: 54.0713%, Training Loss: 0.9316%\n",
      "Epoch [8/300], Step [143/225], Training Accuracy: 54.0756%, Training Loss: 0.9317%\n",
      "Epoch [8/300], Step [144/225], Training Accuracy: 54.1124%, Training Loss: 0.9312%\n",
      "Epoch [8/300], Step [145/225], Training Accuracy: 54.1272%, Training Loss: 0.9310%\n",
      "Epoch [8/300], Step [146/225], Training Accuracy: 54.0668%, Training Loss: 0.9319%\n",
      "Epoch [8/300], Step [147/225], Training Accuracy: 54.0604%, Training Loss: 0.9316%\n",
      "Epoch [8/300], Step [148/225], Training Accuracy: 54.1068%, Training Loss: 0.9312%\n",
      "Epoch [8/300], Step [149/225], Training Accuracy: 54.0898%, Training Loss: 0.9315%\n",
      "Epoch [8/300], Step [150/225], Training Accuracy: 54.2083%, Training Loss: 0.9302%\n",
      "Epoch [8/300], Step [151/225], Training Accuracy: 54.2736%, Training Loss: 0.9294%\n",
      "Epoch [8/300], Step [152/225], Training Accuracy: 54.3174%, Training Loss: 0.9295%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [153/225], Training Accuracy: 54.3505%, Training Loss: 0.9292%\n",
      "Epoch [8/300], Step [154/225], Training Accuracy: 54.3527%, Training Loss: 0.9288%\n",
      "Epoch [8/300], Step [155/225], Training Accuracy: 54.3750%, Training Loss: 0.9287%\n",
      "Epoch [8/300], Step [156/225], Training Accuracy: 54.4371%, Training Loss: 0.9282%\n",
      "Epoch [8/300], Step [157/225], Training Accuracy: 54.4188%, Training Loss: 0.9282%\n",
      "Epoch [8/300], Step [158/225], Training Accuracy: 54.3809%, Training Loss: 0.9294%\n",
      "Epoch [8/300], Step [159/225], Training Accuracy: 54.3239%, Training Loss: 0.9300%\n",
      "Epoch [8/300], Step [160/225], Training Accuracy: 54.3164%, Training Loss: 0.9299%\n",
      "Epoch [8/300], Step [161/225], Training Accuracy: 54.3769%, Training Loss: 0.9296%\n",
      "Epoch [8/300], Step [162/225], Training Accuracy: 54.4560%, Training Loss: 0.9290%\n",
      "Epoch [8/300], Step [163/225], Training Accuracy: 54.4383%, Training Loss: 0.9290%\n",
      "Epoch [8/300], Step [164/225], Training Accuracy: 54.4588%, Training Loss: 0.9283%\n",
      "Epoch [8/300], Step [165/225], Training Accuracy: 54.4697%, Training Loss: 0.9280%\n",
      "Epoch [8/300], Step [166/225], Training Accuracy: 54.4898%, Training Loss: 0.9275%\n",
      "Epoch [8/300], Step [167/225], Training Accuracy: 54.5191%, Training Loss: 0.9268%\n",
      "Epoch [8/300], Step [168/225], Training Accuracy: 54.4829%, Training Loss: 0.9270%\n",
      "Epoch [8/300], Step [169/225], Training Accuracy: 54.4841%, Training Loss: 0.9267%\n",
      "Epoch [8/300], Step [170/225], Training Accuracy: 54.4485%, Training Loss: 0.9269%\n",
      "Epoch [8/300], Step [171/225], Training Accuracy: 54.5139%, Training Loss: 0.9265%\n",
      "Epoch [8/300], Step [172/225], Training Accuracy: 54.5149%, Training Loss: 0.9265%\n",
      "Epoch [8/300], Step [173/225], Training Accuracy: 54.4888%, Training Loss: 0.9264%\n",
      "Epoch [8/300], Step [174/225], Training Accuracy: 54.4989%, Training Loss: 0.9265%\n",
      "Epoch [8/300], Step [175/225], Training Accuracy: 54.5446%, Training Loss: 0.9262%\n",
      "Epoch [8/300], Step [176/225], Training Accuracy: 54.5099%, Training Loss: 0.9262%\n",
      "Epoch [8/300], Step [177/225], Training Accuracy: 54.4933%, Training Loss: 0.9259%\n",
      "Epoch [8/300], Step [178/225], Training Accuracy: 54.4505%, Training Loss: 0.9258%\n",
      "Epoch [8/300], Step [179/225], Training Accuracy: 54.4693%, Training Loss: 0.9250%\n",
      "Epoch [8/300], Step [180/225], Training Accuracy: 54.5312%, Training Loss: 0.9244%\n",
      "Epoch [8/300], Step [181/225], Training Accuracy: 54.4976%, Training Loss: 0.9249%\n",
      "Epoch [8/300], Step [182/225], Training Accuracy: 54.5158%, Training Loss: 0.9250%\n",
      "Epoch [8/300], Step [183/225], Training Accuracy: 54.5338%, Training Loss: 0.9244%\n",
      "Epoch [8/300], Step [184/225], Training Accuracy: 54.5007%, Training Loss: 0.9243%\n",
      "Epoch [8/300], Step [185/225], Training Accuracy: 54.5608%, Training Loss: 0.9239%\n",
      "Epoch [8/300], Step [186/225], Training Accuracy: 54.5867%, Training Loss: 0.9237%\n",
      "Epoch [8/300], Step [187/225], Training Accuracy: 54.5956%, Training Loss: 0.9232%\n",
      "Epoch [8/300], Step [188/225], Training Accuracy: 54.5795%, Training Loss: 0.9231%\n",
      "Epoch [8/300], Step [189/225], Training Accuracy: 54.6296%, Training Loss: 0.9221%\n",
      "Epoch [8/300], Step [190/225], Training Accuracy: 54.6628%, Training Loss: 0.9225%\n",
      "Epoch [8/300], Step [191/225], Training Accuracy: 54.6957%, Training Loss: 0.9225%\n",
      "Epoch [8/300], Step [192/225], Training Accuracy: 54.7852%, Training Loss: 0.9217%\n",
      "Epoch [8/300], Step [193/225], Training Accuracy: 54.7361%, Training Loss: 0.9223%\n",
      "Epoch [8/300], Step [194/225], Training Accuracy: 54.7278%, Training Loss: 0.9227%\n",
      "Epoch [8/300], Step [195/225], Training Accuracy: 54.7676%, Training Loss: 0.9221%\n",
      "Epoch [8/300], Step [196/225], Training Accuracy: 54.7114%, Training Loss: 0.9228%\n",
      "Epoch [8/300], Step [197/225], Training Accuracy: 54.7430%, Training Loss: 0.9228%\n",
      "Epoch [8/300], Step [198/225], Training Accuracy: 54.7585%, Training Loss: 0.9222%\n",
      "Epoch [8/300], Step [199/225], Training Accuracy: 54.8367%, Training Loss: 0.9215%\n",
      "Epoch [8/300], Step [200/225], Training Accuracy: 54.8281%, Training Loss: 0.9219%\n",
      "Epoch [8/300], Step [201/225], Training Accuracy: 54.8119%, Training Loss: 0.9221%\n",
      "Epoch [8/300], Step [202/225], Training Accuracy: 54.8035%, Training Loss: 0.9222%\n",
      "Epoch [8/300], Step [203/225], Training Accuracy: 54.7799%, Training Loss: 0.9222%\n",
      "Epoch [8/300], Step [204/225], Training Accuracy: 54.7947%, Training Loss: 0.9222%\n",
      "Epoch [8/300], Step [205/225], Training Accuracy: 54.8857%, Training Loss: 0.9213%\n",
      "Epoch [8/300], Step [206/225], Training Accuracy: 54.8544%, Training Loss: 0.9216%\n",
      "Epoch [8/300], Step [207/225], Training Accuracy: 54.8385%, Training Loss: 0.9217%\n",
      "Epoch [8/300], Step [208/225], Training Accuracy: 54.8678%, Training Loss: 0.9210%\n",
      "Epoch [8/300], Step [209/225], Training Accuracy: 54.8894%, Training Loss: 0.9211%\n",
      "Epoch [8/300], Step [210/225], Training Accuracy: 54.8661%, Training Loss: 0.9211%\n",
      "Epoch [8/300], Step [211/225], Training Accuracy: 54.9171%, Training Loss: 0.9207%\n",
      "Epoch [8/300], Step [212/225], Training Accuracy: 54.9307%, Training Loss: 0.9207%\n",
      "Epoch [8/300], Step [213/225], Training Accuracy: 54.9369%, Training Loss: 0.9214%\n",
      "Epoch [8/300], Step [214/225], Training Accuracy: 54.9723%, Training Loss: 0.9211%\n",
      "Epoch [8/300], Step [215/225], Training Accuracy: 54.9346%, Training Loss: 0.9214%\n",
      "Epoch [8/300], Step [216/225], Training Accuracy: 54.9479%, Training Loss: 0.9214%\n",
      "Epoch [8/300], Step [217/225], Training Accuracy: 54.9323%, Training Loss: 0.9219%\n",
      "Epoch [8/300], Step [218/225], Training Accuracy: 54.9025%, Training Loss: 0.9225%\n",
      "Epoch [8/300], Step [219/225], Training Accuracy: 54.9087%, Training Loss: 0.9223%\n",
      "Epoch [8/300], Step [220/225], Training Accuracy: 54.8935%, Training Loss: 0.9221%\n",
      "Epoch [8/300], Step [221/225], Training Accuracy: 54.8784%, Training Loss: 0.9227%\n",
      "Epoch [8/300], Step [222/225], Training Accuracy: 54.8705%, Training Loss: 0.9226%\n",
      "Epoch [8/300], Step [223/225], Training Accuracy: 54.8346%, Training Loss: 0.9228%\n",
      "Epoch [8/300], Step [224/225], Training Accuracy: 54.8200%, Training Loss: 0.9226%\n",
      "Epoch [8/300], Step [225/225], Training Accuracy: 54.8082%, Training Loss: 0.9227%\n",
      "Epoch [9/300], Step [1/225], Training Accuracy: 56.2500%, Training Loss: 0.8937%\n",
      "Epoch [9/300], Step [2/225], Training Accuracy: 53.1250%, Training Loss: 0.9513%\n",
      "Epoch [9/300], Step [3/225], Training Accuracy: 51.5625%, Training Loss: 0.9868%\n",
      "Epoch [9/300], Step [4/225], Training Accuracy: 53.5156%, Training Loss: 0.9575%\n",
      "Epoch [9/300], Step [5/225], Training Accuracy: 54.3750%, Training Loss: 0.9366%\n",
      "Epoch [9/300], Step [6/225], Training Accuracy: 54.4271%, Training Loss: 0.9431%\n",
      "Epoch [9/300], Step [7/225], Training Accuracy: 55.8036%, Training Loss: 0.9282%\n",
      "Epoch [9/300], Step [8/225], Training Accuracy: 55.8594%, Training Loss: 0.9310%\n",
      "Epoch [9/300], Step [9/225], Training Accuracy: 55.7292%, Training Loss: 0.9234%\n",
      "Epoch [9/300], Step [10/225], Training Accuracy: 55.3125%, Training Loss: 0.9378%\n",
      "Epoch [9/300], Step [11/225], Training Accuracy: 55.5398%, Training Loss: 0.9425%\n",
      "Epoch [9/300], Step [12/225], Training Accuracy: 56.1198%, Training Loss: 0.9376%\n",
      "Epoch [9/300], Step [13/225], Training Accuracy: 56.6106%, Training Loss: 0.9275%\n",
      "Epoch [9/300], Step [14/225], Training Accuracy: 56.6964%, Training Loss: 0.9233%\n",
      "Epoch [9/300], Step [15/225], Training Accuracy: 56.5625%, Training Loss: 0.9208%\n",
      "Epoch [9/300], Step [16/225], Training Accuracy: 56.6406%, Training Loss: 0.9177%\n",
      "Epoch [9/300], Step [17/225], Training Accuracy: 56.6176%, Training Loss: 0.9115%\n",
      "Epoch [9/300], Step [18/225], Training Accuracy: 56.4236%, Training Loss: 0.9081%\n",
      "Epoch [9/300], Step [19/225], Training Accuracy: 56.4967%, Training Loss: 0.9097%\n",
      "Epoch [9/300], Step [20/225], Training Accuracy: 56.4844%, Training Loss: 0.9093%\n",
      "Epoch [9/300], Step [21/225], Training Accuracy: 56.9940%, Training Loss: 0.9024%\n",
      "Epoch [9/300], Step [22/225], Training Accuracy: 56.7472%, Training Loss: 0.9065%\n",
      "Epoch [9/300], Step [23/225], Training Accuracy: 56.9973%, Training Loss: 0.9030%\n",
      "Epoch [9/300], Step [24/225], Training Accuracy: 56.7057%, Training Loss: 0.9059%\n",
      "Epoch [9/300], Step [25/225], Training Accuracy: 56.4375%, Training Loss: 0.9073%\n",
      "Epoch [9/300], Step [26/225], Training Accuracy: 56.1298%, Training Loss: 0.9103%\n",
      "Epoch [9/300], Step [27/225], Training Accuracy: 55.7870%, Training Loss: 0.9133%\n",
      "Epoch [9/300], Step [28/225], Training Accuracy: 55.7478%, Training Loss: 0.9110%\n",
      "Epoch [9/300], Step [29/225], Training Accuracy: 55.8190%, Training Loss: 0.9094%\n",
      "Epoch [9/300], Step [30/225], Training Accuracy: 55.7812%, Training Loss: 0.9094%\n",
      "Epoch [9/300], Step [31/225], Training Accuracy: 55.3427%, Training Loss: 0.9151%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [32/225], Training Accuracy: 55.1270%, Training Loss: 0.9140%\n",
      "Epoch [9/300], Step [33/225], Training Accuracy: 55.1136%, Training Loss: 0.9115%\n",
      "Epoch [9/300], Step [34/225], Training Accuracy: 54.8254%, Training Loss: 0.9137%\n",
      "Epoch [9/300], Step [35/225], Training Accuracy: 54.8214%, Training Loss: 0.9162%\n",
      "Epoch [9/300], Step [36/225], Training Accuracy: 54.7309%, Training Loss: 0.9168%\n",
      "Epoch [9/300], Step [37/225], Training Accuracy: 54.9831%, Training Loss: 0.9152%\n",
      "Epoch [9/300], Step [38/225], Training Accuracy: 54.8520%, Training Loss: 0.9144%\n",
      "Epoch [9/300], Step [39/225], Training Accuracy: 54.8077%, Training Loss: 0.9152%\n",
      "Epoch [9/300], Step [40/225], Training Accuracy: 54.7266%, Training Loss: 0.9174%\n",
      "Epoch [9/300], Step [41/225], Training Accuracy: 54.4588%, Training Loss: 0.9203%\n",
      "Epoch [9/300], Step [42/225], Training Accuracy: 54.5015%, Training Loss: 0.9200%\n",
      "Epoch [9/300], Step [43/225], Training Accuracy: 54.4331%, Training Loss: 0.9201%\n",
      "Epoch [9/300], Step [44/225], Training Accuracy: 54.6520%, Training Loss: 0.9187%\n",
      "Epoch [9/300], Step [45/225], Training Accuracy: 54.7569%, Training Loss: 0.9166%\n",
      "Epoch [9/300], Step [46/225], Training Accuracy: 55.0272%, Training Loss: 0.9150%\n",
      "Epoch [9/300], Step [47/225], Training Accuracy: 54.9867%, Training Loss: 0.9150%\n",
      "Epoch [9/300], Step [48/225], Training Accuracy: 54.9805%, Training Loss: 0.9131%\n",
      "Epoch [9/300], Step [49/225], Training Accuracy: 55.0702%, Training Loss: 0.9138%\n",
      "Epoch [9/300], Step [50/225], Training Accuracy: 55.1250%, Training Loss: 0.9128%\n",
      "Epoch [9/300], Step [51/225], Training Accuracy: 55.2390%, Training Loss: 0.9116%\n",
      "Epoch [9/300], Step [52/225], Training Accuracy: 55.4087%, Training Loss: 0.9090%\n",
      "Epoch [9/300], Step [53/225], Training Accuracy: 55.4835%, Training Loss: 0.9074%\n",
      "Epoch [9/300], Step [54/225], Training Accuracy: 55.4398%, Training Loss: 0.9085%\n",
      "Epoch [9/300], Step [55/225], Training Accuracy: 55.4261%, Training Loss: 0.9096%\n",
      "Epoch [9/300], Step [56/225], Training Accuracy: 55.3850%, Training Loss: 0.9091%\n",
      "Epoch [9/300], Step [57/225], Training Accuracy: 55.2906%, Training Loss: 0.9075%\n",
      "Epoch [9/300], Step [58/225], Training Accuracy: 55.3071%, Training Loss: 0.9080%\n",
      "Epoch [9/300], Step [59/225], Training Accuracy: 55.4290%, Training Loss: 0.9061%\n",
      "Epoch [9/300], Step [60/225], Training Accuracy: 55.5208%, Training Loss: 0.9044%\n",
      "Epoch [9/300], Step [61/225], Training Accuracy: 55.4816%, Training Loss: 0.9034%\n",
      "Epoch [9/300], Step [62/225], Training Accuracy: 55.5948%, Training Loss: 0.9026%\n",
      "Epoch [9/300], Step [63/225], Training Accuracy: 55.5308%, Training Loss: 0.9036%\n",
      "Epoch [9/300], Step [64/225], Training Accuracy: 55.4932%, Training Loss: 0.9039%\n",
      "Epoch [9/300], Step [65/225], Training Accuracy: 55.3846%, Training Loss: 0.9039%\n",
      "Epoch [9/300], Step [66/225], Training Accuracy: 55.4451%, Training Loss: 0.9039%\n",
      "Epoch [9/300], Step [67/225], Training Accuracy: 55.4571%, Training Loss: 0.9039%\n",
      "Epoch [9/300], Step [68/225], Training Accuracy: 55.4228%, Training Loss: 0.9048%\n",
      "Epoch [9/300], Step [69/225], Training Accuracy: 55.2763%, Training Loss: 0.9046%\n",
      "Epoch [9/300], Step [70/225], Training Accuracy: 55.2009%, Training Loss: 0.9063%\n",
      "Epoch [9/300], Step [71/225], Training Accuracy: 55.2157%, Training Loss: 0.9067%\n",
      "Epoch [9/300], Step [72/225], Training Accuracy: 55.2300%, Training Loss: 0.9074%\n",
      "Epoch [9/300], Step [73/225], Training Accuracy: 55.1370%, Training Loss: 0.9092%\n",
      "Epoch [9/300], Step [74/225], Training Accuracy: 55.1098%, Training Loss: 0.9082%\n",
      "Epoch [9/300], Step [75/225], Training Accuracy: 55.1458%, Training Loss: 0.9070%\n",
      "Epoch [9/300], Step [76/225], Training Accuracy: 55.0370%, Training Loss: 0.9084%\n",
      "Epoch [9/300], Step [77/225], Training Accuracy: 55.0731%, Training Loss: 0.9084%\n",
      "Epoch [9/300], Step [78/225], Training Accuracy: 55.0280%, Training Loss: 0.9082%\n",
      "Epoch [9/300], Step [79/225], Training Accuracy: 54.9842%, Training Loss: 0.9105%\n",
      "Epoch [9/300], Step [80/225], Training Accuracy: 55.0977%, Training Loss: 0.9102%\n",
      "Epoch [9/300], Step [81/225], Training Accuracy: 55.0926%, Training Loss: 0.9097%\n",
      "Epoch [9/300], Step [82/225], Training Accuracy: 55.1448%, Training Loss: 0.9092%\n",
      "Epoch [9/300], Step [83/225], Training Accuracy: 55.1958%, Training Loss: 0.9082%\n",
      "Epoch [9/300], Step [84/225], Training Accuracy: 55.2827%, Training Loss: 0.9075%\n",
      "Epoch [9/300], Step [85/225], Training Accuracy: 55.4412%, Training Loss: 0.9065%\n",
      "Epoch [9/300], Step [86/225], Training Accuracy: 55.3961%, Training Loss: 0.9073%\n",
      "Epoch [9/300], Step [87/225], Training Accuracy: 55.3879%, Training Loss: 0.9078%\n",
      "Epoch [9/300], Step [88/225], Training Accuracy: 55.3977%, Training Loss: 0.9077%\n",
      "Epoch [9/300], Step [89/225], Training Accuracy: 55.4073%, Training Loss: 0.9088%\n",
      "Epoch [9/300], Step [90/225], Training Accuracy: 55.4167%, Training Loss: 0.9099%\n",
      "Epoch [9/300], Step [91/225], Training Accuracy: 55.3915%, Training Loss: 0.9096%\n",
      "Epoch [9/300], Step [92/225], Training Accuracy: 55.3329%, Training Loss: 0.9101%\n",
      "Epoch [9/300], Step [93/225], Training Accuracy: 55.4099%, Training Loss: 0.9096%\n",
      "Epoch [9/300], Step [94/225], Training Accuracy: 55.5020%, Training Loss: 0.9080%\n",
      "Epoch [9/300], Step [95/225], Training Accuracy: 55.4441%, Training Loss: 0.9088%\n",
      "Epoch [9/300], Step [96/225], Training Accuracy: 55.5176%, Training Loss: 0.9076%\n",
      "Epoch [9/300], Step [97/225], Training Accuracy: 55.5090%, Training Loss: 0.9065%\n",
      "Epoch [9/300], Step [98/225], Training Accuracy: 55.5325%, Training Loss: 0.9062%\n",
      "Epoch [9/300], Step [99/225], Training Accuracy: 55.4924%, Training Loss: 0.9065%\n",
      "Epoch [9/300], Step [100/225], Training Accuracy: 55.4062%, Training Loss: 0.9073%\n",
      "Epoch [9/300], Step [101/225], Training Accuracy: 55.3837%, Training Loss: 0.9079%\n",
      "Epoch [9/300], Step [102/225], Training Accuracy: 55.3156%, Training Loss: 0.9088%\n",
      "Epoch [9/300], Step [103/225], Training Accuracy: 55.2791%, Training Loss: 0.9082%\n",
      "Epoch [9/300], Step [104/225], Training Accuracy: 55.1683%, Training Loss: 0.9087%\n",
      "Epoch [9/300], Step [105/225], Training Accuracy: 55.1488%, Training Loss: 0.9085%\n",
      "Epoch [9/300], Step [106/225], Training Accuracy: 55.1739%, Training Loss: 0.9078%\n",
      "Epoch [9/300], Step [107/225], Training Accuracy: 55.1256%, Training Loss: 0.9085%\n",
      "Epoch [9/300], Step [108/225], Training Accuracy: 55.0781%, Training Loss: 0.9090%\n",
      "Epoch [9/300], Step [109/225], Training Accuracy: 55.0602%, Training Loss: 0.9091%\n",
      "Epoch [9/300], Step [110/225], Training Accuracy: 54.9858%, Training Loss: 0.9089%\n",
      "Epoch [9/300], Step [111/225], Training Accuracy: 54.9690%, Training Loss: 0.9087%\n",
      "Epoch [9/300], Step [112/225], Training Accuracy: 54.9526%, Training Loss: 0.9087%\n",
      "Epoch [9/300], Step [113/225], Training Accuracy: 54.9640%, Training Loss: 0.9095%\n",
      "Epoch [9/300], Step [114/225], Training Accuracy: 54.9205%, Training Loss: 0.9096%\n",
      "Epoch [9/300], Step [115/225], Training Accuracy: 54.9728%, Training Loss: 0.9091%\n",
      "Epoch [9/300], Step [116/225], Training Accuracy: 54.9569%, Training Loss: 0.9094%\n",
      "Epoch [9/300], Step [117/225], Training Accuracy: 54.9012%, Training Loss: 0.9099%\n",
      "Epoch [9/300], Step [118/225], Training Accuracy: 54.9523%, Training Loss: 0.9095%\n",
      "Epoch [9/300], Step [119/225], Training Accuracy: 54.9107%, Training Loss: 0.9098%\n",
      "Epoch [9/300], Step [120/225], Training Accuracy: 54.9219%, Training Loss: 0.9099%\n",
      "Epoch [9/300], Step [121/225], Training Accuracy: 54.7650%, Training Loss: 0.9106%\n",
      "Epoch [9/300], Step [122/225], Training Accuracy: 54.7772%, Training Loss: 0.9104%\n",
      "Epoch [9/300], Step [123/225], Training Accuracy: 54.7637%, Training Loss: 0.9100%\n",
      "Epoch [9/300], Step [124/225], Training Accuracy: 54.7505%, Training Loss: 0.9093%\n",
      "Epoch [9/300], Step [125/225], Training Accuracy: 54.7875%, Training Loss: 0.9101%\n",
      "Epoch [9/300], Step [126/225], Training Accuracy: 54.7743%, Training Loss: 0.9107%\n",
      "Epoch [9/300], Step [127/225], Training Accuracy: 54.6629%, Training Loss: 0.9116%\n",
      "Epoch [9/300], Step [128/225], Training Accuracy: 54.5532%, Training Loss: 0.9132%\n",
      "Epoch [9/300], Step [129/225], Training Accuracy: 54.5179%, Training Loss: 0.9139%\n",
      "Epoch [9/300], Step [130/225], Training Accuracy: 54.5312%, Training Loss: 0.9147%\n",
      "Epoch [9/300], Step [131/225], Training Accuracy: 54.4847%, Training Loss: 0.9151%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [132/225], Training Accuracy: 54.5099%, Training Loss: 0.9155%\n",
      "Epoch [9/300], Step [133/225], Training Accuracy: 54.4878%, Training Loss: 0.9161%\n",
      "Epoch [9/300], Step [134/225], Training Accuracy: 54.4776%, Training Loss: 0.9168%\n",
      "Epoch [9/300], Step [135/225], Training Accuracy: 54.5255%, Training Loss: 0.9163%\n",
      "Epoch [9/300], Step [136/225], Training Accuracy: 54.5267%, Training Loss: 0.9160%\n",
      "Epoch [9/300], Step [137/225], Training Accuracy: 54.5278%, Training Loss: 0.9161%\n",
      "Epoch [9/300], Step [138/225], Training Accuracy: 54.5516%, Training Loss: 0.9153%\n",
      "Epoch [9/300], Step [139/225], Training Accuracy: 54.5301%, Training Loss: 0.9155%\n",
      "Epoch [9/300], Step [140/225], Training Accuracy: 54.5424%, Training Loss: 0.9150%\n",
      "Epoch [9/300], Step [141/225], Training Accuracy: 54.5878%, Training Loss: 0.9145%\n",
      "Epoch [9/300], Step [142/225], Training Accuracy: 54.6215%, Training Loss: 0.9142%\n",
      "Epoch [9/300], Step [143/225], Training Accuracy: 54.6329%, Training Loss: 0.9138%\n",
      "Epoch [9/300], Step [144/225], Training Accuracy: 54.7201%, Training Loss: 0.9131%\n",
      "Epoch [9/300], Step [145/225], Training Accuracy: 54.7091%, Training Loss: 0.9133%\n",
      "Epoch [9/300], Step [146/225], Training Accuracy: 54.6447%, Training Loss: 0.9136%\n",
      "Epoch [9/300], Step [147/225], Training Accuracy: 54.6131%, Training Loss: 0.9134%\n",
      "Epoch [9/300], Step [148/225], Training Accuracy: 54.6875%, Training Loss: 0.9124%\n",
      "Epoch [9/300], Step [149/225], Training Accuracy: 54.6665%, Training Loss: 0.9128%\n",
      "Epoch [9/300], Step [150/225], Training Accuracy: 54.7604%, Training Loss: 0.9116%\n",
      "Epoch [9/300], Step [151/225], Training Accuracy: 54.7599%, Training Loss: 0.9113%\n",
      "Epoch [9/300], Step [152/225], Training Accuracy: 54.7389%, Training Loss: 0.9115%\n",
      "Epoch [9/300], Step [153/225], Training Accuracy: 54.7386%, Training Loss: 0.9112%\n",
      "Epoch [9/300], Step [154/225], Training Accuracy: 54.7585%, Training Loss: 0.9108%\n",
      "Epoch [9/300], Step [155/225], Training Accuracy: 54.7480%, Training Loss: 0.9110%\n",
      "Epoch [9/300], Step [156/225], Training Accuracy: 54.7376%, Training Loss: 0.9111%\n",
      "Epoch [9/300], Step [157/225], Training Accuracy: 54.7074%, Training Loss: 0.9113%\n",
      "Epoch [9/300], Step [158/225], Training Accuracy: 54.6677%, Training Loss: 0.9123%\n",
      "Epoch [9/300], Step [159/225], Training Accuracy: 54.6187%, Training Loss: 0.9127%\n",
      "Epoch [9/300], Step [160/225], Training Accuracy: 54.5996%, Training Loss: 0.9126%\n",
      "Epoch [9/300], Step [161/225], Training Accuracy: 54.6293%, Training Loss: 0.9130%\n",
      "Epoch [9/300], Step [162/225], Training Accuracy: 54.6971%, Training Loss: 0.9122%\n",
      "Epoch [9/300], Step [163/225], Training Accuracy: 54.6683%, Training Loss: 0.9121%\n",
      "Epoch [9/300], Step [164/225], Training Accuracy: 54.7066%, Training Loss: 0.9114%\n",
      "Epoch [9/300], Step [165/225], Training Accuracy: 54.6875%, Training Loss: 0.9114%\n",
      "Epoch [9/300], Step [166/225], Training Accuracy: 54.6969%, Training Loss: 0.9111%\n",
      "Epoch [9/300], Step [167/225], Training Accuracy: 54.7156%, Training Loss: 0.9107%\n",
      "Epoch [9/300], Step [168/225], Training Accuracy: 54.7061%, Training Loss: 0.9106%\n",
      "Epoch [9/300], Step [169/225], Training Accuracy: 54.7152%, Training Loss: 0.9105%\n",
      "Epoch [9/300], Step [170/225], Training Accuracy: 54.7243%, Training Loss: 0.9104%\n",
      "Epoch [9/300], Step [171/225], Training Accuracy: 54.7789%, Training Loss: 0.9100%\n",
      "Epoch [9/300], Step [172/225], Training Accuracy: 54.7783%, Training Loss: 0.9102%\n",
      "Epoch [9/300], Step [173/225], Training Accuracy: 54.7778%, Training Loss: 0.9102%\n",
      "Epoch [9/300], Step [174/225], Training Accuracy: 54.7953%, Training Loss: 0.9106%\n",
      "Epoch [9/300], Step [175/225], Training Accuracy: 54.8304%, Training Loss: 0.9104%\n",
      "Epoch [9/300], Step [176/225], Training Accuracy: 54.8562%, Training Loss: 0.9102%\n",
      "Epoch [9/300], Step [177/225], Training Accuracy: 54.8817%, Training Loss: 0.9097%\n",
      "Epoch [9/300], Step [178/225], Training Accuracy: 54.8806%, Training Loss: 0.9099%\n",
      "Epoch [9/300], Step [179/225], Training Accuracy: 54.9232%, Training Loss: 0.9092%\n",
      "Epoch [9/300], Step [180/225], Training Accuracy: 54.9740%, Training Loss: 0.9085%\n",
      "Epoch [9/300], Step [181/225], Training Accuracy: 54.9637%, Training Loss: 0.9092%\n",
      "Epoch [9/300], Step [182/225], Training Accuracy: 54.9794%, Training Loss: 0.9092%\n",
      "Epoch [9/300], Step [183/225], Training Accuracy: 54.9778%, Training Loss: 0.9086%\n",
      "Epoch [9/300], Step [184/225], Training Accuracy: 54.9847%, Training Loss: 0.9086%\n",
      "Epoch [9/300], Step [185/225], Training Accuracy: 54.9831%, Training Loss: 0.9087%\n",
      "Epoch [9/300], Step [186/225], Training Accuracy: 55.0487%, Training Loss: 0.9075%\n",
      "Epoch [9/300], Step [187/225], Training Accuracy: 55.0301%, Training Loss: 0.9070%\n",
      "Epoch [9/300], Step [188/225], Training Accuracy: 55.0033%, Training Loss: 0.9072%\n",
      "Epoch [9/300], Step [189/225], Training Accuracy: 55.0182%, Training Loss: 0.9067%\n",
      "Epoch [9/300], Step [190/225], Training Accuracy: 55.0247%, Training Loss: 0.9070%\n",
      "Epoch [9/300], Step [191/225], Training Accuracy: 54.9984%, Training Loss: 0.9073%\n",
      "Epoch [9/300], Step [192/225], Training Accuracy: 55.0700%, Training Loss: 0.9063%\n",
      "Epoch [9/300], Step [193/225], Training Accuracy: 55.0275%, Training Loss: 0.9066%\n",
      "Epoch [9/300], Step [194/225], Training Accuracy: 55.0016%, Training Loss: 0.9066%\n",
      "Epoch [9/300], Step [195/225], Training Accuracy: 55.0721%, Training Loss: 0.9058%\n",
      "Epoch [9/300], Step [196/225], Training Accuracy: 55.0303%, Training Loss: 0.9063%\n",
      "Epoch [9/300], Step [197/225], Training Accuracy: 55.0127%, Training Loss: 0.9062%\n",
      "Epoch [9/300], Step [198/225], Training Accuracy: 55.0821%, Training Loss: 0.9056%\n",
      "Epoch [9/300], Step [199/225], Training Accuracy: 55.1508%, Training Loss: 0.9050%\n",
      "Epoch [9/300], Step [200/225], Training Accuracy: 55.1641%, Training Loss: 0.9049%\n",
      "Epoch [9/300], Step [201/225], Training Accuracy: 55.1384%, Training Loss: 0.9057%\n",
      "Epoch [9/300], Step [202/225], Training Accuracy: 55.1207%, Training Loss: 0.9057%\n",
      "Epoch [9/300], Step [203/225], Training Accuracy: 55.1339%, Training Loss: 0.9054%\n",
      "Epoch [9/300], Step [204/225], Training Accuracy: 55.1700%, Training Loss: 0.9051%\n",
      "Epoch [9/300], Step [205/225], Training Accuracy: 55.2515%, Training Loss: 0.9046%\n",
      "Epoch [9/300], Step [206/225], Training Accuracy: 55.2791%, Training Loss: 0.9048%\n",
      "Epoch [9/300], Step [207/225], Training Accuracy: 55.2612%, Training Loss: 0.9049%\n",
      "Epoch [9/300], Step [208/225], Training Accuracy: 55.2960%, Training Loss: 0.9044%\n",
      "Epoch [9/300], Step [209/225], Training Accuracy: 55.3155%, Training Loss: 0.9047%\n",
      "Epoch [9/300], Step [210/225], Training Accuracy: 55.3051%, Training Loss: 0.9051%\n",
      "Epoch [9/300], Step [211/225], Training Accuracy: 55.3614%, Training Loss: 0.9045%\n",
      "Epoch [9/300], Step [212/225], Training Accuracy: 55.4024%, Training Loss: 0.9045%\n",
      "Epoch [9/300], Step [213/225], Training Accuracy: 55.3330%, Training Loss: 0.9056%\n",
      "Epoch [9/300], Step [214/225], Training Accuracy: 55.3957%, Training Loss: 0.9052%\n",
      "Epoch [9/300], Step [215/225], Training Accuracy: 55.3852%, Training Loss: 0.9049%\n",
      "Epoch [9/300], Step [216/225], Training Accuracy: 55.4109%, Training Loss: 0.9050%\n",
      "Epoch [9/300], Step [217/225], Training Accuracy: 55.4075%, Training Loss: 0.9055%\n",
      "Epoch [9/300], Step [218/225], Training Accuracy: 55.3827%, Training Loss: 0.9061%\n",
      "Epoch [9/300], Step [219/225], Training Accuracy: 55.3653%, Training Loss: 0.9058%\n",
      "Epoch [9/300], Step [220/225], Training Accuracy: 55.3409%, Training Loss: 0.9061%\n",
      "Epoch [9/300], Step [221/225], Training Accuracy: 55.3026%, Training Loss: 0.9067%\n",
      "Epoch [9/300], Step [222/225], Training Accuracy: 55.2928%, Training Loss: 0.9065%\n",
      "Epoch [9/300], Step [223/225], Training Accuracy: 55.2831%, Training Loss: 0.9065%\n",
      "Epoch [9/300], Step [224/225], Training Accuracy: 55.2944%, Training Loss: 0.9063%\n",
      "Epoch [9/300], Step [225/225], Training Accuracy: 55.2668%, Training Loss: 0.9064%\n",
      "Epoch [10/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8321%\n",
      "Epoch [10/300], Step [2/225], Training Accuracy: 62.5000%, Training Loss: 0.8885%\n",
      "Epoch [10/300], Step [3/225], Training Accuracy: 59.8958%, Training Loss: 0.9059%\n",
      "Epoch [10/300], Step [4/225], Training Accuracy: 58.2031%, Training Loss: 0.8975%\n",
      "Epoch [10/300], Step [5/225], Training Accuracy: 59.0625%, Training Loss: 0.8871%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [6/225], Training Accuracy: 59.1146%, Training Loss: 0.8890%\n",
      "Epoch [10/300], Step [7/225], Training Accuracy: 58.2589%, Training Loss: 0.8872%\n",
      "Epoch [10/300], Step [8/225], Training Accuracy: 58.3984%, Training Loss: 0.8911%\n",
      "Epoch [10/300], Step [9/225], Training Accuracy: 58.5069%, Training Loss: 0.8945%\n",
      "Epoch [10/300], Step [10/225], Training Accuracy: 57.9688%, Training Loss: 0.9118%\n",
      "Epoch [10/300], Step [11/225], Training Accuracy: 57.9545%, Training Loss: 0.9093%\n",
      "Epoch [10/300], Step [12/225], Training Accuracy: 58.0729%, Training Loss: 0.9047%\n",
      "Epoch [10/300], Step [13/225], Training Accuracy: 59.1346%, Training Loss: 0.8939%\n",
      "Epoch [10/300], Step [14/225], Training Accuracy: 59.3750%, Training Loss: 0.8886%\n",
      "Epoch [10/300], Step [15/225], Training Accuracy: 59.3750%, Training Loss: 0.8895%\n",
      "Epoch [10/300], Step [16/225], Training Accuracy: 59.0820%, Training Loss: 0.8879%\n",
      "Epoch [10/300], Step [17/225], Training Accuracy: 59.0074%, Training Loss: 0.8822%\n",
      "Epoch [10/300], Step [18/225], Training Accuracy: 58.7674%, Training Loss: 0.8831%\n",
      "Epoch [10/300], Step [19/225], Training Accuracy: 58.5526%, Training Loss: 0.8809%\n",
      "Epoch [10/300], Step [20/225], Training Accuracy: 58.4375%, Training Loss: 0.8801%\n",
      "Epoch [10/300], Step [21/225], Training Accuracy: 59.0774%, Training Loss: 0.8718%\n",
      "Epoch [10/300], Step [22/225], Training Accuracy: 58.2386%, Training Loss: 0.8802%\n",
      "Epoch [10/300], Step [23/225], Training Accuracy: 58.6957%, Training Loss: 0.8743%\n",
      "Epoch [10/300], Step [24/225], Training Accuracy: 58.3984%, Training Loss: 0.8764%\n",
      "Epoch [10/300], Step [25/225], Training Accuracy: 58.4375%, Training Loss: 0.8760%\n",
      "Epoch [10/300], Step [26/225], Training Accuracy: 58.1731%, Training Loss: 0.8792%\n",
      "Epoch [10/300], Step [27/225], Training Accuracy: 57.9282%, Training Loss: 0.8809%\n",
      "Epoch [10/300], Step [28/225], Training Accuracy: 58.0915%, Training Loss: 0.8771%\n",
      "Epoch [10/300], Step [29/225], Training Accuracy: 58.2435%, Training Loss: 0.8752%\n",
      "Epoch [10/300], Step [30/225], Training Accuracy: 58.1250%, Training Loss: 0.8744%\n",
      "Epoch [10/300], Step [31/225], Training Accuracy: 57.8125%, Training Loss: 0.8786%\n",
      "Epoch [10/300], Step [32/225], Training Accuracy: 57.8125%, Training Loss: 0.8750%\n",
      "Epoch [10/300], Step [33/225], Training Accuracy: 57.9545%, Training Loss: 0.8725%\n",
      "Epoch [10/300], Step [34/225], Training Accuracy: 57.6746%, Training Loss: 0.8759%\n",
      "Epoch [10/300], Step [35/225], Training Accuracy: 57.6339%, Training Loss: 0.8787%\n",
      "Epoch [10/300], Step [36/225], Training Accuracy: 57.4653%, Training Loss: 0.8802%\n",
      "Epoch [10/300], Step [37/225], Training Accuracy: 57.6014%, Training Loss: 0.8775%\n",
      "Epoch [10/300], Step [38/225], Training Accuracy: 57.5658%, Training Loss: 0.8772%\n",
      "Epoch [10/300], Step [39/225], Training Accuracy: 57.5721%, Training Loss: 0.8777%\n",
      "Epoch [10/300], Step [40/225], Training Accuracy: 57.5000%, Training Loss: 0.8804%\n",
      "Epoch [10/300], Step [41/225], Training Accuracy: 57.3933%, Training Loss: 0.8835%\n",
      "Epoch [10/300], Step [42/225], Training Accuracy: 57.3661%, Training Loss: 0.8830%\n",
      "Epoch [10/300], Step [43/225], Training Accuracy: 57.1948%, Training Loss: 0.8832%\n",
      "Epoch [10/300], Step [44/225], Training Accuracy: 57.2088%, Training Loss: 0.8810%\n",
      "Epoch [10/300], Step [45/225], Training Accuracy: 57.3264%, Training Loss: 0.8787%\n",
      "Epoch [10/300], Step [46/225], Training Accuracy: 57.6766%, Training Loss: 0.8766%\n",
      "Epoch [10/300], Step [47/225], Training Accuracy: 57.7128%, Training Loss: 0.8763%\n",
      "Epoch [10/300], Step [48/225], Training Accuracy: 57.7799%, Training Loss: 0.8747%\n",
      "Epoch [10/300], Step [49/225], Training Accuracy: 57.8444%, Training Loss: 0.8757%\n",
      "Epoch [10/300], Step [50/225], Training Accuracy: 57.9062%, Training Loss: 0.8749%\n",
      "Epoch [10/300], Step [51/225], Training Accuracy: 58.0576%, Training Loss: 0.8732%\n",
      "Epoch [10/300], Step [52/225], Training Accuracy: 58.0529%, Training Loss: 0.8725%\n",
      "Epoch [10/300], Step [53/225], Training Accuracy: 58.1368%, Training Loss: 0.8720%\n",
      "Epoch [10/300], Step [54/225], Training Accuracy: 58.0440%, Training Loss: 0.8725%\n",
      "Epoch [10/300], Step [55/225], Training Accuracy: 57.9545%, Training Loss: 0.8736%\n",
      "Epoch [10/300], Step [56/225], Training Accuracy: 57.8962%, Training Loss: 0.8729%\n",
      "Epoch [10/300], Step [57/225], Training Accuracy: 57.8673%, Training Loss: 0.8721%\n",
      "Epoch [10/300], Step [58/225], Training Accuracy: 58.0280%, Training Loss: 0.8721%\n",
      "Epoch [10/300], Step [59/225], Training Accuracy: 58.0244%, Training Loss: 0.8711%\n",
      "Epoch [10/300], Step [60/225], Training Accuracy: 58.0729%, Training Loss: 0.8696%\n",
      "Epoch [10/300], Step [61/225], Training Accuracy: 58.1711%, Training Loss: 0.8691%\n",
      "Epoch [10/300], Step [62/225], Training Accuracy: 58.1149%, Training Loss: 0.8683%\n",
      "Epoch [10/300], Step [63/225], Training Accuracy: 58.0605%, Training Loss: 0.8695%\n",
      "Epoch [10/300], Step [64/225], Training Accuracy: 58.1055%, Training Loss: 0.8693%\n",
      "Epoch [10/300], Step [65/225], Training Accuracy: 58.0048%, Training Loss: 0.8700%\n",
      "Epoch [10/300], Step [66/225], Training Accuracy: 57.9782%, Training Loss: 0.8704%\n",
      "Epoch [10/300], Step [67/225], Training Accuracy: 57.9757%, Training Loss: 0.8700%\n",
      "Epoch [10/300], Step [68/225], Training Accuracy: 57.8814%, Training Loss: 0.8706%\n",
      "Epoch [10/300], Step [69/225], Training Accuracy: 57.7672%, Training Loss: 0.8702%\n",
      "Epoch [10/300], Step [70/225], Training Accuracy: 57.6339%, Training Loss: 0.8712%\n",
      "Epoch [10/300], Step [71/225], Training Accuracy: 57.5704%, Training Loss: 0.8712%\n",
      "Epoch [10/300], Step [72/225], Training Accuracy: 57.4870%, Training Loss: 0.8724%\n",
      "Epoch [10/300], Step [73/225], Training Accuracy: 57.4272%, Training Loss: 0.8734%\n",
      "Epoch [10/300], Step [74/225], Training Accuracy: 57.3691%, Training Loss: 0.8729%\n",
      "Epoch [10/300], Step [75/225], Training Accuracy: 57.3750%, Training Loss: 0.8723%\n",
      "Epoch [10/300], Step [76/225], Training Accuracy: 57.2574%, Training Loss: 0.8747%\n",
      "Epoch [10/300], Step [77/225], Training Accuracy: 57.3458%, Training Loss: 0.8737%\n",
      "Epoch [10/300], Step [78/225], Training Accuracy: 57.3117%, Training Loss: 0.8734%\n",
      "Epoch [10/300], Step [79/225], Training Accuracy: 57.2389%, Training Loss: 0.8746%\n",
      "Epoch [10/300], Step [80/225], Training Accuracy: 57.2656%, Training Loss: 0.8745%\n",
      "Epoch [10/300], Step [81/225], Training Accuracy: 57.2917%, Training Loss: 0.8738%\n",
      "Epoch [10/300], Step [82/225], Training Accuracy: 57.2599%, Training Loss: 0.8733%\n",
      "Epoch [10/300], Step [83/225], Training Accuracy: 57.3042%, Training Loss: 0.8727%\n",
      "Epoch [10/300], Step [84/225], Training Accuracy: 57.3103%, Training Loss: 0.8724%\n",
      "Epoch [10/300], Step [85/225], Training Accuracy: 57.3529%, Training Loss: 0.8715%\n",
      "Epoch [10/300], Step [86/225], Training Accuracy: 57.3038%, Training Loss: 0.8719%\n",
      "Epoch [10/300], Step [87/225], Training Accuracy: 57.3635%, Training Loss: 0.8725%\n",
      "Epoch [10/300], Step [88/225], Training Accuracy: 57.2976%, Training Loss: 0.8734%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss = 0\n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #scheduler.step() \n",
    "        #print(scheduler.get_last_lr()[0])\n",
    "      \n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        #print(optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += Y.size(0)\n",
    "        correct += predicted.eq(Y).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        accu=100.*correct/total\n",
    "        train_loss = running_loss/(i+1)\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.4f}%, Training Loss: {:.4f}%'.format(epoch+1, num_epochs, i+1, total_step, accu, train_loss))\n",
    "    \n",
    "   \n",
    "        #writer.add_scalar(f'train/accuracy', accu, epoch)\n",
    "        #writer.add_scalar(f'train/loss', train_loss, epoch)\n",
    "        writer.add_scalars(f'train/accuracy_loss', {\n",
    "            'accuracy': accu,\n",
    "            'loss': train_loss,\n",
    "        }, epoch)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, Y in test_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += Y.size(0)\n",
    "        correct += (predicted == Y).sum().item()\n",
    "\n",
    "    print('Test Accuracy : {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce71586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8487164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
