{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce5e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "72630224",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58eded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a3116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/final_format/train_set.csv\",header=None).to_numpy()\n",
    "train_label = pd.read_csv(\"data/final_format/train_label.csv\",header=None).to_numpy()\n",
    "test_set = pd.read_csv(\"data/final_format/test_set.csv\",header=None).to_numpy()\n",
    "test_label = pd.read_csv(\"data/final_format/test_label.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f64e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14393, 4096) (14393, 1) (3599, 4096) (3599, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d522cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 4096) (14392, 1) (3598, 4096) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "#delet first row data\n",
    "train_set = train_set[1:]\n",
    "train_label = train_label[1:]\n",
    "test_set = test_set[1:]\n",
    "test_label = test_label[1:]\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a84a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 64, 64) (14392, 1) (3598, 64, 64) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.reshape((-1,64,64))\n",
    "test_set = test_set.reshape((-1,64,64))\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e23a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 64, 64) (14392,) (3598, 64, 64) (3598,)\n"
     ]
    }
   ],
   "source": [
    "train_label = train_label.reshape(-1)\n",
    "test_label = test_label.reshape(-1)\n",
    "\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f62253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 300\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d66b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tensor = Tensor(train_set) \n",
    "train_label_tensor = Tensor(train_label).type(torch.LongTensor)\n",
    "\n",
    "train_dataset = TensorDataset(train_set_tensor,train_label_tensor) \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size) \n",
    "\n",
    "test_set_tensor = Tensor(test_set) \n",
    "test_label_tensor = Tensor(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_dataset = TensorDataset(test_set_tensor,test_label_tensor) \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33820b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6f6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Patryk Chrabaszcz\n",
    "#          Lukas Gemein <l.gemein@gmail.com>\n",
    "#\n",
    "# License: BSD-3\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from function.modules import Ensure4d, Expression\n",
    "from function.functions import squeeze_final_output\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"Temporal Convolutional Network (TCN) from Bai et al 2018.\n",
    "\n",
    "    See [Bai2018]_ for details.\n",
    "\n",
    "    Code adapted from https://github.com/locuslab/TCN/blob/master/TCN/tcn.py\n",
    "\n",
    "    Parameters\n",
    "    ---------- \n",
    "    n_in_chans: int\n",
    "        number of input EEG channels\n",
    "    n_outputs: int\n",
    "        number of outputs of the decoding task (for example number of classes in\n",
    "        classification)\n",
    "    n_filters: int\n",
    "        number of output filters of each convolution\n",
    "    n_blocks: int\n",
    "        number of temporal blocks in the network\n",
    "    kernel_size: int\n",
    "        kernel size of the convolutions\n",
    "    drop_prob: float\n",
    "        dropout probability\n",
    "    add_log_softmax: bool\n",
    "        whether to add a log softmax layer\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [Bai2018] Bai, S., Kolter, J. Z., & Koltun, V. (2018).\n",
    "       An empirical evaluation of generic convolutional and recurrent networks\n",
    "       for sequence modeling.\n",
    "       arXiv preprint arXiv:1803.01271.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in_chans=64, n_outputs=4, n_blocks=4\n",
    "                 , n_filters=24, kernel_size=3,\n",
    "                 drop_prob=0.25, add_log_softmax=False):\n",
    "        super().__init__()\n",
    "        self.ensuredims = Ensure4d()\n",
    "        t_blocks = nn.Sequential()\n",
    "        for i in range(n_blocks):\n",
    "            n_inputs = n_in_chans if i == 0 else n_filters\n",
    "            dilation_size = 2 ** i\n",
    "            t_blocks.add_module(\"temporal_block_{:d}\".format(i), TemporalBlock(\n",
    "                n_inputs=n_inputs,\n",
    "                n_outputs=n_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                dilation=dilation_size,\n",
    "                padding=(kernel_size - 1) * dilation_size,\n",
    "                drop_prob=drop_prob\n",
    "            ))\n",
    "        self.temporal_blocks = t_blocks\n",
    "        self.fc = nn.Linear(in_features=n_filters, out_features=n_outputs)\n",
    "        self.fc_last = nn.Linear(4*4, out_features=n_outputs)\n",
    "        if add_log_softmax:\n",
    "            self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.squeeze = Expression(squeeze_final_output)\n",
    "\n",
    "        self.min_len = 1\n",
    "        for i in range(n_blocks):\n",
    "            dilation = 2 ** i\n",
    "            self.min_len += 2 * (kernel_size - 1) * dilation\n",
    "\n",
    "        # start in eval mode\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "        # b 64 64\n",
    "        x = self.ensuredims(x)\n",
    "        # x is in format: B x C x T x 1\n",
    "        (batch_size, _, time_size, _) = x.size()\n",
    "        \n",
    "        assert time_size >= self.min_len\n",
    "        # remove empty trailing dimension\n",
    "        x = x.squeeze(3)\n",
    "        # b 64 64\n",
    "        x = self.temporal_blocks(x)\n",
    "        #  if n_blocks = 2 -> b 20 64\n",
    "        # Convert to: B x T x C\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        # b 64 20\n",
    "        fc_out = self.fc(x.view(batch_size * time_size, x.size(2)))\n",
    "        # b*64 4\n",
    "        fc_out = fc_out.view(batch_size, time_size, fc_out.size(1))\n",
    "        #b 64 4\n",
    "        out_size = 1 + max(0, time_size - self.min_len)\n",
    "        out = fc_out[:, -out_size:, :].transpose(1, 2)\n",
    "        #b 4 52\n",
    "        out = self.fc_last(out.reshape(out.size(0),-1))\n",
    "       \n",
    "        return out\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation,\n",
    "                 padding, drop_prob):\n",
    "        super().__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(\n",
    "            n_inputs, n_outputs, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout2d(drop_prob)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(\n",
    "            n_outputs, n_outputs, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout2d(drop_prob)\n",
    "\n",
    "        self.downsample = (nn.Conv1d(n_inputs, n_outputs, 1)\n",
    "                           if n_inputs != n_outputs else None)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        init.normal_(self.conv1.weight, 0, 0.01)\n",
    "        init.normal_(self.conv2.weight, 0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            init.normal_(self.downsample.weight, 0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #b 64 64, b 20 64 \n",
    "        out = self.conv1(x)\n",
    "        #b 20 66, b 20 68\n",
    "        out = self.chomp1(out)\n",
    "        #b 20 64, b 20 64\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.conv2(out)\n",
    "        #b 20 66, b 20 68 \n",
    "        out = self.chomp2(out)\n",
    "        #b 20 64, b 20 64\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        res = x if self.downsample is None else self.downsample(x)# b 20 64\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'chomp_size={}'.format(self.chomp_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d4db97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 1, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.randn((64,1,10))\n",
    "test[None,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247664f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((10,64,64))\n",
    "model = TCN()\n",
    "b = model(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc880d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN(\n",
      "  (ensuredims): Ensure4d()\n",
      "  (temporal_blocks): Sequential(\n",
      "    (temporal_block_0): TemporalBlock(\n",
      "      (conv1): Conv1d(64, 24, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "      (chomp1): Chomp1d(chomp_size=2)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "      (conv2): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "      (chomp2): Chomp1d(chomp_size=2)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout2d(p=0.25, inplace=False)\n",
      "      (downsample): Conv1d(64, 24, kernel_size=(1,), stride=(1,))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (temporal_block_1): TemporalBlock(\n",
      "      (conv1): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "      (chomp1): Chomp1d(chomp_size=4)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "      (conv2): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "      (chomp2): Chomp1d(chomp_size=4)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout2d(p=0.25, inplace=False)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (temporal_block_2): TemporalBlock(\n",
      "      (conv1): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "      (chomp1): Chomp1d(chomp_size=8)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "      (conv2): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "      (chomp2): Chomp1d(chomp_size=8)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout2d(p=0.25, inplace=False)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (temporal_block_3): TemporalBlock(\n",
      "      (conv1): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "      (chomp1): Chomp1d(chomp_size=16)\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "      (conv2): Conv1d(24, 24, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "      (chomp2): Chomp1d(chomp_size=16)\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout2d(p=0.25, inplace=False)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=24, out_features=4, bias=True)\n",
      "  (fc_last): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (squeeze): Expression(expression=squeeze_final_output) \n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "901366a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ae0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3) \n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "milestones = [50,100,150,200,250]\n",
    "milestones = [a * len(train_loader) for a in milestones]\n",
    "scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b03775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [1/225], Training Accuracy: 26.5625%, Training Loss: 1.3893%\n",
      "Epoch [1/300], Step [2/225], Training Accuracy: 25.0000%, Training Loss: 1.3875%\n",
      "Epoch [1/300], Step [3/225], Training Accuracy: 23.4375%, Training Loss: 1.3872%\n",
      "Epoch [1/300], Step [4/225], Training Accuracy: 24.2188%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [5/225], Training Accuracy: 22.1875%, Training Loss: 1.3896%\n",
      "Epoch [1/300], Step [6/225], Training Accuracy: 24.2188%, Training Loss: 1.3875%\n",
      "Epoch [1/300], Step [7/225], Training Accuracy: 24.1071%, Training Loss: 1.3869%\n",
      "Epoch [1/300], Step [8/225], Training Accuracy: 25.0000%, Training Loss: 1.3859%\n",
      "Epoch [1/300], Step [9/225], Training Accuracy: 25.1736%, Training Loss: 1.3864%\n",
      "Epoch [1/300], Step [10/225], Training Accuracy: 25.4688%, Training Loss: 1.3854%\n",
      "Epoch [1/300], Step [11/225], Training Accuracy: 26.1364%, Training Loss: 1.3859%\n",
      "Epoch [1/300], Step [12/225], Training Accuracy: 26.3021%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [13/225], Training Accuracy: 26.5625%, Training Loss: 1.3848%\n",
      "Epoch [1/300], Step [14/225], Training Accuracy: 26.2277%, Training Loss: 1.3858%\n",
      "Epoch [1/300], Step [15/225], Training Accuracy: 25.9375%, Training Loss: 1.3863%\n",
      "Epoch [1/300], Step [16/225], Training Accuracy: 26.1719%, Training Loss: 1.3858%\n",
      "Epoch [1/300], Step [17/225], Training Accuracy: 25.8272%, Training Loss: 1.3861%\n",
      "Epoch [1/300], Step [18/225], Training Accuracy: 25.5208%, Training Loss: 1.3873%\n",
      "Epoch [1/300], Step [19/225], Training Accuracy: 25.1645%, Training Loss: 1.3880%\n",
      "Epoch [1/300], Step [20/225], Training Accuracy: 25.0781%, Training Loss: 1.3885%\n",
      "Epoch [1/300], Step [21/225], Training Accuracy: 25.4464%, Training Loss: 1.3879%\n",
      "Epoch [1/300], Step [22/225], Training Accuracy: 25.3551%, Training Loss: 1.3877%\n",
      "Epoch [1/300], Step [23/225], Training Accuracy: 25.5435%, Training Loss: 1.3873%\n",
      "Epoch [1/300], Step [24/225], Training Accuracy: 25.3906%, Training Loss: 1.3876%\n",
      "Epoch [1/300], Step [25/225], Training Accuracy: 25.5000%, Training Loss: 1.3877%\n",
      "Epoch [1/300], Step [26/225], Training Accuracy: 25.3005%, Training Loss: 1.3881%\n",
      "Epoch [1/300], Step [27/225], Training Accuracy: 25.6944%, Training Loss: 1.3874%\n",
      "Epoch [1/300], Step [28/225], Training Accuracy: 25.8929%, Training Loss: 1.3872%\n",
      "Epoch [1/300], Step [29/225], Training Accuracy: 25.9698%, Training Loss: 1.3873%\n",
      "Epoch [1/300], Step [30/225], Training Accuracy: 26.0938%, Training Loss: 1.3873%\n",
      "Epoch [1/300], Step [31/225], Training Accuracy: 26.3609%, Training Loss: 1.3871%\n",
      "Epoch [1/300], Step [32/225], Training Accuracy: 26.3184%, Training Loss: 1.3873%\n",
      "Epoch [1/300], Step [33/225], Training Accuracy: 26.3731%, Training Loss: 1.3873%\n",
      "Epoch [1/300], Step [34/225], Training Accuracy: 26.3327%, Training Loss: 1.3871%\n",
      "Epoch [1/300], Step [35/225], Training Accuracy: 26.2500%, Training Loss: 1.3872%\n",
      "Epoch [1/300], Step [36/225], Training Accuracy: 26.3021%, Training Loss: 1.3871%\n",
      "Epoch [1/300], Step [37/225], Training Accuracy: 26.1402%, Training Loss: 1.3871%\n",
      "Epoch [1/300], Step [38/225], Training Accuracy: 26.0691%, Training Loss: 1.3870%\n",
      "Epoch [1/300], Step [39/225], Training Accuracy: 26.1218%, Training Loss: 1.3869%\n",
      "Epoch [1/300], Step [40/225], Training Accuracy: 26.0547%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [41/225], Training Accuracy: 26.0671%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [42/225], Training Accuracy: 26.0417%, Training Loss: 1.3869%\n",
      "Epoch [1/300], Step [43/225], Training Accuracy: 25.9448%, Training Loss: 1.3870%\n",
      "Epoch [1/300], Step [44/225], Training Accuracy: 25.9943%, Training Loss: 1.3870%\n",
      "Epoch [1/300], Step [45/225], Training Accuracy: 25.8681%, Training Loss: 1.3870%\n",
      "Epoch [1/300], Step [46/225], Training Accuracy: 25.8832%, Training Loss: 1.3869%\n",
      "Epoch [1/300], Step [47/225], Training Accuracy: 25.6981%, Training Loss: 1.3870%\n",
      "Epoch [1/300], Step [48/225], Training Accuracy: 25.6836%, Training Loss: 1.3869%\n",
      "Epoch [1/300], Step [49/225], Training Accuracy: 25.8291%, Training Loss: 1.3867%\n",
      "Epoch [1/300], Step [50/225], Training Accuracy: 25.7188%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [51/225], Training Accuracy: 25.7047%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [52/225], Training Accuracy: 25.7212%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [53/225], Training Accuracy: 25.7370%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [54/225], Training Accuracy: 25.8102%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [55/225], Training Accuracy: 25.7670%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [56/225], Training Accuracy: 25.6138%, Training Loss: 1.3867%\n",
      "Epoch [1/300], Step [57/225], Training Accuracy: 25.6305%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [58/225], Training Accuracy: 25.7543%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [59/225], Training Accuracy: 25.8210%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [60/225], Training Accuracy: 25.6771%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [61/225], Training Accuracy: 25.7172%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [62/225], Training Accuracy: 25.6552%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [63/225], Training Accuracy: 25.4960%, Training Loss: 1.3868%\n",
      "Epoch [1/300], Step [64/225], Training Accuracy: 25.5615%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [65/225], Training Accuracy: 25.6010%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [66/225], Training Accuracy: 25.6629%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [67/225], Training Accuracy: 25.6996%, Training Loss: 1.3866%\n",
      "Epoch [1/300], Step [68/225], Training Accuracy: 25.7353%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [69/225], Training Accuracy: 25.8379%, Training Loss: 1.3864%\n",
      "Epoch [1/300], Step [70/225], Training Accuracy: 25.8705%, Training Loss: 1.3864%\n",
      "Epoch [1/300], Step [71/225], Training Accuracy: 25.8363%, Training Loss: 1.3864%\n",
      "Epoch [1/300], Step [72/225], Training Accuracy: 25.7812%, Training Loss: 1.3865%\n",
      "Epoch [1/300], Step [73/225], Training Accuracy: 25.8562%, Training Loss: 1.3863%\n",
      "Epoch [1/300], Step [74/225], Training Accuracy: 25.8235%, Training Loss: 1.3863%\n",
      "Epoch [1/300], Step [75/225], Training Accuracy: 25.8750%, Training Loss: 1.3861%\n",
      "Epoch [1/300], Step [76/225], Training Accuracy: 25.9252%, Training Loss: 1.3861%\n",
      "Epoch [1/300], Step [77/225], Training Accuracy: 25.9131%, Training Loss: 1.3861%\n",
      "Epoch [1/300], Step [78/225], Training Accuracy: 25.9014%, Training Loss: 1.3861%\n",
      "Epoch [1/300], Step [79/225], Training Accuracy: 26.0087%, Training Loss: 1.3860%\n",
      "Epoch [1/300], Step [80/225], Training Accuracy: 26.0352%, Training Loss: 1.3860%\n",
      "Epoch [1/300], Step [81/225], Training Accuracy: 26.1574%, Training Loss: 1.3859%\n",
      "Epoch [1/300], Step [82/225], Training Accuracy: 26.2767%, Training Loss: 1.3858%\n",
      "Epoch [1/300], Step [83/225], Training Accuracy: 26.3178%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [84/225], Training Accuracy: 26.2835%, Training Loss: 1.3858%\n",
      "Epoch [1/300], Step [85/225], Training Accuracy: 26.3051%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [86/225], Training Accuracy: 26.3081%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [87/225], Training Accuracy: 26.3649%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [88/225], Training Accuracy: 26.4027%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [89/225], Training Accuracy: 26.5274%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [90/225], Training Accuracy: 26.4931%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [91/225], Training Accuracy: 26.4938%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [92/225], Training Accuracy: 26.5795%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [93/225], Training Accuracy: 26.5625%, Training Loss: 1.3855%\n",
      "Epoch [1/300], Step [94/225], Training Accuracy: 26.4960%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [95/225], Training Accuracy: 26.5789%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [96/225], Training Accuracy: 26.5462%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [97/225], Training Accuracy: 26.6108%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [98/225], Training Accuracy: 26.5944%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [99/225], Training Accuracy: 26.6256%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [100/225], Training Accuracy: 26.6094%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [101/225], Training Accuracy: 26.5625%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [102/225], Training Accuracy: 26.5778%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [103/225], Training Accuracy: 26.5322%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [104/225], Training Accuracy: 26.5325%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [105/225], Training Accuracy: 26.5327%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [106/225], Training Accuracy: 26.5035%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [107/225], Training Accuracy: 26.4895%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [108/225], Training Accuracy: 26.5046%, Training Loss: 1.3857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [109/225], Training Accuracy: 26.4908%, Training Loss: 1.3857%\n",
      "Epoch [1/300], Step [110/225], Training Accuracy: 26.5909%, Training Loss: 1.3856%\n",
      "Epoch [1/300], Step [111/225], Training Accuracy: 26.6188%, Training Loss: 1.3855%\n",
      "Epoch [1/300], Step [112/225], Training Accuracy: 26.6323%, Training Loss: 1.3855%\n",
      "Epoch [1/300], Step [113/225], Training Accuracy: 26.6316%, Training Loss: 1.3855%\n",
      "Epoch [1/300], Step [114/225], Training Accuracy: 26.6721%, Training Loss: 1.3854%\n",
      "Epoch [1/300], Step [115/225], Training Accuracy: 26.6984%, Training Loss: 1.3854%\n",
      "Epoch [1/300], Step [116/225], Training Accuracy: 26.7915%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [117/225], Training Accuracy: 26.7228%, Training Loss: 1.3854%\n",
      "Epoch [1/300], Step [118/225], Training Accuracy: 26.7744%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [119/225], Training Accuracy: 26.8251%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [120/225], Training Accuracy: 26.8099%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [121/225], Training Accuracy: 26.7562%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [122/225], Training Accuracy: 26.7674%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [123/225], Training Accuracy: 26.7785%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [124/225], Training Accuracy: 26.8019%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [125/225], Training Accuracy: 26.7250%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [126/225], Training Accuracy: 26.6989%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [127/225], Training Accuracy: 26.6363%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [128/225], Training Accuracy: 26.6357%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [129/225], Training Accuracy: 26.6109%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [130/225], Training Accuracy: 26.6106%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [131/225], Training Accuracy: 26.6102%, Training Loss: 1.3854%\n",
      "Epoch [1/300], Step [132/225], Training Accuracy: 26.6098%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [133/225], Training Accuracy: 26.6095%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [134/225], Training Accuracy: 26.5858%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [135/225], Training Accuracy: 26.5394%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [136/225], Training Accuracy: 26.5510%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [137/225], Training Accuracy: 26.6423%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [138/225], Training Accuracy: 26.6078%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [139/225], Training Accuracy: 26.6075%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [140/225], Training Accuracy: 26.6741%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [141/225], Training Accuracy: 26.7509%, Training Loss: 1.3850%\n",
      "Epoch [1/300], Step [142/225], Training Accuracy: 26.7055%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [143/225], Training Accuracy: 26.6499%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [144/225], Training Accuracy: 26.6493%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [145/225], Training Accuracy: 26.6703%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [146/225], Training Accuracy: 26.6695%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [147/225], Training Accuracy: 26.6156%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [148/225], Training Accuracy: 26.6681%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [149/225], Training Accuracy: 26.6254%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [150/225], Training Accuracy: 26.6146%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [151/225], Training Accuracy: 26.6453%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [152/225], Training Accuracy: 26.6447%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [153/225], Training Accuracy: 26.6953%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [154/225], Training Accuracy: 26.6538%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [155/225], Training Accuracy: 26.6532%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [156/225], Training Accuracy: 26.6827%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [157/225], Training Accuracy: 26.6421%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [158/225], Training Accuracy: 26.6713%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [159/225], Training Accuracy: 26.6706%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [160/225], Training Accuracy: 26.6602%, Training Loss: 1.3853%\n",
      "Epoch [1/300], Step [161/225], Training Accuracy: 26.6693%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [162/225], Training Accuracy: 26.6782%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [163/225], Training Accuracy: 26.6775%, Training Loss: 1.3852%\n",
      "Epoch [1/300], Step [164/225], Training Accuracy: 26.7626%, Training Loss: 1.3851%\n",
      "Epoch [1/300], Step [165/225], Training Accuracy: 26.7614%, Training Loss: 1.3850%\n",
      "Epoch [1/300], Step [166/225], Training Accuracy: 26.8449%, Training Loss: 1.3849%\n",
      "Epoch [1/300], Step [167/225], Training Accuracy: 26.8432%, Training Loss: 1.3849%\n",
      "Epoch [1/300], Step [168/225], Training Accuracy: 26.8694%, Training Loss: 1.3848%\n",
      "Epoch [1/300], Step [169/225], Training Accuracy: 26.9138%, Training Loss: 1.3848%\n",
      "Epoch [1/300], Step [170/225], Training Accuracy: 26.9210%, Training Loss: 1.3848%\n",
      "Epoch [1/300], Step [171/225], Training Accuracy: 26.9645%, Training Loss: 1.3847%\n",
      "Epoch [1/300], Step [172/225], Training Accuracy: 26.9713%, Training Loss: 1.3847%\n",
      "Epoch [1/300], Step [173/225], Training Accuracy: 26.9960%, Training Loss: 1.3847%\n",
      "Epoch [1/300], Step [174/225], Training Accuracy: 26.9576%, Training Loss: 1.3847%\n",
      "Epoch [1/300], Step [175/225], Training Accuracy: 26.9554%, Training Loss: 1.3847%\n",
      "Epoch [1/300], Step [176/225], Training Accuracy: 26.9442%, Training Loss: 1.3847%\n",
      "Epoch [1/300], Step [177/225], Training Accuracy: 26.9333%, Training Loss: 1.3846%\n",
      "Epoch [1/300], Step [178/225], Training Accuracy: 26.9487%, Training Loss: 1.3846%\n",
      "Epoch [1/300], Step [179/225], Training Accuracy: 26.9291%, Training Loss: 1.3845%\n",
      "Epoch [1/300], Step [180/225], Training Accuracy: 27.0139%, Training Loss: 1.3844%\n",
      "Epoch [1/300], Step [181/225], Training Accuracy: 27.0200%, Training Loss: 1.3844%\n",
      "Epoch [1/300], Step [182/225], Training Accuracy: 27.0519%, Training Loss: 1.3843%\n",
      "Epoch [1/300], Step [183/225], Training Accuracy: 27.0663%, Training Loss: 1.3843%\n",
      "Epoch [1/300], Step [184/225], Training Accuracy: 27.0890%, Training Loss: 1.3843%\n",
      "Epoch [1/300], Step [185/225], Training Accuracy: 27.1368%, Training Loss: 1.3842%\n",
      "Epoch [1/300], Step [186/225], Training Accuracy: 27.1925%, Training Loss: 1.3841%\n",
      "Epoch [1/300], Step [187/225], Training Accuracy: 27.2226%, Training Loss: 1.3841%\n",
      "Epoch [1/300], Step [188/225], Training Accuracy: 27.2689%, Training Loss: 1.3840%\n",
      "Epoch [1/300], Step [189/225], Training Accuracy: 27.3065%, Training Loss: 1.3839%\n",
      "Epoch [1/300], Step [190/225], Training Accuracy: 27.3438%, Training Loss: 1.3839%\n",
      "Epoch [1/300], Step [191/225], Training Accuracy: 27.3560%, Training Loss: 1.3839%\n",
      "Epoch [1/300], Step [192/225], Training Accuracy: 27.3438%, Training Loss: 1.3839%\n",
      "Epoch [1/300], Step [193/225], Training Accuracy: 27.3478%, Training Loss: 1.3838%\n",
      "Epoch [1/300], Step [194/225], Training Accuracy: 27.4001%, Training Loss: 1.3837%\n",
      "Epoch [1/300], Step [195/225], Training Accuracy: 27.3878%, Training Loss: 1.3838%\n",
      "Epoch [1/300], Step [196/225], Training Accuracy: 27.3756%, Training Loss: 1.3838%\n",
      "Epoch [1/300], Step [197/225], Training Accuracy: 27.4112%, Training Loss: 1.3838%\n",
      "Epoch [1/300], Step [198/225], Training Accuracy: 27.4621%, Training Loss: 1.3837%\n",
      "Epoch [1/300], Step [199/225], Training Accuracy: 27.5204%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [200/225], Training Accuracy: 27.5625%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [201/225], Training Accuracy: 27.5575%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [202/225], Training Accuracy: 27.5835%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [203/225], Training Accuracy: 27.5939%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [204/225], Training Accuracy: 27.5965%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [205/225], Training Accuracy: 27.5610%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [206/225], Training Accuracy: 27.5713%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [207/225], Training Accuracy: 27.5891%, Training Loss: 1.3836%\n",
      "Epoch [1/300], Step [208/225], Training Accuracy: 27.6367%, Training Loss: 1.3834%\n",
      "Epoch [1/300], Step [209/225], Training Accuracy: 27.6316%, Training Loss: 1.3835%\n",
      "Epoch [1/300], Step [210/225], Training Accuracy: 27.6339%, Training Loss: 1.3834%\n",
      "Epoch [1/300], Step [211/225], Training Accuracy: 27.6363%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [212/225], Training Accuracy: 27.6386%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [213/225], Training Accuracy: 27.6629%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [214/225], Training Accuracy: 27.6942%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [215/225], Training Accuracy: 27.7180%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [216/225], Training Accuracy: 27.7416%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [217/225], Training Accuracy: 27.7506%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [218/225], Training Accuracy: 27.7451%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [219/225], Training Accuracy: 27.7397%, Training Loss: 1.3833%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [220/225], Training Accuracy: 27.7841%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [221/225], Training Accuracy: 27.7574%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [222/225], Training Accuracy: 27.7942%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [223/225], Training Accuracy: 27.7536%, Training Loss: 1.3833%\n",
      "Epoch [1/300], Step [224/225], Training Accuracy: 27.7204%, Training Loss: 1.3834%\n",
      "Epoch [1/300], Step [225/225], Training Accuracy: 27.6959%, Training Loss: 1.3835%\n",
      "Epoch [2/300], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 1.3715%\n",
      "Epoch [2/300], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 1.3712%\n",
      "Epoch [2/300], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 1.3718%\n",
      "Epoch [2/300], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 1.3709%\n",
      "Epoch [2/300], Step [5/225], Training Accuracy: 28.7500%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [6/225], Training Accuracy: 28.6458%, Training Loss: 1.3761%\n",
      "Epoch [2/300], Step [7/225], Training Accuracy: 28.3482%, Training Loss: 1.3759%\n",
      "Epoch [2/300], Step [8/225], Training Accuracy: 28.3203%, Training Loss: 1.3763%\n",
      "Epoch [2/300], Step [9/225], Training Accuracy: 28.4722%, Training Loss: 1.3779%\n",
      "Epoch [2/300], Step [10/225], Training Accuracy: 28.9062%, Training Loss: 1.3782%\n",
      "Epoch [2/300], Step [11/225], Training Accuracy: 28.5511%, Training Loss: 1.3783%\n",
      "Epoch [2/300], Step [12/225], Training Accuracy: 29.1667%, Training Loss: 1.3774%\n",
      "Epoch [2/300], Step [13/225], Training Accuracy: 29.4471%, Training Loss: 1.3769%\n",
      "Epoch [2/300], Step [14/225], Training Accuracy: 29.1295%, Training Loss: 1.3786%\n",
      "Epoch [2/300], Step [15/225], Training Accuracy: 28.7500%, Training Loss: 1.3796%\n",
      "Epoch [2/300], Step [16/225], Training Accuracy: 29.3945%, Training Loss: 1.3781%\n",
      "Epoch [2/300], Step [17/225], Training Accuracy: 29.6875%, Training Loss: 1.3781%\n",
      "Epoch [2/300], Step [18/225], Training Accuracy: 29.6875%, Training Loss: 1.3796%\n",
      "Epoch [2/300], Step [19/225], Training Accuracy: 29.5230%, Training Loss: 1.3802%\n",
      "Epoch [2/300], Step [20/225], Training Accuracy: 29.6094%, Training Loss: 1.3804%\n",
      "Epoch [2/300], Step [21/225], Training Accuracy: 30.2083%, Training Loss: 1.3792%\n",
      "Epoch [2/300], Step [22/225], Training Accuracy: 30.1847%, Training Loss: 1.3789%\n",
      "Epoch [2/300], Step [23/225], Training Accuracy: 30.0272%, Training Loss: 1.3782%\n",
      "Epoch [2/300], Step [24/225], Training Accuracy: 29.7526%, Training Loss: 1.3788%\n",
      "Epoch [2/300], Step [25/225], Training Accuracy: 30.0625%, Training Loss: 1.3784%\n",
      "Epoch [2/300], Step [26/225], Training Accuracy: 29.6274%, Training Loss: 1.3786%\n",
      "Epoch [2/300], Step [27/225], Training Accuracy: 29.8611%, Training Loss: 1.3777%\n",
      "Epoch [2/300], Step [28/225], Training Accuracy: 29.9107%, Training Loss: 1.3776%\n",
      "Epoch [2/300], Step [29/225], Training Accuracy: 30.0647%, Training Loss: 1.3769%\n",
      "Epoch [2/300], Step [30/225], Training Accuracy: 30.2083%, Training Loss: 1.3766%\n",
      "Epoch [2/300], Step [31/225], Training Accuracy: 30.4435%, Training Loss: 1.3767%\n",
      "Epoch [2/300], Step [32/225], Training Accuracy: 30.3711%, Training Loss: 1.3769%\n",
      "Epoch [2/300], Step [33/225], Training Accuracy: 30.5871%, Training Loss: 1.3768%\n",
      "Epoch [2/300], Step [34/225], Training Accuracy: 30.8364%, Training Loss: 1.3766%\n",
      "Epoch [2/300], Step [35/225], Training Accuracy: 30.6250%, Training Loss: 1.3767%\n",
      "Epoch [2/300], Step [36/225], Training Accuracy: 30.5556%, Training Loss: 1.3771%\n",
      "Epoch [2/300], Step [37/225], Training Accuracy: 30.5743%, Training Loss: 1.3772%\n",
      "Epoch [2/300], Step [38/225], Training Accuracy: 30.9622%, Training Loss: 1.3768%\n",
      "Epoch [2/300], Step [39/225], Training Accuracy: 31.1298%, Training Loss: 1.3771%\n",
      "Epoch [2/300], Step [40/225], Training Accuracy: 31.0938%, Training Loss: 1.3770%\n",
      "Epoch [2/300], Step [41/225], Training Accuracy: 31.0213%, Training Loss: 1.3771%\n",
      "Epoch [2/300], Step [42/225], Training Accuracy: 30.7664%, Training Loss: 1.3773%\n",
      "Epoch [2/300], Step [43/225], Training Accuracy: 30.7049%, Training Loss: 1.3773%\n",
      "Epoch [2/300], Step [44/225], Training Accuracy: 30.7173%, Training Loss: 1.3772%\n",
      "Epoch [2/300], Step [45/225], Training Accuracy: 30.7292%, Training Loss: 1.3771%\n",
      "Epoch [2/300], Step [46/225], Training Accuracy: 30.7065%, Training Loss: 1.3772%\n",
      "Epoch [2/300], Step [47/225], Training Accuracy: 30.5851%, Training Loss: 1.3774%\n",
      "Epoch [2/300], Step [48/225], Training Accuracy: 30.6315%, Training Loss: 1.3772%\n",
      "Epoch [2/300], Step [49/225], Training Accuracy: 30.8355%, Training Loss: 1.3769%\n",
      "Epoch [2/300], Step [50/225], Training Accuracy: 30.8750%, Training Loss: 1.3768%\n",
      "Epoch [2/300], Step [51/225], Training Accuracy: 30.8517%, Training Loss: 1.3766%\n",
      "Epoch [2/300], Step [52/225], Training Accuracy: 30.8293%, Training Loss: 1.3768%\n",
      "Epoch [2/300], Step [53/225], Training Accuracy: 30.8667%, Training Loss: 1.3769%\n",
      "Epoch [2/300], Step [54/225], Training Accuracy: 30.8449%, Training Loss: 1.3770%\n",
      "Epoch [2/300], Step [55/225], Training Accuracy: 30.7955%, Training Loss: 1.3770%\n",
      "Epoch [2/300], Step [56/225], Training Accuracy: 30.6641%, Training Loss: 1.3770%\n",
      "Epoch [2/300], Step [57/225], Training Accuracy: 30.8114%, Training Loss: 1.3766%\n",
      "Epoch [2/300], Step [58/225], Training Accuracy: 30.8190%, Training Loss: 1.3766%\n",
      "Epoch [2/300], Step [59/225], Training Accuracy: 30.9587%, Training Loss: 1.3763%\n",
      "Epoch [2/300], Step [60/225], Training Accuracy: 31.0156%, Training Loss: 1.3760%\n",
      "Epoch [2/300], Step [61/225], Training Accuracy: 31.0707%, Training Loss: 1.3760%\n",
      "Epoch [2/300], Step [62/225], Training Accuracy: 31.0736%, Training Loss: 1.3760%\n",
      "Epoch [2/300], Step [63/225], Training Accuracy: 31.0020%, Training Loss: 1.3763%\n",
      "Epoch [2/300], Step [64/225], Training Accuracy: 31.1035%, Training Loss: 1.3762%\n",
      "Epoch [2/300], Step [65/225], Training Accuracy: 31.0337%, Training Loss: 1.3763%\n",
      "Epoch [2/300], Step [66/225], Training Accuracy: 31.1316%, Training Loss: 1.3761%\n",
      "Epoch [2/300], Step [67/225], Training Accuracy: 31.1101%, Training Loss: 1.3760%\n",
      "Epoch [2/300], Step [68/225], Training Accuracy: 31.0662%, Training Loss: 1.3759%\n",
      "Epoch [2/300], Step [69/225], Training Accuracy: 31.2274%, Training Loss: 1.3757%\n",
      "Epoch [2/300], Step [70/225], Training Accuracy: 31.2277%, Training Loss: 1.3757%\n",
      "Epoch [2/300], Step [71/225], Training Accuracy: 31.2060%, Training Loss: 1.3758%\n",
      "Epoch [2/300], Step [72/225], Training Accuracy: 31.2066%, Training Loss: 1.3760%\n",
      "Epoch [2/300], Step [73/225], Training Accuracy: 31.2500%, Training Loss: 1.3757%\n",
      "Epoch [2/300], Step [74/225], Training Accuracy: 31.3556%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [75/225], Training Accuracy: 31.4167%, Training Loss: 1.3753%\n",
      "Epoch [2/300], Step [76/225], Training Accuracy: 31.3322%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [77/225], Training Accuracy: 31.2500%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [78/225], Training Accuracy: 31.3101%, Training Loss: 1.3756%\n",
      "Epoch [2/300], Step [79/225], Training Accuracy: 31.2896%, Training Loss: 1.3756%\n",
      "Epoch [2/300], Step [80/225], Training Accuracy: 31.1914%, Training Loss: 1.3757%\n",
      "Epoch [2/300], Step [81/225], Training Accuracy: 31.2886%, Training Loss: 1.3756%\n",
      "Epoch [2/300], Step [82/225], Training Accuracy: 31.2881%, Training Loss: 1.3755%\n",
      "Epoch [2/300], Step [83/225], Training Accuracy: 31.2123%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [84/225], Training Accuracy: 31.2314%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [85/225], Training Accuracy: 31.2684%, Training Loss: 1.3753%\n",
      "Epoch [2/300], Step [86/225], Training Accuracy: 31.2682%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [87/225], Training Accuracy: 31.2680%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [88/225], Training Accuracy: 31.1790%, Training Loss: 1.3753%\n",
      "Epoch [2/300], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [90/225], Training Accuracy: 31.1285%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [91/225], Training Accuracy: 31.1470%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [92/225], Training Accuracy: 31.1990%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [93/225], Training Accuracy: 31.0988%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [94/225], Training Accuracy: 31.1004%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [95/225], Training Accuracy: 31.1020%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [96/225], Training Accuracy: 31.0547%, Training Loss: 1.3753%\n",
      "Epoch [2/300], Step [97/225], Training Accuracy: 31.0567%, Training Loss: 1.3753%\n",
      "Epoch [2/300], Step [98/225], Training Accuracy: 31.0587%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [99/225], Training Accuracy: 31.0764%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [100/225], Training Accuracy: 31.1250%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [101/225], Training Accuracy: 31.0644%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [102/225], Training Accuracy: 31.0968%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [103/225], Training Accuracy: 31.1438%, Training Loss: 1.3753%\n",
      "Epoch [2/300], Step [104/225], Training Accuracy: 31.1298%, Training Loss: 1.3755%\n",
      "Epoch [2/300], Step [105/225], Training Accuracy: 31.0863%, Training Loss: 1.3754%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [106/225], Training Accuracy: 30.9994%, Training Loss: 1.3755%\n",
      "Epoch [2/300], Step [107/225], Training Accuracy: 30.9725%, Training Loss: 1.3755%\n",
      "Epoch [2/300], Step [108/225], Training Accuracy: 30.9896%, Training Loss: 1.3755%\n",
      "Epoch [2/300], Step [109/225], Training Accuracy: 30.9776%, Training Loss: 1.3754%\n",
      "Epoch [2/300], Step [110/225], Training Accuracy: 31.0227%, Training Loss: 1.3752%\n",
      "Epoch [2/300], Step [111/225], Training Accuracy: 31.0670%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [112/225], Training Accuracy: 31.1523%, Training Loss: 1.3749%\n",
      "Epoch [2/300], Step [113/225], Training Accuracy: 31.1117%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [114/225], Training Accuracy: 31.0992%, Training Loss: 1.3749%\n",
      "Epoch [2/300], Step [115/225], Training Accuracy: 31.0870%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [116/225], Training Accuracy: 31.1422%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [117/225], Training Accuracy: 31.0897%, Training Loss: 1.3749%\n",
      "Epoch [2/300], Step [118/225], Training Accuracy: 31.1573%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [119/225], Training Accuracy: 31.1712%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [120/225], Training Accuracy: 31.1849%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [121/225], Training Accuracy: 31.0950%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [122/225], Training Accuracy: 31.1347%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [123/225], Training Accuracy: 31.0976%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [124/225], Training Accuracy: 31.1114%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [125/225], Training Accuracy: 31.0250%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [126/225], Training Accuracy: 30.9400%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [127/225], Training Accuracy: 30.9424%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [128/225], Training Accuracy: 30.9326%, Training Loss: 1.3749%\n",
      "Epoch [2/300], Step [129/225], Training Accuracy: 30.9109%, Training Loss: 1.3750%\n",
      "Epoch [2/300], Step [130/225], Training Accuracy: 30.8534%, Training Loss: 1.3750%\n",
      "Epoch [2/300], Step [131/225], Training Accuracy: 30.8445%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [132/225], Training Accuracy: 30.8475%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [133/225], Training Accuracy: 30.8388%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [134/225], Training Accuracy: 30.8069%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [135/225], Training Accuracy: 30.7755%, Training Loss: 1.3751%\n",
      "Epoch [2/300], Step [136/225], Training Accuracy: 30.7790%, Training Loss: 1.3750%\n",
      "Epoch [2/300], Step [137/225], Training Accuracy: 30.8508%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [138/225], Training Accuracy: 30.8311%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [139/225], Training Accuracy: 30.8341%, Training Loss: 1.3748%\n",
      "Epoch [2/300], Step [140/225], Training Accuracy: 30.8705%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [141/225], Training Accuracy: 30.9397%, Training Loss: 1.3745%\n",
      "Epoch [2/300], Step [142/225], Training Accuracy: 30.8869%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [143/225], Training Accuracy: 30.7911%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [144/225], Training Accuracy: 30.8051%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [145/225], Training Accuracy: 30.8297%, Training Loss: 1.3745%\n",
      "Epoch [2/300], Step [146/225], Training Accuracy: 30.8647%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [147/225], Training Accuracy: 30.8248%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [148/225], Training Accuracy: 30.8805%, Training Loss: 1.3745%\n",
      "Epoch [2/300], Step [149/225], Training Accuracy: 30.8515%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [150/225], Training Accuracy: 30.8229%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [151/225], Training Accuracy: 30.8464%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [152/225], Training Accuracy: 30.8285%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [153/225], Training Accuracy: 30.8721%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [154/225], Training Accuracy: 30.8340%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [155/225], Training Accuracy: 30.8266%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [156/225], Training Accuracy: 30.8093%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [157/225], Training Accuracy: 30.7822%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [158/225], Training Accuracy: 30.7951%, Training Loss: 1.3747%\n",
      "Epoch [2/300], Step [159/225], Training Accuracy: 30.7685%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [160/225], Training Accuracy: 30.7520%, Training Loss: 1.3746%\n",
      "Epoch [2/300], Step [161/225], Training Accuracy: 30.7842%, Training Loss: 1.3745%\n",
      "Epoch [2/300], Step [162/225], Training Accuracy: 30.8256%, Training Loss: 1.3744%\n",
      "Epoch [2/300], Step [163/225], Training Accuracy: 30.7899%, Training Loss: 1.3744%\n",
      "Epoch [2/300], Step [164/225], Training Accuracy: 30.8594%, Training Loss: 1.3743%\n",
      "Epoch [2/300], Step [165/225], Training Accuracy: 30.8239%, Training Loss: 1.3742%\n",
      "Epoch [2/300], Step [166/225], Training Accuracy: 30.8735%, Training Loss: 1.3740%\n",
      "Epoch [2/300], Step [167/225], Training Accuracy: 30.8945%, Training Loss: 1.3740%\n",
      "Epoch [2/300], Step [168/225], Training Accuracy: 30.9245%, Training Loss: 1.3739%\n",
      "Epoch [2/300], Step [169/225], Training Accuracy: 30.9634%, Training Loss: 1.3739%\n",
      "Epoch [2/300], Step [170/225], Training Accuracy: 30.9743%, Training Loss: 1.3738%\n",
      "Epoch [2/300], Step [171/225], Training Accuracy: 31.0398%, Training Loss: 1.3737%\n",
      "Epoch [2/300], Step [172/225], Training Accuracy: 31.0229%, Training Loss: 1.3737%\n",
      "Epoch [2/300], Step [173/225], Training Accuracy: 31.0152%, Training Loss: 1.3736%\n",
      "Epoch [2/300], Step [174/225], Training Accuracy: 30.9716%, Training Loss: 1.3736%\n",
      "Epoch [2/300], Step [175/225], Training Accuracy: 30.9732%, Training Loss: 1.3736%\n",
      "Epoch [2/300], Step [176/225], Training Accuracy: 30.9837%, Training Loss: 1.3736%\n",
      "Epoch [2/300], Step [177/225], Training Accuracy: 30.9763%, Training Loss: 1.3736%\n",
      "Epoch [2/300], Step [178/225], Training Accuracy: 30.9954%, Training Loss: 1.3735%\n",
      "Epoch [2/300], Step [179/225], Training Accuracy: 30.9881%, Training Loss: 1.3734%\n",
      "Epoch [2/300], Step [180/225], Training Accuracy: 31.0764%, Training Loss: 1.3731%\n",
      "Epoch [2/300], Step [181/225], Training Accuracy: 31.0687%, Training Loss: 1.3731%\n",
      "Epoch [2/300], Step [182/225], Training Accuracy: 31.0869%, Training Loss: 1.3730%\n",
      "Epoch [2/300], Step [183/225], Training Accuracy: 31.1134%, Training Loss: 1.3729%\n",
      "Epoch [2/300], Step [184/225], Training Accuracy: 31.1396%, Training Loss: 1.3729%\n",
      "Epoch [2/300], Step [185/225], Training Accuracy: 31.1824%, Training Loss: 1.3728%\n",
      "Epoch [2/300], Step [186/225], Training Accuracy: 31.2500%, Training Loss: 1.3725%\n",
      "Epoch [2/300], Step [187/225], Training Accuracy: 31.2751%, Training Loss: 1.3724%\n",
      "Epoch [2/300], Step [188/225], Training Accuracy: 31.2832%, Training Loss: 1.3724%\n",
      "Epoch [2/300], Step [189/225], Training Accuracy: 31.2996%, Training Loss: 1.3722%\n",
      "Epoch [2/300], Step [190/225], Training Accuracy: 31.3158%, Training Loss: 1.3722%\n",
      "Epoch [2/300], Step [191/225], Training Accuracy: 31.3073%, Training Loss: 1.3722%\n",
      "Epoch [2/300], Step [192/225], Training Accuracy: 31.2907%, Training Loss: 1.3721%\n",
      "Epoch [2/300], Step [193/225], Training Accuracy: 31.3472%, Training Loss: 1.3720%\n",
      "Epoch [2/300], Step [194/225], Training Accuracy: 31.3869%, Training Loss: 1.3718%\n",
      "Epoch [2/300], Step [195/225], Training Accuracy: 31.3622%, Training Loss: 1.3718%\n",
      "Epoch [2/300], Step [196/225], Training Accuracy: 31.3536%, Training Loss: 1.3719%\n",
      "Epoch [2/300], Step [197/225], Training Accuracy: 31.4086%, Training Loss: 1.3719%\n",
      "Epoch [2/300], Step [198/225], Training Accuracy: 31.4394%, Training Loss: 1.3717%\n",
      "Epoch [2/300], Step [199/225], Training Accuracy: 31.4698%, Training Loss: 1.3715%\n",
      "Epoch [2/300], Step [200/225], Training Accuracy: 31.5078%, Training Loss: 1.3714%\n",
      "Epoch [2/300], Step [201/225], Training Accuracy: 31.5299%, Training Loss: 1.3714%\n",
      "Epoch [2/300], Step [202/225], Training Accuracy: 31.5207%, Training Loss: 1.3714%\n",
      "Epoch [2/300], Step [203/225], Training Accuracy: 31.5040%, Training Loss: 1.3714%\n",
      "Epoch [2/300], Step [204/225], Training Accuracy: 31.5334%, Training Loss: 1.3713%\n",
      "Epoch [2/300], Step [205/225], Training Accuracy: 31.5168%, Training Loss: 1.3713%\n",
      "Epoch [2/300], Step [206/225], Training Accuracy: 31.4927%, Training Loss: 1.3713%\n",
      "Epoch [2/300], Step [207/225], Training Accuracy: 31.5142%, Training Loss: 1.3714%\n",
      "Epoch [2/300], Step [208/225], Training Accuracy: 31.5430%, Training Loss: 1.3711%\n",
      "Epoch [2/300], Step [209/225], Training Accuracy: 31.5191%, Training Loss: 1.3711%\n",
      "Epoch [2/300], Step [210/225], Training Accuracy: 31.5253%, Training Loss: 1.3709%\n",
      "Epoch [2/300], Step [211/225], Training Accuracy: 31.5462%, Training Loss: 1.3708%\n",
      "Epoch [2/300], Step [212/225], Training Accuracy: 31.5522%, Training Loss: 1.3707%\n",
      "Epoch [2/300], Step [213/225], Training Accuracy: 31.5581%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [214/225], Training Accuracy: 31.5421%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [215/225], Training Accuracy: 31.5988%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [216/225], Training Accuracy: 31.6117%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [217/225], Training Accuracy: 31.6244%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [218/225], Training Accuracy: 31.6012%, Training Loss: 1.3705%\n",
      "Epoch [2/300], Step [219/225], Training Accuracy: 31.5925%, Training Loss: 1.3705%\n",
      "Epoch [2/300], Step [220/225], Training Accuracy: 31.6335%, Training Loss: 1.3704%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [221/225], Training Accuracy: 31.6035%, Training Loss: 1.3705%\n",
      "Epoch [2/300], Step [222/225], Training Accuracy: 31.6512%, Training Loss: 1.3704%\n",
      "Epoch [2/300], Step [223/225], Training Accuracy: 31.6284%, Training Loss: 1.3705%\n",
      "Epoch [2/300], Step [224/225], Training Accuracy: 31.5918%, Training Loss: 1.3706%\n",
      "Epoch [2/300], Step [225/225], Training Accuracy: 31.6078%, Training Loss: 1.3707%\n",
      "Epoch [3/300], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 1.3550%\n",
      "Epoch [3/300], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 1.3526%\n",
      "Epoch [3/300], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 1.3554%\n",
      "Epoch [3/300], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 1.3507%\n",
      "Epoch [3/300], Step [5/225], Training Accuracy: 31.2500%, Training Loss: 1.3586%\n",
      "Epoch [3/300], Step [6/225], Training Accuracy: 30.4688%, Training Loss: 1.3606%\n",
      "Epoch [3/300], Step [7/225], Training Accuracy: 31.0268%, Training Loss: 1.3587%\n",
      "Epoch [3/300], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 1.3577%\n",
      "Epoch [3/300], Step [9/225], Training Accuracy: 30.9028%, Training Loss: 1.3620%\n",
      "Epoch [3/300], Step [10/225], Training Accuracy: 31.5625%, Training Loss: 1.3606%\n",
      "Epoch [3/300], Step [11/225], Training Accuracy: 31.6761%, Training Loss: 1.3604%\n",
      "Epoch [3/300], Step [12/225], Training Accuracy: 32.2917%, Training Loss: 1.3590%\n",
      "Epoch [3/300], Step [13/225], Training Accuracy: 32.0913%, Training Loss: 1.3590%\n",
      "Epoch [3/300], Step [14/225], Training Accuracy: 31.9196%, Training Loss: 1.3615%\n",
      "Epoch [3/300], Step [15/225], Training Accuracy: 31.5625%, Training Loss: 1.3632%\n",
      "Epoch [3/300], Step [16/225], Training Accuracy: 31.8359%, Training Loss: 1.3612%\n",
      "Epoch [3/300], Step [17/225], Training Accuracy: 32.2610%, Training Loss: 1.3612%\n",
      "Epoch [3/300], Step [18/225], Training Accuracy: 31.9444%, Training Loss: 1.3621%\n",
      "Epoch [3/300], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 1.3634%\n",
      "Epoch [3/300], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 1.3632%\n",
      "Epoch [3/300], Step [21/225], Training Accuracy: 32.3661%, Training Loss: 1.3606%\n",
      "Epoch [3/300], Step [22/225], Training Accuracy: 32.3153%, Training Loss: 1.3599%\n",
      "Epoch [3/300], Step [23/225], Training Accuracy: 32.6766%, Training Loss: 1.3588%\n",
      "Epoch [3/300], Step [24/225], Training Accuracy: 32.2266%, Training Loss: 1.3598%\n",
      "Epoch [3/300], Step [25/225], Training Accuracy: 32.6250%, Training Loss: 1.3590%\n",
      "Epoch [3/300], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 1.3591%\n",
      "Epoch [3/300], Step [27/225], Training Accuracy: 32.4653%, Training Loss: 1.3577%\n",
      "Epoch [3/300], Step [28/225], Training Accuracy: 32.7009%, Training Loss: 1.3578%\n",
      "Epoch [3/300], Step [29/225], Training Accuracy: 32.8125%, Training Loss: 1.3558%\n",
      "Epoch [3/300], Step [30/225], Training Accuracy: 33.0729%, Training Loss: 1.3552%\n",
      "Epoch [3/300], Step [31/225], Training Accuracy: 33.2157%, Training Loss: 1.3547%\n",
      "Epoch [3/300], Step [32/225], Training Accuracy: 33.0078%, Training Loss: 1.3549%\n",
      "Epoch [3/300], Step [33/225], Training Accuracy: 33.0966%, Training Loss: 1.3537%\n",
      "Epoch [3/300], Step [34/225], Training Accuracy: 33.0882%, Training Loss: 1.3534%\n",
      "Epoch [3/300], Step [35/225], Training Accuracy: 32.9911%, Training Loss: 1.3533%\n",
      "Epoch [3/300], Step [36/225], Training Accuracy: 32.8993%, Training Loss: 1.3547%\n",
      "Epoch [3/300], Step [37/225], Training Accuracy: 32.8547%, Training Loss: 1.3548%\n",
      "Epoch [3/300], Step [38/225], Training Accuracy: 33.1414%, Training Loss: 1.3544%\n",
      "Epoch [3/300], Step [39/225], Training Accuracy: 33.2532%, Training Loss: 1.3548%\n",
      "Epoch [3/300], Step [40/225], Training Accuracy: 33.3203%, Training Loss: 1.3545%\n",
      "Epoch [3/300], Step [41/225], Training Accuracy: 33.2317%, Training Loss: 1.3547%\n",
      "Epoch [3/300], Step [42/225], Training Accuracy: 32.9985%, Training Loss: 1.3555%\n",
      "Epoch [3/300], Step [43/225], Training Accuracy: 33.0305%, Training Loss: 1.3555%\n",
      "Epoch [3/300], Step [44/225], Training Accuracy: 33.1321%, Training Loss: 1.3551%\n",
      "Epoch [3/300], Step [45/225], Training Accuracy: 33.2292%, Training Loss: 1.3547%\n",
      "Epoch [3/300], Step [46/225], Training Accuracy: 33.0503%, Training Loss: 1.3551%\n",
      "Epoch [3/300], Step [47/225], Training Accuracy: 33.0120%, Training Loss: 1.3553%\n",
      "Epoch [3/300], Step [48/225], Training Accuracy: 32.9102%, Training Loss: 1.3549%\n",
      "Epoch [3/300], Step [49/225], Training Accuracy: 33.1633%, Training Loss: 1.3549%\n",
      "Epoch [3/300], Step [50/225], Training Accuracy: 33.1562%, Training Loss: 1.3547%\n",
      "Epoch [3/300], Step [51/225], Training Accuracy: 33.0576%, Training Loss: 1.3545%\n",
      "Epoch [3/300], Step [52/225], Training Accuracy: 32.9928%, Training Loss: 1.3547%\n",
      "Epoch [3/300], Step [53/225], Training Accuracy: 33.0483%, Training Loss: 1.3546%\n",
      "Epoch [3/300], Step [54/225], Training Accuracy: 32.9282%, Training Loss: 1.3549%\n",
      "Epoch [3/300], Step [55/225], Training Accuracy: 32.9830%, Training Loss: 1.3548%\n",
      "Epoch [3/300], Step [56/225], Training Accuracy: 32.8962%, Training Loss: 1.3548%\n",
      "Epoch [3/300], Step [57/225], Training Accuracy: 33.0592%, Training Loss: 1.3534%\n",
      "Epoch [3/300], Step [58/225], Training Accuracy: 33.0819%, Training Loss: 1.3529%\n",
      "Epoch [3/300], Step [59/225], Training Accuracy: 33.2627%, Training Loss: 1.3519%\n",
      "Epoch [3/300], Step [60/225], Training Accuracy: 33.3073%, Training Loss: 1.3509%\n",
      "Epoch [3/300], Step [61/225], Training Accuracy: 33.3504%, Training Loss: 1.3506%\n",
      "Epoch [3/300], Step [62/225], Training Accuracy: 33.3669%, Training Loss: 1.3507%\n",
      "Epoch [3/300], Step [63/225], Training Accuracy: 33.2837%, Training Loss: 1.3510%\n",
      "Epoch [3/300], Step [64/225], Training Accuracy: 33.4229%, Training Loss: 1.3507%\n",
      "Epoch [3/300], Step [65/225], Training Accuracy: 33.2692%, Training Loss: 1.3509%\n",
      "Epoch [3/300], Step [66/225], Training Accuracy: 33.3807%, Training Loss: 1.3502%\n",
      "Epoch [3/300], Step [67/225], Training Accuracy: 33.3722%, Training Loss: 1.3497%\n",
      "Epoch [3/300], Step [68/225], Training Accuracy: 33.2721%, Training Loss: 1.3496%\n",
      "Epoch [3/300], Step [69/225], Training Accuracy: 33.2880%, Training Loss: 1.3488%\n",
      "Epoch [3/300], Step [70/225], Training Accuracy: 33.2589%, Training Loss: 1.3490%\n",
      "Epoch [3/300], Step [71/225], Training Accuracy: 33.3187%, Training Loss: 1.3488%\n",
      "Epoch [3/300], Step [72/225], Training Accuracy: 33.3333%, Training Loss: 1.3495%\n",
      "Epoch [3/300], Step [73/225], Training Accuracy: 33.3262%, Training Loss: 1.3493%\n",
      "Epoch [3/300], Step [74/225], Training Accuracy: 33.3404%, Training Loss: 1.3485%\n",
      "Epoch [3/300], Step [75/225], Training Accuracy: 33.3958%, Training Loss: 1.3480%\n",
      "Epoch [3/300], Step [76/225], Training Accuracy: 33.3059%, Training Loss: 1.3482%\n",
      "Epoch [3/300], Step [77/225], Training Accuracy: 33.1778%, Training Loss: 1.3487%\n",
      "Epoch [3/300], Step [78/225], Training Accuracy: 33.2131%, Training Loss: 1.3487%\n",
      "Epoch [3/300], Step [79/225], Training Accuracy: 33.2278%, Training Loss: 1.3488%\n",
      "Epoch [3/300], Step [80/225], Training Accuracy: 33.1250%, Training Loss: 1.3486%\n",
      "Epoch [3/300], Step [81/225], Training Accuracy: 33.1597%, Training Loss: 1.3485%\n",
      "Epoch [3/300], Step [82/225], Training Accuracy: 33.2127%, Training Loss: 1.3481%\n",
      "Epoch [3/300], Step [83/225], Training Accuracy: 33.1325%, Training Loss: 1.3478%\n",
      "Epoch [3/300], Step [84/225], Training Accuracy: 33.1287%, Training Loss: 1.3477%\n",
      "Epoch [3/300], Step [85/225], Training Accuracy: 33.1250%, Training Loss: 1.3480%\n",
      "Epoch [3/300], Step [86/225], Training Accuracy: 33.1577%, Training Loss: 1.3479%\n",
      "Epoch [3/300], Step [87/225], Training Accuracy: 33.1717%, Training Loss: 1.3477%\n",
      "Epoch [3/300], Step [88/225], Training Accuracy: 33.0611%, Training Loss: 1.3480%\n",
      "Epoch [3/300], Step [89/225], Training Accuracy: 33.1110%, Training Loss: 1.3479%\n",
      "Epoch [3/300], Step [90/225], Training Accuracy: 33.1597%, Training Loss: 1.3475%\n",
      "Epoch [3/300], Step [91/225], Training Accuracy: 33.1731%, Training Loss: 1.3477%\n",
      "Epoch [3/300], Step [92/225], Training Accuracy: 33.2541%, Training Loss: 1.3476%\n",
      "Epoch [3/300], Step [93/225], Training Accuracy: 33.1989%, Training Loss: 1.3473%\n",
      "Epoch [3/300], Step [94/225], Training Accuracy: 33.1782%, Training Loss: 1.3474%\n",
      "Epoch [3/300], Step [95/225], Training Accuracy: 33.1250%, Training Loss: 1.3478%\n",
      "Epoch [3/300], Step [96/225], Training Accuracy: 33.1380%, Training Loss: 1.3476%\n",
      "Epoch [3/300], Step [97/225], Training Accuracy: 33.1830%, Training Loss: 1.3475%\n",
      "Epoch [3/300], Step [98/225], Training Accuracy: 33.1314%, Training Loss: 1.3473%\n",
      "Epoch [3/300], Step [99/225], Training Accuracy: 33.1124%, Training Loss: 1.3474%\n",
      "Epoch [3/300], Step [100/225], Training Accuracy: 33.0625%, Training Loss: 1.3473%\n",
      "Epoch [3/300], Step [101/225], Training Accuracy: 33.0600%, Training Loss: 1.3473%\n",
      "Epoch [3/300], Step [102/225], Training Accuracy: 33.1648%, Training Loss: 1.3471%\n",
      "Epoch [3/300], Step [103/225], Training Accuracy: 33.1614%, Training Loss: 1.3474%\n",
      "Epoch [3/300], Step [104/225], Training Accuracy: 33.1881%, Training Loss: 1.3476%\n",
      "Epoch [3/300], Step [105/225], Training Accuracy: 33.1696%, Training Loss: 1.3474%\n",
      "Epoch [3/300], Step [106/225], Training Accuracy: 33.0926%, Training Loss: 1.3473%\n",
      "Epoch [3/300], Step [107/225], Training Accuracy: 33.0900%, Training Loss: 1.3473%\n",
      "Epoch [3/300], Step [108/225], Training Accuracy: 33.0584%, Training Loss: 1.3471%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [109/225], Training Accuracy: 33.0275%, Training Loss: 1.3470%\n",
      "Epoch [3/300], Step [110/225], Training Accuracy: 33.0966%, Training Loss: 1.3466%\n",
      "Epoch [3/300], Step [111/225], Training Accuracy: 33.1222%, Training Loss: 1.3463%\n",
      "Epoch [3/300], Step [112/225], Training Accuracy: 33.1613%, Training Loss: 1.3460%\n",
      "Epoch [3/300], Step [113/225], Training Accuracy: 33.1444%, Training Loss: 1.3463%\n",
      "Epoch [3/300], Step [114/225], Training Accuracy: 33.1689%, Training Loss: 1.3457%\n",
      "Epoch [3/300], Step [115/225], Training Accuracy: 33.1658%, Training Loss: 1.3455%\n",
      "Epoch [3/300], Step [116/225], Training Accuracy: 33.2435%, Training Loss: 1.3450%\n",
      "Epoch [3/300], Step [117/225], Training Accuracy: 33.1998%, Training Loss: 1.3457%\n",
      "Epoch [3/300], Step [118/225], Training Accuracy: 33.2495%, Training Loss: 1.3455%\n",
      "Epoch [3/300], Step [119/225], Training Accuracy: 33.2589%, Training Loss: 1.3455%\n",
      "Epoch [3/300], Step [120/225], Training Accuracy: 33.2943%, Training Loss: 1.3456%\n",
      "Epoch [3/300], Step [121/225], Training Accuracy: 33.2903%, Training Loss: 1.3455%\n",
      "Epoch [3/300], Step [122/225], Training Accuracy: 33.3760%, Training Loss: 1.3454%\n",
      "Epoch [3/300], Step [123/225], Training Accuracy: 33.3460%, Training Loss: 1.3454%\n",
      "Epoch [3/300], Step [124/225], Training Accuracy: 33.3921%, Training Loss: 1.3452%\n",
      "Epoch [3/300], Step [125/225], Training Accuracy: 33.2750%, Training Loss: 1.3458%\n",
      "Epoch [3/300], Step [126/225], Training Accuracy: 33.2589%, Training Loss: 1.3458%\n",
      "Epoch [3/300], Step [127/225], Training Accuracy: 33.2431%, Training Loss: 1.3456%\n",
      "Epoch [3/300], Step [128/225], Training Accuracy: 33.2275%, Training Loss: 1.3457%\n",
      "Epoch [3/300], Step [129/225], Training Accuracy: 33.2243%, Training Loss: 1.3458%\n",
      "Epoch [3/300], Step [130/225], Training Accuracy: 33.1370%, Training Loss: 1.3460%\n",
      "Epoch [3/300], Step [131/225], Training Accuracy: 33.1703%, Training Loss: 1.3460%\n",
      "Epoch [3/300], Step [132/225], Training Accuracy: 33.1439%, Training Loss: 1.3459%\n",
      "Epoch [3/300], Step [133/225], Training Accuracy: 33.1062%, Training Loss: 1.3461%\n",
      "Epoch [3/300], Step [134/225], Training Accuracy: 33.1040%, Training Loss: 1.3460%\n",
      "Epoch [3/300], Step [135/225], Training Accuracy: 33.1019%, Training Loss: 1.3459%\n",
      "Epoch [3/300], Step [136/225], Training Accuracy: 33.1572%, Training Loss: 1.3456%\n",
      "Epoch [3/300], Step [137/225], Training Accuracy: 33.2231%, Training Loss: 1.3451%\n",
      "Epoch [3/300], Step [138/225], Training Accuracy: 33.2314%, Training Loss: 1.3450%\n",
      "Epoch [3/300], Step [139/225], Training Accuracy: 33.1947%, Training Loss: 1.3450%\n",
      "Epoch [3/300], Step [140/225], Training Accuracy: 33.2589%, Training Loss: 1.3445%\n",
      "Epoch [3/300], Step [141/225], Training Accuracy: 33.3444%, Training Loss: 1.3441%\n",
      "Epoch [3/300], Step [142/225], Training Accuracy: 33.3187%, Training Loss: 1.3440%\n",
      "Epoch [3/300], Step [143/225], Training Accuracy: 33.3260%, Training Loss: 1.3438%\n",
      "Epoch [3/300], Step [144/225], Training Accuracy: 33.3767%, Training Loss: 1.3437%\n",
      "Epoch [3/300], Step [145/225], Training Accuracy: 33.4483%, Training Loss: 1.3434%\n",
      "Epoch [3/300], Step [146/225], Training Accuracy: 33.5081%, Training Loss: 1.3433%\n",
      "Epoch [3/300], Step [147/225], Training Accuracy: 33.5140%, Training Loss: 1.3431%\n",
      "Epoch [3/300], Step [148/225], Training Accuracy: 33.5832%, Training Loss: 1.3428%\n",
      "Epoch [3/300], Step [149/225], Training Accuracy: 33.5990%, Training Loss: 1.3429%\n",
      "Epoch [3/300], Step [150/225], Training Accuracy: 33.5625%, Training Loss: 1.3430%\n",
      "Epoch [3/300], Step [151/225], Training Accuracy: 33.5886%, Training Loss: 1.3427%\n",
      "Epoch [3/300], Step [152/225], Training Accuracy: 33.6554%, Training Loss: 1.3424%\n",
      "Epoch [3/300], Step [153/225], Training Accuracy: 33.7010%, Training Loss: 1.3420%\n",
      "Epoch [3/300], Step [154/225], Training Accuracy: 33.6648%, Training Loss: 1.3421%\n",
      "Epoch [3/300], Step [155/225], Training Accuracy: 33.6593%, Training Loss: 1.3420%\n",
      "Epoch [3/300], Step [156/225], Training Accuracy: 33.6639%, Training Loss: 1.3420%\n",
      "Epoch [3/300], Step [157/225], Training Accuracy: 33.6783%, Training Loss: 1.3418%\n",
      "Epoch [3/300], Step [158/225], Training Accuracy: 33.7025%, Training Loss: 1.3416%\n",
      "Epoch [3/300], Step [159/225], Training Accuracy: 33.6773%, Training Loss: 1.3415%\n",
      "Epoch [3/300], Step [160/225], Training Accuracy: 33.6426%, Training Loss: 1.3415%\n",
      "Epoch [3/300], Step [161/225], Training Accuracy: 33.7151%, Training Loss: 1.3409%\n",
      "Epoch [3/300], Step [162/225], Training Accuracy: 33.7384%, Training Loss: 1.3407%\n",
      "Epoch [3/300], Step [163/225], Training Accuracy: 33.7423%, Training Loss: 1.3404%\n",
      "Epoch [3/300], Step [164/225], Training Accuracy: 33.8415%, Training Loss: 1.3401%\n",
      "Epoch [3/300], Step [165/225], Training Accuracy: 33.8542%, Training Loss: 1.3399%\n",
      "Epoch [3/300], Step [166/225], Training Accuracy: 33.8761%, Training Loss: 1.3396%\n",
      "Epoch [3/300], Step [167/225], Training Accuracy: 33.8978%, Training Loss: 1.3395%\n",
      "Epoch [3/300], Step [168/225], Training Accuracy: 33.9193%, Training Loss: 1.3394%\n",
      "Epoch [3/300], Step [169/225], Training Accuracy: 33.9682%, Training Loss: 1.3393%\n",
      "Epoch [3/300], Step [170/225], Training Accuracy: 33.9706%, Training Loss: 1.3391%\n",
      "Epoch [3/300], Step [171/225], Training Accuracy: 33.9912%, Training Loss: 1.3387%\n",
      "Epoch [3/300], Step [172/225], Training Accuracy: 34.0207%, Training Loss: 1.3385%\n",
      "Epoch [3/300], Step [173/225], Training Accuracy: 33.9686%, Training Loss: 1.3385%\n",
      "Epoch [3/300], Step [174/225], Training Accuracy: 33.9799%, Training Loss: 1.3384%\n",
      "Epoch [3/300], Step [175/225], Training Accuracy: 33.9554%, Training Loss: 1.3383%\n",
      "Epoch [3/300], Step [176/225], Training Accuracy: 33.9400%, Training Loss: 1.3383%\n",
      "Epoch [3/300], Step [177/225], Training Accuracy: 33.9248%, Training Loss: 1.3383%\n",
      "Epoch [3/300], Step [178/225], Training Accuracy: 33.9712%, Training Loss: 1.3381%\n",
      "Epoch [3/300], Step [179/225], Training Accuracy: 33.9909%, Training Loss: 1.3379%\n",
      "Epoch [3/300], Step [180/225], Training Accuracy: 34.0972%, Training Loss: 1.3373%\n",
      "Epoch [3/300], Step [181/225], Training Accuracy: 34.0901%, Training Loss: 1.3373%\n",
      "Epoch [3/300], Step [182/225], Training Accuracy: 34.1346%, Training Loss: 1.3372%\n",
      "Epoch [3/300], Step [183/225], Training Accuracy: 34.1445%, Training Loss: 1.3367%\n",
      "Epoch [3/300], Step [184/225], Training Accuracy: 34.1712%, Training Loss: 1.3366%\n",
      "Epoch [3/300], Step [185/225], Training Accuracy: 34.1723%, Training Loss: 1.3364%\n",
      "Epoch [3/300], Step [186/225], Training Accuracy: 34.2322%, Training Loss: 1.3357%\n",
      "Epoch [3/300], Step [187/225], Training Accuracy: 34.2497%, Training Loss: 1.3355%\n",
      "Epoch [3/300], Step [188/225], Training Accuracy: 34.3085%, Training Loss: 1.3352%\n",
      "Epoch [3/300], Step [189/225], Training Accuracy: 34.3171%, Training Loss: 1.3348%\n",
      "Epoch [3/300], Step [190/225], Training Accuracy: 34.3503%, Training Loss: 1.3348%\n",
      "Epoch [3/300], Step [191/225], Training Accuracy: 34.3750%, Training Loss: 1.3346%\n",
      "Epoch [3/300], Step [192/225], Training Accuracy: 34.3506%, Training Loss: 1.3346%\n",
      "Epoch [3/300], Step [193/225], Training Accuracy: 34.3669%, Training Loss: 1.3342%\n",
      "Epoch [3/300], Step [194/225], Training Accuracy: 34.3992%, Training Loss: 1.3336%\n",
      "Epoch [3/300], Step [195/225], Training Accuracy: 34.3750%, Training Loss: 1.3335%\n",
      "Epoch [3/300], Step [196/225], Training Accuracy: 34.3591%, Training Loss: 1.3334%\n",
      "Epoch [3/300], Step [197/225], Training Accuracy: 34.3512%, Training Loss: 1.3333%\n",
      "Epoch [3/300], Step [198/225], Training Accuracy: 34.3987%, Training Loss: 1.3328%\n",
      "Epoch [3/300], Step [199/225], Training Accuracy: 34.4692%, Training Loss: 1.3326%\n",
      "Epoch [3/300], Step [200/225], Training Accuracy: 34.5000%, Training Loss: 1.3324%\n",
      "Epoch [3/300], Step [201/225], Training Accuracy: 34.5149%, Training Loss: 1.3321%\n",
      "Epoch [3/300], Step [202/225], Training Accuracy: 34.5374%, Training Loss: 1.3319%\n",
      "Epoch [3/300], Step [203/225], Training Accuracy: 34.5366%, Training Loss: 1.3320%\n",
      "Epoch [3/300], Step [204/225], Training Accuracy: 34.5512%, Training Loss: 1.3315%\n",
      "Epoch [3/300], Step [205/225], Training Accuracy: 34.5274%, Training Loss: 1.3317%\n",
      "Epoch [3/300], Step [206/225], Training Accuracy: 34.5343%, Training Loss: 1.3317%\n",
      "Epoch [3/300], Step [207/225], Training Accuracy: 34.5486%, Training Loss: 1.3317%\n",
      "Epoch [3/300], Step [208/225], Training Accuracy: 34.6229%, Training Loss: 1.3312%\n",
      "Epoch [3/300], Step [209/225], Training Accuracy: 34.5843%, Training Loss: 1.3311%\n",
      "Epoch [3/300], Step [210/225], Training Accuracy: 34.5982%, Training Loss: 1.3306%\n",
      "Epoch [3/300], Step [211/225], Training Accuracy: 34.6712%, Training Loss: 1.3301%\n",
      "Epoch [3/300], Step [212/225], Training Accuracy: 34.6993%, Training Loss: 1.3298%\n",
      "Epoch [3/300], Step [213/225], Training Accuracy: 34.6978%, Training Loss: 1.3299%\n",
      "Epoch [3/300], Step [214/225], Training Accuracy: 34.6817%, Training Loss: 1.3297%\n",
      "Epoch [3/300], Step [215/225], Training Accuracy: 34.7093%, Training Loss: 1.3294%\n",
      "Epoch [3/300], Step [216/225], Training Accuracy: 34.6933%, Training Loss: 1.3295%\n",
      "Epoch [3/300], Step [217/225], Training Accuracy: 34.6918%, Training Loss: 1.3292%\n",
      "Epoch [3/300], Step [218/225], Training Accuracy: 34.6760%, Training Loss: 1.3291%\n",
      "Epoch [3/300], Step [219/225], Training Accuracy: 34.6747%, Training Loss: 1.3288%\n",
      "Epoch [3/300], Step [220/225], Training Accuracy: 34.6946%, Training Loss: 1.3285%\n",
      "Epoch [3/300], Step [221/225], Training Accuracy: 34.6719%, Training Loss: 1.3286%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [222/225], Training Accuracy: 34.6847%, Training Loss: 1.3285%\n",
      "Epoch [3/300], Step [223/225], Training Accuracy: 34.6623%, Training Loss: 1.3284%\n",
      "Epoch [3/300], Step [224/225], Training Accuracy: 34.6191%, Training Loss: 1.3283%\n",
      "Epoch [3/300], Step [225/225], Training Accuracy: 34.6304%, Training Loss: 1.3283%\n",
      "Epoch [4/300], Step [1/225], Training Accuracy: 40.6250%, Training Loss: 1.3006%\n",
      "Epoch [4/300], Step [2/225], Training Accuracy: 39.8438%, Training Loss: 1.3168%\n",
      "Epoch [4/300], Step [3/225], Training Accuracy: 35.9375%, Training Loss: 1.3237%\n",
      "Epoch [4/300], Step [4/225], Training Accuracy: 38.2812%, Training Loss: 1.3030%\n",
      "Epoch [4/300], Step [5/225], Training Accuracy: 39.0625%, Training Loss: 1.3097%\n",
      "Epoch [4/300], Step [6/225], Training Accuracy: 38.8021%, Training Loss: 1.3192%\n",
      "Epoch [4/300], Step [7/225], Training Accuracy: 38.1696%, Training Loss: 1.3141%\n",
      "Epoch [4/300], Step [8/225], Training Accuracy: 36.5234%, Training Loss: 1.3103%\n",
      "Epoch [4/300], Step [9/225], Training Accuracy: 35.7639%, Training Loss: 1.3183%\n",
      "Epoch [4/300], Step [10/225], Training Accuracy: 37.0312%, Training Loss: 1.3120%\n",
      "Epoch [4/300], Step [11/225], Training Accuracy: 37.2159%, Training Loss: 1.3135%\n",
      "Epoch [4/300], Step [12/225], Training Accuracy: 36.9792%, Training Loss: 1.3087%\n",
      "Epoch [4/300], Step [13/225], Training Accuracy: 37.7404%, Training Loss: 1.3082%\n",
      "Epoch [4/300], Step [14/225], Training Accuracy: 37.1652%, Training Loss: 1.3151%\n",
      "Epoch [4/300], Step [15/225], Training Accuracy: 37.1875%, Training Loss: 1.3164%\n",
      "Epoch [4/300], Step [16/225], Training Accuracy: 37.0117%, Training Loss: 1.3129%\n",
      "Epoch [4/300], Step [17/225], Training Accuracy: 36.7647%, Training Loss: 1.3115%\n",
      "Epoch [4/300], Step [18/225], Training Accuracy: 36.8056%, Training Loss: 1.3092%\n",
      "Epoch [4/300], Step [19/225], Training Accuracy: 36.4309%, Training Loss: 1.3122%\n",
      "Epoch [4/300], Step [20/225], Training Accuracy: 36.4062%, Training Loss: 1.3116%\n",
      "Epoch [4/300], Step [21/225], Training Accuracy: 36.9048%, Training Loss: 1.3075%\n",
      "Epoch [4/300], Step [22/225], Training Accuracy: 36.7188%, Training Loss: 1.3063%\n",
      "Epoch [4/300], Step [23/225], Training Accuracy: 37.0924%, Training Loss: 1.3033%\n",
      "Epoch [4/300], Step [24/225], Training Accuracy: 36.7839%, Training Loss: 1.3038%\n",
      "Epoch [4/300], Step [25/225], Training Accuracy: 36.9375%, Training Loss: 1.3017%\n",
      "Epoch [4/300], Step [26/225], Training Accuracy: 36.5986%, Training Loss: 1.3015%\n",
      "Epoch [4/300], Step [27/225], Training Accuracy: 36.8056%, Training Loss: 1.3005%\n",
      "Epoch [4/300], Step [28/225], Training Accuracy: 36.9420%, Training Loss: 1.2986%\n",
      "Epoch [4/300], Step [29/225], Training Accuracy: 37.0151%, Training Loss: 1.2952%\n",
      "Epoch [4/300], Step [30/225], Training Accuracy: 37.2396%, Training Loss: 1.2927%\n",
      "Epoch [4/300], Step [31/225], Training Accuracy: 37.1976%, Training Loss: 1.2925%\n",
      "Epoch [4/300], Step [32/225], Training Accuracy: 37.2070%, Training Loss: 1.2912%\n",
      "Epoch [4/300], Step [33/225], Training Accuracy: 37.4053%, Training Loss: 1.2893%\n",
      "Epoch [4/300], Step [34/225], Training Accuracy: 37.4540%, Training Loss: 1.2890%\n",
      "Epoch [4/300], Step [35/225], Training Accuracy: 37.3214%, Training Loss: 1.2899%\n",
      "Epoch [4/300], Step [36/225], Training Accuracy: 37.2830%, Training Loss: 1.2917%\n",
      "Epoch [4/300], Step [37/225], Training Accuracy: 37.3311%, Training Loss: 1.2917%\n",
      "Epoch [4/300], Step [38/225], Training Accuracy: 37.4589%, Training Loss: 1.2912%\n",
      "Epoch [4/300], Step [39/225], Training Accuracy: 37.6603%, Training Loss: 1.2903%\n",
      "Epoch [4/300], Step [40/225], Training Accuracy: 37.6562%, Training Loss: 1.2895%\n",
      "Epoch [4/300], Step [41/225], Training Accuracy: 37.6524%, Training Loss: 1.2895%\n",
      "Epoch [4/300], Step [42/225], Training Accuracy: 37.3140%, Training Loss: 1.2909%\n",
      "Epoch [4/300], Step [43/225], Training Accuracy: 37.1730%, Training Loss: 1.2911%\n",
      "Epoch [4/300], Step [44/225], Training Accuracy: 37.3935%, Training Loss: 1.2899%\n",
      "Epoch [4/300], Step [45/225], Training Accuracy: 37.4306%, Training Loss: 1.2891%\n",
      "Epoch [4/300], Step [46/225], Training Accuracy: 37.4660%, Training Loss: 1.2895%\n",
      "Epoch [4/300], Step [47/225], Training Accuracy: 37.2673%, Training Loss: 1.2892%\n",
      "Epoch [4/300], Step [48/225], Training Accuracy: 37.1419%, Training Loss: 1.2882%\n",
      "Epoch [4/300], Step [49/225], Training Accuracy: 37.1811%, Training Loss: 1.2888%\n",
      "Epoch [4/300], Step [50/225], Training Accuracy: 37.2500%, Training Loss: 1.2894%\n",
      "Epoch [4/300], Step [51/225], Training Accuracy: 37.1630%, Training Loss: 1.2896%\n",
      "Epoch [4/300], Step [52/225], Training Accuracy: 37.1995%, Training Loss: 1.2894%\n",
      "Epoch [4/300], Step [53/225], Training Accuracy: 37.2936%, Training Loss: 1.2893%\n",
      "Epoch [4/300], Step [54/225], Training Accuracy: 37.1817%, Training Loss: 1.2892%\n",
      "Epoch [4/300], Step [55/225], Training Accuracy: 37.1307%, Training Loss: 1.2889%\n",
      "Epoch [4/300], Step [56/225], Training Accuracy: 36.9420%, Training Loss: 1.2897%\n",
      "Epoch [4/300], Step [57/225], Training Accuracy: 37.1436%, Training Loss: 1.2873%\n",
      "Epoch [4/300], Step [58/225], Training Accuracy: 37.1767%, Training Loss: 1.2865%\n",
      "Epoch [4/300], Step [59/225], Training Accuracy: 37.2087%, Training Loss: 1.2843%\n",
      "Epoch [4/300], Step [60/225], Training Accuracy: 37.3438%, Training Loss: 1.2831%\n",
      "Epoch [4/300], Step [61/225], Training Accuracy: 37.3719%, Training Loss: 1.2827%\n",
      "Epoch [4/300], Step [62/225], Training Accuracy: 37.2480%, Training Loss: 1.2831%\n",
      "Epoch [4/300], Step [63/225], Training Accuracy: 37.1280%, Training Loss: 1.2840%\n",
      "Epoch [4/300], Step [64/225], Training Accuracy: 37.2803%, Training Loss: 1.2839%\n",
      "Epoch [4/300], Step [65/225], Training Accuracy: 37.2115%, Training Loss: 1.2837%\n",
      "Epoch [4/300], Step [66/225], Training Accuracy: 37.2633%, Training Loss: 1.2831%\n",
      "Epoch [4/300], Step [67/225], Training Accuracy: 37.3368%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [68/225], Training Accuracy: 37.2472%, Training Loss: 1.2825%\n",
      "Epoch [4/300], Step [69/225], Training Accuracy: 37.1603%, Training Loss: 1.2811%\n",
      "Epoch [4/300], Step [70/225], Training Accuracy: 37.1429%, Training Loss: 1.2815%\n",
      "Epoch [4/300], Step [71/225], Training Accuracy: 37.1039%, Training Loss: 1.2816%\n",
      "Epoch [4/300], Step [72/225], Training Accuracy: 37.1745%, Training Loss: 1.2834%\n",
      "Epoch [4/300], Step [73/225], Training Accuracy: 37.1147%, Training Loss: 1.2845%\n",
      "Epoch [4/300], Step [74/225], Training Accuracy: 37.1410%, Training Loss: 1.2834%\n",
      "Epoch [4/300], Step [75/225], Training Accuracy: 37.2917%, Training Loss: 1.2825%\n",
      "Epoch [4/300], Step [76/225], Training Accuracy: 37.2327%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [77/225], Training Accuracy: 37.2159%, Training Loss: 1.2831%\n",
      "Epoch [4/300], Step [78/225], Training Accuracy: 37.2196%, Training Loss: 1.2830%\n",
      "Epoch [4/300], Step [79/225], Training Accuracy: 37.1835%, Training Loss: 1.2838%\n",
      "Epoch [4/300], Step [80/225], Training Accuracy: 37.2461%, Training Loss: 1.2836%\n",
      "Epoch [4/300], Step [81/225], Training Accuracy: 37.2685%, Training Loss: 1.2838%\n",
      "Epoch [4/300], Step [82/225], Training Accuracy: 37.3476%, Training Loss: 1.2830%\n",
      "Epoch [4/300], Step [83/225], Training Accuracy: 37.3117%, Training Loss: 1.2825%\n",
      "Epoch [4/300], Step [84/225], Training Accuracy: 37.2396%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [85/225], Training Accuracy: 37.2059%, Training Loss: 1.2832%\n",
      "Epoch [4/300], Step [86/225], Training Accuracy: 37.1911%, Training Loss: 1.2834%\n",
      "Epoch [4/300], Step [87/225], Training Accuracy: 37.1767%, Training Loss: 1.2836%\n",
      "Epoch [4/300], Step [88/225], Training Accuracy: 37.1804%, Training Loss: 1.2838%\n",
      "Epoch [4/300], Step [89/225], Training Accuracy: 37.2015%, Training Loss: 1.2835%\n",
      "Epoch [4/300], Step [90/225], Training Accuracy: 37.2743%, Training Loss: 1.2827%\n",
      "Epoch [4/300], Step [91/225], Training Accuracy: 37.3111%, Training Loss: 1.2823%\n",
      "Epoch [4/300], Step [92/225], Training Accuracy: 37.3641%, Training Loss: 1.2823%\n",
      "Epoch [4/300], Step [93/225], Training Accuracy: 37.3656%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [94/225], Training Accuracy: 37.3836%, Training Loss: 1.2820%\n",
      "Epoch [4/300], Step [95/225], Training Accuracy: 37.2697%, Training Loss: 1.2834%\n",
      "Epoch [4/300], Step [96/225], Training Accuracy: 37.2721%, Training Loss: 1.2831%\n",
      "Epoch [4/300], Step [97/225], Training Accuracy: 37.2906%, Training Loss: 1.2830%\n",
      "Epoch [4/300], Step [98/225], Training Accuracy: 37.2927%, Training Loss: 1.2823%\n",
      "Epoch [4/300], Step [99/225], Training Accuracy: 37.2317%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [100/225], Training Accuracy: 37.1875%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [101/225], Training Accuracy: 37.1751%, Training Loss: 1.2818%\n",
      "Epoch [4/300], Step [102/225], Training Accuracy: 37.2702%, Training Loss: 1.2813%\n",
      "Epoch [4/300], Step [103/225], Training Accuracy: 37.2573%, Training Loss: 1.2819%\n",
      "Epoch [4/300], Step [104/225], Training Accuracy: 37.3197%, Training Loss: 1.2819%\n",
      "Epoch [4/300], Step [105/225], Training Accuracy: 37.3214%, Training Loss: 1.2821%\n",
      "Epoch [4/300], Step [106/225], Training Accuracy: 37.2347%, Training Loss: 1.2819%\n",
      "Epoch [4/300], Step [107/225], Training Accuracy: 37.1933%, Training Loss: 1.2820%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [108/225], Training Accuracy: 37.1238%, Training Loss: 1.2820%\n",
      "Epoch [4/300], Step [109/225], Training Accuracy: 37.0700%, Training Loss: 1.2820%\n",
      "Epoch [4/300], Step [110/225], Training Accuracy: 37.0739%, Training Loss: 1.2822%\n",
      "Epoch [4/300], Step [111/225], Training Accuracy: 37.0636%, Training Loss: 1.2822%\n",
      "Epoch [4/300], Step [112/225], Training Accuracy: 37.0675%, Training Loss: 1.2819%\n",
      "Epoch [4/300], Step [113/225], Training Accuracy: 37.0299%, Training Loss: 1.2826%\n",
      "Epoch [4/300], Step [114/225], Training Accuracy: 36.9929%, Training Loss: 1.2820%\n",
      "Epoch [4/300], Step [115/225], Training Accuracy: 36.9701%, Training Loss: 1.2820%\n",
      "Epoch [4/300], Step [116/225], Training Accuracy: 37.0286%, Training Loss: 1.2813%\n",
      "Epoch [4/300], Step [117/225], Training Accuracy: 36.9525%, Training Loss: 1.2822%\n",
      "Epoch [4/300], Step [118/225], Training Accuracy: 36.9836%, Training Loss: 1.2821%\n",
      "Epoch [4/300], Step [119/225], Training Accuracy: 36.9879%, Training Loss: 1.2822%\n",
      "Epoch [4/300], Step [120/225], Training Accuracy: 37.0312%, Training Loss: 1.2820%\n",
      "Epoch [4/300], Step [121/225], Training Accuracy: 36.9964%, Training Loss: 1.2821%\n",
      "Epoch [4/300], Step [122/225], Training Accuracy: 37.0133%, Training Loss: 1.2821%\n",
      "Epoch [4/300], Step [123/225], Training Accuracy: 36.9792%, Training Loss: 1.2824%\n",
      "Epoch [4/300], Step [124/225], Training Accuracy: 37.0086%, Training Loss: 1.2821%\n",
      "Epoch [4/300], Step [125/225], Training Accuracy: 36.9750%, Training Loss: 1.2830%\n",
      "Epoch [4/300], Step [126/225], Training Accuracy: 36.9668%, Training Loss: 1.2830%\n",
      "Epoch [4/300], Step [127/225], Training Accuracy: 36.9341%, Training Loss: 1.2828%\n",
      "Epoch [4/300], Step [128/225], Training Accuracy: 36.9263%, Training Loss: 1.2830%\n",
      "Epoch [4/300], Step [129/225], Training Accuracy: 36.9186%, Training Loss: 1.2833%\n",
      "Epoch [4/300], Step [130/225], Training Accuracy: 36.8269%, Training Loss: 1.2839%\n",
      "Epoch [4/300], Step [131/225], Training Accuracy: 36.7963%, Training Loss: 1.2838%\n",
      "Epoch [4/300], Step [132/225], Training Accuracy: 36.7779%, Training Loss: 1.2838%\n",
      "Epoch [4/300], Step [133/225], Training Accuracy: 36.7481%, Training Loss: 1.2840%\n",
      "Epoch [4/300], Step [134/225], Training Accuracy: 36.7188%, Training Loss: 1.2839%\n",
      "Epoch [4/300], Step [135/225], Training Accuracy: 36.6782%, Training Loss: 1.2842%\n",
      "Epoch [4/300], Step [136/225], Training Accuracy: 36.7073%, Training Loss: 1.2837%\n",
      "Epoch [4/300], Step [137/225], Training Accuracy: 36.7473%, Training Loss: 1.2836%\n",
      "Epoch [4/300], Step [138/225], Training Accuracy: 36.7188%, Training Loss: 1.2836%\n",
      "Epoch [4/300], Step [139/225], Training Accuracy: 36.7469%, Training Loss: 1.2837%\n",
      "Epoch [4/300], Step [140/225], Training Accuracy: 36.7299%, Training Loss: 1.2834%\n",
      "Epoch [4/300], Step [141/225], Training Accuracy: 36.8019%, Training Loss: 1.2831%\n",
      "Epoch [4/300], Step [142/225], Training Accuracy: 36.7518%, Training Loss: 1.2829%\n",
      "Epoch [4/300], Step [143/225], Training Accuracy: 36.7898%, Training Loss: 1.2826%\n",
      "Epoch [4/300], Step [144/225], Training Accuracy: 36.8056%, Training Loss: 1.2826%\n",
      "Epoch [4/300], Step [145/225], Training Accuracy: 36.8750%, Training Loss: 1.2818%\n",
      "Epoch [4/300], Step [146/225], Training Accuracy: 36.9114%, Training Loss: 1.2813%\n",
      "Epoch [4/300], Step [147/225], Training Accuracy: 36.9048%, Training Loss: 1.2810%\n",
      "Epoch [4/300], Step [148/225], Training Accuracy: 36.9616%, Training Loss: 1.2809%\n",
      "Epoch [4/300], Step [149/225], Training Accuracy: 36.9757%, Training Loss: 1.2811%\n",
      "Epoch [4/300], Step [150/225], Training Accuracy: 37.0000%, Training Loss: 1.2813%\n",
      "Epoch [4/300], Step [151/225], Training Accuracy: 37.0550%, Training Loss: 1.2807%\n",
      "Epoch [4/300], Step [152/225], Training Accuracy: 37.0785%, Training Loss: 1.2804%\n",
      "Epoch [4/300], Step [153/225], Training Accuracy: 37.1017%, Training Loss: 1.2799%\n",
      "Epoch [4/300], Step [154/225], Training Accuracy: 37.0333%, Training Loss: 1.2801%\n",
      "Epoch [4/300], Step [155/225], Training Accuracy: 37.0464%, Training Loss: 1.2801%\n",
      "Epoch [4/300], Step [156/225], Training Accuracy: 37.0493%, Training Loss: 1.2801%\n",
      "Epoch [4/300], Step [157/225], Training Accuracy: 37.0820%, Training Loss: 1.2799%\n",
      "Epoch [4/300], Step [158/225], Training Accuracy: 37.0945%, Training Loss: 1.2795%\n",
      "Epoch [4/300], Step [159/225], Training Accuracy: 37.0676%, Training Loss: 1.2794%\n",
      "Epoch [4/300], Step [160/225], Training Accuracy: 37.0410%, Training Loss: 1.2796%\n",
      "Epoch [4/300], Step [161/225], Training Accuracy: 37.0924%, Training Loss: 1.2787%\n",
      "Epoch [4/300], Step [162/225], Training Accuracy: 37.1431%, Training Loss: 1.2784%\n",
      "Epoch [4/300], Step [163/225], Training Accuracy: 37.1549%, Training Loss: 1.2779%\n",
      "Epoch [4/300], Step [164/225], Training Accuracy: 37.1856%, Training Loss: 1.2778%\n",
      "Epoch [4/300], Step [165/225], Training Accuracy: 37.2159%, Training Loss: 1.2778%\n",
      "Epoch [4/300], Step [166/225], Training Accuracy: 37.2176%, Training Loss: 1.2776%\n",
      "Epoch [4/300], Step [167/225], Training Accuracy: 37.2380%, Training Loss: 1.2777%\n",
      "Epoch [4/300], Step [168/225], Training Accuracy: 37.2210%, Training Loss: 1.2776%\n",
      "Epoch [4/300], Step [169/225], Training Accuracy: 37.2874%, Training Loss: 1.2775%\n",
      "Epoch [4/300], Step [170/225], Training Accuracy: 37.2610%, Training Loss: 1.2775%\n",
      "Epoch [4/300], Step [171/225], Training Accuracy: 37.2533%, Training Loss: 1.2773%\n",
      "Epoch [4/300], Step [172/225], Training Accuracy: 37.2729%, Training Loss: 1.2772%\n",
      "Epoch [4/300], Step [173/225], Training Accuracy: 37.2471%, Training Loss: 1.2772%\n",
      "Epoch [4/300], Step [174/225], Training Accuracy: 37.2306%, Training Loss: 1.2772%\n",
      "Epoch [4/300], Step [175/225], Training Accuracy: 37.2411%, Training Loss: 1.2770%\n",
      "Epoch [4/300], Step [176/225], Training Accuracy: 37.2248%, Training Loss: 1.2771%\n",
      "Epoch [4/300], Step [177/225], Training Accuracy: 37.2175%, Training Loss: 1.2772%\n",
      "Epoch [4/300], Step [178/225], Training Accuracy: 37.2367%, Training Loss: 1.2768%\n",
      "Epoch [4/300], Step [179/225], Training Accuracy: 37.2556%, Training Loss: 1.2769%\n",
      "Epoch [4/300], Step [180/225], Training Accuracy: 37.3524%, Training Loss: 1.2758%\n",
      "Epoch [4/300], Step [181/225], Training Accuracy: 37.3273%, Training Loss: 1.2762%\n",
      "Epoch [4/300], Step [182/225], Training Accuracy: 37.3455%, Training Loss: 1.2761%\n",
      "Epoch [4/300], Step [183/225], Training Accuracy: 37.3548%, Training Loss: 1.2756%\n",
      "Epoch [4/300], Step [184/225], Training Accuracy: 37.3641%, Training Loss: 1.2755%\n",
      "Epoch [4/300], Step [185/225], Training Accuracy: 37.3986%, Training Loss: 1.2754%\n",
      "Epoch [4/300], Step [186/225], Training Accuracy: 37.4328%, Training Loss: 1.2746%\n",
      "Epoch [4/300], Step [187/225], Training Accuracy: 37.4666%, Training Loss: 1.2742%\n",
      "Epoch [4/300], Step [188/225], Training Accuracy: 37.5083%, Training Loss: 1.2737%\n",
      "Epoch [4/300], Step [189/225], Training Accuracy: 37.5248%, Training Loss: 1.2735%\n",
      "Epoch [4/300], Step [190/225], Training Accuracy: 37.5576%, Training Loss: 1.2735%\n",
      "Epoch [4/300], Step [191/225], Training Accuracy: 37.5327%, Training Loss: 1.2735%\n",
      "Epoch [4/300], Step [192/225], Training Accuracy: 37.5326%, Training Loss: 1.2737%\n",
      "Epoch [4/300], Step [193/225], Training Accuracy: 37.5324%, Training Loss: 1.2735%\n",
      "Epoch [4/300], Step [194/225], Training Accuracy: 37.5242%, Training Loss: 1.2731%\n",
      "Epoch [4/300], Step [195/225], Training Accuracy: 37.5321%, Training Loss: 1.2729%\n",
      "Epoch [4/300], Step [196/225], Training Accuracy: 37.5319%, Training Loss: 1.2729%\n",
      "Epoch [4/300], Step [197/225], Training Accuracy: 37.5317%, Training Loss: 1.2727%\n",
      "Epoch [4/300], Step [198/225], Training Accuracy: 37.5789%, Training Loss: 1.2721%\n",
      "Epoch [4/300], Step [199/225], Training Accuracy: 37.6413%, Training Loss: 1.2718%\n",
      "Epoch [4/300], Step [200/225], Training Accuracy: 37.6484%, Training Loss: 1.2718%\n",
      "Epoch [4/300], Step [201/225], Training Accuracy: 37.6866%, Training Loss: 1.2715%\n",
      "Epoch [4/300], Step [202/225], Training Accuracy: 37.6779%, Training Loss: 1.2714%\n",
      "Epoch [4/300], Step [203/225], Training Accuracy: 37.7001%, Training Loss: 1.2715%\n",
      "Epoch [4/300], Step [204/225], Training Accuracy: 37.7298%, Training Loss: 1.2710%\n",
      "Epoch [4/300], Step [205/225], Training Accuracy: 37.7058%, Training Loss: 1.2713%\n",
      "Epoch [4/300], Step [206/225], Training Accuracy: 37.7048%, Training Loss: 1.2713%\n",
      "Epoch [4/300], Step [207/225], Training Accuracy: 37.7038%, Training Loss: 1.2714%\n",
      "Epoch [4/300], Step [208/225], Training Accuracy: 37.7479%, Training Loss: 1.2709%\n",
      "Epoch [4/300], Step [209/225], Training Accuracy: 37.7168%, Training Loss: 1.2707%\n",
      "Epoch [4/300], Step [210/225], Training Accuracy: 37.7381%, Training Loss: 1.2702%\n",
      "Epoch [4/300], Step [211/225], Training Accuracy: 37.8036%, Training Loss: 1.2697%\n",
      "Epoch [4/300], Step [212/225], Training Accuracy: 37.8096%, Training Loss: 1.2695%\n",
      "Epoch [4/300], Step [213/225], Training Accuracy: 37.7861%, Training Loss: 1.2697%\n",
      "Epoch [4/300], Step [214/225], Training Accuracy: 37.7629%, Training Loss: 1.2695%\n",
      "Epoch [4/300], Step [215/225], Training Accuracy: 37.7834%, Training Loss: 1.2692%\n",
      "Epoch [4/300], Step [216/225], Training Accuracy: 37.7677%, Training Loss: 1.2694%\n",
      "Epoch [4/300], Step [217/225], Training Accuracy: 37.7592%, Training Loss: 1.2690%\n",
      "Epoch [4/300], Step [218/225], Training Accuracy: 37.7437%, Training Loss: 1.2691%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [219/225], Training Accuracy: 37.7212%, Training Loss: 1.2688%\n",
      "Epoch [4/300], Step [220/225], Training Accuracy: 37.7273%, Training Loss: 1.2684%\n",
      "Epoch [4/300], Step [221/225], Training Accuracy: 37.7121%, Training Loss: 1.2684%\n",
      "Epoch [4/300], Step [222/225], Training Accuracy: 37.7252%, Training Loss: 1.2684%\n",
      "Epoch [4/300], Step [223/225], Training Accuracy: 37.6682%, Training Loss: 1.2684%\n",
      "Epoch [4/300], Step [224/225], Training Accuracy: 37.6186%, Training Loss: 1.2684%\n",
      "Epoch [4/300], Step [225/225], Training Accuracy: 37.6181%, Training Loss: 1.2687%\n",
      "Epoch [5/300], Step [1/225], Training Accuracy: 50.0000%, Training Loss: 1.2485%\n",
      "Epoch [5/300], Step [2/225], Training Accuracy: 46.0938%, Training Loss: 1.2749%\n",
      "Epoch [5/300], Step [3/225], Training Accuracy: 40.6250%, Training Loss: 1.2901%\n",
      "Epoch [5/300], Step [4/225], Training Accuracy: 39.4531%, Training Loss: 1.2633%\n",
      "Epoch [5/300], Step [5/225], Training Accuracy: 39.6875%, Training Loss: 1.2637%\n",
      "Epoch [5/300], Step [6/225], Training Accuracy: 40.1042%, Training Loss: 1.2753%\n",
      "Epoch [5/300], Step [7/225], Training Accuracy: 39.2857%, Training Loss: 1.2721%\n",
      "Epoch [5/300], Step [8/225], Training Accuracy: 38.8672%, Training Loss: 1.2711%\n",
      "Epoch [5/300], Step [9/225], Training Accuracy: 38.1944%, Training Loss: 1.2782%\n",
      "Epoch [5/300], Step [10/225], Training Accuracy: 39.2188%, Training Loss: 1.2704%\n",
      "Epoch [5/300], Step [11/225], Training Accuracy: 39.7727%, Training Loss: 1.2719%\n",
      "Epoch [5/300], Step [12/225], Training Accuracy: 40.3646%, Training Loss: 1.2670%\n",
      "Epoch [5/300], Step [13/225], Training Accuracy: 40.9856%, Training Loss: 1.2642%\n",
      "Epoch [5/300], Step [14/225], Training Accuracy: 40.4018%, Training Loss: 1.2734%\n",
      "Epoch [5/300], Step [15/225], Training Accuracy: 40.3125%, Training Loss: 1.2749%\n",
      "Epoch [5/300], Step [16/225], Training Accuracy: 40.1367%, Training Loss: 1.2706%\n",
      "Epoch [5/300], Step [17/225], Training Accuracy: 40.1654%, Training Loss: 1.2683%\n",
      "Epoch [5/300], Step [18/225], Training Accuracy: 39.8438%, Training Loss: 1.2656%\n",
      "Epoch [5/300], Step [19/225], Training Accuracy: 39.4737%, Training Loss: 1.2676%\n",
      "Epoch [5/300], Step [20/225], Training Accuracy: 39.3750%, Training Loss: 1.2648%\n",
      "Epoch [5/300], Step [21/225], Training Accuracy: 39.6577%, Training Loss: 1.2616%\n",
      "Epoch [5/300], Step [22/225], Training Accuracy: 39.6307%, Training Loss: 1.2603%\n",
      "Epoch [5/300], Step [23/225], Training Accuracy: 39.8777%, Training Loss: 1.2569%\n",
      "Epoch [5/300], Step [24/225], Training Accuracy: 39.5182%, Training Loss: 1.2575%\n",
      "Epoch [5/300], Step [25/225], Training Accuracy: 39.6875%, Training Loss: 1.2542%\n",
      "Epoch [5/300], Step [26/225], Training Accuracy: 39.3630%, Training Loss: 1.2549%\n",
      "Epoch [5/300], Step [27/225], Training Accuracy: 39.5833%, Training Loss: 1.2542%\n",
      "Epoch [5/300], Step [28/225], Training Accuracy: 39.6763%, Training Loss: 1.2523%\n",
      "Epoch [5/300], Step [29/225], Training Accuracy: 39.7091%, Training Loss: 1.2499%\n",
      "Epoch [5/300], Step [30/225], Training Accuracy: 39.8438%, Training Loss: 1.2467%\n",
      "Epoch [5/300], Step [31/225], Training Accuracy: 39.7681%, Training Loss: 1.2466%\n",
      "Epoch [5/300], Step [32/225], Training Accuracy: 39.6973%, Training Loss: 1.2456%\n",
      "Epoch [5/300], Step [33/225], Training Accuracy: 39.8674%, Training Loss: 1.2434%\n",
      "Epoch [5/300], Step [34/225], Training Accuracy: 39.9816%, Training Loss: 1.2436%\n",
      "Epoch [5/300], Step [35/225], Training Accuracy: 39.9107%, Training Loss: 1.2455%\n",
      "Epoch [5/300], Step [36/225], Training Accuracy: 39.8003%, Training Loss: 1.2472%\n",
      "Epoch [5/300], Step [37/225], Training Accuracy: 39.8226%, Training Loss: 1.2471%\n",
      "Epoch [5/300], Step [38/225], Training Accuracy: 39.9260%, Training Loss: 1.2469%\n",
      "Epoch [5/300], Step [39/225], Training Accuracy: 40.1042%, Training Loss: 1.2457%\n",
      "Epoch [5/300], Step [40/225], Training Accuracy: 40.2344%, Training Loss: 1.2447%\n",
      "Epoch [5/300], Step [41/225], Training Accuracy: 40.2439%, Training Loss: 1.2445%\n",
      "Epoch [5/300], Step [42/225], Training Accuracy: 40.0298%, Training Loss: 1.2455%\n",
      "Epoch [5/300], Step [43/225], Training Accuracy: 39.7892%, Training Loss: 1.2462%\n",
      "Epoch [5/300], Step [44/225], Training Accuracy: 40.1634%, Training Loss: 1.2447%\n",
      "Epoch [5/300], Step [45/225], Training Accuracy: 40.2431%, Training Loss: 1.2439%\n",
      "Epoch [5/300], Step [46/225], Training Accuracy: 40.1834%, Training Loss: 1.2439%\n",
      "Epoch [5/300], Step [47/225], Training Accuracy: 40.0931%, Training Loss: 1.2438%\n",
      "Epoch [5/300], Step [48/225], Training Accuracy: 40.0065%, Training Loss: 1.2427%\n",
      "Epoch [5/300], Step [49/225], Training Accuracy: 40.0510%, Training Loss: 1.2439%\n",
      "Epoch [5/300], Step [50/225], Training Accuracy: 40.0625%, Training Loss: 1.2440%\n",
      "Epoch [5/300], Step [51/225], Training Accuracy: 40.1348%, Training Loss: 1.2436%\n",
      "Epoch [5/300], Step [52/225], Training Accuracy: 40.2644%, Training Loss: 1.2431%\n",
      "Epoch [5/300], Step [53/225], Training Accuracy: 40.3007%, Training Loss: 1.2428%\n",
      "Epoch [5/300], Step [54/225], Training Accuracy: 40.2199%, Training Loss: 1.2429%\n",
      "Epoch [5/300], Step [55/225], Training Accuracy: 40.1420%, Training Loss: 1.2427%\n",
      "Epoch [5/300], Step [56/225], Training Accuracy: 39.9275%, Training Loss: 1.2436%\n",
      "Epoch [5/300], Step [57/225], Training Accuracy: 40.0768%, Training Loss: 1.2416%\n",
      "Epoch [5/300], Step [58/225], Training Accuracy: 39.9784%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [59/225], Training Accuracy: 40.0159%, Training Loss: 1.2387%\n",
      "Epoch [5/300], Step [60/225], Training Accuracy: 40.0000%, Training Loss: 1.2376%\n",
      "Epoch [5/300], Step [61/225], Training Accuracy: 39.9078%, Training Loss: 1.2378%\n",
      "Epoch [5/300], Step [62/225], Training Accuracy: 39.8942%, Training Loss: 1.2381%\n",
      "Epoch [5/300], Step [63/225], Training Accuracy: 39.8065%, Training Loss: 1.2394%\n",
      "Epoch [5/300], Step [64/225], Training Accuracy: 39.8193%, Training Loss: 1.2400%\n",
      "Epoch [5/300], Step [65/225], Training Accuracy: 39.6635%, Training Loss: 1.2395%\n",
      "Epoch [5/300], Step [66/225], Training Accuracy: 39.6780%, Training Loss: 1.2390%\n",
      "Epoch [5/300], Step [67/225], Training Accuracy: 39.6688%, Training Loss: 1.2383%\n",
      "Epoch [5/300], Step [68/225], Training Accuracy: 39.5680%, Training Loss: 1.2383%\n",
      "Epoch [5/300], Step [69/225], Training Accuracy: 39.5607%, Training Loss: 1.2371%\n",
      "Epoch [5/300], Step [70/225], Training Accuracy: 39.4420%, Training Loss: 1.2379%\n",
      "Epoch [5/300], Step [71/225], Training Accuracy: 39.4146%, Training Loss: 1.2383%\n",
      "Epoch [5/300], Step [72/225], Training Accuracy: 39.3663%, Training Loss: 1.2403%\n",
      "Epoch [5/300], Step [73/225], Training Accuracy: 39.3408%, Training Loss: 1.2424%\n",
      "Epoch [5/300], Step [74/225], Training Accuracy: 39.4003%, Training Loss: 1.2414%\n",
      "Epoch [5/300], Step [75/225], Training Accuracy: 39.5208%, Training Loss: 1.2401%\n",
      "Epoch [5/300], Step [76/225], Training Accuracy: 39.4531%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [77/225], Training Accuracy: 39.4886%, Training Loss: 1.2406%\n",
      "Epoch [5/300], Step [78/225], Training Accuracy: 39.4431%, Training Loss: 1.2406%\n",
      "Epoch [5/300], Step [79/225], Training Accuracy: 39.3790%, Training Loss: 1.2417%\n",
      "Epoch [5/300], Step [80/225], Training Accuracy: 39.3945%, Training Loss: 1.2414%\n",
      "Epoch [5/300], Step [81/225], Training Accuracy: 39.4097%, Training Loss: 1.2418%\n",
      "Epoch [5/300], Step [82/225], Training Accuracy: 39.4245%, Training Loss: 1.2413%\n",
      "Epoch [5/300], Step [83/225], Training Accuracy: 39.4202%, Training Loss: 1.2409%\n",
      "Epoch [5/300], Step [84/225], Training Accuracy: 39.3601%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [85/225], Training Accuracy: 39.3382%, Training Loss: 1.2417%\n",
      "Epoch [5/300], Step [86/225], Training Accuracy: 39.2805%, Training Loss: 1.2420%\n",
      "Epoch [5/300], Step [87/225], Training Accuracy: 39.2241%, Training Loss: 1.2424%\n",
      "Epoch [5/300], Step [88/225], Training Accuracy: 39.3111%, Training Loss: 1.2423%\n",
      "Epoch [5/300], Step [89/225], Training Accuracy: 39.2907%, Training Loss: 1.2421%\n",
      "Epoch [5/300], Step [90/225], Training Accuracy: 39.3229%, Training Loss: 1.2413%\n",
      "Epoch [5/300], Step [91/225], Training Accuracy: 39.3201%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [92/225], Training Accuracy: 39.3682%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [93/225], Training Accuracy: 39.2641%, Training Loss: 1.2409%\n",
      "Epoch [5/300], Step [94/225], Training Accuracy: 39.3451%, Training Loss: 1.2401%\n",
      "Epoch [5/300], Step [95/225], Training Accuracy: 39.2763%, Training Loss: 1.2417%\n",
      "Epoch [5/300], Step [96/225], Training Accuracy: 39.2578%, Training Loss: 1.2415%\n",
      "Epoch [5/300], Step [97/225], Training Accuracy: 39.2558%, Training Loss: 1.2414%\n",
      "Epoch [5/300], Step [98/225], Training Accuracy: 39.3495%, Training Loss: 1.2406%\n",
      "Epoch [5/300], Step [99/225], Training Accuracy: 39.2835%, Training Loss: 1.2409%\n",
      "Epoch [5/300], Step [100/225], Training Accuracy: 39.2656%, Training Loss: 1.2409%\n",
      "Epoch [5/300], Step [101/225], Training Accuracy: 39.2636%, Training Loss: 1.2401%\n",
      "Epoch [5/300], Step [102/225], Training Accuracy: 39.3382%, Training Loss: 1.2396%\n",
      "Epoch [5/300], Step [103/225], Training Accuracy: 39.3356%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [104/225], Training Accuracy: 39.4231%, Training Loss: 1.2400%\n",
      "Epoch [5/300], Step [105/225], Training Accuracy: 39.3750%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [106/225], Training Accuracy: 39.3278%, Training Loss: 1.2402%\n",
      "Epoch [5/300], Step [107/225], Training Accuracy: 39.3546%, Training Loss: 1.2401%\n",
      "Epoch [5/300], Step [108/225], Training Accuracy: 39.3374%, Training Loss: 1.2404%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [109/225], Training Accuracy: 39.2775%, Training Loss: 1.2405%\n",
      "Epoch [5/300], Step [110/225], Training Accuracy: 39.2898%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [111/225], Training Accuracy: 39.2877%, Training Loss: 1.2407%\n",
      "Epoch [5/300], Step [112/225], Training Accuracy: 39.3276%, Training Loss: 1.2405%\n",
      "Epoch [5/300], Step [113/225], Training Accuracy: 39.3252%, Training Loss: 1.2414%\n",
      "Epoch [5/300], Step [114/225], Training Accuracy: 39.3092%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [115/225], Training Accuracy: 39.2527%, Training Loss: 1.2408%\n",
      "Epoch [5/300], Step [116/225], Training Accuracy: 39.3588%, Training Loss: 1.2400%\n",
      "Epoch [5/300], Step [117/225], Training Accuracy: 39.2762%, Training Loss: 1.2410%\n",
      "Epoch [5/300], Step [118/225], Training Accuracy: 39.3008%, Training Loss: 1.2411%\n",
      "Epoch [5/300], Step [119/225], Training Accuracy: 39.3251%, Training Loss: 1.2409%\n",
      "Epoch [5/300], Step [120/225], Training Accuracy: 39.2969%, Training Loss: 1.2410%\n",
      "Epoch [5/300], Step [121/225], Training Accuracy: 39.2691%, Training Loss: 1.2411%\n",
      "Epoch [5/300], Step [122/225], Training Accuracy: 39.2930%, Training Loss: 1.2410%\n",
      "Epoch [5/300], Step [123/225], Training Accuracy: 39.2658%, Training Loss: 1.2414%\n",
      "Epoch [5/300], Step [124/225], Training Accuracy: 39.2515%, Training Loss: 1.2414%\n",
      "Epoch [5/300], Step [125/225], Training Accuracy: 39.2250%, Training Loss: 1.2420%\n",
      "Epoch [5/300], Step [126/225], Training Accuracy: 39.2113%, Training Loss: 1.2420%\n",
      "Epoch [5/300], Step [127/225], Training Accuracy: 39.1732%, Training Loss: 1.2420%\n",
      "Epoch [5/300], Step [128/225], Training Accuracy: 39.1602%, Training Loss: 1.2424%\n",
      "Epoch [5/300], Step [129/225], Training Accuracy: 39.1352%, Training Loss: 1.2425%\n",
      "Epoch [5/300], Step [130/225], Training Accuracy: 39.0505%, Training Loss: 1.2430%\n",
      "Epoch [5/300], Step [131/225], Training Accuracy: 39.0506%, Training Loss: 1.2430%\n",
      "Epoch [5/300], Step [132/225], Training Accuracy: 39.0507%, Training Loss: 1.2431%\n",
      "Epoch [5/300], Step [133/225], Training Accuracy: 39.0390%, Training Loss: 1.2432%\n",
      "Epoch [5/300], Step [134/225], Training Accuracy: 39.0159%, Training Loss: 1.2428%\n",
      "Epoch [5/300], Step [135/225], Training Accuracy: 38.9699%, Training Loss: 1.2434%\n",
      "Epoch [5/300], Step [136/225], Training Accuracy: 39.0051%, Training Loss: 1.2428%\n",
      "Epoch [5/300], Step [137/225], Training Accuracy: 39.0739%, Training Loss: 1.2427%\n",
      "Epoch [5/300], Step [138/225], Training Accuracy: 39.0399%, Training Loss: 1.2428%\n",
      "Epoch [5/300], Step [139/225], Training Accuracy: 39.0175%, Training Loss: 1.2431%\n",
      "Epoch [5/300], Step [140/225], Training Accuracy: 39.0513%, Training Loss: 1.2430%\n",
      "Epoch [5/300], Step [141/225], Training Accuracy: 39.1068%, Training Loss: 1.2429%\n",
      "Epoch [5/300], Step [142/225], Training Accuracy: 39.0625%, Training Loss: 1.2428%\n",
      "Epoch [5/300], Step [143/225], Training Accuracy: 39.0516%, Training Loss: 1.2426%\n",
      "Epoch [5/300], Step [144/225], Training Accuracy: 39.0408%, Training Loss: 1.2427%\n",
      "Epoch [5/300], Step [145/225], Training Accuracy: 39.1703%, Training Loss: 1.2416%\n",
      "Epoch [5/300], Step [146/225], Training Accuracy: 39.1802%, Training Loss: 1.2412%\n",
      "Epoch [5/300], Step [147/225], Training Accuracy: 39.1582%, Training Loss: 1.2410%\n",
      "Epoch [5/300], Step [148/225], Training Accuracy: 39.2209%, Training Loss: 1.2409%\n",
      "Epoch [5/300], Step [149/225], Training Accuracy: 39.2303%, Training Loss: 1.2413%\n",
      "Epoch [5/300], Step [150/225], Training Accuracy: 39.2708%, Training Loss: 1.2416%\n",
      "Epoch [5/300], Step [151/225], Training Accuracy: 39.3108%, Training Loss: 1.2407%\n",
      "Epoch [5/300], Step [152/225], Training Accuracy: 39.3092%, Training Loss: 1.2405%\n",
      "Epoch [5/300], Step [153/225], Training Accuracy: 39.3382%, Training Loss: 1.2401%\n",
      "Epoch [5/300], Step [154/225], Training Accuracy: 39.2553%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [155/225], Training Accuracy: 39.2540%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [156/225], Training Accuracy: 39.2628%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [157/225], Training Accuracy: 39.2914%, Training Loss: 1.2404%\n",
      "Epoch [5/300], Step [158/225], Training Accuracy: 39.3295%, Training Loss: 1.2399%\n",
      "Epoch [5/300], Step [159/225], Training Accuracy: 39.3278%, Training Loss: 1.2397%\n",
      "Epoch [5/300], Step [160/225], Training Accuracy: 39.2676%, Training Loss: 1.2400%\n",
      "Epoch [5/300], Step [161/225], Training Accuracy: 39.2954%, Training Loss: 1.2390%\n",
      "Epoch [5/300], Step [162/225], Training Accuracy: 39.3711%, Training Loss: 1.2388%\n",
      "Epoch [5/300], Step [163/225], Training Accuracy: 39.3597%, Training Loss: 1.2384%\n",
      "Epoch [5/300], Step [164/225], Training Accuracy: 39.3674%, Training Loss: 1.2385%\n",
      "Epoch [5/300], Step [165/225], Training Accuracy: 39.3750%, Training Loss: 1.2386%\n",
      "Epoch [5/300], Step [166/225], Training Accuracy: 39.3731%, Training Loss: 1.2386%\n",
      "Epoch [5/300], Step [167/225], Training Accuracy: 39.4087%, Training Loss: 1.2385%\n",
      "Epoch [5/300], Step [168/225], Training Accuracy: 39.3880%, Training Loss: 1.2385%\n",
      "Epoch [5/300], Step [169/225], Training Accuracy: 39.4046%, Training Loss: 1.2384%\n",
      "Epoch [5/300], Step [170/225], Training Accuracy: 39.3842%, Training Loss: 1.2386%\n",
      "Epoch [5/300], Step [171/225], Training Accuracy: 39.3914%, Training Loss: 1.2385%\n",
      "Epoch [5/300], Step [172/225], Training Accuracy: 39.3714%, Training Loss: 1.2386%\n",
      "Epoch [5/300], Step [173/225], Training Accuracy: 39.3515%, Training Loss: 1.2386%\n",
      "Epoch [5/300], Step [174/225], Training Accuracy: 39.3229%, Training Loss: 1.2385%\n",
      "Epoch [5/300], Step [175/225], Training Accuracy: 39.3125%, Training Loss: 1.2382%\n",
      "Epoch [5/300], Step [176/225], Training Accuracy: 39.3022%, Training Loss: 1.2384%\n",
      "Epoch [5/300], Step [177/225], Training Accuracy: 39.3008%, Training Loss: 1.2385%\n",
      "Epoch [5/300], Step [178/225], Training Accuracy: 39.2907%, Training Loss: 1.2382%\n",
      "Epoch [5/300], Step [179/225], Training Accuracy: 39.2895%, Training Loss: 1.2383%\n",
      "Epoch [5/300], Step [180/225], Training Accuracy: 39.3924%, Training Loss: 1.2371%\n",
      "Epoch [5/300], Step [181/225], Training Accuracy: 39.3560%, Training Loss: 1.2377%\n",
      "Epoch [5/300], Step [182/225], Training Accuracy: 39.3973%, Training Loss: 1.2376%\n",
      "Epoch [5/300], Step [183/225], Training Accuracy: 39.4040%, Training Loss: 1.2371%\n",
      "Epoch [5/300], Step [184/225], Training Accuracy: 39.4192%, Training Loss: 1.2372%\n",
      "Epoch [5/300], Step [185/225], Training Accuracy: 39.4510%, Training Loss: 1.2370%\n",
      "Epoch [5/300], Step [186/225], Training Accuracy: 39.4993%, Training Loss: 1.2363%\n",
      "Epoch [5/300], Step [187/225], Training Accuracy: 39.5388%, Training Loss: 1.2359%\n",
      "Epoch [5/300], Step [188/225], Training Accuracy: 39.5861%, Training Loss: 1.2354%\n",
      "Epoch [5/300], Step [189/225], Training Accuracy: 39.6081%, Training Loss: 1.2352%\n",
      "Epoch [5/300], Step [190/225], Training Accuracy: 39.6299%, Training Loss: 1.2353%\n",
      "Epoch [5/300], Step [191/225], Training Accuracy: 39.5942%, Training Loss: 1.2353%\n",
      "Epoch [5/300], Step [192/225], Training Accuracy: 39.5752%, Training Loss: 1.2357%\n",
      "Epoch [5/300], Step [193/225], Training Accuracy: 39.5483%, Training Loss: 1.2353%\n",
      "Epoch [5/300], Step [194/225], Training Accuracy: 39.5619%, Training Loss: 1.2350%\n",
      "Epoch [5/300], Step [195/225], Training Accuracy: 39.5513%, Training Loss: 1.2348%\n",
      "Epoch [5/300], Step [196/225], Training Accuracy: 39.5488%, Training Loss: 1.2348%\n",
      "Epoch [5/300], Step [197/225], Training Accuracy: 39.5384%, Training Loss: 1.2346%\n",
      "Epoch [5/300], Step [198/225], Training Accuracy: 39.5833%, Training Loss: 1.2340%\n",
      "Epoch [5/300], Step [199/225], Training Accuracy: 39.6278%, Training Loss: 1.2337%\n",
      "Epoch [5/300], Step [200/225], Training Accuracy: 39.6406%, Training Loss: 1.2339%\n",
      "Epoch [5/300], Step [201/225], Training Accuracy: 39.6222%, Training Loss: 1.2337%\n",
      "Epoch [5/300], Step [202/225], Training Accuracy: 39.6504%, Training Loss: 1.2335%\n",
      "Epoch [5/300], Step [203/225], Training Accuracy: 39.6706%, Training Loss: 1.2337%\n",
      "Epoch [5/300], Step [204/225], Training Accuracy: 39.6982%, Training Loss: 1.2332%\n",
      "Epoch [5/300], Step [205/225], Training Accuracy: 39.6951%, Training Loss: 1.2335%\n",
      "Epoch [5/300], Step [206/225], Training Accuracy: 39.7072%, Training Loss: 1.2336%\n",
      "Epoch [5/300], Step [207/225], Training Accuracy: 39.6966%, Training Loss: 1.2337%\n",
      "Epoch [5/300], Step [208/225], Training Accuracy: 39.7236%, Training Loss: 1.2332%\n",
      "Epoch [5/300], Step [209/225], Training Accuracy: 39.7129%, Training Loss: 1.2331%\n",
      "Epoch [5/300], Step [210/225], Training Accuracy: 39.7173%, Training Loss: 1.2326%\n",
      "Epoch [5/300], Step [211/225], Training Accuracy: 39.7882%, Training Loss: 1.2323%\n",
      "Epoch [5/300], Step [212/225], Training Accuracy: 39.7700%, Training Loss: 1.2322%\n",
      "Epoch [5/300], Step [213/225], Training Accuracy: 39.7521%, Training Loss: 1.2323%\n",
      "Epoch [5/300], Step [214/225], Training Accuracy: 39.7488%, Training Loss: 1.2322%\n",
      "Epoch [5/300], Step [215/225], Training Accuracy: 39.7747%, Training Loss: 1.2318%\n",
      "Epoch [5/300], Step [216/225], Training Accuracy: 39.7569%, Training Loss: 1.2321%\n",
      "Epoch [5/300], Step [217/225], Training Accuracy: 39.7393%, Training Loss: 1.2318%\n",
      "Epoch [5/300], Step [218/225], Training Accuracy: 39.7291%, Training Loss: 1.2320%\n",
      "Epoch [5/300], Step [219/225], Training Accuracy: 39.7189%, Training Loss: 1.2317%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [220/225], Training Accuracy: 39.7230%, Training Loss: 1.2314%\n",
      "Epoch [5/300], Step [221/225], Training Accuracy: 39.7059%, Training Loss: 1.2314%\n",
      "Epoch [5/300], Step [222/225], Training Accuracy: 39.7523%, Training Loss: 1.2314%\n",
      "Epoch [5/300], Step [223/225], Training Accuracy: 39.6861%, Training Loss: 1.2315%\n",
      "Epoch [5/300], Step [224/225], Training Accuracy: 39.6345%, Training Loss: 1.2317%\n",
      "Epoch [5/300], Step [225/225], Training Accuracy: 39.6331%, Training Loss: 1.2321%\n",
      "Epoch [6/300], Step [1/225], Training Accuracy: 53.1250%, Training Loss: 1.2046%\n",
      "Epoch [6/300], Step [2/225], Training Accuracy: 43.7500%, Training Loss: 1.2484%\n",
      "Epoch [6/300], Step [3/225], Training Accuracy: 39.5833%, Training Loss: 1.2656%\n",
      "Epoch [6/300], Step [4/225], Training Accuracy: 39.4531%, Training Loss: 1.2381%\n",
      "Epoch [6/300], Step [5/225], Training Accuracy: 40.9375%, Training Loss: 1.2351%\n",
      "Epoch [6/300], Step [6/225], Training Accuracy: 42.1875%, Training Loss: 1.2437%\n",
      "Epoch [6/300], Step [7/225], Training Accuracy: 41.2946%, Training Loss: 1.2412%\n",
      "Epoch [6/300], Step [8/225], Training Accuracy: 41.6016%, Training Loss: 1.2425%\n",
      "Epoch [6/300], Step [9/225], Training Accuracy: 40.6250%, Training Loss: 1.2489%\n",
      "Epoch [6/300], Step [10/225], Training Accuracy: 41.2500%, Training Loss: 1.2407%\n",
      "Epoch [6/300], Step [11/225], Training Accuracy: 41.7614%, Training Loss: 1.2402%\n",
      "Epoch [6/300], Step [12/225], Training Accuracy: 41.7969%, Training Loss: 1.2364%\n",
      "Epoch [6/300], Step [13/225], Training Accuracy: 42.3077%, Training Loss: 1.2326%\n",
      "Epoch [6/300], Step [14/225], Training Accuracy: 42.0759%, Training Loss: 1.2414%\n",
      "Epoch [6/300], Step [15/225], Training Accuracy: 41.9792%, Training Loss: 1.2444%\n",
      "Epoch [6/300], Step [16/225], Training Accuracy: 41.6992%, Training Loss: 1.2406%\n",
      "Epoch [6/300], Step [17/225], Training Accuracy: 41.9118%, Training Loss: 1.2370%\n",
      "Epoch [6/300], Step [18/225], Training Accuracy: 41.4062%, Training Loss: 1.2343%\n",
      "Epoch [6/300], Step [19/225], Training Accuracy: 40.9539%, Training Loss: 1.2364%\n",
      "Epoch [6/300], Step [20/225], Training Accuracy: 40.8594%, Training Loss: 1.2326%\n",
      "Epoch [6/300], Step [21/225], Training Accuracy: 40.9226%, Training Loss: 1.2290%\n",
      "Epoch [6/300], Step [22/225], Training Accuracy: 40.8381%, Training Loss: 1.2280%\n",
      "Epoch [6/300], Step [23/225], Training Accuracy: 40.8967%, Training Loss: 1.2244%\n",
      "Epoch [6/300], Step [24/225], Training Accuracy: 40.4948%, Training Loss: 1.2253%\n",
      "Epoch [6/300], Step [25/225], Training Accuracy: 40.7500%, Training Loss: 1.2215%\n",
      "Epoch [6/300], Step [26/225], Training Accuracy: 40.4447%, Training Loss: 1.2231%\n",
      "Epoch [6/300], Step [27/225], Training Accuracy: 40.3935%, Training Loss: 1.2228%\n",
      "Epoch [6/300], Step [28/225], Training Accuracy: 40.5134%, Training Loss: 1.2214%\n",
      "Epoch [6/300], Step [29/225], Training Accuracy: 40.5172%, Training Loss: 1.2199%\n",
      "Epoch [6/300], Step [30/225], Training Accuracy: 40.7292%, Training Loss: 1.2172%\n",
      "Epoch [6/300], Step [31/225], Training Accuracy: 40.7762%, Training Loss: 1.2174%\n",
      "Epoch [6/300], Step [32/225], Training Accuracy: 40.7227%, Training Loss: 1.2166%\n",
      "Epoch [6/300], Step [33/225], Training Accuracy: 40.9091%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [34/225], Training Accuracy: 40.9926%, Training Loss: 1.2149%\n",
      "Epoch [6/300], Step [35/225], Training Accuracy: 40.8929%, Training Loss: 1.2167%\n",
      "Epoch [6/300], Step [36/225], Training Accuracy: 40.7986%, Training Loss: 1.2181%\n",
      "Epoch [6/300], Step [37/225], Training Accuracy: 41.0051%, Training Loss: 1.2180%\n",
      "Epoch [6/300], Step [38/225], Training Accuracy: 41.1595%, Training Loss: 1.2175%\n",
      "Epoch [6/300], Step [39/225], Training Accuracy: 41.3061%, Training Loss: 1.2163%\n",
      "Epoch [6/300], Step [40/225], Training Accuracy: 41.2891%, Training Loss: 1.2154%\n",
      "Epoch [6/300], Step [41/225], Training Accuracy: 41.2729%, Training Loss: 1.2153%\n",
      "Epoch [6/300], Step [42/225], Training Accuracy: 41.1458%, Training Loss: 1.2157%\n",
      "Epoch [6/300], Step [43/225], Training Accuracy: 40.9157%, Training Loss: 1.2164%\n",
      "Epoch [6/300], Step [44/225], Training Accuracy: 41.2287%, Training Loss: 1.2147%\n",
      "Epoch [6/300], Step [45/225], Training Accuracy: 41.2847%, Training Loss: 1.2140%\n",
      "Epoch [6/300], Step [46/225], Training Accuracy: 41.3043%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [47/225], Training Accuracy: 41.1569%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [48/225], Training Accuracy: 41.1784%, Training Loss: 1.2123%\n",
      "Epoch [6/300], Step [49/225], Training Accuracy: 41.0714%, Training Loss: 1.2141%\n",
      "Epoch [6/300], Step [50/225], Training Accuracy: 41.0312%, Training Loss: 1.2142%\n",
      "Epoch [6/300], Step [51/225], Training Accuracy: 41.1765%, Training Loss: 1.2137%\n",
      "Epoch [6/300], Step [52/225], Training Accuracy: 41.2560%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [53/225], Training Accuracy: 41.2441%, Training Loss: 1.2130%\n",
      "Epoch [6/300], Step [54/225], Training Accuracy: 41.2037%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [55/225], Training Accuracy: 41.1364%, Training Loss: 1.2132%\n",
      "Epoch [6/300], Step [56/225], Training Accuracy: 40.9877%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [57/225], Training Accuracy: 41.2281%, Training Loss: 1.2119%\n",
      "Epoch [6/300], Step [58/225], Training Accuracy: 41.1369%, Training Loss: 1.2113%\n",
      "Epoch [6/300], Step [59/225], Training Accuracy: 41.2076%, Training Loss: 1.2096%\n",
      "Epoch [6/300], Step [60/225], Training Accuracy: 41.1458%, Training Loss: 1.2088%\n",
      "Epoch [6/300], Step [61/225], Training Accuracy: 41.0861%, Training Loss: 1.2093%\n",
      "Epoch [6/300], Step [62/225], Training Accuracy: 41.0786%, Training Loss: 1.2094%\n",
      "Epoch [6/300], Step [63/225], Training Accuracy: 40.8730%, Training Loss: 1.2110%\n",
      "Epoch [6/300], Step [64/225], Training Accuracy: 40.8936%, Training Loss: 1.2118%\n",
      "Epoch [6/300], Step [65/225], Training Accuracy: 40.7933%, Training Loss: 1.2113%\n",
      "Epoch [6/300], Step [66/225], Training Accuracy: 40.8144%, Training Loss: 1.2108%\n",
      "Epoch [6/300], Step [67/225], Training Accuracy: 40.8349%, Training Loss: 1.2099%\n",
      "Epoch [6/300], Step [68/225], Training Accuracy: 40.7858%, Training Loss: 1.2100%\n",
      "Epoch [6/300], Step [69/225], Training Accuracy: 40.8062%, Training Loss: 1.2094%\n",
      "Epoch [6/300], Step [70/225], Training Accuracy: 40.6920%, Training Loss: 1.2102%\n",
      "Epoch [6/300], Step [71/225], Training Accuracy: 40.7130%, Training Loss: 1.2106%\n",
      "Epoch [6/300], Step [72/225], Training Accuracy: 40.6250%, Training Loss: 1.2122%\n",
      "Epoch [6/300], Step [73/225], Training Accuracy: 40.5822%, Training Loss: 1.2149%\n",
      "Epoch [6/300], Step [74/225], Training Accuracy: 40.6039%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [75/225], Training Accuracy: 40.7083%, Training Loss: 1.2123%\n",
      "Epoch [6/300], Step [76/225], Training Accuracy: 40.6456%, Training Loss: 1.2129%\n",
      "Epoch [6/300], Step [77/225], Training Accuracy: 40.6656%, Training Loss: 1.2130%\n",
      "Epoch [6/300], Step [78/225], Training Accuracy: 40.5248%, Training Loss: 1.2132%\n",
      "Epoch [6/300], Step [79/225], Training Accuracy: 40.4470%, Training Loss: 1.2143%\n",
      "Epoch [6/300], Step [80/225], Training Accuracy: 40.4883%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [81/225], Training Accuracy: 40.4321%, Training Loss: 1.2143%\n",
      "Epoch [6/300], Step [82/225], Training Accuracy: 40.3773%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [83/225], Training Accuracy: 40.3238%, Training Loss: 1.2136%\n",
      "Epoch [6/300], Step [84/225], Training Accuracy: 40.2716%, Training Loss: 1.2135%\n",
      "Epoch [6/300], Step [85/225], Training Accuracy: 40.2206%, Training Loss: 1.2143%\n",
      "Epoch [6/300], Step [86/225], Training Accuracy: 40.2253%, Training Loss: 1.2146%\n",
      "Epoch [6/300], Step [87/225], Training Accuracy: 40.1940%, Training Loss: 1.2148%\n",
      "Epoch [6/300], Step [88/225], Training Accuracy: 40.2876%, Training Loss: 1.2146%\n",
      "Epoch [6/300], Step [89/225], Training Accuracy: 40.3265%, Training Loss: 1.2146%\n",
      "Epoch [6/300], Step [90/225], Training Accuracy: 40.3472%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [91/225], Training Accuracy: 40.3674%, Training Loss: 1.2131%\n",
      "Epoch [6/300], Step [92/225], Training Accuracy: 40.4552%, Training Loss: 1.2131%\n",
      "Epoch [6/300], Step [93/225], Training Accuracy: 40.3730%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [94/225], Training Accuracy: 40.5253%, Training Loss: 1.2123%\n",
      "Epoch [6/300], Step [95/225], Training Accuracy: 40.4770%, Training Loss: 1.2141%\n",
      "Epoch [6/300], Step [96/225], Training Accuracy: 40.4785%, Training Loss: 1.2140%\n",
      "Epoch [6/300], Step [97/225], Training Accuracy: 40.4961%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [98/225], Training Accuracy: 40.5134%, Training Loss: 1.2129%\n",
      "Epoch [6/300], Step [99/225], Training Accuracy: 40.5145%, Training Loss: 1.2132%\n",
      "Epoch [6/300], Step [100/225], Training Accuracy: 40.4531%, Training Loss: 1.2131%\n",
      "Epoch [6/300], Step [101/225], Training Accuracy: 40.4084%, Training Loss: 1.2125%\n",
      "Epoch [6/300], Step [102/225], Training Accuracy: 40.4871%, Training Loss: 1.2120%\n",
      "Epoch [6/300], Step [103/225], Training Accuracy: 40.4885%, Training Loss: 1.2128%\n",
      "Epoch [6/300], Step [104/225], Training Accuracy: 40.5799%, Training Loss: 1.2122%\n",
      "Epoch [6/300], Step [105/225], Training Accuracy: 40.5060%, Training Loss: 1.2124%\n",
      "Epoch [6/300], Step [106/225], Training Accuracy: 40.4481%, Training Loss: 1.2123%\n",
      "Epoch [6/300], Step [107/225], Training Accuracy: 40.4936%, Training Loss: 1.2122%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [108/225], Training Accuracy: 40.4803%, Training Loss: 1.2125%\n",
      "Epoch [6/300], Step [109/225], Training Accuracy: 40.4243%, Training Loss: 1.2126%\n",
      "Epoch [6/300], Step [110/225], Training Accuracy: 40.4545%, Training Loss: 1.2130%\n",
      "Epoch [6/300], Step [111/225], Training Accuracy: 40.4702%, Training Loss: 1.2128%\n",
      "Epoch [6/300], Step [112/225], Training Accuracy: 40.5552%, Training Loss: 1.2124%\n",
      "Epoch [6/300], Step [113/225], Training Accuracy: 40.5282%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [114/225], Training Accuracy: 40.5428%, Training Loss: 1.2126%\n",
      "Epoch [6/300], Step [115/225], Training Accuracy: 40.5299%, Training Loss: 1.2125%\n",
      "Epoch [6/300], Step [116/225], Training Accuracy: 40.6115%, Training Loss: 1.2116%\n",
      "Epoch [6/300], Step [117/225], Training Accuracy: 40.5582%, Training Loss: 1.2128%\n",
      "Epoch [6/300], Step [118/225], Training Accuracy: 40.5323%, Training Loss: 1.2129%\n",
      "Epoch [6/300], Step [119/225], Training Accuracy: 40.5593%, Training Loss: 1.2125%\n",
      "Epoch [6/300], Step [120/225], Training Accuracy: 40.5729%, Training Loss: 1.2127%\n",
      "Epoch [6/300], Step [121/225], Training Accuracy: 40.5604%, Training Loss: 1.2127%\n",
      "Epoch [6/300], Step [122/225], Training Accuracy: 40.5866%, Training Loss: 1.2124%\n",
      "Epoch [6/300], Step [123/225], Training Accuracy: 40.5361%, Training Loss: 1.2128%\n",
      "Epoch [6/300], Step [124/225], Training Accuracy: 40.5242%, Training Loss: 1.2128%\n",
      "Epoch [6/300], Step [125/225], Training Accuracy: 40.5000%, Training Loss: 1.2132%\n",
      "Epoch [6/300], Step [126/225], Training Accuracy: 40.4762%, Training Loss: 1.2133%\n",
      "Epoch [6/300], Step [127/225], Training Accuracy: 40.4405%, Training Loss: 1.2134%\n",
      "Epoch [6/300], Step [128/225], Training Accuracy: 40.4663%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [129/225], Training Accuracy: 40.4554%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [130/225], Training Accuracy: 40.4087%, Training Loss: 1.2143%\n",
      "Epoch [6/300], Step [131/225], Training Accuracy: 40.3984%, Training Loss: 1.2143%\n",
      "Epoch [6/300], Step [132/225], Training Accuracy: 40.4001%, Training Loss: 1.2145%\n",
      "Epoch [6/300], Step [133/225], Training Accuracy: 40.4253%, Training Loss: 1.2145%\n",
      "Epoch [6/300], Step [134/225], Training Accuracy: 40.4151%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [135/225], Training Accuracy: 40.3588%, Training Loss: 1.2143%\n",
      "Epoch [6/300], Step [136/225], Training Accuracy: 40.3837%, Training Loss: 1.2137%\n",
      "Epoch [6/300], Step [137/225], Training Accuracy: 40.4425%, Training Loss: 1.2135%\n",
      "Epoch [6/300], Step [138/225], Training Accuracy: 40.4325%, Training Loss: 1.2136%\n",
      "Epoch [6/300], Step [139/225], Training Accuracy: 40.4227%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [140/225], Training Accuracy: 40.4688%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [141/225], Training Accuracy: 40.5142%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [142/225], Training Accuracy: 40.4599%, Training Loss: 1.2139%\n",
      "Epoch [6/300], Step [143/225], Training Accuracy: 40.4830%, Training Loss: 1.2136%\n",
      "Epoch [6/300], Step [144/225], Training Accuracy: 40.4514%, Training Loss: 1.2138%\n",
      "Epoch [6/300], Step [145/225], Training Accuracy: 40.5496%, Training Loss: 1.2127%\n",
      "Epoch [6/300], Step [146/225], Training Accuracy: 40.5501%, Training Loss: 1.2123%\n",
      "Epoch [6/300], Step [147/225], Training Accuracy: 40.5400%, Training Loss: 1.2123%\n",
      "Epoch [6/300], Step [148/225], Training Accuracy: 40.5617%, Training Loss: 1.2121%\n",
      "Epoch [6/300], Step [149/225], Training Accuracy: 40.5621%, Training Loss: 1.2126%\n",
      "Epoch [6/300], Step [150/225], Training Accuracy: 40.5729%, Training Loss: 1.2128%\n",
      "Epoch [6/300], Step [151/225], Training Accuracy: 40.6147%, Training Loss: 1.2119%\n",
      "Epoch [6/300], Step [152/225], Training Accuracy: 40.5942%, Training Loss: 1.2118%\n",
      "Epoch [6/300], Step [153/225], Training Accuracy: 40.6148%, Training Loss: 1.2114%\n",
      "Epoch [6/300], Step [154/225], Training Accuracy: 40.5337%, Training Loss: 1.2117%\n",
      "Epoch [6/300], Step [155/225], Training Accuracy: 40.5343%, Training Loss: 1.2118%\n",
      "Epoch [6/300], Step [156/225], Training Accuracy: 40.5248%, Training Loss: 1.2119%\n",
      "Epoch [6/300], Step [157/225], Training Accuracy: 40.5255%, Training Loss: 1.2119%\n",
      "Epoch [6/300], Step [158/225], Training Accuracy: 40.5854%, Training Loss: 1.2115%\n",
      "Epoch [6/300], Step [159/225], Training Accuracy: 40.5759%, Training Loss: 1.2112%\n",
      "Epoch [6/300], Step [160/225], Training Accuracy: 40.5078%, Training Loss: 1.2115%\n",
      "Epoch [6/300], Step [161/225], Training Accuracy: 40.5085%, Training Loss: 1.2106%\n",
      "Epoch [6/300], Step [162/225], Training Accuracy: 40.5768%, Training Loss: 1.2104%\n",
      "Epoch [6/300], Step [163/225], Training Accuracy: 40.5771%, Training Loss: 1.2100%\n",
      "Epoch [6/300], Step [164/225], Training Accuracy: 40.5774%, Training Loss: 1.2103%\n",
      "Epoch [6/300], Step [165/225], Training Accuracy: 40.5777%, Training Loss: 1.2104%\n",
      "Epoch [6/300], Step [166/225], Training Accuracy: 40.5591%, Training Loss: 1.2105%\n",
      "Epoch [6/300], Step [167/225], Training Accuracy: 40.6063%, Training Loss: 1.2103%\n",
      "Epoch [6/300], Step [168/225], Training Accuracy: 40.5506%, Training Loss: 1.2103%\n",
      "Epoch [6/300], Step [169/225], Training Accuracy: 40.5788%, Training Loss: 1.2101%\n",
      "Epoch [6/300], Step [170/225], Training Accuracy: 40.5882%, Training Loss: 1.2105%\n",
      "Epoch [6/300], Step [171/225], Training Accuracy: 40.5885%, Training Loss: 1.2104%\n",
      "Epoch [6/300], Step [172/225], Training Accuracy: 40.5887%, Training Loss: 1.2106%\n",
      "Epoch [6/300], Step [173/225], Training Accuracy: 40.5708%, Training Loss: 1.2106%\n",
      "Epoch [6/300], Step [174/225], Training Accuracy: 40.5532%, Training Loss: 1.2104%\n",
      "Epoch [6/300], Step [175/225], Training Accuracy: 40.5536%, Training Loss: 1.2101%\n",
      "Epoch [6/300], Step [176/225], Training Accuracy: 40.5273%, Training Loss: 1.2103%\n",
      "Epoch [6/300], Step [177/225], Training Accuracy: 40.5456%, Training Loss: 1.2103%\n",
      "Epoch [6/300], Step [178/225], Training Accuracy: 40.5460%, Training Loss: 1.2101%\n",
      "Epoch [6/300], Step [179/225], Training Accuracy: 40.5639%, Training Loss: 1.2104%\n",
      "Epoch [6/300], Step [180/225], Training Accuracy: 40.6424%, Training Loss: 1.2092%\n",
      "Epoch [6/300], Step [181/225], Training Accuracy: 40.5991%, Training Loss: 1.2098%\n",
      "Epoch [6/300], Step [182/225], Training Accuracy: 40.6422%, Training Loss: 1.2097%\n",
      "Epoch [6/300], Step [183/225], Training Accuracy: 40.6421%, Training Loss: 1.2092%\n",
      "Epoch [6/300], Step [184/225], Training Accuracy: 40.6590%, Training Loss: 1.2093%\n",
      "Epoch [6/300], Step [185/225], Training Accuracy: 40.6926%, Training Loss: 1.2092%\n",
      "Epoch [6/300], Step [186/225], Training Accuracy: 40.7006%, Training Loss: 1.2086%\n",
      "Epoch [6/300], Step [187/225], Training Accuracy: 40.7336%, Training Loss: 1.2082%\n",
      "Epoch [6/300], Step [188/225], Training Accuracy: 40.7746%, Training Loss: 1.2078%\n",
      "Epoch [6/300], Step [189/225], Training Accuracy: 40.8151%, Training Loss: 1.2075%\n",
      "Epoch [6/300], Step [190/225], Training Accuracy: 40.8470%, Training Loss: 1.2077%\n",
      "Epoch [6/300], Step [191/225], Training Accuracy: 40.8459%, Training Loss: 1.2076%\n",
      "Epoch [6/300], Step [192/225], Training Accuracy: 40.8447%, Training Loss: 1.2080%\n",
      "Epoch [6/300], Step [193/225], Training Accuracy: 40.8355%, Training Loss: 1.2075%\n",
      "Epoch [6/300], Step [194/225], Training Accuracy: 40.8586%, Training Loss: 1.2073%\n",
      "Epoch [6/300], Step [195/225], Training Accuracy: 40.8333%, Training Loss: 1.2071%\n",
      "Epoch [6/300], Step [196/225], Training Accuracy: 40.8243%, Training Loss: 1.2070%\n",
      "Epoch [6/300], Step [197/225], Training Accuracy: 40.8233%, Training Loss: 1.2069%\n",
      "Epoch [6/300], Step [198/225], Training Accuracy: 40.8617%, Training Loss: 1.2062%\n",
      "Epoch [6/300], Step [199/225], Training Accuracy: 40.9077%, Training Loss: 1.2059%\n",
      "Epoch [6/300], Step [200/225], Training Accuracy: 40.9062%, Training Loss: 1.2062%\n",
      "Epoch [6/300], Step [201/225], Training Accuracy: 40.8815%, Training Loss: 1.2061%\n",
      "Epoch [6/300], Step [202/225], Training Accuracy: 40.9112%, Training Loss: 1.2059%\n",
      "Epoch [6/300], Step [203/225], Training Accuracy: 40.9406%, Training Loss: 1.2060%\n",
      "Epoch [6/300], Step [204/225], Training Accuracy: 40.9773%, Training Loss: 1.2055%\n",
      "Epoch [6/300], Step [205/225], Training Accuracy: 40.9680%, Training Loss: 1.2059%\n",
      "Epoch [6/300], Step [206/225], Training Accuracy: 41.0042%, Training Loss: 1.2061%\n",
      "Epoch [6/300], Step [207/225], Training Accuracy: 40.9647%, Training Loss: 1.2063%\n",
      "Epoch [6/300], Step [208/225], Training Accuracy: 40.9856%, Training Loss: 1.2058%\n",
      "Epoch [6/300], Step [209/225], Training Accuracy: 40.9839%, Training Loss: 1.2057%\n",
      "Epoch [6/300], Step [210/225], Training Accuracy: 40.9970%, Training Loss: 1.2053%\n",
      "Epoch [6/300], Step [211/225], Training Accuracy: 41.0397%, Training Loss: 1.2051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [212/225], Training Accuracy: 41.0083%, Training Loss: 1.2051%\n",
      "Epoch [6/300], Step [213/225], Training Accuracy: 40.9844%, Training Loss: 1.2053%\n",
      "Epoch [6/300], Step [214/225], Training Accuracy: 40.9828%, Training Loss: 1.2052%\n",
      "Epoch [6/300], Step [215/225], Training Accuracy: 41.0320%, Training Loss: 1.2048%\n",
      "Epoch [6/300], Step [216/225], Training Accuracy: 40.9939%, Training Loss: 1.2051%\n",
      "Epoch [6/300], Step [217/225], Training Accuracy: 40.9850%, Training Loss: 1.2049%\n",
      "Epoch [6/300], Step [218/225], Training Accuracy: 40.9619%, Training Loss: 1.2052%\n",
      "Epoch [6/300], Step [219/225], Training Accuracy: 40.9461%, Training Loss: 1.2050%\n",
      "Epoch [6/300], Step [220/225], Training Accuracy: 40.9517%, Training Loss: 1.2046%\n",
      "Epoch [6/300], Step [221/225], Training Accuracy: 40.9290%, Training Loss: 1.2047%\n",
      "Epoch [6/300], Step [222/225], Training Accuracy: 40.9628%, Training Loss: 1.2046%\n",
      "Epoch [6/300], Step [223/225], Training Accuracy: 40.8842%, Training Loss: 1.2049%\n",
      "Epoch [6/300], Step [224/225], Training Accuracy: 40.8343%, Training Loss: 1.2051%\n",
      "Epoch [6/300], Step [225/225], Training Accuracy: 40.8213%, Training Loss: 1.2056%\n",
      "Epoch [7/300], Step [1/225], Training Accuracy: 57.8125%, Training Loss: 1.1707%\n",
      "Epoch [7/300], Step [2/225], Training Accuracy: 46.8750%, Training Loss: 1.2254%\n",
      "Epoch [7/300], Step [3/225], Training Accuracy: 41.6667%, Training Loss: 1.2412%\n",
      "Epoch [7/300], Step [4/225], Training Accuracy: 41.7969%, Training Loss: 1.2164%\n",
      "Epoch [7/300], Step [5/225], Training Accuracy: 42.8125%, Training Loss: 1.2086%\n",
      "Epoch [7/300], Step [6/225], Training Accuracy: 42.7083%, Training Loss: 1.2194%\n",
      "Epoch [7/300], Step [7/225], Training Accuracy: 42.4107%, Training Loss: 1.2167%\n",
      "Epoch [7/300], Step [8/225], Training Accuracy: 42.7734%, Training Loss: 1.2191%\n",
      "Epoch [7/300], Step [9/225], Training Accuracy: 42.0139%, Training Loss: 1.2224%\n",
      "Epoch [7/300], Step [10/225], Training Accuracy: 42.8125%, Training Loss: 1.2131%\n",
      "Epoch [7/300], Step [11/225], Training Accuracy: 43.1818%, Training Loss: 1.2115%\n",
      "Epoch [7/300], Step [12/225], Training Accuracy: 43.3594%, Training Loss: 1.2085%\n",
      "Epoch [7/300], Step [13/225], Training Accuracy: 43.8702%, Training Loss: 1.2047%\n",
      "Epoch [7/300], Step [14/225], Training Accuracy: 43.7500%, Training Loss: 1.2125%\n",
      "Epoch [7/300], Step [15/225], Training Accuracy: 43.6458%, Training Loss: 1.2167%\n",
      "Epoch [7/300], Step [16/225], Training Accuracy: 43.5547%, Training Loss: 1.2132%\n",
      "Epoch [7/300], Step [17/225], Training Accuracy: 43.9338%, Training Loss: 1.2094%\n",
      "Epoch [7/300], Step [18/225], Training Accuracy: 43.4028%, Training Loss: 1.2068%\n",
      "Epoch [7/300], Step [19/225], Training Accuracy: 42.8454%, Training Loss: 1.2085%\n",
      "Epoch [7/300], Step [20/225], Training Accuracy: 42.7344%, Training Loss: 1.2045%\n",
      "Epoch [7/300], Step [21/225], Training Accuracy: 42.9315%, Training Loss: 1.1998%\n",
      "Epoch [7/300], Step [22/225], Training Accuracy: 42.6847%, Training Loss: 1.1996%\n",
      "Epoch [7/300], Step [23/225], Training Accuracy: 42.8668%, Training Loss: 1.1961%\n",
      "Epoch [7/300], Step [24/225], Training Accuracy: 42.4479%, Training Loss: 1.1974%\n",
      "Epoch [7/300], Step [25/225], Training Accuracy: 42.5000%, Training Loss: 1.1936%\n",
      "Epoch [7/300], Step [26/225], Training Accuracy: 42.1274%, Training Loss: 1.1959%\n",
      "Epoch [7/300], Step [27/225], Training Accuracy: 42.0718%, Training Loss: 1.1959%\n",
      "Epoch [7/300], Step [28/225], Training Accuracy: 42.1317%, Training Loss: 1.1944%\n",
      "Epoch [7/300], Step [29/225], Training Accuracy: 42.1336%, Training Loss: 1.1934%\n",
      "Epoch [7/300], Step [30/225], Training Accuracy: 42.3438%, Training Loss: 1.1908%\n",
      "Epoch [7/300], Step [31/225], Training Accuracy: 42.3387%, Training Loss: 1.1911%\n",
      "Epoch [7/300], Step [32/225], Training Accuracy: 42.2852%, Training Loss: 1.1906%\n",
      "Epoch [7/300], Step [33/225], Training Accuracy: 42.3769%, Training Loss: 1.1876%\n",
      "Epoch [7/300], Step [34/225], Training Accuracy: 42.5551%, Training Loss: 1.1892%\n",
      "Epoch [7/300], Step [35/225], Training Accuracy: 42.4107%, Training Loss: 1.1906%\n",
      "Epoch [7/300], Step [36/225], Training Accuracy: 42.4045%, Training Loss: 1.1917%\n",
      "Epoch [7/300], Step [37/225], Training Accuracy: 42.4831%, Training Loss: 1.1916%\n",
      "Epoch [7/300], Step [38/225], Training Accuracy: 42.6398%, Training Loss: 1.1910%\n",
      "Epoch [7/300], Step [39/225], Training Accuracy: 42.6282%, Training Loss: 1.1897%\n",
      "Epoch [7/300], Step [40/225], Training Accuracy: 42.5781%, Training Loss: 1.1891%\n",
      "Epoch [7/300], Step [41/225], Training Accuracy: 42.5686%, Training Loss: 1.1894%\n",
      "Epoch [7/300], Step [42/225], Training Accuracy: 42.6339%, Training Loss: 1.1895%\n",
      "Epoch [7/300], Step [43/225], Training Accuracy: 42.4419%, Training Loss: 1.1903%\n",
      "Epoch [7/300], Step [44/225], Training Accuracy: 42.6491%, Training Loss: 1.1885%\n",
      "Epoch [7/300], Step [45/225], Training Accuracy: 42.7083%, Training Loss: 1.1878%\n",
      "Epoch [7/300], Step [46/225], Training Accuracy: 42.8329%, Training Loss: 1.1861%\n",
      "Epoch [7/300], Step [47/225], Training Accuracy: 42.7194%, Training Loss: 1.1861%\n",
      "Epoch [7/300], Step [48/225], Training Accuracy: 42.7734%, Training Loss: 1.1852%\n",
      "Epoch [7/300], Step [49/225], Training Accuracy: 42.7296%, Training Loss: 1.1875%\n",
      "Epoch [7/300], Step [50/225], Training Accuracy: 42.7812%, Training Loss: 1.1875%\n",
      "Epoch [7/300], Step [51/225], Training Accuracy: 42.8922%, Training Loss: 1.1871%\n",
      "Epoch [7/300], Step [52/225], Training Accuracy: 42.9087%, Training Loss: 1.1869%\n",
      "Epoch [7/300], Step [53/225], Training Accuracy: 42.8950%, Training Loss: 1.1868%\n",
      "Epoch [7/300], Step [54/225], Training Accuracy: 42.8530%, Training Loss: 1.1872%\n",
      "Epoch [7/300], Step [55/225], Training Accuracy: 42.8409%, Training Loss: 1.1873%\n",
      "Epoch [7/300], Step [56/225], Training Accuracy: 42.7455%, Training Loss: 1.1876%\n",
      "Epoch [7/300], Step [57/225], Training Accuracy: 42.9550%, Training Loss: 1.1855%\n",
      "Epoch [7/300], Step [58/225], Training Accuracy: 42.8879%, Training Loss: 1.1852%\n",
      "Epoch [7/300], Step [59/225], Training Accuracy: 42.9290%, Training Loss: 1.1838%\n",
      "Epoch [7/300], Step [60/225], Training Accuracy: 42.8906%, Training Loss: 1.1833%\n",
      "Epoch [7/300], Step [61/225], Training Accuracy: 42.8535%, Training Loss: 1.1839%\n",
      "Epoch [7/300], Step [62/225], Training Accuracy: 42.7671%, Training Loss: 1.1840%\n",
      "Epoch [7/300], Step [63/225], Training Accuracy: 42.5843%, Training Loss: 1.1857%\n",
      "Epoch [7/300], Step [64/225], Training Accuracy: 42.5781%, Training Loss: 1.1866%\n",
      "Epoch [7/300], Step [65/225], Training Accuracy: 42.5000%, Training Loss: 1.1862%\n",
      "Epoch [7/300], Step [66/225], Training Accuracy: 42.5900%, Training Loss: 1.1858%\n",
      "Epoch [7/300], Step [67/225], Training Accuracy: 42.5606%, Training Loss: 1.1849%\n",
      "Epoch [7/300], Step [68/225], Training Accuracy: 42.5551%, Training Loss: 1.1851%\n",
      "Epoch [7/300], Step [69/225], Training Accuracy: 42.5725%, Training Loss: 1.1847%\n",
      "Epoch [7/300], Step [70/225], Training Accuracy: 42.4107%, Training Loss: 1.1854%\n",
      "Epoch [7/300], Step [71/225], Training Accuracy: 42.4296%, Training Loss: 1.1858%\n",
      "Epoch [7/300], Step [72/225], Training Accuracy: 42.4045%, Training Loss: 1.1873%\n",
      "Epoch [7/300], Step [73/225], Training Accuracy: 42.2945%, Training Loss: 1.1901%\n",
      "Epoch [7/300], Step [74/225], Training Accuracy: 42.2931%, Training Loss: 1.1888%\n",
      "Epoch [7/300], Step [75/225], Training Accuracy: 42.4167%, Training Loss: 1.1873%\n",
      "Epoch [7/300], Step [76/225], Training Accuracy: 42.3931%, Training Loss: 1.1880%\n",
      "Epoch [7/300], Step [77/225], Training Accuracy: 42.4107%, Training Loss: 1.1880%\n",
      "Epoch [7/300], Step [78/225], Training Accuracy: 42.3277%, Training Loss: 1.1884%\n",
      "Epoch [7/300], Step [79/225], Training Accuracy: 42.2864%, Training Loss: 1.1896%\n",
      "Epoch [7/300], Step [80/225], Training Accuracy: 42.2656%, Training Loss: 1.1891%\n",
      "Epoch [7/300], Step [81/225], Training Accuracy: 42.2454%, Training Loss: 1.1895%\n",
      "Epoch [7/300], Step [82/225], Training Accuracy: 42.2066%, Training Loss: 1.1891%\n",
      "Epoch [7/300], Step [83/225], Training Accuracy: 42.0934%, Training Loss: 1.1888%\n",
      "Epoch [7/300], Step [84/225], Training Accuracy: 42.0201%, Training Loss: 1.1890%\n",
      "Epoch [7/300], Step [85/225], Training Accuracy: 42.0221%, Training Loss: 1.1895%\n",
      "Epoch [7/300], Step [86/225], Training Accuracy: 42.0422%, Training Loss: 1.1898%\n",
      "Epoch [7/300], Step [87/225], Training Accuracy: 42.0618%, Training Loss: 1.1898%\n",
      "Epoch [7/300], Step [88/225], Training Accuracy: 42.1342%, Training Loss: 1.1894%\n",
      "Epoch [7/300], Step [89/225], Training Accuracy: 42.1875%, Training Loss: 1.1895%\n",
      "Epoch [7/300], Step [90/225], Training Accuracy: 42.2222%, Training Loss: 1.1887%\n",
      "Epoch [7/300], Step [91/225], Training Accuracy: 42.2905%, Training Loss: 1.1879%\n",
      "Epoch [7/300], Step [92/225], Training Accuracy: 42.3573%, Training Loss: 1.1879%\n",
      "Epoch [7/300], Step [93/225], Training Accuracy: 42.2883%, Training Loss: 1.1882%\n",
      "Epoch [7/300], Step [94/225], Training Accuracy: 42.4036%, Training Loss: 1.1872%\n",
      "Epoch [7/300], Step [95/225], Training Accuracy: 42.3191%, Training Loss: 1.1890%\n",
      "Epoch [7/300], Step [96/225], Training Accuracy: 42.3177%, Training Loss: 1.1889%\n",
      "Epoch [7/300], Step [97/225], Training Accuracy: 42.3003%, Training Loss: 1.1888%\n",
      "Epoch [7/300], Step [98/225], Training Accuracy: 42.2991%, Training Loss: 1.1878%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Step [99/225], Training Accuracy: 42.3295%, Training Loss: 1.1881%\n",
      "Epoch [7/300], Step [100/225], Training Accuracy: 42.2500%, Training Loss: 1.1880%\n",
      "Epoch [7/300], Step [101/225], Training Accuracy: 42.1875%, Training Loss: 1.1877%\n",
      "Epoch [7/300], Step [102/225], Training Accuracy: 42.2488%, Training Loss: 1.1872%\n",
      "Epoch [7/300], Step [103/225], Training Accuracy: 42.1875%, Training Loss: 1.1880%\n",
      "Epoch [7/300], Step [104/225], Training Accuracy: 42.2476%, Training Loss: 1.1873%\n",
      "Epoch [7/300], Step [105/225], Training Accuracy: 42.1875%, Training Loss: 1.1874%\n",
      "Epoch [7/300], Step [106/225], Training Accuracy: 42.1138%, Training Loss: 1.1874%\n",
      "Epoch [7/300], Step [107/225], Training Accuracy: 42.1437%, Training Loss: 1.1872%\n",
      "Epoch [7/300], Step [108/225], Training Accuracy: 42.0862%, Training Loss: 1.1877%\n",
      "Epoch [7/300], Step [109/225], Training Accuracy: 42.0155%, Training Loss: 1.1878%\n",
      "Epoch [7/300], Step [110/225], Training Accuracy: 42.0028%, Training Loss: 1.1883%\n",
      "Epoch [7/300], Step [111/225], Training Accuracy: 42.0327%, Training Loss: 1.1882%\n",
      "Epoch [7/300], Step [112/225], Training Accuracy: 42.1456%, Training Loss: 1.1876%\n",
      "Epoch [7/300], Step [113/225], Training Accuracy: 42.1322%, Training Loss: 1.1886%\n",
      "Epoch [7/300], Step [114/225], Training Accuracy: 42.1738%, Training Loss: 1.1878%\n",
      "Epoch [7/300], Step [115/225], Training Accuracy: 42.2011%, Training Loss: 1.1877%\n",
      "Epoch [7/300], Step [116/225], Training Accuracy: 42.3087%, Training Loss: 1.1867%\n",
      "Epoch [7/300], Step [117/225], Training Accuracy: 42.2810%, Training Loss: 1.1880%\n",
      "Epoch [7/300], Step [118/225], Training Accuracy: 42.2537%, Training Loss: 1.1881%\n",
      "Epoch [7/300], Step [119/225], Training Accuracy: 42.2794%, Training Loss: 1.1876%\n",
      "Epoch [7/300], Step [120/225], Training Accuracy: 42.2917%, Training Loss: 1.1875%\n",
      "Epoch [7/300], Step [121/225], Training Accuracy: 42.2262%, Training Loss: 1.1876%\n",
      "Epoch [7/300], Step [122/225], Training Accuracy: 42.2131%, Training Loss: 1.1872%\n",
      "Epoch [7/300], Step [123/225], Training Accuracy: 42.1367%, Training Loss: 1.1876%\n",
      "Epoch [7/300], Step [124/225], Training Accuracy: 42.1119%, Training Loss: 1.1874%\n",
      "Epoch [7/300], Step [125/225], Training Accuracy: 42.1000%, Training Loss: 1.1879%\n",
      "Epoch [7/300], Step [126/225], Training Accuracy: 42.0635%, Training Loss: 1.1880%\n",
      "Epoch [7/300], Step [127/225], Training Accuracy: 42.0030%, Training Loss: 1.1881%\n",
      "Epoch [7/300], Step [128/225], Training Accuracy: 42.0410%, Training Loss: 1.1887%\n",
      "Epoch [7/300], Step [129/225], Training Accuracy: 42.0179%, Training Loss: 1.1885%\n",
      "Epoch [7/300], Step [130/225], Training Accuracy: 41.9591%, Training Loss: 1.1890%\n",
      "Epoch [7/300], Step [131/225], Training Accuracy: 41.9370%, Training Loss: 1.1890%\n",
      "Epoch [7/300], Step [132/225], Training Accuracy: 41.9626%, Training Loss: 1.1893%\n",
      "Epoch [7/300], Step [133/225], Training Accuracy: 41.9995%, Training Loss: 1.1892%\n",
      "Epoch [7/300], Step [134/225], Training Accuracy: 42.0243%, Training Loss: 1.1884%\n",
      "Epoch [7/300], Step [135/225], Training Accuracy: 42.0023%, Training Loss: 1.1887%\n",
      "Epoch [7/300], Step [136/225], Training Accuracy: 42.0152%, Training Loss: 1.1881%\n",
      "Epoch [7/300], Step [137/225], Training Accuracy: 42.0506%, Training Loss: 1.1879%\n",
      "Epoch [7/300], Step [138/225], Training Accuracy: 42.0630%, Training Loss: 1.1879%\n",
      "Epoch [7/300], Step [139/225], Training Accuracy: 42.0638%, Training Loss: 1.1882%\n",
      "Epoch [7/300], Step [140/225], Training Accuracy: 42.1205%, Training Loss: 1.1882%\n",
      "Epoch [7/300], Step [141/225], Training Accuracy: 42.1543%, Training Loss: 1.1883%\n",
      "Epoch [7/300], Step [142/225], Training Accuracy: 42.1655%, Training Loss: 1.1883%\n",
      "Epoch [7/300], Step [143/225], Training Accuracy: 42.1875%, Training Loss: 1.1879%\n",
      "Epoch [7/300], Step [144/225], Training Accuracy: 42.1658%, Training Loss: 1.1881%\n",
      "Epoch [7/300], Step [145/225], Training Accuracy: 42.2522%, Training Loss: 1.1870%\n",
      "Epoch [7/300], Step [146/225], Training Accuracy: 42.2517%, Training Loss: 1.1868%\n",
      "Epoch [7/300], Step [147/225], Training Accuracy: 42.2300%, Training Loss: 1.1868%\n",
      "Epoch [7/300], Step [148/225], Training Accuracy: 42.2297%, Training Loss: 1.1867%\n",
      "Epoch [7/300], Step [149/225], Training Accuracy: 42.2399%, Training Loss: 1.1872%\n",
      "Epoch [7/300], Step [150/225], Training Accuracy: 42.2396%, Training Loss: 1.1873%\n",
      "Epoch [7/300], Step [151/225], Training Accuracy: 42.2910%, Training Loss: 1.1865%\n",
      "Epoch [7/300], Step [152/225], Training Accuracy: 42.2595%, Training Loss: 1.1864%\n",
      "Epoch [7/300], Step [153/225], Training Accuracy: 42.2896%, Training Loss: 1.1859%\n",
      "Epoch [7/300], Step [154/225], Training Accuracy: 42.2179%, Training Loss: 1.1863%\n",
      "Epoch [7/300], Step [155/225], Training Accuracy: 42.2177%, Training Loss: 1.1865%\n",
      "Epoch [7/300], Step [156/225], Training Accuracy: 42.1975%, Training Loss: 1.1865%\n",
      "Epoch [7/300], Step [157/225], Training Accuracy: 42.1676%, Training Loss: 1.1867%\n",
      "Epoch [7/300], Step [158/225], Training Accuracy: 42.2073%, Training Loss: 1.1863%\n",
      "Epoch [7/300], Step [159/225], Training Accuracy: 42.1875%, Training Loss: 1.1860%\n",
      "Epoch [7/300], Step [160/225], Training Accuracy: 42.1094%, Training Loss: 1.1863%\n",
      "Epoch [7/300], Step [161/225], Training Accuracy: 42.1584%, Training Loss: 1.1854%\n",
      "Epoch [7/300], Step [162/225], Training Accuracy: 42.2357%, Training Loss: 1.1853%\n",
      "Epoch [7/300], Step [163/225], Training Accuracy: 42.2354%, Training Loss: 1.1849%\n",
      "Epoch [7/300], Step [164/225], Training Accuracy: 42.2256%, Training Loss: 1.1853%\n",
      "Epoch [7/300], Step [165/225], Training Accuracy: 42.2538%, Training Loss: 1.1853%\n",
      "Epoch [7/300], Step [166/225], Training Accuracy: 42.2534%, Training Loss: 1.1855%\n",
      "Epoch [7/300], Step [167/225], Training Accuracy: 42.2998%, Training Loss: 1.1851%\n",
      "Epoch [7/300], Step [168/225], Training Accuracy: 42.2433%, Training Loss: 1.1852%\n",
      "Epoch [7/300], Step [169/225], Training Accuracy: 42.2800%, Training Loss: 1.1851%\n",
      "Epoch [7/300], Step [170/225], Training Accuracy: 42.2610%, Training Loss: 1.1855%\n",
      "Epoch [7/300], Step [171/225], Training Accuracy: 42.2515%, Training Loss: 1.1854%\n",
      "Epoch [7/300], Step [172/225], Training Accuracy: 42.2238%, Training Loss: 1.1857%\n",
      "Epoch [7/300], Step [173/225], Training Accuracy: 42.1875%, Training Loss: 1.1857%\n",
      "Epoch [7/300], Step [174/225], Training Accuracy: 42.1965%, Training Loss: 1.1855%\n",
      "Epoch [7/300], Step [175/225], Training Accuracy: 42.1696%, Training Loss: 1.1853%\n",
      "Epoch [7/300], Step [176/225], Training Accuracy: 42.1431%, Training Loss: 1.1854%\n",
      "Epoch [7/300], Step [177/225], Training Accuracy: 42.1610%, Training Loss: 1.1853%\n",
      "Epoch [7/300], Step [178/225], Training Accuracy: 42.1436%, Training Loss: 1.1851%\n",
      "Epoch [7/300], Step [179/225], Training Accuracy: 42.1788%, Training Loss: 1.1854%\n",
      "Epoch [7/300], Step [180/225], Training Accuracy: 42.2483%, Training Loss: 1.1843%\n",
      "Epoch [7/300], Step [181/225], Training Accuracy: 42.2220%, Training Loss: 1.1850%\n",
      "Epoch [7/300], Step [182/225], Training Accuracy: 42.2476%, Training Loss: 1.1849%\n",
      "Epoch [7/300], Step [183/225], Training Accuracy: 42.2558%, Training Loss: 1.1844%\n",
      "Epoch [7/300], Step [184/225], Training Accuracy: 42.2639%, Training Loss: 1.1846%\n",
      "Epoch [7/300], Step [185/225], Training Accuracy: 42.2973%, Training Loss: 1.1843%\n",
      "Epoch [7/300], Step [186/225], Training Accuracy: 42.3051%, Training Loss: 1.1838%\n",
      "Epoch [7/300], Step [187/225], Training Accuracy: 42.3212%, Training Loss: 1.1836%\n",
      "Epoch [7/300], Step [188/225], Training Accuracy: 42.3703%, Training Loss: 1.1831%\n",
      "Epoch [7/300], Step [189/225], Training Accuracy: 42.4190%, Training Loss: 1.1829%\n",
      "Epoch [7/300], Step [190/225], Training Accuracy: 42.4424%, Training Loss: 1.1830%\n",
      "Epoch [7/300], Step [191/225], Training Accuracy: 42.4247%, Training Loss: 1.1830%\n",
      "Epoch [7/300], Step [192/225], Training Accuracy: 42.4154%, Training Loss: 1.1833%\n",
      "Epoch [7/300], Step [193/225], Training Accuracy: 42.3818%, Training Loss: 1.1829%\n",
      "Epoch [7/300], Step [194/225], Training Accuracy: 42.4291%, Training Loss: 1.1827%\n",
      "Epoch [7/300], Step [195/225], Training Accuracy: 42.4599%, Training Loss: 1.1824%\n",
      "Epoch [7/300], Step [196/225], Training Accuracy: 42.4426%, Training Loss: 1.1824%\n",
      "Epoch [7/300], Step [197/225], Training Accuracy: 42.4492%, Training Loss: 1.1822%\n",
      "Epoch [7/300], Step [198/225], Training Accuracy: 42.4637%, Training Loss: 1.1815%\n",
      "Epoch [7/300], Step [199/225], Training Accuracy: 42.5094%, Training Loss: 1.1812%\n",
      "Epoch [7/300], Step [200/225], Training Accuracy: 42.5000%, Training Loss: 1.1816%\n",
      "Epoch [7/300], Step [201/225], Training Accuracy: 42.4751%, Training Loss: 1.1815%\n",
      "Epoch [7/300], Step [202/225], Training Accuracy: 42.5046%, Training Loss: 1.1813%\n",
      "Epoch [7/300], Step [203/225], Training Accuracy: 42.5262%, Training Loss: 1.1814%\n",
      "Epoch [7/300], Step [204/225], Training Accuracy: 42.5628%, Training Loss: 1.1810%\n",
      "Epoch [7/300], Step [205/225], Training Accuracy: 42.5381%, Training Loss: 1.1813%\n",
      "Epoch [7/300], Step [206/225], Training Accuracy: 42.5516%, Training Loss: 1.1817%\n",
      "Epoch [7/300], Step [207/225], Training Accuracy: 42.5045%, Training Loss: 1.1819%\n",
      "Epoch [7/300], Step [208/225], Training Accuracy: 42.5180%, Training Loss: 1.1814%\n",
      "Epoch [7/300], Step [209/225], Training Accuracy: 42.5015%, Training Loss: 1.1813%\n",
      "Epoch [7/300], Step [210/225], Training Accuracy: 42.5223%, Training Loss: 1.1810%\n",
      "Epoch [7/300], Step [211/225], Training Accuracy: 42.5504%, Training Loss: 1.1808%\n",
      "Epoch [7/300], Step [212/225], Training Accuracy: 42.5339%, Training Loss: 1.1810%\n",
      "Epoch [7/300], Step [213/225], Training Accuracy: 42.5323%, Training Loss: 1.1812%\n",
      "Epoch [7/300], Step [214/225], Training Accuracy: 42.5453%, Training Loss: 1.1811%\n",
      "Epoch [7/300], Step [215/225], Training Accuracy: 42.5654%, Training Loss: 1.1806%\n",
      "Epoch [7/300], Step [216/225], Training Accuracy: 42.5203%, Training Loss: 1.1810%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Step [217/225], Training Accuracy: 42.5187%, Training Loss: 1.1809%\n",
      "Epoch [7/300], Step [218/225], Training Accuracy: 42.4742%, Training Loss: 1.1812%\n",
      "Epoch [7/300], Step [219/225], Training Accuracy: 42.4586%, Training Loss: 1.1810%\n",
      "Epoch [7/300], Step [220/225], Training Accuracy: 42.4716%, Training Loss: 1.1807%\n",
      "Epoch [7/300], Step [221/225], Training Accuracy: 42.4491%, Training Loss: 1.1808%\n",
      "Epoch [7/300], Step [222/225], Training Accuracy: 42.4901%, Training Loss: 1.1807%\n",
      "Epoch [7/300], Step [223/225], Training Accuracy: 42.4187%, Training Loss: 1.1811%\n",
      "Epoch [7/300], Step [224/225], Training Accuracy: 42.3619%, Training Loss: 1.1813%\n",
      "Epoch [7/300], Step [225/225], Training Accuracy: 42.3360%, Training Loss: 1.1818%\n",
      "Epoch [8/300], Step [1/225], Training Accuracy: 54.6875%, Training Loss: 1.1408%\n",
      "Epoch [8/300], Step [2/225], Training Accuracy: 45.3125%, Training Loss: 1.2074%\n",
      "Epoch [8/300], Step [3/225], Training Accuracy: 41.1458%, Training Loss: 1.2238%\n",
      "Epoch [8/300], Step [4/225], Training Accuracy: 41.4062%, Training Loss: 1.1999%\n",
      "Epoch [8/300], Step [5/225], Training Accuracy: 42.5000%, Training Loss: 1.1878%\n",
      "Epoch [8/300], Step [6/225], Training Accuracy: 42.9688%, Training Loss: 1.2022%\n",
      "Epoch [8/300], Step [7/225], Training Accuracy: 43.3036%, Training Loss: 1.1983%\n",
      "Epoch [8/300], Step [8/225], Training Accuracy: 43.5547%, Training Loss: 1.2007%\n",
      "Epoch [8/300], Step [9/225], Training Accuracy: 43.0556%, Training Loss: 1.2024%\n",
      "Epoch [8/300], Step [10/225], Training Accuracy: 43.9062%, Training Loss: 1.1916%\n",
      "Epoch [8/300], Step [11/225], Training Accuracy: 44.3182%, Training Loss: 1.1892%\n",
      "Epoch [8/300], Step [12/225], Training Accuracy: 44.5312%, Training Loss: 1.1869%\n",
      "Epoch [8/300], Step [13/225], Training Accuracy: 44.9519%, Training Loss: 1.1835%\n",
      "Epoch [8/300], Step [14/225], Training Accuracy: 44.6429%, Training Loss: 1.1912%\n",
      "Epoch [8/300], Step [15/225], Training Accuracy: 44.7917%, Training Loss: 1.1959%\n",
      "Epoch [8/300], Step [16/225], Training Accuracy: 44.9219%, Training Loss: 1.1925%\n",
      "Epoch [8/300], Step [17/225], Training Accuracy: 45.1287%, Training Loss: 1.1889%\n",
      "Epoch [8/300], Step [18/225], Training Accuracy: 44.6181%, Training Loss: 1.1862%\n",
      "Epoch [8/300], Step [19/225], Training Accuracy: 43.9967%, Training Loss: 1.1874%\n",
      "Epoch [8/300], Step [20/225], Training Accuracy: 43.9062%, Training Loss: 1.1834%\n",
      "Epoch [8/300], Step [21/225], Training Accuracy: 44.1964%, Training Loss: 1.1777%\n",
      "Epoch [8/300], Step [22/225], Training Accuracy: 43.9631%, Training Loss: 1.1780%\n",
      "Epoch [8/300], Step [23/225], Training Accuracy: 44.0897%, Training Loss: 1.1747%\n",
      "Epoch [8/300], Step [24/225], Training Accuracy: 43.8151%, Training Loss: 1.1763%\n",
      "Epoch [8/300], Step [25/225], Training Accuracy: 43.9375%, Training Loss: 1.1725%\n",
      "Epoch [8/300], Step [26/225], Training Accuracy: 43.5096%, Training Loss: 1.1749%\n",
      "Epoch [8/300], Step [27/225], Training Accuracy: 43.3449%, Training Loss: 1.1751%\n",
      "Epoch [8/300], Step [28/225], Training Accuracy: 43.1920%, Training Loss: 1.1734%\n",
      "Epoch [8/300], Step [29/225], Training Accuracy: 43.1573%, Training Loss: 1.1723%\n",
      "Epoch [8/300], Step [30/225], Training Accuracy: 43.2292%, Training Loss: 1.1699%\n",
      "Epoch [8/300], Step [31/225], Training Accuracy: 43.1452%, Training Loss: 1.1701%\n",
      "Epoch [8/300], Step [32/225], Training Accuracy: 43.0664%, Training Loss: 1.1697%\n",
      "Epoch [8/300], Step [33/225], Training Accuracy: 43.1818%, Training Loss: 1.1668%\n",
      "Epoch [8/300], Step [34/225], Training Accuracy: 43.2904%, Training Loss: 1.1687%\n",
      "Epoch [8/300], Step [35/225], Training Accuracy: 43.2589%, Training Loss: 1.1695%\n",
      "Epoch [8/300], Step [36/225], Training Accuracy: 43.2292%, Training Loss: 1.1701%\n",
      "Epoch [8/300], Step [37/225], Training Accuracy: 43.2010%, Training Loss: 1.1700%\n",
      "Epoch [8/300], Step [38/225], Training Accuracy: 43.4211%, Training Loss: 1.1690%\n",
      "Epoch [8/300], Step [39/225], Training Accuracy: 43.4696%, Training Loss: 1.1677%\n",
      "Epoch [8/300], Step [40/225], Training Accuracy: 43.4375%, Training Loss: 1.1673%\n",
      "Epoch [8/300], Step [41/225], Training Accuracy: 43.4451%, Training Loss: 1.1680%\n",
      "Epoch [8/300], Step [42/225], Training Accuracy: 43.4524%, Training Loss: 1.1677%\n",
      "Epoch [8/300], Step [43/225], Training Accuracy: 43.2776%, Training Loss: 1.1684%\n",
      "Epoch [8/300], Step [44/225], Training Accuracy: 43.5014%, Training Loss: 1.1665%\n",
      "Epoch [8/300], Step [45/225], Training Accuracy: 43.5764%, Training Loss: 1.1656%\n",
      "Epoch [8/300], Step [46/225], Training Accuracy: 43.7160%, Training Loss: 1.1633%\n",
      "Epoch [8/300], Step [47/225], Training Accuracy: 43.7168%, Training Loss: 1.1632%\n",
      "Epoch [8/300], Step [48/225], Training Accuracy: 43.7500%, Training Loss: 1.1624%\n",
      "Epoch [8/300], Step [49/225], Training Accuracy: 43.7819%, Training Loss: 1.1647%\n",
      "Epoch [8/300], Step [50/225], Training Accuracy: 43.7500%, Training Loss: 1.1647%\n",
      "Epoch [8/300], Step [51/225], Training Accuracy: 43.8725%, Training Loss: 1.1643%\n",
      "Epoch [8/300], Step [52/225], Training Accuracy: 43.8702%, Training Loss: 1.1643%\n",
      "Epoch [8/300], Step [53/225], Training Accuracy: 43.8090%, Training Loss: 1.1643%\n",
      "Epoch [8/300], Step [54/225], Training Accuracy: 43.7211%, Training Loss: 1.1648%\n",
      "Epoch [8/300], Step [55/225], Training Accuracy: 43.7216%, Training Loss: 1.1651%\n",
      "Epoch [8/300], Step [56/225], Training Accuracy: 43.6663%, Training Loss: 1.1653%\n",
      "Epoch [8/300], Step [57/225], Training Accuracy: 43.8596%, Training Loss: 1.1629%\n",
      "Epoch [8/300], Step [58/225], Training Accuracy: 43.7231%, Training Loss: 1.1627%\n",
      "Epoch [8/300], Step [59/225], Training Accuracy: 43.6970%, Training Loss: 1.1616%\n",
      "Epoch [8/300], Step [60/225], Training Accuracy: 43.6458%, Training Loss: 1.1614%\n",
      "Epoch [8/300], Step [61/225], Training Accuracy: 43.6219%, Training Loss: 1.1621%\n",
      "Epoch [8/300], Step [62/225], Training Accuracy: 43.5484%, Training Loss: 1.1621%\n",
      "Epoch [8/300], Step [63/225], Training Accuracy: 43.3532%, Training Loss: 1.1639%\n",
      "Epoch [8/300], Step [64/225], Training Accuracy: 43.2861%, Training Loss: 1.1649%\n",
      "Epoch [8/300], Step [65/225], Training Accuracy: 43.2452%, Training Loss: 1.1645%\n",
      "Epoch [8/300], Step [66/225], Training Accuracy: 43.3239%, Training Loss: 1.1642%\n",
      "Epoch [8/300], Step [67/225], Training Accuracy: 43.3069%, Training Loss: 1.1633%\n",
      "Epoch [8/300], Step [68/225], Training Accuracy: 43.2675%, Training Loss: 1.1637%\n",
      "Epoch [8/300], Step [69/225], Training Accuracy: 43.2518%, Training Loss: 1.1635%\n",
      "Epoch [8/300], Step [70/225], Training Accuracy: 43.1250%, Training Loss: 1.1641%\n",
      "Epoch [8/300], Step [71/225], Training Accuracy: 43.1778%, Training Loss: 1.1644%\n",
      "Epoch [8/300], Step [72/225], Training Accuracy: 43.0773%, Training Loss: 1.1660%\n",
      "Epoch [8/300], Step [73/225], Training Accuracy: 43.0009%, Training Loss: 1.1688%\n",
      "Epoch [8/300], Step [74/225], Training Accuracy: 43.0321%, Training Loss: 1.1672%\n",
      "Epoch [8/300], Step [75/225], Training Accuracy: 43.1458%, Training Loss: 1.1658%\n",
      "Epoch [8/300], Step [76/225], Training Accuracy: 43.1332%, Training Loss: 1.1666%\n",
      "Epoch [8/300], Step [77/225], Training Accuracy: 43.1615%, Training Loss: 1.1666%\n",
      "Epoch [8/300], Step [78/225], Training Accuracy: 43.0689%, Training Loss: 1.1671%\n",
      "Epoch [8/300], Step [79/225], Training Accuracy: 42.9786%, Training Loss: 1.1682%\n",
      "Epoch [8/300], Step [80/225], Training Accuracy: 42.9883%, Training Loss: 1.1677%\n",
      "Epoch [8/300], Step [81/225], Training Accuracy: 42.9205%, Training Loss: 1.1681%\n",
      "Epoch [8/300], Step [82/225], Training Accuracy: 42.8925%, Training Loss: 1.1677%\n",
      "Epoch [8/300], Step [83/225], Training Accuracy: 42.8464%, Training Loss: 1.1675%\n",
      "Epoch [8/300], Step [84/225], Training Accuracy: 42.8013%, Training Loss: 1.1677%\n",
      "Epoch [8/300], Step [85/225], Training Accuracy: 42.8309%, Training Loss: 1.1680%\n",
      "Epoch [8/300], Step [86/225], Training Accuracy: 42.8416%, Training Loss: 1.1684%\n",
      "Epoch [8/300], Step [87/225], Training Accuracy: 42.8700%, Training Loss: 1.1682%\n",
      "Epoch [8/300], Step [88/225], Training Accuracy: 42.9155%, Training Loss: 1.1679%\n",
      "Epoch [8/300], Step [89/225], Training Accuracy: 42.9424%, Training Loss: 1.1681%\n",
      "Epoch [8/300], Step [90/225], Training Accuracy: 42.9688%, Training Loss: 1.1674%\n",
      "Epoch [8/300], Step [91/225], Training Accuracy: 43.0288%, Training Loss: 1.1663%\n",
      "Epoch [8/300], Step [92/225], Training Accuracy: 43.0537%, Training Loss: 1.1664%\n",
      "Epoch [8/300], Step [93/225], Training Accuracy: 43.0108%, Training Loss: 1.1667%\n",
      "Epoch [8/300], Step [94/225], Training Accuracy: 43.1184%, Training Loss: 1.1658%\n",
      "Epoch [8/300], Step [95/225], Training Accuracy: 43.0428%, Training Loss: 1.1675%\n",
      "Epoch [8/300], Step [96/225], Training Accuracy: 43.0501%, Training Loss: 1.1675%\n",
      "Epoch [8/300], Step [97/225], Training Accuracy: 43.0573%, Training Loss: 1.1674%\n",
      "Epoch [8/300], Step [98/225], Training Accuracy: 43.0644%, Training Loss: 1.1664%\n",
      "Epoch [8/300], Step [99/225], Training Accuracy: 43.1187%, Training Loss: 1.1667%\n",
      "Epoch [8/300], Step [100/225], Training Accuracy: 43.0469%, Training Loss: 1.1667%\n",
      "Epoch [8/300], Step [101/225], Training Accuracy: 42.9610%, Training Loss: 1.1664%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [102/225], Training Accuracy: 42.9841%, Training Loss: 1.1661%\n",
      "Epoch [8/300], Step [103/225], Training Accuracy: 42.9005%, Training Loss: 1.1669%\n",
      "Epoch [8/300], Step [104/225], Training Accuracy: 42.9537%, Training Loss: 1.1661%\n",
      "Epoch [8/300], Step [105/225], Training Accuracy: 42.9315%, Training Loss: 1.1660%\n",
      "Epoch [8/300], Step [106/225], Training Accuracy: 42.8361%, Training Loss: 1.1661%\n",
      "Epoch [8/300], Step [107/225], Training Accuracy: 42.8300%, Training Loss: 1.1659%\n",
      "Epoch [8/300], Step [108/225], Training Accuracy: 42.7662%, Training Loss: 1.1664%\n",
      "Epoch [8/300], Step [109/225], Training Accuracy: 42.7036%, Training Loss: 1.1666%\n",
      "Epoch [8/300], Step [110/225], Training Accuracy: 42.6847%, Training Loss: 1.1670%\n",
      "Epoch [8/300], Step [111/225], Training Accuracy: 42.7646%, Training Loss: 1.1669%\n",
      "Epoch [8/300], Step [112/225], Training Accuracy: 42.8432%, Training Loss: 1.1664%\n",
      "Epoch [8/300], Step [113/225], Training Accuracy: 42.8097%, Training Loss: 1.1675%\n",
      "Epoch [8/300], Step [114/225], Training Accuracy: 42.8591%, Training Loss: 1.1667%\n",
      "Epoch [8/300], Step [115/225], Training Accuracy: 42.8804%, Training Loss: 1.1665%\n",
      "Epoch [8/300], Step [116/225], Training Accuracy: 42.9957%, Training Loss: 1.1655%\n",
      "Epoch [8/300], Step [117/225], Training Accuracy: 42.9754%, Training Loss: 1.1667%\n",
      "Epoch [8/300], Step [118/225], Training Accuracy: 42.9820%, Training Loss: 1.1669%\n",
      "Epoch [8/300], Step [119/225], Training Accuracy: 43.0147%, Training Loss: 1.1663%\n",
      "Epoch [8/300], Step [120/225], Training Accuracy: 43.0859%, Training Loss: 1.1659%\n",
      "Epoch [8/300], Step [121/225], Training Accuracy: 42.9881%, Training Loss: 1.1661%\n",
      "Epoch [8/300], Step [122/225], Training Accuracy: 43.0328%, Training Loss: 1.1657%\n",
      "Epoch [8/300], Step [123/225], Training Accuracy: 42.9751%, Training Loss: 1.1659%\n",
      "Epoch [8/300], Step [124/225], Training Accuracy: 42.9688%, Training Loss: 1.1656%\n",
      "Epoch [8/300], Step [125/225], Training Accuracy: 42.9500%, Training Loss: 1.1661%\n",
      "Epoch [8/300], Step [126/225], Training Accuracy: 42.8943%, Training Loss: 1.1663%\n",
      "Epoch [8/300], Step [127/225], Training Accuracy: 42.8396%, Training Loss: 1.1664%\n",
      "Epoch [8/300], Step [128/225], Training Accuracy: 42.8589%, Training Loss: 1.1669%\n",
      "Epoch [8/300], Step [129/225], Training Accuracy: 42.8173%, Training Loss: 1.1668%\n",
      "Epoch [8/300], Step [130/225], Training Accuracy: 42.7644%, Training Loss: 1.1672%\n",
      "Epoch [8/300], Step [131/225], Training Accuracy: 42.7600%, Training Loss: 1.1671%\n",
      "Epoch [8/300], Step [132/225], Training Accuracy: 42.8030%, Training Loss: 1.1675%\n",
      "Epoch [8/300], Step [133/225], Training Accuracy: 42.8571%, Training Loss: 1.1673%\n",
      "Epoch [8/300], Step [134/225], Training Accuracy: 42.8755%, Training Loss: 1.1665%\n",
      "Epoch [8/300], Step [135/225], Training Accuracy: 42.8704%, Training Loss: 1.1666%\n",
      "Epoch [8/300], Step [136/225], Training Accuracy: 42.8768%, Training Loss: 1.1660%\n",
      "Epoch [8/300], Step [137/225], Training Accuracy: 42.9060%, Training Loss: 1.1658%\n",
      "Epoch [8/300], Step [138/225], Training Accuracy: 42.9348%, Training Loss: 1.1656%\n",
      "Epoch [8/300], Step [139/225], Training Accuracy: 42.9406%, Training Loss: 1.1659%\n",
      "Epoch [8/300], Step [140/225], Training Accuracy: 43.0022%, Training Loss: 1.1658%\n",
      "Epoch [8/300], Step [141/225], Training Accuracy: 43.0408%, Training Loss: 1.1659%\n",
      "Epoch [8/300], Step [142/225], Training Accuracy: 43.0348%, Training Loss: 1.1658%\n",
      "Epoch [8/300], Step [143/225], Training Accuracy: 43.0835%, Training Loss: 1.1654%\n",
      "Epoch [8/300], Step [144/225], Training Accuracy: 43.0447%, Training Loss: 1.1656%\n",
      "Epoch [8/300], Step [145/225], Training Accuracy: 43.1250%, Training Loss: 1.1646%\n",
      "Epoch [8/300], Step [146/225], Training Accuracy: 43.1079%, Training Loss: 1.1644%\n",
      "Epoch [8/300], Step [147/225], Training Accuracy: 43.0804%, Training Loss: 1.1646%\n",
      "Epoch [8/300], Step [148/225], Training Accuracy: 43.1060%, Training Loss: 1.1644%\n",
      "Epoch [8/300], Step [149/225], Training Accuracy: 43.1103%, Training Loss: 1.1649%\n",
      "Epoch [8/300], Step [150/225], Training Accuracy: 43.1354%, Training Loss: 1.1651%\n",
      "Epoch [8/300], Step [151/225], Training Accuracy: 43.1705%, Training Loss: 1.1642%\n",
      "Epoch [8/300], Step [152/225], Training Accuracy: 43.1229%, Training Loss: 1.1642%\n",
      "Epoch [8/300], Step [153/225], Training Accuracy: 43.1475%, Training Loss: 1.1636%\n",
      "Epoch [8/300], Step [154/225], Training Accuracy: 43.0601%, Training Loss: 1.1640%\n",
      "Epoch [8/300], Step [155/225], Training Accuracy: 43.0444%, Training Loss: 1.1642%\n",
      "Epoch [8/300], Step [156/225], Training Accuracy: 43.0389%, Training Loss: 1.1642%\n",
      "Epoch [8/300], Step [157/225], Training Accuracy: 43.0036%, Training Loss: 1.1645%\n",
      "Epoch [8/300], Step [158/225], Training Accuracy: 43.0479%, Training Loss: 1.1641%\n",
      "Epoch [8/300], Step [159/225], Training Accuracy: 43.0228%, Training Loss: 1.1638%\n",
      "Epoch [8/300], Step [160/225], Training Accuracy: 42.9688%, Training Loss: 1.1642%\n",
      "Epoch [8/300], Step [161/225], Training Accuracy: 43.0027%, Training Loss: 1.1632%\n",
      "Epoch [8/300], Step [162/225], Training Accuracy: 43.0652%, Training Loss: 1.1631%\n",
      "Epoch [8/300], Step [163/225], Training Accuracy: 43.0790%, Training Loss: 1.1628%\n",
      "Epoch [8/300], Step [164/225], Training Accuracy: 43.0640%, Training Loss: 1.1630%\n",
      "Epoch [8/300], Step [165/225], Training Accuracy: 43.0871%, Training Loss: 1.1630%\n",
      "Epoch [8/300], Step [166/225], Training Accuracy: 43.1099%, Training Loss: 1.1633%\n",
      "Epoch [8/300], Step [167/225], Training Accuracy: 43.1699%, Training Loss: 1.1628%\n",
      "Epoch [8/300], Step [168/225], Training Accuracy: 43.1269%, Training Loss: 1.1629%\n",
      "Epoch [8/300], Step [169/225], Training Accuracy: 43.1398%, Training Loss: 1.1627%\n",
      "Epoch [8/300], Step [170/225], Training Accuracy: 43.1158%, Training Loss: 1.1632%\n",
      "Epoch [8/300], Step [171/225], Training Accuracy: 43.1104%, Training Loss: 1.1631%\n",
      "Epoch [8/300], Step [172/225], Training Accuracy: 43.0959%, Training Loss: 1.1634%\n",
      "Epoch [8/300], Step [173/225], Training Accuracy: 43.0816%, Training Loss: 1.1634%\n",
      "Epoch [8/300], Step [174/225], Training Accuracy: 43.1034%, Training Loss: 1.1632%\n",
      "Epoch [8/300], Step [175/225], Training Accuracy: 43.1071%, Training Loss: 1.1629%\n",
      "Epoch [8/300], Step [176/225], Training Accuracy: 43.0753%, Training Loss: 1.1630%\n",
      "Epoch [8/300], Step [177/225], Training Accuracy: 43.0703%, Training Loss: 1.1629%\n",
      "Epoch [8/300], Step [178/225], Training Accuracy: 43.0478%, Training Loss: 1.1628%\n",
      "Epoch [8/300], Step [179/225], Training Accuracy: 43.0691%, Training Loss: 1.1631%\n",
      "Epoch [8/300], Step [180/225], Training Accuracy: 43.1424%, Training Loss: 1.1619%\n",
      "Epoch [8/300], Step [181/225], Training Accuracy: 43.1026%, Training Loss: 1.1626%\n",
      "Epoch [8/300], Step [182/225], Training Accuracy: 43.1147%, Training Loss: 1.1626%\n",
      "Epoch [8/300], Step [183/225], Training Accuracy: 43.1096%, Training Loss: 1.1622%\n",
      "Epoch [8/300], Step [184/225], Training Accuracy: 43.1131%, Training Loss: 1.1623%\n",
      "Epoch [8/300], Step [185/225], Training Accuracy: 43.1250%, Training Loss: 1.1621%\n",
      "Epoch [8/300], Step [186/225], Training Accuracy: 43.1452%, Training Loss: 1.1617%\n",
      "Epoch [8/300], Step [187/225], Training Accuracy: 43.1651%, Training Loss: 1.1615%\n",
      "Epoch [8/300], Step [188/225], Training Accuracy: 43.2098%, Training Loss: 1.1611%\n",
      "Epoch [8/300], Step [189/225], Training Accuracy: 43.2374%, Training Loss: 1.1608%\n",
      "Epoch [8/300], Step [190/225], Training Accuracy: 43.2566%, Training Loss: 1.1610%\n",
      "Epoch [8/300], Step [191/225], Training Accuracy: 43.2428%, Training Loss: 1.1609%\n",
      "Epoch [8/300], Step [192/225], Training Accuracy: 43.2454%, Training Loss: 1.1613%\n",
      "Epoch [8/300], Step [193/225], Training Accuracy: 43.2238%, Training Loss: 1.1608%\n",
      "Epoch [8/300], Step [194/225], Training Accuracy: 43.2506%, Training Loss: 1.1608%\n",
      "Epoch [8/300], Step [195/225], Training Accuracy: 43.2772%, Training Loss: 1.1605%\n",
      "Epoch [8/300], Step [196/225], Training Accuracy: 43.2717%, Training Loss: 1.1604%\n",
      "Epoch [8/300], Step [197/225], Training Accuracy: 43.2820%, Training Loss: 1.1602%\n",
      "Epoch [8/300], Step [198/225], Training Accuracy: 43.2844%, Training Loss: 1.1595%\n",
      "Epoch [8/300], Step [199/225], Training Accuracy: 43.3260%, Training Loss: 1.1592%\n",
      "Epoch [8/300], Step [200/225], Training Accuracy: 43.3047%, Training Loss: 1.1597%\n",
      "Epoch [8/300], Step [201/225], Training Accuracy: 43.2836%, Training Loss: 1.1596%\n",
      "Epoch [8/300], Step [202/225], Training Accuracy: 43.2936%, Training Loss: 1.1593%\n",
      "Epoch [8/300], Step [203/225], Training Accuracy: 43.3036%, Training Loss: 1.1594%\n",
      "Epoch [8/300], Step [204/225], Training Accuracy: 43.3517%, Training Loss: 1.1591%\n",
      "Epoch [8/300], Step [205/225], Training Accuracy: 43.3232%, Training Loss: 1.1595%\n",
      "Epoch [8/300], Step [206/225], Training Accuracy: 43.3328%, Training Loss: 1.1598%\n",
      "Epoch [8/300], Step [207/225], Training Accuracy: 43.2971%, Training Loss: 1.1601%\n",
      "Epoch [8/300], Step [208/225], Training Accuracy: 43.3293%, Training Loss: 1.1596%\n",
      "Epoch [8/300], Step [209/225], Training Accuracy: 43.3089%, Training Loss: 1.1596%\n",
      "Epoch [8/300], Step [210/225], Training Accuracy: 43.3185%, Training Loss: 1.1592%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [211/225], Training Accuracy: 43.3575%, Training Loss: 1.1591%\n",
      "Epoch [8/300], Step [212/225], Training Accuracy: 43.3373%, Training Loss: 1.1593%\n",
      "Epoch [8/300], Step [213/225], Training Accuracy: 43.3319%, Training Loss: 1.1596%\n",
      "Epoch [8/300], Step [214/225], Training Accuracy: 43.3557%, Training Loss: 1.1595%\n",
      "Epoch [8/300], Step [215/225], Training Accuracy: 43.3866%, Training Loss: 1.1590%\n",
      "Epoch [8/300], Step [216/225], Training Accuracy: 43.3594%, Training Loss: 1.1594%\n",
      "Epoch [8/300], Step [217/225], Training Accuracy: 43.3540%, Training Loss: 1.1594%\n",
      "Epoch [8/300], Step [218/225], Training Accuracy: 43.3128%, Training Loss: 1.1598%\n",
      "Epoch [8/300], Step [219/225], Training Accuracy: 43.2791%, Training Loss: 1.1596%\n",
      "Epoch [8/300], Step [220/225], Training Accuracy: 43.2955%, Training Loss: 1.1593%\n",
      "Epoch [8/300], Step [221/225], Training Accuracy: 43.2834%, Training Loss: 1.1594%\n",
      "Epoch [8/300], Step [222/225], Training Accuracy: 43.3066%, Training Loss: 1.1593%\n",
      "Epoch [8/300], Step [223/225], Training Accuracy: 43.2525%, Training Loss: 1.1597%\n",
      "Epoch [8/300], Step [224/225], Training Accuracy: 43.2059%, Training Loss: 1.1599%\n",
      "Epoch [8/300], Step [225/225], Training Accuracy: 43.1907%, Training Loss: 1.1605%\n",
      "Epoch [9/300], Step [1/225], Training Accuracy: 54.6875%, Training Loss: 1.1131%\n",
      "Epoch [9/300], Step [2/225], Training Accuracy: 44.5312%, Training Loss: 1.1920%\n",
      "Epoch [9/300], Step [3/225], Training Accuracy: 41.6667%, Training Loss: 1.2107%\n",
      "Epoch [9/300], Step [4/225], Training Accuracy: 42.1875%, Training Loss: 1.1851%\n",
      "Epoch [9/300], Step [5/225], Training Accuracy: 43.4375%, Training Loss: 1.1691%\n",
      "Epoch [9/300], Step [6/225], Training Accuracy: 43.4896%, Training Loss: 1.1864%\n",
      "Epoch [9/300], Step [7/225], Training Accuracy: 43.5268%, Training Loss: 1.1814%\n",
      "Epoch [9/300], Step [8/225], Training Accuracy: 43.7500%, Training Loss: 1.1833%\n",
      "Epoch [9/300], Step [9/225], Training Accuracy: 43.4028%, Training Loss: 1.1838%\n",
      "Epoch [9/300], Step [10/225], Training Accuracy: 43.7500%, Training Loss: 1.1722%\n",
      "Epoch [9/300], Step [11/225], Training Accuracy: 44.0341%, Training Loss: 1.1692%\n",
      "Epoch [9/300], Step [12/225], Training Accuracy: 44.4010%, Training Loss: 1.1676%\n",
      "Epoch [9/300], Step [13/225], Training Accuracy: 45.0721%, Training Loss: 1.1648%\n",
      "Epoch [9/300], Step [14/225], Training Accuracy: 44.8661%, Training Loss: 1.1730%\n",
      "Epoch [9/300], Step [15/225], Training Accuracy: 44.7917%, Training Loss: 1.1781%\n",
      "Epoch [9/300], Step [16/225], Training Accuracy: 45.1172%, Training Loss: 1.1749%\n",
      "Epoch [9/300], Step [17/225], Training Accuracy: 45.2206%, Training Loss: 1.1714%\n",
      "Epoch [9/300], Step [18/225], Training Accuracy: 44.7049%, Training Loss: 1.1687%\n",
      "Epoch [9/300], Step [19/225], Training Accuracy: 44.1612%, Training Loss: 1.1695%\n",
      "Epoch [9/300], Step [20/225], Training Accuracy: 43.9844%, Training Loss: 1.1656%\n",
      "Epoch [9/300], Step [21/225], Training Accuracy: 44.1220%, Training Loss: 1.1592%\n",
      "Epoch [9/300], Step [22/225], Training Accuracy: 44.1051%, Training Loss: 1.1598%\n",
      "Epoch [9/300], Step [23/225], Training Accuracy: 44.2935%, Training Loss: 1.1565%\n",
      "Epoch [9/300], Step [24/225], Training Accuracy: 44.0104%, Training Loss: 1.1587%\n",
      "Epoch [9/300], Step [25/225], Training Accuracy: 44.2500%, Training Loss: 1.1548%\n",
      "Epoch [9/300], Step [26/225], Training Accuracy: 43.9303%, Training Loss: 1.1570%\n",
      "Epoch [9/300], Step [27/225], Training Accuracy: 43.8657%, Training Loss: 1.1572%\n",
      "Epoch [9/300], Step [28/225], Training Accuracy: 43.8058%, Training Loss: 1.1555%\n",
      "Epoch [9/300], Step [29/225], Training Accuracy: 43.7500%, Training Loss: 1.1539%\n",
      "Epoch [9/300], Step [30/225], Training Accuracy: 43.8542%, Training Loss: 1.1517%\n",
      "Epoch [9/300], Step [31/225], Training Accuracy: 43.7500%, Training Loss: 1.1518%\n",
      "Epoch [9/300], Step [32/225], Training Accuracy: 43.7500%, Training Loss: 1.1514%\n",
      "Epoch [9/300], Step [33/225], Training Accuracy: 43.9394%, Training Loss: 1.1484%\n",
      "Epoch [9/300], Step [34/225], Training Accuracy: 43.9798%, Training Loss: 1.1505%\n",
      "Epoch [9/300], Step [35/225], Training Accuracy: 43.8839%, Training Loss: 1.1508%\n",
      "Epoch [9/300], Step [36/225], Training Accuracy: 43.8368%, Training Loss: 1.1511%\n",
      "Epoch [9/300], Step [37/225], Training Accuracy: 43.8345%, Training Loss: 1.1505%\n",
      "Epoch [9/300], Step [38/225], Training Accuracy: 44.1201%, Training Loss: 1.1492%\n",
      "Epoch [9/300], Step [39/225], Training Accuracy: 44.1506%, Training Loss: 1.1478%\n",
      "Epoch [9/300], Step [40/225], Training Accuracy: 44.1797%, Training Loss: 1.1476%\n",
      "Epoch [9/300], Step [41/225], Training Accuracy: 44.1311%, Training Loss: 1.1484%\n",
      "Epoch [9/300], Step [42/225], Training Accuracy: 44.1592%, Training Loss: 1.1477%\n",
      "Epoch [9/300], Step [43/225], Training Accuracy: 44.0407%, Training Loss: 1.1481%\n",
      "Epoch [9/300], Step [44/225], Training Accuracy: 44.2472%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [45/225], Training Accuracy: 44.2708%, Training Loss: 1.1452%\n",
      "Epoch [9/300], Step [46/225], Training Accuracy: 44.4973%, Training Loss: 1.1424%\n",
      "Epoch [9/300], Step [47/225], Training Accuracy: 44.4814%, Training Loss: 1.1422%\n",
      "Epoch [9/300], Step [48/225], Training Accuracy: 44.4987%, Training Loss: 1.1415%\n",
      "Epoch [9/300], Step [49/225], Training Accuracy: 44.4834%, Training Loss: 1.1436%\n",
      "Epoch [9/300], Step [50/225], Training Accuracy: 44.5312%, Training Loss: 1.1434%\n",
      "Epoch [9/300], Step [51/225], Training Accuracy: 44.6998%, Training Loss: 1.1431%\n",
      "Epoch [9/300], Step [52/225], Training Accuracy: 44.6815%, Training Loss: 1.1430%\n",
      "Epoch [9/300], Step [53/225], Training Accuracy: 44.5460%, Training Loss: 1.1432%\n",
      "Epoch [9/300], Step [54/225], Training Accuracy: 44.4734%, Training Loss: 1.1438%\n",
      "Epoch [9/300], Step [55/225], Training Accuracy: 44.4034%, Training Loss: 1.1442%\n",
      "Epoch [9/300], Step [56/225], Training Accuracy: 44.3638%, Training Loss: 1.1444%\n",
      "Epoch [9/300], Step [57/225], Training Accuracy: 44.5450%, Training Loss: 1.1418%\n",
      "Epoch [9/300], Step [58/225], Training Accuracy: 44.3427%, Training Loss: 1.1417%\n",
      "Epoch [9/300], Step [59/225], Training Accuracy: 44.3326%, Training Loss: 1.1407%\n",
      "Epoch [9/300], Step [60/225], Training Accuracy: 44.2969%, Training Loss: 1.1409%\n",
      "Epoch [9/300], Step [61/225], Training Accuracy: 44.2879%, Training Loss: 1.1415%\n",
      "Epoch [9/300], Step [62/225], Training Accuracy: 44.2540%, Training Loss: 1.1415%\n",
      "Epoch [9/300], Step [63/225], Training Accuracy: 44.0724%, Training Loss: 1.1435%\n",
      "Epoch [9/300], Step [64/225], Training Accuracy: 44.0674%, Training Loss: 1.1445%\n",
      "Epoch [9/300], Step [65/225], Training Accuracy: 44.0385%, Training Loss: 1.1441%\n",
      "Epoch [9/300], Step [66/225], Training Accuracy: 44.0814%, Training Loss: 1.1439%\n",
      "Epoch [9/300], Step [67/225], Training Accuracy: 44.0299%, Training Loss: 1.1431%\n",
      "Epoch [9/300], Step [68/225], Training Accuracy: 44.0028%, Training Loss: 1.1435%\n",
      "Epoch [9/300], Step [69/225], Training Accuracy: 43.9538%, Training Loss: 1.1435%\n",
      "Epoch [9/300], Step [70/225], Training Accuracy: 43.8393%, Training Loss: 1.1444%\n",
      "Epoch [9/300], Step [71/225], Training Accuracy: 43.8820%, Training Loss: 1.1445%\n",
      "Epoch [9/300], Step [72/225], Training Accuracy: 43.8151%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [73/225], Training Accuracy: 43.7286%, Training Loss: 1.1489%\n",
      "Epoch [9/300], Step [74/225], Training Accuracy: 43.7500%, Training Loss: 1.1473%\n",
      "Epoch [9/300], Step [75/225], Training Accuracy: 43.8125%, Training Loss: 1.1460%\n",
      "Epoch [9/300], Step [76/225], Training Accuracy: 43.7911%, Training Loss: 1.1467%\n",
      "Epoch [9/300], Step [77/225], Training Accuracy: 43.8515%, Training Loss: 1.1467%\n",
      "Epoch [9/300], Step [78/225], Training Accuracy: 43.7700%, Training Loss: 1.1474%\n",
      "Epoch [9/300], Step [79/225], Training Accuracy: 43.6709%, Training Loss: 1.1485%\n",
      "Epoch [9/300], Step [80/225], Training Accuracy: 43.7109%, Training Loss: 1.1481%\n",
      "Epoch [9/300], Step [81/225], Training Accuracy: 43.6728%, Training Loss: 1.1484%\n",
      "Epoch [9/300], Step [82/225], Training Accuracy: 43.6547%, Training Loss: 1.1480%\n",
      "Epoch [9/300], Step [83/225], Training Accuracy: 43.5994%, Training Loss: 1.1478%\n",
      "Epoch [9/300], Step [84/225], Training Accuracy: 43.5454%, Training Loss: 1.1481%\n",
      "Epoch [9/300], Step [85/225], Training Accuracy: 43.5662%, Training Loss: 1.1483%\n",
      "Epoch [9/300], Step [86/225], Training Accuracy: 43.5683%, Training Loss: 1.1486%\n",
      "Epoch [9/300], Step [87/225], Training Accuracy: 43.6063%, Training Loss: 1.1482%\n",
      "Epoch [9/300], Step [88/225], Training Accuracy: 43.6612%, Training Loss: 1.1480%\n",
      "Epoch [9/300], Step [89/225], Training Accuracy: 43.6973%, Training Loss: 1.1483%\n",
      "Epoch [9/300], Step [90/225], Training Accuracy: 43.7326%, Training Loss: 1.1477%\n",
      "Epoch [9/300], Step [91/225], Training Accuracy: 43.7843%, Training Loss: 1.1464%\n",
      "Epoch [9/300], Step [92/225], Training Accuracy: 43.7670%, Training Loss: 1.1466%\n",
      "Epoch [9/300], Step [93/225], Training Accuracy: 43.6996%, Training Loss: 1.1469%\n",
      "Epoch [9/300], Step [94/225], Training Accuracy: 43.7999%, Training Loss: 1.1460%\n",
      "Epoch [9/300], Step [95/225], Training Accuracy: 43.7007%, Training Loss: 1.1475%\n",
      "Epoch [9/300], Step [96/225], Training Accuracy: 43.7337%, Training Loss: 1.1473%\n",
      "Epoch [9/300], Step [97/225], Training Accuracy: 43.7661%, Training Loss: 1.1472%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [98/225], Training Accuracy: 43.7500%, Training Loss: 1.1463%\n",
      "Epoch [9/300], Step [99/225], Training Accuracy: 43.7973%, Training Loss: 1.1466%\n",
      "Epoch [9/300], Step [100/225], Training Accuracy: 43.7031%, Training Loss: 1.1465%\n",
      "Epoch [9/300], Step [101/225], Training Accuracy: 43.6726%, Training Loss: 1.1465%\n",
      "Epoch [9/300], Step [102/225], Training Accuracy: 43.7040%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [103/225], Training Accuracy: 43.6590%, Training Loss: 1.1471%\n",
      "Epoch [9/300], Step [104/225], Training Accuracy: 43.7049%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [105/225], Training Accuracy: 43.6905%, Training Loss: 1.1460%\n",
      "Epoch [9/300], Step [106/225], Training Accuracy: 43.6026%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [107/225], Training Accuracy: 43.6186%, Training Loss: 1.1460%\n",
      "Epoch [9/300], Step [108/225], Training Accuracy: 43.5619%, Training Loss: 1.1465%\n",
      "Epoch [9/300], Step [109/225], Training Accuracy: 43.4920%, Training Loss: 1.1467%\n",
      "Epoch [9/300], Step [110/225], Training Accuracy: 43.4801%, Training Loss: 1.1471%\n",
      "Epoch [9/300], Step [111/225], Training Accuracy: 43.5529%, Training Loss: 1.1470%\n",
      "Epoch [9/300], Step [112/225], Training Accuracy: 43.6105%, Training Loss: 1.1465%\n",
      "Epoch [9/300], Step [113/225], Training Accuracy: 43.5702%, Training Loss: 1.1478%\n",
      "Epoch [9/300], Step [114/225], Training Accuracy: 43.6404%, Training Loss: 1.1470%\n",
      "Epoch [9/300], Step [115/225], Training Accuracy: 43.6685%, Training Loss: 1.1467%\n",
      "Epoch [9/300], Step [116/225], Training Accuracy: 43.7769%, Training Loss: 1.1457%\n",
      "Epoch [9/300], Step [117/225], Training Accuracy: 43.7366%, Training Loss: 1.1469%\n",
      "Epoch [9/300], Step [118/225], Training Accuracy: 43.7500%, Training Loss: 1.1471%\n",
      "Epoch [9/300], Step [119/225], Training Accuracy: 43.7763%, Training Loss: 1.1466%\n",
      "Epoch [9/300], Step [120/225], Training Accuracy: 43.8281%, Training Loss: 1.1460%\n",
      "Epoch [9/300], Step [121/225], Training Accuracy: 43.7242%, Training Loss: 1.1461%\n",
      "Epoch [9/300], Step [122/225], Training Accuracy: 43.7628%, Training Loss: 1.1458%\n",
      "Epoch [9/300], Step [123/225], Training Accuracy: 43.6992%, Training Loss: 1.1460%\n",
      "Epoch [9/300], Step [124/225], Training Accuracy: 43.7122%, Training Loss: 1.1456%\n",
      "Epoch [9/300], Step [125/225], Training Accuracy: 43.7125%, Training Loss: 1.1461%\n",
      "Epoch [9/300], Step [126/225], Training Accuracy: 43.6756%, Training Loss: 1.1463%\n",
      "Epoch [9/300], Step [127/225], Training Accuracy: 43.6270%, Training Loss: 1.1463%\n",
      "Epoch [9/300], Step [128/225], Training Accuracy: 43.6279%, Training Loss: 1.1468%\n",
      "Epoch [9/300], Step [129/225], Training Accuracy: 43.6410%, Training Loss: 1.1467%\n",
      "Epoch [9/300], Step [130/225], Training Accuracy: 43.6178%, Training Loss: 1.1472%\n",
      "Epoch [9/300], Step [131/225], Training Accuracy: 43.6307%, Training Loss: 1.1470%\n",
      "Epoch [9/300], Step [132/225], Training Accuracy: 43.6435%, Training Loss: 1.1474%\n",
      "Epoch [9/300], Step [133/225], Training Accuracy: 43.6913%, Training Loss: 1.1470%\n",
      "Epoch [9/300], Step [134/225], Training Accuracy: 43.7150%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [135/225], Training Accuracy: 43.7037%, Training Loss: 1.1462%\n",
      "Epoch [9/300], Step [136/225], Training Accuracy: 43.7040%, Training Loss: 1.1456%\n",
      "Epoch [9/300], Step [137/225], Training Accuracy: 43.7044%, Training Loss: 1.1454%\n",
      "Epoch [9/300], Step [138/225], Training Accuracy: 43.7387%, Training Loss: 1.1450%\n",
      "Epoch [9/300], Step [139/225], Training Accuracy: 43.7275%, Training Loss: 1.1453%\n",
      "Epoch [9/300], Step [140/225], Training Accuracy: 43.8170%, Training Loss: 1.1451%\n",
      "Epoch [9/300], Step [141/225], Training Accuracy: 43.8608%, Training Loss: 1.1451%\n",
      "Epoch [9/300], Step [142/225], Training Accuracy: 43.8600%, Training Loss: 1.1450%\n",
      "Epoch [9/300], Step [143/225], Training Accuracy: 43.9030%, Training Loss: 1.1446%\n",
      "Epoch [9/300], Step [144/225], Training Accuracy: 43.8585%, Training Loss: 1.1447%\n",
      "Epoch [9/300], Step [145/225], Training Accuracy: 43.9332%, Training Loss: 1.1437%\n",
      "Epoch [9/300], Step [146/225], Training Accuracy: 43.9212%, Training Loss: 1.1436%\n",
      "Epoch [9/300], Step [147/225], Training Accuracy: 43.8669%, Training Loss: 1.1440%\n",
      "Epoch [9/300], Step [148/225], Training Accuracy: 43.8872%, Training Loss: 1.1438%\n",
      "Epoch [9/300], Step [149/225], Training Accuracy: 43.8654%, Training Loss: 1.1442%\n",
      "Epoch [9/300], Step [150/225], Training Accuracy: 43.8750%, Training Loss: 1.1445%\n",
      "Epoch [9/300], Step [151/225], Training Accuracy: 43.9156%, Training Loss: 1.1437%\n",
      "Epoch [9/300], Step [152/225], Training Accuracy: 43.8631%, Training Loss: 1.1437%\n",
      "Epoch [9/300], Step [153/225], Training Accuracy: 43.9032%, Training Loss: 1.1430%\n",
      "Epoch [9/300], Step [154/225], Training Accuracy: 43.8413%, Training Loss: 1.1434%\n",
      "Epoch [9/300], Step [155/225], Training Accuracy: 43.8407%, Training Loss: 1.1436%\n",
      "Epoch [9/300], Step [156/225], Training Accuracy: 43.8301%, Training Loss: 1.1436%\n",
      "Epoch [9/300], Step [157/225], Training Accuracy: 43.8296%, Training Loss: 1.1439%\n",
      "Epoch [9/300], Step [158/225], Training Accuracy: 43.8687%, Training Loss: 1.1436%\n",
      "Epoch [9/300], Step [159/225], Training Accuracy: 43.8679%, Training Loss: 1.1432%\n",
      "Epoch [9/300], Step [160/225], Training Accuracy: 43.8086%, Training Loss: 1.1436%\n",
      "Epoch [9/300], Step [161/225], Training Accuracy: 43.8470%, Training Loss: 1.1426%\n",
      "Epoch [9/300], Step [162/225], Training Accuracy: 43.9043%, Training Loss: 1.1425%\n",
      "Epoch [9/300], Step [163/225], Training Accuracy: 43.9034%, Training Loss: 1.1422%\n",
      "Epoch [9/300], Step [164/225], Training Accuracy: 43.9501%, Training Loss: 1.1422%\n",
      "Epoch [9/300], Step [165/225], Training Accuracy: 43.9773%, Training Loss: 1.1421%\n",
      "Epoch [9/300], Step [166/225], Training Accuracy: 44.0041%, Training Loss: 1.1424%\n",
      "Epoch [9/300], Step [167/225], Training Accuracy: 44.0494%, Training Loss: 1.1418%\n",
      "Epoch [9/300], Step [168/225], Training Accuracy: 44.0011%, Training Loss: 1.1420%\n",
      "Epoch [9/300], Step [169/225], Training Accuracy: 44.0089%, Training Loss: 1.1418%\n",
      "Epoch [9/300], Step [170/225], Training Accuracy: 43.9798%, Training Loss: 1.1424%\n",
      "Epoch [9/300], Step [171/225], Training Accuracy: 43.9419%, Training Loss: 1.1422%\n",
      "Epoch [9/300], Step [172/225], Training Accuracy: 43.9317%, Training Loss: 1.1425%\n",
      "Epoch [9/300], Step [173/225], Training Accuracy: 43.9126%, Training Loss: 1.1424%\n",
      "Epoch [9/300], Step [174/225], Training Accuracy: 43.9386%, Training Loss: 1.1422%\n",
      "Epoch [9/300], Step [175/225], Training Accuracy: 43.9554%, Training Loss: 1.1421%\n",
      "Epoch [9/300], Step [176/225], Training Accuracy: 43.9187%, Training Loss: 1.1422%\n",
      "Epoch [9/300], Step [177/225], Training Accuracy: 43.9001%, Training Loss: 1.1419%\n",
      "Epoch [9/300], Step [178/225], Training Accuracy: 43.8729%, Training Loss: 1.1419%\n",
      "Epoch [9/300], Step [179/225], Training Accuracy: 43.8897%, Training Loss: 1.1423%\n",
      "Epoch [9/300], Step [180/225], Training Accuracy: 43.9844%, Training Loss: 1.1411%\n",
      "Epoch [9/300], Step [181/225], Training Accuracy: 43.9313%, Training Loss: 1.1418%\n",
      "Epoch [9/300], Step [182/225], Training Accuracy: 43.9475%, Training Loss: 1.1418%\n",
      "Epoch [9/300], Step [183/225], Training Accuracy: 43.9549%, Training Loss: 1.1415%\n",
      "Epoch [9/300], Step [184/225], Training Accuracy: 43.9623%, Training Loss: 1.1415%\n",
      "Epoch [9/300], Step [185/225], Training Accuracy: 43.9865%, Training Loss: 1.1413%\n",
      "Epoch [9/300], Step [186/225], Training Accuracy: 44.0272%, Training Loss: 1.1410%\n",
      "Epoch [9/300], Step [187/225], Training Accuracy: 44.0424%, Training Loss: 1.1408%\n",
      "Epoch [9/300], Step [188/225], Training Accuracy: 44.0991%, Training Loss: 1.1405%\n",
      "Epoch [9/300], Step [189/225], Training Accuracy: 44.1138%, Training Loss: 1.1401%\n",
      "Epoch [9/300], Step [190/225], Training Accuracy: 44.1118%, Training Loss: 1.1403%\n",
      "Epoch [9/300], Step [191/225], Training Accuracy: 44.1018%, Training Loss: 1.1403%\n",
      "Epoch [9/300], Step [192/225], Training Accuracy: 44.1243%, Training Loss: 1.1406%\n",
      "Epoch [9/300], Step [193/225], Training Accuracy: 44.1143%, Training Loss: 1.1402%\n",
      "Epoch [9/300], Step [194/225], Training Accuracy: 44.1608%, Training Loss: 1.1403%\n",
      "Epoch [9/300], Step [195/225], Training Accuracy: 44.1907%, Training Loss: 1.1399%\n",
      "Epoch [9/300], Step [196/225], Training Accuracy: 44.1964%, Training Loss: 1.1398%\n",
      "Epoch [9/300], Step [197/225], Training Accuracy: 44.1942%, Training Loss: 1.1396%\n",
      "Epoch [9/300], Step [198/225], Training Accuracy: 44.1998%, Training Loss: 1.1389%\n",
      "Epoch [9/300], Step [199/225], Training Accuracy: 44.2368%, Training Loss: 1.1386%\n",
      "Epoch [9/300], Step [200/225], Training Accuracy: 44.2188%, Training Loss: 1.1391%\n",
      "Epoch [9/300], Step [201/225], Training Accuracy: 44.1853%, Training Loss: 1.1390%\n",
      "Epoch [9/300], Step [202/225], Training Accuracy: 44.2141%, Training Loss: 1.1387%\n",
      "Epoch [9/300], Step [203/225], Training Accuracy: 44.2118%, Training Loss: 1.1389%\n",
      "Epoch [9/300], Step [204/225], Training Accuracy: 44.2708%, Training Loss: 1.1386%\n",
      "Epoch [9/300], Step [205/225], Training Accuracy: 44.2607%, Training Loss: 1.1390%\n",
      "Epoch [9/300], Step [206/225], Training Accuracy: 44.2885%, Training Loss: 1.1393%\n",
      "Epoch [9/300], Step [207/225], Training Accuracy: 44.2406%, Training Loss: 1.1396%\n",
      "Epoch [9/300], Step [208/225], Training Accuracy: 44.2834%, Training Loss: 1.1391%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [209/225], Training Accuracy: 44.2434%, Training Loss: 1.1391%\n",
      "Epoch [9/300], Step [210/225], Training Accuracy: 44.2411%, Training Loss: 1.1388%\n",
      "Epoch [9/300], Step [211/225], Training Accuracy: 44.2758%, Training Loss: 1.1386%\n",
      "Epoch [9/300], Step [212/225], Training Accuracy: 44.2438%, Training Loss: 1.1390%\n",
      "Epoch [9/300], Step [213/225], Training Accuracy: 44.2268%, Training Loss: 1.1394%\n",
      "Epoch [9/300], Step [214/225], Training Accuracy: 44.2465%, Training Loss: 1.1392%\n",
      "Epoch [9/300], Step [215/225], Training Accuracy: 44.2878%, Training Loss: 1.1388%\n",
      "Epoch [9/300], Step [216/225], Training Accuracy: 44.2708%, Training Loss: 1.1391%\n",
      "Epoch [9/300], Step [217/225], Training Accuracy: 44.2468%, Training Loss: 1.1391%\n",
      "Epoch [9/300], Step [218/225], Training Accuracy: 44.2087%, Training Loss: 1.1396%\n",
      "Epoch [9/300], Step [219/225], Training Accuracy: 44.1781%, Training Loss: 1.1395%\n",
      "Epoch [9/300], Step [220/225], Training Accuracy: 44.2116%, Training Loss: 1.1392%\n",
      "Epoch [9/300], Step [221/225], Training Accuracy: 44.1954%, Training Loss: 1.1392%\n",
      "Epoch [9/300], Step [222/225], Training Accuracy: 44.2286%, Training Loss: 1.1391%\n",
      "Epoch [9/300], Step [223/225], Training Accuracy: 44.1704%, Training Loss: 1.1396%\n",
      "Epoch [9/300], Step [224/225], Training Accuracy: 44.1267%, Training Loss: 1.1398%\n",
      "Epoch [9/300], Step [225/225], Training Accuracy: 44.1148%, Training Loss: 1.1403%\n",
      "Epoch [10/300], Step [1/225], Training Accuracy: 53.1250%, Training Loss: 1.0849%\n",
      "Epoch [10/300], Step [2/225], Training Accuracy: 44.5312%, Training Loss: 1.1754%\n",
      "Epoch [10/300], Step [3/225], Training Accuracy: 42.1875%, Training Loss: 1.1989%\n",
      "Epoch [10/300], Step [4/225], Training Accuracy: 42.1875%, Training Loss: 1.1714%\n",
      "Epoch [10/300], Step [5/225], Training Accuracy: 44.3750%, Training Loss: 1.1524%\n",
      "Epoch [10/300], Step [6/225], Training Accuracy: 44.0104%, Training Loss: 1.1722%\n",
      "Epoch [10/300], Step [7/225], Training Accuracy: 44.4196%, Training Loss: 1.1655%\n",
      "Epoch [10/300], Step [8/225], Training Accuracy: 44.7266%, Training Loss: 1.1668%\n",
      "Epoch [10/300], Step [9/225], Training Accuracy: 44.2708%, Training Loss: 1.1668%\n",
      "Epoch [10/300], Step [10/225], Training Accuracy: 44.6875%, Training Loss: 1.1548%\n",
      "Epoch [10/300], Step [11/225], Training Accuracy: 45.0284%, Training Loss: 1.1509%\n",
      "Epoch [10/300], Step [12/225], Training Accuracy: 45.5729%, Training Loss: 1.1502%\n",
      "Epoch [10/300], Step [13/225], Training Accuracy: 46.1538%, Training Loss: 1.1482%\n",
      "Epoch [10/300], Step [14/225], Training Accuracy: 45.6473%, Training Loss: 1.1574%\n",
      "Epoch [10/300], Step [15/225], Training Accuracy: 45.5208%, Training Loss: 1.1628%\n",
      "Epoch [10/300], Step [16/225], Training Accuracy: 45.8984%, Training Loss: 1.1599%\n",
      "Epoch [10/300], Step [17/225], Training Accuracy: 46.0478%, Training Loss: 1.1563%\n",
      "Epoch [10/300], Step [18/225], Training Accuracy: 45.4861%, Training Loss: 1.1532%\n",
      "Epoch [10/300], Step [19/225], Training Accuracy: 44.9013%, Training Loss: 1.1540%\n",
      "Epoch [10/300], Step [20/225], Training Accuracy: 44.6875%, Training Loss: 1.1502%\n",
      "Epoch [10/300], Step [21/225], Training Accuracy: 44.8661%, Training Loss: 1.1434%\n",
      "Epoch [10/300], Step [22/225], Training Accuracy: 44.8153%, Training Loss: 1.1444%\n",
      "Epoch [10/300], Step [23/225], Training Accuracy: 45.0408%, Training Loss: 1.1408%\n",
      "Epoch [10/300], Step [24/225], Training Accuracy: 44.8568%, Training Loss: 1.1433%\n",
      "Epoch [10/300], Step [25/225], Training Accuracy: 45.1875%, Training Loss: 1.1393%\n",
      "Epoch [10/300], Step [26/225], Training Accuracy: 44.8317%, Training Loss: 1.1413%\n",
      "Epoch [10/300], Step [27/225], Training Accuracy: 44.7917%, Training Loss: 1.1413%\n",
      "Epoch [10/300], Step [28/225], Training Accuracy: 44.6987%, Training Loss: 1.1395%\n",
      "Epoch [10/300], Step [29/225], Training Accuracy: 44.7198%, Training Loss: 1.1373%\n",
      "Epoch [10/300], Step [30/225], Training Accuracy: 44.7396%, Training Loss: 1.1354%\n",
      "Epoch [10/300], Step [31/225], Training Accuracy: 44.7581%, Training Loss: 1.1353%\n",
      "Epoch [10/300], Step [32/225], Training Accuracy: 44.7754%, Training Loss: 1.1348%\n",
      "Epoch [10/300], Step [33/225], Training Accuracy: 44.9811%, Training Loss: 1.1318%\n",
      "Epoch [10/300], Step [34/225], Training Accuracy: 44.9908%, Training Loss: 1.1340%\n",
      "Epoch [10/300], Step [35/225], Training Accuracy: 44.9107%, Training Loss: 1.1338%\n",
      "Epoch [10/300], Step [36/225], Training Accuracy: 44.7483%, Training Loss: 1.1338%\n",
      "Epoch [10/300], Step [37/225], Training Accuracy: 44.7213%, Training Loss: 1.1327%\n",
      "Epoch [10/300], Step [38/225], Training Accuracy: 44.9424%, Training Loss: 1.1312%\n",
      "Epoch [10/300], Step [39/225], Training Accuracy: 44.9920%, Training Loss: 1.1297%\n",
      "Epoch [10/300], Step [40/225], Training Accuracy: 45.0391%, Training Loss: 1.1297%\n",
      "Epoch [10/300], Step [41/225], Training Accuracy: 44.9695%, Training Loss: 1.1306%\n",
      "Epoch [10/300], Step [42/225], Training Accuracy: 44.9405%, Training Loss: 1.1296%\n",
      "Epoch [10/300], Step [43/225], Training Accuracy: 44.8401%, Training Loss: 1.1298%\n",
      "Epoch [10/300], Step [44/225], Training Accuracy: 44.9929%, Training Loss: 1.1278%\n",
      "Epoch [10/300], Step [45/225], Training Accuracy: 44.9653%, Training Loss: 1.1267%\n",
      "Epoch [10/300], Step [46/225], Training Accuracy: 45.2446%, Training Loss: 1.1234%\n",
      "Epoch [10/300], Step [47/225], Training Accuracy: 45.2793%, Training Loss: 1.1233%\n",
      "Epoch [10/300], Step [48/225], Training Accuracy: 45.3125%, Training Loss: 1.1227%\n",
      "Epoch [10/300], Step [49/225], Training Accuracy: 45.3125%, Training Loss: 1.1245%\n",
      "Epoch [10/300], Step [50/225], Training Accuracy: 45.3750%, Training Loss: 1.1241%\n",
      "Epoch [10/300], Step [51/225], Training Accuracy: 45.5270%, Training Loss: 1.1237%\n",
      "Epoch [10/300], Step [52/225], Training Accuracy: 45.5829%, Training Loss: 1.1235%\n",
      "Epoch [10/300], Step [53/225], Training Accuracy: 45.4599%, Training Loss: 1.1237%\n",
      "Epoch [10/300], Step [54/225], Training Accuracy: 45.3704%, Training Loss: 1.1243%\n",
      "Epoch [10/300], Step [55/225], Training Accuracy: 45.2841%, Training Loss: 1.1248%\n",
      "Epoch [10/300], Step [56/225], Training Accuracy: 45.2288%, Training Loss: 1.1252%\n",
      "Epoch [10/300], Step [57/225], Training Accuracy: 45.3947%, Training Loss: 1.1224%\n",
      "Epoch [10/300], Step [58/225], Training Accuracy: 45.1778%, Training Loss: 1.1224%\n",
      "Epoch [10/300], Step [59/225], Training Accuracy: 45.1536%, Training Loss: 1.1215%\n",
      "Epoch [10/300], Step [60/225], Training Accuracy: 45.1042%, Training Loss: 1.1220%\n",
      "Epoch [10/300], Step [61/225], Training Accuracy: 45.1076%, Training Loss: 1.1225%\n",
      "Epoch [10/300], Step [62/225], Training Accuracy: 45.0605%, Training Loss: 1.1224%\n",
      "Epoch [10/300], Step [63/225], Training Accuracy: 44.9405%, Training Loss: 1.1246%\n",
      "Epoch [10/300], Step [64/225], Training Accuracy: 44.9219%, Training Loss: 1.1257%\n",
      "Epoch [10/300], Step [65/225], Training Accuracy: 44.8558%, Training Loss: 1.1253%\n",
      "Epoch [10/300], Step [66/225], Training Accuracy: 44.8864%, Training Loss: 1.1250%\n",
      "Epoch [10/300], Step [67/225], Training Accuracy: 44.7994%, Training Loss: 1.1244%\n",
      "Epoch [10/300], Step [68/225], Training Accuracy: 44.7381%, Training Loss: 1.1246%\n",
      "Epoch [10/300], Step [69/225], Training Accuracy: 44.7011%, Training Loss: 1.1248%\n",
      "Epoch [10/300], Step [70/225], Training Accuracy: 44.5982%, Training Loss: 1.1258%\n",
      "Epoch [10/300], Step [71/225], Training Accuracy: 44.6083%, Training Loss: 1.1257%\n",
      "Epoch [10/300], Step [72/225], Training Accuracy: 44.5312%, Training Loss: 1.1274%\n",
      "Epoch [10/300], Step [73/225], Training Accuracy: 44.4563%, Training Loss: 1.1301%\n",
      "Epoch [10/300], Step [74/225], Training Accuracy: 44.4890%, Training Loss: 1.1283%\n",
      "Epoch [10/300], Step [75/225], Training Accuracy: 44.5417%, Training Loss: 1.1271%\n",
      "Epoch [10/300], Step [76/225], Training Accuracy: 44.5312%, Training Loss: 1.1278%\n",
      "Epoch [10/300], Step [77/225], Training Accuracy: 44.6023%, Training Loss: 1.1277%\n",
      "Epoch [10/300], Step [78/225], Training Accuracy: 44.5112%, Training Loss: 1.1285%\n",
      "Epoch [10/300], Step [79/225], Training Accuracy: 44.3829%, Training Loss: 1.1297%\n",
      "Epoch [10/300], Step [80/225], Training Accuracy: 44.3945%, Training Loss: 1.1293%\n",
      "Epoch [10/300], Step [81/225], Training Accuracy: 44.4252%, Training Loss: 1.1297%\n",
      "Epoch [10/300], Step [82/225], Training Accuracy: 44.4360%, Training Loss: 1.1291%\n",
      "Epoch [10/300], Step [83/225], Training Accuracy: 44.4277%, Training Loss: 1.1290%\n",
      "Epoch [10/300], Step [84/225], Training Accuracy: 44.3824%, Training Loss: 1.1295%\n",
      "Epoch [10/300], Step [85/225], Training Accuracy: 44.4118%, Training Loss: 1.1295%\n",
      "Epoch [10/300], Step [86/225], Training Accuracy: 44.4586%, Training Loss: 1.1297%\n",
      "Epoch [10/300], Step [87/225], Training Accuracy: 44.5043%, Training Loss: 1.1294%\n",
      "Epoch [10/300], Step [88/225], Training Accuracy: 44.4957%, Training Loss: 1.1291%\n",
      "Epoch [10/300], Step [89/225], Training Accuracy: 44.5400%, Training Loss: 1.1297%\n",
      "Epoch [10/300], Step [90/225], Training Accuracy: 44.4965%, Training Loss: 1.1291%\n",
      "Epoch [10/300], Step [91/225], Training Accuracy: 44.5570%, Training Loss: 1.1277%\n",
      "Epoch [10/300], Step [92/225], Training Accuracy: 44.5312%, Training Loss: 1.1279%\n",
      "Epoch [10/300], Step [93/225], Training Accuracy: 44.5060%, Training Loss: 1.1282%\n",
      "Epoch [10/300], Step [94/225], Training Accuracy: 44.5977%, Training Loss: 1.1272%\n",
      "Epoch [10/300], Step [95/225], Training Accuracy: 44.4243%, Training Loss: 1.1285%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [96/225], Training Accuracy: 44.4499%, Training Loss: 1.1282%\n",
      "Epoch [10/300], Step [97/225], Training Accuracy: 44.4749%, Training Loss: 1.1280%\n",
      "Epoch [10/300], Step [98/225], Training Accuracy: 44.4515%, Training Loss: 1.1271%\n",
      "Epoch [10/300], Step [99/225], Training Accuracy: 44.5076%, Training Loss: 1.1274%\n",
      "Epoch [10/300], Step [100/225], Training Accuracy: 44.3906%, Training Loss: 1.1273%\n",
      "Epoch [10/300], Step [101/225], Training Accuracy: 44.3379%, Training Loss: 1.1274%\n",
      "Epoch [10/300], Step [102/225], Training Accuracy: 44.3321%, Training Loss: 1.1273%\n",
      "Epoch [10/300], Step [103/225], Training Accuracy: 44.3113%, Training Loss: 1.1281%\n",
      "Epoch [10/300], Step [104/225], Training Accuracy: 44.3359%, Training Loss: 1.1273%\n",
      "Epoch [10/300], Step [105/225], Training Accuracy: 44.3452%, Training Loss: 1.1270%\n",
      "Epoch [10/300], Step [106/225], Training Accuracy: 44.2512%, Training Loss: 1.1273%\n",
      "Epoch [10/300], Step [107/225], Training Accuracy: 44.2465%, Training Loss: 1.1270%\n",
      "Epoch [10/300], Step [108/225], Training Accuracy: 44.1985%, Training Loss: 1.1275%\n",
      "Epoch [10/300], Step [109/225], Training Accuracy: 44.1084%, Training Loss: 1.1278%\n",
      "Epoch [10/300], Step [110/225], Training Accuracy: 44.1051%, Training Loss: 1.1281%\n",
      "Epoch [10/300], Step [111/225], Training Accuracy: 44.1723%, Training Loss: 1.1281%\n",
      "Epoch [10/300], Step [112/225], Training Accuracy: 44.2243%, Training Loss: 1.1277%\n",
      "Epoch [10/300], Step [113/225], Training Accuracy: 44.1648%, Training Loss: 1.1290%\n",
      "Epoch [10/300], Step [114/225], Training Accuracy: 44.2023%, Training Loss: 1.1283%\n",
      "Epoch [10/300], Step [115/225], Training Accuracy: 44.2255%, Training Loss: 1.1279%\n",
      "Epoch [10/300], Step [116/225], Training Accuracy: 44.3561%, Training Loss: 1.1270%\n",
      "Epoch [10/300], Step [117/225], Training Accuracy: 44.3243%, Training Loss: 1.1282%\n",
      "Epoch [10/300], Step [118/225], Training Accuracy: 44.3724%, Training Loss: 1.1284%\n",
      "Epoch [10/300], Step [119/225], Training Accuracy: 44.3934%, Training Loss: 1.1279%\n",
      "Epoch [10/300], Step [120/225], Training Accuracy: 44.4401%, Training Loss: 1.1272%\n",
      "Epoch [10/300], Step [121/225], Training Accuracy: 44.3311%, Training Loss: 1.1274%\n",
      "Epoch [10/300], Step [122/225], Training Accuracy: 44.3519%, Training Loss: 1.1271%\n",
      "Epoch [10/300], Step [123/225], Training Accuracy: 44.3089%, Training Loss: 1.1273%\n",
      "Epoch [10/300], Step [124/225], Training Accuracy: 44.3422%, Training Loss: 1.1268%\n",
      "Epoch [10/300], Step [125/225], Training Accuracy: 44.3375%, Training Loss: 1.1274%\n",
      "Epoch [10/300], Step [126/225], Training Accuracy: 44.3204%, Training Loss: 1.1276%\n",
      "Epoch [10/300], Step [127/225], Training Accuracy: 44.3159%, Training Loss: 1.1275%\n",
      "Epoch [10/300], Step [128/225], Training Accuracy: 44.2749%, Training Loss: 1.1280%\n",
      "Epoch [10/300], Step [129/225], Training Accuracy: 44.2587%, Training Loss: 1.1279%\n",
      "Epoch [10/300], Step [130/225], Training Accuracy: 44.2308%, Training Loss: 1.1284%\n",
      "Epoch [10/300], Step [131/225], Training Accuracy: 44.2629%, Training Loss: 1.1282%\n",
      "Epoch [10/300], Step [132/225], Training Accuracy: 44.2590%, Training Loss: 1.1286%\n",
      "Epoch [10/300], Step [133/225], Training Accuracy: 44.2904%, Training Loss: 1.1281%\n",
      "Epoch [10/300], Step [134/225], Training Accuracy: 44.3097%, Training Loss: 1.1273%\n",
      "Epoch [10/300], Step [135/225], Training Accuracy: 44.2940%, Training Loss: 1.1272%\n",
      "Epoch [10/300], Step [136/225], Training Accuracy: 44.2900%, Training Loss: 1.1266%\n",
      "Epoch [10/300], Step [137/225], Training Accuracy: 44.2974%, Training Loss: 1.1264%\n",
      "Epoch [10/300], Step [138/225], Training Accuracy: 44.3274%, Training Loss: 1.1259%\n",
      "Epoch [10/300], Step [139/225], Training Accuracy: 44.3345%, Training Loss: 1.1262%\n",
      "Epoch [10/300], Step [140/225], Training Accuracy: 44.4196%, Training Loss: 1.1260%\n",
      "Epoch [10/300], Step [141/225], Training Accuracy: 44.4592%, Training Loss: 1.1258%\n",
      "Epoch [10/300], Step [142/225], Training Accuracy: 44.4432%, Training Loss: 1.1258%\n",
      "Epoch [10/300], Step [143/225], Training Accuracy: 44.5039%, Training Loss: 1.1253%\n",
      "Epoch [10/300], Step [144/225], Training Accuracy: 44.4878%, Training Loss: 1.1254%\n",
      "Epoch [10/300], Step [145/225], Training Accuracy: 44.5905%, Training Loss: 1.1244%\n",
      "Epoch [10/300], Step [146/225], Training Accuracy: 44.5634%, Training Loss: 1.1242%\n",
      "Epoch [10/300], Step [147/225], Training Accuracy: 44.5153%, Training Loss: 1.1248%\n",
      "Epoch [10/300], Step [148/225], Training Accuracy: 44.5312%, Training Loss: 1.1246%\n",
      "Epoch [10/300], Step [149/225], Training Accuracy: 44.4945%, Training Loss: 1.1250%\n",
      "Epoch [10/300], Step [150/225], Training Accuracy: 44.4896%, Training Loss: 1.1253%\n",
      "Epoch [10/300], Step [151/225], Training Accuracy: 44.5364%, Training Loss: 1.1245%\n",
      "Epoch [10/300], Step [152/225], Training Accuracy: 44.5004%, Training Loss: 1.1246%\n",
      "Epoch [10/300], Step [153/225], Training Accuracy: 44.5568%, Training Loss: 1.1239%\n",
      "Epoch [10/300], Step [154/225], Training Accuracy: 44.5008%, Training Loss: 1.1242%\n",
      "Epoch [10/300], Step [155/225], Training Accuracy: 44.4758%, Training Loss: 1.1244%\n",
      "Epoch [10/300], Step [156/225], Training Accuracy: 44.4611%, Training Loss: 1.1243%\n",
      "Epoch [10/300], Step [157/225], Training Accuracy: 44.4666%, Training Loss: 1.1247%\n",
      "Epoch [10/300], Step [158/225], Training Accuracy: 44.5115%, Training Loss: 1.1244%\n",
      "Epoch [10/300], Step [159/225], Training Accuracy: 44.5165%, Training Loss: 1.1240%\n",
      "Epoch [10/300], Step [160/225], Training Accuracy: 44.4629%, Training Loss: 1.1245%\n",
      "Epoch [10/300], Step [161/225], Training Accuracy: 44.5264%, Training Loss: 1.1234%\n",
      "Epoch [10/300], Step [162/225], Training Accuracy: 44.5988%, Training Loss: 1.1233%\n",
      "Epoch [10/300], Step [163/225], Training Accuracy: 44.6031%, Training Loss: 1.1230%\n",
      "Epoch [10/300], Step [164/225], Training Accuracy: 44.6361%, Training Loss: 1.1229%\n",
      "Epoch [10/300], Step [165/225], Training Accuracy: 44.6686%, Training Loss: 1.1228%\n",
      "Epoch [10/300], Step [166/225], Training Accuracy: 44.6819%, Training Loss: 1.1230%\n",
      "Epoch [10/300], Step [167/225], Training Accuracy: 44.7137%, Training Loss: 1.1225%\n",
      "Epoch [10/300], Step [168/225], Training Accuracy: 44.6987%, Training Loss: 1.1226%\n",
      "Epoch [10/300], Step [169/225], Training Accuracy: 44.7023%, Training Loss: 1.1225%\n",
      "Epoch [10/300], Step [170/225], Training Accuracy: 44.6875%, Training Loss: 1.1230%\n",
      "Epoch [10/300], Step [171/225], Training Accuracy: 44.6637%, Training Loss: 1.1228%\n",
      "Epoch [10/300], Step [172/225], Training Accuracy: 44.6312%, Training Loss: 1.1232%\n",
      "Epoch [10/300], Step [173/225], Training Accuracy: 44.6261%, Training Loss: 1.1231%\n",
      "Epoch [10/300], Step [174/225], Training Accuracy: 44.6480%, Training Loss: 1.1229%\n",
      "Epoch [10/300], Step [175/225], Training Accuracy: 44.6607%, Training Loss: 1.1228%\n",
      "Epoch [10/300], Step [176/225], Training Accuracy: 44.6200%, Training Loss: 1.1229%\n",
      "Epoch [10/300], Step [177/225], Training Accuracy: 44.5886%, Training Loss: 1.1226%\n",
      "Epoch [10/300], Step [178/225], Training Accuracy: 44.5576%, Training Loss: 1.1227%\n",
      "Epoch [10/300], Step [179/225], Training Accuracy: 44.5705%, Training Loss: 1.1230%\n",
      "Epoch [10/300], Step [180/225], Training Accuracy: 44.6615%, Training Loss: 1.1218%\n",
      "Epoch [10/300], Step [181/225], Training Accuracy: 44.6219%, Training Loss: 1.1225%\n",
      "Epoch [10/300], Step [182/225], Training Accuracy: 44.6257%, Training Loss: 1.1226%\n",
      "Epoch [10/300], Step [183/225], Training Accuracy: 44.6124%, Training Loss: 1.1223%\n",
      "Epoch [10/300], Step [184/225], Training Accuracy: 44.6162%, Training Loss: 1.1223%\n",
      "Epoch [10/300], Step [185/225], Training Accuracy: 44.6284%, Training Loss: 1.1221%\n",
      "Epoch [10/300], Step [186/225], Training Accuracy: 44.6741%, Training Loss: 1.1218%\n",
      "Epoch [10/300], Step [187/225], Training Accuracy: 44.7109%, Training Loss: 1.1216%\n",
      "Epoch [10/300], Step [188/225], Training Accuracy: 44.7640%, Training Loss: 1.1213%\n",
      "Epoch [10/300], Step [189/225], Training Accuracy: 44.7999%, Training Loss: 1.1209%\n",
      "Epoch [10/300], Step [190/225], Training Accuracy: 44.8109%, Training Loss: 1.1212%\n",
      "Epoch [10/300], Step [191/225], Training Accuracy: 44.7808%, Training Loss: 1.1212%\n",
      "Epoch [10/300], Step [192/225], Training Accuracy: 44.7998%, Training Loss: 1.1214%\n",
      "Epoch [10/300], Step [193/225], Training Accuracy: 44.7944%, Training Loss: 1.1210%\n",
      "Epoch [10/300], Step [194/225], Training Accuracy: 44.8212%, Training Loss: 1.1211%\n",
      "Epoch [10/300], Step [195/225], Training Accuracy: 44.8397%, Training Loss: 1.1208%\n",
      "Epoch [10/300], Step [196/225], Training Accuracy: 44.8501%, Training Loss: 1.1207%\n",
      "Epoch [10/300], Step [197/225], Training Accuracy: 44.8525%, Training Loss: 1.1204%\n",
      "Epoch [10/300], Step [198/225], Training Accuracy: 44.8548%, Training Loss: 1.1198%\n",
      "Epoch [10/300], Step [199/225], Training Accuracy: 44.8807%, Training Loss: 1.1195%\n",
      "Epoch [10/300], Step [200/225], Training Accuracy: 44.8750%, Training Loss: 1.1199%\n",
      "Epoch [10/300], Step [201/225], Training Accuracy: 44.8539%, Training Loss: 1.1200%\n",
      "Epoch [10/300], Step [202/225], Training Accuracy: 44.8871%, Training Loss: 1.1196%\n",
      "Epoch [10/300], Step [203/225], Training Accuracy: 44.8969%, Training Loss: 1.1199%\n",
      "Epoch [10/300], Step [204/225], Training Accuracy: 44.9219%, Training Loss: 1.1196%\n",
      "Epoch [10/300], Step [205/225], Training Accuracy: 44.9162%, Training Loss: 1.1199%\n",
      "Epoch [10/300], Step [206/225], Training Accuracy: 44.9484%, Training Loss: 1.1203%\n",
      "Epoch [10/300], Step [207/225], Training Accuracy: 44.9124%, Training Loss: 1.1206%\n",
      "Epoch [10/300], Step [208/225], Training Accuracy: 44.9669%, Training Loss: 1.1200%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [209/225], Training Accuracy: 44.9387%, Training Loss: 1.1201%\n",
      "Epoch [10/300], Step [210/225], Training Accuracy: 44.9330%, Training Loss: 1.1197%\n",
      "Epoch [10/300], Step [211/225], Training Accuracy: 44.9719%, Training Loss: 1.1196%\n",
      "Epoch [10/300], Step [212/225], Training Accuracy: 44.9219%, Training Loss: 1.1200%\n",
      "Epoch [10/300], Step [213/225], Training Accuracy: 44.8944%, Training Loss: 1.1205%\n",
      "Epoch [10/300], Step [214/225], Training Accuracy: 44.9182%, Training Loss: 1.1203%\n",
      "Epoch [10/300], Step [215/225], Training Accuracy: 44.9491%, Training Loss: 1.1200%\n",
      "Epoch [10/300], Step [216/225], Training Accuracy: 44.9363%, Training Loss: 1.1202%\n",
      "Epoch [10/300], Step [217/225], Training Accuracy: 44.9093%, Training Loss: 1.1202%\n",
      "Epoch [10/300], Step [218/225], Training Accuracy: 44.8753%, Training Loss: 1.1208%\n",
      "Epoch [10/300], Step [219/225], Training Accuracy: 44.8416%, Training Loss: 1.1207%\n",
      "Epoch [10/300], Step [220/225], Training Accuracy: 44.8935%, Training Loss: 1.1204%\n",
      "Epoch [10/300], Step [221/225], Training Accuracy: 44.8671%, Training Loss: 1.1205%\n",
      "Epoch [10/300], Step [222/225], Training Accuracy: 44.9043%, Training Loss: 1.1204%\n",
      "Epoch [10/300], Step [223/225], Training Accuracy: 44.8360%, Training Loss: 1.1209%\n",
      "Epoch [10/300], Step [224/225], Training Accuracy: 44.7963%, Training Loss: 1.1210%\n",
      "Epoch [10/300], Step [225/225], Training Accuracy: 44.7888%, Training Loss: 1.1215%\n",
      "Epoch [11/300], Step [1/225], Training Accuracy: 54.6875%, Training Loss: 1.0572%\n",
      "Epoch [11/300], Step [2/225], Training Accuracy: 42.9688%, Training Loss: 1.1560%\n",
      "Epoch [11/300], Step [3/225], Training Accuracy: 41.6667%, Training Loss: 1.1852%\n",
      "Epoch [11/300], Step [4/225], Training Accuracy: 42.9688%, Training Loss: 1.1569%\n",
      "Epoch [11/300], Step [5/225], Training Accuracy: 45.0000%, Training Loss: 1.1364%\n",
      "Epoch [11/300], Step [6/225], Training Accuracy: 45.0521%, Training Loss: 1.1575%\n",
      "Epoch [11/300], Step [7/225], Training Accuracy: 45.7589%, Training Loss: 1.1495%\n",
      "Epoch [11/300], Step [8/225], Training Accuracy: 45.7031%, Training Loss: 1.1504%\n",
      "Epoch [11/300], Step [9/225], Training Accuracy: 44.9653%, Training Loss: 1.1499%\n",
      "Epoch [11/300], Step [10/225], Training Accuracy: 45.1562%, Training Loss: 1.1379%\n",
      "Epoch [11/300], Step [11/225], Training Accuracy: 45.5966%, Training Loss: 1.1331%\n",
      "Epoch [11/300], Step [12/225], Training Accuracy: 45.9635%, Training Loss: 1.1335%\n",
      "Epoch [11/300], Step [13/225], Training Accuracy: 46.3942%, Training Loss: 1.1323%\n",
      "Epoch [11/300], Step [14/225], Training Accuracy: 45.7589%, Training Loss: 1.1425%\n",
      "Epoch [11/300], Step [15/225], Training Accuracy: 45.6250%, Training Loss: 1.1477%\n",
      "Epoch [11/300], Step [16/225], Training Accuracy: 45.8984%, Training Loss: 1.1455%\n",
      "Epoch [11/300], Step [17/225], Training Accuracy: 46.1397%, Training Loss: 1.1413%\n",
      "Epoch [11/300], Step [18/225], Training Accuracy: 45.5729%, Training Loss: 1.1381%\n",
      "Epoch [11/300], Step [19/225], Training Accuracy: 44.9836%, Training Loss: 1.1388%\n",
      "Epoch [11/300], Step [20/225], Training Accuracy: 45.0000%, Training Loss: 1.1350%\n",
      "Epoch [11/300], Step [21/225], Training Accuracy: 45.3125%, Training Loss: 1.1280%\n",
      "Epoch [11/300], Step [22/225], Training Accuracy: 45.2415%, Training Loss: 1.1293%\n",
      "Epoch [11/300], Step [23/225], Training Accuracy: 45.4484%, Training Loss: 1.1255%\n",
      "Epoch [11/300], Step [24/225], Training Accuracy: 45.3125%, Training Loss: 1.1286%\n",
      "Epoch [11/300], Step [25/225], Training Accuracy: 45.6250%, Training Loss: 1.1246%\n",
      "Epoch [11/300], Step [26/225], Training Accuracy: 45.3125%, Training Loss: 1.1265%\n",
      "Epoch [11/300], Step [27/225], Training Accuracy: 45.2546%, Training Loss: 1.1263%\n",
      "Epoch [11/300], Step [28/225], Training Accuracy: 45.3125%, Training Loss: 1.1244%\n",
      "Epoch [11/300], Step [29/225], Training Accuracy: 45.2586%, Training Loss: 1.1216%\n",
      "Epoch [11/300], Step [30/225], Training Accuracy: 45.1562%, Training Loss: 1.1199%\n",
      "Epoch [11/300], Step [31/225], Training Accuracy: 45.1613%, Training Loss: 1.1198%\n",
      "Epoch [11/300], Step [32/225], Training Accuracy: 45.2148%, Training Loss: 1.1192%\n",
      "Epoch [11/300], Step [33/225], Training Accuracy: 45.4545%, Training Loss: 1.1162%\n",
      "Epoch [11/300], Step [34/225], Training Accuracy: 45.4963%, Training Loss: 1.1185%\n",
      "Epoch [11/300], Step [35/225], Training Accuracy: 45.4911%, Training Loss: 1.1178%\n",
      "Epoch [11/300], Step [36/225], Training Accuracy: 45.3993%, Training Loss: 1.1178%\n",
      "Epoch [11/300], Step [37/225], Training Accuracy: 45.3547%, Training Loss: 1.1162%\n",
      "Epoch [11/300], Step [38/225], Training Accuracy: 45.5592%, Training Loss: 1.1146%\n",
      "Epoch [11/300], Step [39/225], Training Accuracy: 45.5529%, Training Loss: 1.1130%\n",
      "Epoch [11/300], Step [40/225], Training Accuracy: 45.6250%, Training Loss: 1.1130%\n",
      "Epoch [11/300], Step [41/225], Training Accuracy: 45.4649%, Training Loss: 1.1140%\n",
      "Epoch [11/300], Step [42/225], Training Accuracy: 45.4613%, Training Loss: 1.1129%\n",
      "Epoch [11/300], Step [43/225], Training Accuracy: 45.3852%, Training Loss: 1.1129%\n",
      "Epoch [11/300], Step [44/225], Training Accuracy: 45.5256%, Training Loss: 1.1108%\n",
      "Epoch [11/300], Step [45/225], Training Accuracy: 45.5556%, Training Loss: 1.1096%\n",
      "Epoch [11/300], Step [46/225], Training Accuracy: 45.8899%, Training Loss: 1.1061%\n",
      "Epoch [11/300], Step [47/225], Training Accuracy: 45.9109%, Training Loss: 1.1060%\n",
      "Epoch [11/300], Step [48/225], Training Accuracy: 45.9635%, Training Loss: 1.1056%\n",
      "Epoch [11/300], Step [49/225], Training Accuracy: 45.9503%, Training Loss: 1.1071%\n",
      "Epoch [11/300], Step [50/225], Training Accuracy: 46.0312%, Training Loss: 1.1064%\n",
      "Epoch [11/300], Step [51/225], Training Accuracy: 46.1397%, Training Loss: 1.1060%\n",
      "Epoch [11/300], Step [52/225], Training Accuracy: 46.2740%, Training Loss: 1.1057%\n",
      "Epoch [11/300], Step [53/225], Training Accuracy: 46.1675%, Training Loss: 1.1058%\n",
      "Epoch [11/300], Step [54/225], Training Accuracy: 46.0648%, Training Loss: 1.1063%\n",
      "Epoch [11/300], Step [55/225], Training Accuracy: 45.9659%, Training Loss: 1.1070%\n",
      "Epoch [11/300], Step [56/225], Training Accuracy: 45.8705%, Training Loss: 1.1074%\n",
      "Epoch [11/300], Step [57/225], Training Accuracy: 46.0800%, Training Loss: 1.1044%\n",
      "Epoch [11/300], Step [58/225], Training Accuracy: 45.9591%, Training Loss: 1.1046%\n",
      "Epoch [11/300], Step [59/225], Training Accuracy: 46.0275%, Training Loss: 1.1039%\n",
      "Epoch [11/300], Step [60/225], Training Accuracy: 45.9635%, Training Loss: 1.1044%\n",
      "Epoch [11/300], Step [61/225], Training Accuracy: 45.9529%, Training Loss: 1.1047%\n",
      "Epoch [11/300], Step [62/225], Training Accuracy: 45.9425%, Training Loss: 1.1046%\n",
      "Epoch [11/300], Step [63/225], Training Accuracy: 45.8085%, Training Loss: 1.1067%\n",
      "Epoch [11/300], Step [64/225], Training Accuracy: 45.8252%, Training Loss: 1.1079%\n",
      "Epoch [11/300], Step [65/225], Training Accuracy: 45.7692%, Training Loss: 1.1076%\n",
      "Epoch [11/300], Step [66/225], Training Accuracy: 45.7623%, Training Loss: 1.1072%\n",
      "Epoch [11/300], Step [67/225], Training Accuracy: 45.6390%, Training Loss: 1.1067%\n",
      "Epoch [11/300], Step [68/225], Training Accuracy: 45.5882%, Training Loss: 1.1068%\n",
      "Epoch [11/300], Step [69/225], Training Accuracy: 45.5163%, Training Loss: 1.1071%\n",
      "Epoch [11/300], Step [70/225], Training Accuracy: 45.4464%, Training Loss: 1.1083%\n",
      "Epoch [11/300], Step [71/225], Training Accuracy: 45.4445%, Training Loss: 1.1079%\n",
      "Epoch [11/300], Step [72/225], Training Accuracy: 45.3559%, Training Loss: 1.1097%\n",
      "Epoch [11/300], Step [73/225], Training Accuracy: 45.3553%, Training Loss: 1.1124%\n",
      "Epoch [11/300], Step [74/225], Training Accuracy: 45.3970%, Training Loss: 1.1105%\n",
      "Epoch [11/300], Step [75/225], Training Accuracy: 45.4792%, Training Loss: 1.1094%\n",
      "Epoch [11/300], Step [76/225], Training Accuracy: 45.4770%, Training Loss: 1.1099%\n",
      "Epoch [11/300], Step [77/225], Training Accuracy: 45.5560%, Training Loss: 1.1097%\n",
      "Epoch [11/300], Step [78/225], Training Accuracy: 45.4728%, Training Loss: 1.1107%\n",
      "Epoch [11/300], Step [79/225], Training Accuracy: 45.3521%, Training Loss: 1.1118%\n",
      "Epoch [11/300], Step [80/225], Training Accuracy: 45.3906%, Training Loss: 1.1115%\n",
      "Epoch [11/300], Step [81/225], Training Accuracy: 45.4668%, Training Loss: 1.1119%\n",
      "Epoch [11/300], Step [82/225], Training Accuracy: 45.5030%, Training Loss: 1.1112%\n",
      "Epoch [11/300], Step [83/225], Training Accuracy: 45.4819%, Training Loss: 1.1112%\n",
      "Epoch [11/300], Step [84/225], Training Accuracy: 45.4241%, Training Loss: 1.1117%\n",
      "Epoch [11/300], Step [85/225], Training Accuracy: 45.4596%, Training Loss: 1.1116%\n",
      "Epoch [11/300], Step [86/225], Training Accuracy: 45.4942%, Training Loss: 1.1118%\n",
      "Epoch [11/300], Step [87/225], Training Accuracy: 45.5639%, Training Loss: 1.1114%\n",
      "Epoch [11/300], Step [88/225], Training Accuracy: 45.5256%, Training Loss: 1.1112%\n",
      "Epoch [11/300], Step [89/225], Training Accuracy: 45.5758%, Training Loss: 1.1119%\n",
      "Epoch [11/300], Step [90/225], Training Accuracy: 45.5035%, Training Loss: 1.1114%\n",
      "Epoch [11/300], Step [91/225], Training Accuracy: 45.5701%, Training Loss: 1.1099%\n",
      "Epoch [11/300], Step [92/225], Training Accuracy: 45.5673%, Training Loss: 1.1101%\n",
      "Epoch [11/300], Step [93/225], Training Accuracy: 45.5309%, Training Loss: 1.1103%\n",
      "Epoch [11/300], Step [94/225], Training Accuracy: 45.6117%, Training Loss: 1.1092%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Step [95/225], Training Accuracy: 45.4605%, Training Loss: 1.1105%\n",
      "Epoch [11/300], Step [96/225], Training Accuracy: 45.4915%, Training Loss: 1.1101%\n",
      "Epoch [11/300], Step [97/225], Training Accuracy: 45.5058%, Training Loss: 1.1098%\n",
      "Epoch [11/300], Step [98/225], Training Accuracy: 45.5038%, Training Loss: 1.1089%\n",
      "Epoch [11/300], Step [99/225], Training Accuracy: 45.5335%, Training Loss: 1.1092%\n",
      "Epoch [11/300], Step [100/225], Training Accuracy: 45.4062%, Training Loss: 1.1092%\n",
      "Epoch [11/300], Step [101/225], Training Accuracy: 45.3280%, Training Loss: 1.1093%\n",
      "Epoch [11/300], Step [102/225], Training Accuracy: 45.3125%, Training Loss: 1.1094%\n",
      "Epoch [11/300], Step [103/225], Training Accuracy: 45.2973%, Training Loss: 1.1103%\n",
      "Epoch [11/300], Step [104/225], Training Accuracy: 45.2825%, Training Loss: 1.1095%\n",
      "Epoch [11/300], Step [105/225], Training Accuracy: 45.2976%, Training Loss: 1.1090%\n",
      "Epoch [11/300], Step [106/225], Training Accuracy: 45.1946%, Training Loss: 1.1094%\n",
      "Epoch [11/300], Step [107/225], Training Accuracy: 45.1665%, Training Loss: 1.1092%\n",
      "Epoch [11/300], Step [108/225], Training Accuracy: 45.1244%, Training Loss: 1.1097%\n",
      "Epoch [11/300], Step [109/225], Training Accuracy: 45.0258%, Training Loss: 1.1099%\n",
      "Epoch [11/300], Step [110/225], Training Accuracy: 44.9858%, Training Loss: 1.1102%\n",
      "Epoch [11/300], Step [111/225], Training Accuracy: 45.0732%, Training Loss: 1.1102%\n",
      "Epoch [11/300], Step [112/225], Training Accuracy: 45.1172%, Training Loss: 1.1098%\n",
      "Epoch [11/300], Step [113/225], Training Accuracy: 45.0498%, Training Loss: 1.1113%\n",
      "Epoch [11/300], Step [114/225], Training Accuracy: 45.0795%, Training Loss: 1.1107%\n",
      "Epoch [11/300], Step [115/225], Training Accuracy: 45.1223%, Training Loss: 1.1102%\n",
      "Epoch [11/300], Step [116/225], Training Accuracy: 45.2452%, Training Loss: 1.1094%\n",
      "Epoch [11/300], Step [117/225], Training Accuracy: 45.1790%, Training Loss: 1.1107%\n",
      "Epoch [11/300], Step [118/225], Training Accuracy: 45.2198%, Training Loss: 1.1108%\n",
      "Epoch [11/300], Step [119/225], Training Accuracy: 45.1812%, Training Loss: 1.1105%\n",
      "Epoch [11/300], Step [120/225], Training Accuracy: 45.2474%, Training Loss: 1.1096%\n",
      "Epoch [11/300], Step [121/225], Training Accuracy: 45.1575%, Training Loss: 1.1098%\n",
      "Epoch [11/300], Step [122/225], Training Accuracy: 45.1460%, Training Loss: 1.1096%\n",
      "Epoch [11/300], Step [123/225], Training Accuracy: 45.0965%, Training Loss: 1.1097%\n",
      "Epoch [11/300], Step [124/225], Training Accuracy: 45.1361%, Training Loss: 1.1092%\n",
      "Epoch [11/300], Step [125/225], Training Accuracy: 45.1375%, Training Loss: 1.1099%\n",
      "Epoch [11/300], Step [126/225], Training Accuracy: 45.1265%, Training Loss: 1.1101%\n",
      "Epoch [11/300], Step [127/225], Training Accuracy: 45.1526%, Training Loss: 1.1099%\n",
      "Epoch [11/300], Step [128/225], Training Accuracy: 45.0928%, Training Loss: 1.1104%\n",
      "Epoch [11/300], Step [129/225], Training Accuracy: 45.0824%, Training Loss: 1.1103%\n",
      "Epoch [11/300], Step [130/225], Training Accuracy: 45.0601%, Training Loss: 1.1108%\n",
      "Epoch [11/300], Step [131/225], Training Accuracy: 45.0740%, Training Loss: 1.1106%\n",
      "Epoch [11/300], Step [132/225], Training Accuracy: 45.0521%, Training Loss: 1.1110%\n",
      "Epoch [11/300], Step [133/225], Training Accuracy: 45.0893%, Training Loss: 1.1105%\n",
      "Epoch [11/300], Step [134/225], Training Accuracy: 45.1143%, Training Loss: 1.1096%\n",
      "Epoch [11/300], Step [135/225], Training Accuracy: 45.1042%, Training Loss: 1.1095%\n",
      "Epoch [11/300], Step [136/225], Training Accuracy: 45.0827%, Training Loss: 1.1089%\n",
      "Epoch [11/300], Step [137/225], Training Accuracy: 45.1186%, Training Loss: 1.1087%\n",
      "Epoch [11/300], Step [138/225], Training Accuracy: 45.1540%, Training Loss: 1.1082%\n",
      "Epoch [11/300], Step [139/225], Training Accuracy: 45.1664%, Training Loss: 1.1084%\n",
      "Epoch [11/300], Step [140/225], Training Accuracy: 45.2232%, Training Loss: 1.1082%\n",
      "Epoch [11/300], Step [141/225], Training Accuracy: 45.2903%, Training Loss: 1.1081%\n",
      "Epoch [11/300], Step [142/225], Training Accuracy: 45.2795%, Training Loss: 1.1081%\n",
      "Epoch [11/300], Step [143/225], Training Accuracy: 45.3562%, Training Loss: 1.1076%\n",
      "Epoch [11/300], Step [144/225], Training Accuracy: 45.3234%, Training Loss: 1.1075%\n",
      "Epoch [11/300], Step [145/225], Training Accuracy: 45.4203%, Training Loss: 1.1065%\n",
      "Epoch [11/300], Step [146/225], Training Accuracy: 45.3874%, Training Loss: 1.1064%\n",
      "Epoch [11/300], Step [147/225], Training Accuracy: 45.3338%, Training Loss: 1.1071%\n",
      "Epoch [11/300], Step [148/225], Training Accuracy: 45.3653%, Training Loss: 1.1069%\n",
      "Epoch [11/300], Step [149/225], Training Accuracy: 45.3020%, Training Loss: 1.1073%\n",
      "Epoch [11/300], Step [150/225], Training Accuracy: 45.2917%, Training Loss: 1.1077%\n",
      "Epoch [11/300], Step [151/225], Training Accuracy: 45.3435%, Training Loss: 1.1069%\n",
      "Epoch [11/300], Step [152/225], Training Accuracy: 45.3022%, Training Loss: 1.1070%\n",
      "Epoch [11/300], Step [153/225], Training Accuracy: 45.3636%, Training Loss: 1.1062%\n",
      "Epoch [11/300], Step [154/225], Training Accuracy: 45.2922%, Training Loss: 1.1065%\n",
      "Epoch [11/300], Step [155/225], Training Accuracy: 45.2823%, Training Loss: 1.1067%\n",
      "Epoch [11/300], Step [156/225], Training Accuracy: 45.2624%, Training Loss: 1.1065%\n",
      "Epoch [11/300], Step [157/225], Training Accuracy: 45.2627%, Training Loss: 1.1070%\n",
      "Epoch [11/300], Step [158/225], Training Accuracy: 45.3125%, Training Loss: 1.1067%\n",
      "Epoch [11/300], Step [159/225], Training Accuracy: 45.3322%, Training Loss: 1.1063%\n",
      "Epoch [11/300], Step [160/225], Training Accuracy: 45.3027%, Training Loss: 1.1068%\n",
      "Epoch [11/300], Step [161/225], Training Accuracy: 45.3804%, Training Loss: 1.1058%\n",
      "Epoch [11/300], Step [162/225], Training Accuracy: 45.4186%, Training Loss: 1.1057%\n",
      "Epoch [11/300], Step [163/225], Training Accuracy: 45.3988%, Training Loss: 1.1053%\n",
      "Epoch [11/300], Step [164/225], Training Accuracy: 45.4364%, Training Loss: 1.1051%\n",
      "Epoch [11/300], Step [165/225], Training Accuracy: 45.4735%, Training Loss: 1.1050%\n",
      "Epoch [11/300], Step [166/225], Training Accuracy: 45.4913%, Training Loss: 1.1053%\n",
      "Epoch [11/300], Step [167/225], Training Accuracy: 45.5277%, Training Loss: 1.1046%\n",
      "Epoch [11/300], Step [168/225], Training Accuracy: 45.5264%, Training Loss: 1.1049%\n",
      "Epoch [11/300], Step [169/225], Training Accuracy: 45.5251%, Training Loss: 1.1047%\n",
      "Epoch [11/300], Step [170/225], Training Accuracy: 45.5239%, Training Loss: 1.1053%\n",
      "Epoch [11/300], Step [171/225], Training Accuracy: 45.5044%, Training Loss: 1.1050%\n",
      "Epoch [11/300], Step [172/225], Training Accuracy: 45.4669%, Training Loss: 1.1054%\n",
      "Epoch [11/300], Step [173/225], Training Accuracy: 45.4660%, Training Loss: 1.1054%\n",
      "Epoch [11/300], Step [174/225], Training Accuracy: 45.5101%, Training Loss: 1.1052%\n",
      "Epoch [11/300], Step [175/225], Training Accuracy: 45.5179%, Training Loss: 1.1052%\n",
      "Epoch [11/300], Step [176/225], Training Accuracy: 45.4812%, Training Loss: 1.1053%\n",
      "Epoch [11/300], Step [177/225], Training Accuracy: 45.4273%, Training Loss: 1.1050%\n",
      "Epoch [11/300], Step [178/225], Training Accuracy: 45.3915%, Training Loss: 1.1050%\n",
      "Epoch [11/300], Step [179/225], Training Accuracy: 45.4172%, Training Loss: 1.1053%\n",
      "Epoch [11/300], Step [180/225], Training Accuracy: 45.5208%, Training Loss: 1.1040%\n",
      "Epoch [11/300], Step [181/225], Training Accuracy: 45.4765%, Training Loss: 1.1048%\n",
      "Epoch [11/300], Step [182/225], Training Accuracy: 45.4756%, Training Loss: 1.1050%\n",
      "Epoch [11/300], Step [183/225], Training Accuracy: 45.4577%, Training Loss: 1.1047%\n",
      "Epoch [11/300], Step [184/225], Training Accuracy: 45.4569%, Training Loss: 1.1046%\n",
      "Epoch [11/300], Step [185/225], Training Accuracy: 45.4645%, Training Loss: 1.1045%\n",
      "Epoch [11/300], Step [186/225], Training Accuracy: 45.5057%, Training Loss: 1.1043%\n",
      "Epoch [11/300], Step [187/225], Training Accuracy: 45.5548%, Training Loss: 1.1040%\n",
      "Epoch [11/300], Step [188/225], Training Accuracy: 45.6117%, Training Loss: 1.1037%\n",
      "Epoch [11/300], Step [189/225], Training Accuracy: 45.6432%, Training Loss: 1.1032%\n",
      "Epoch [11/300], Step [190/225], Training Accuracy: 45.6497%, Training Loss: 1.1036%\n",
      "Epoch [11/300], Step [191/225], Training Accuracy: 45.6315%, Training Loss: 1.1036%\n",
      "Epoch [11/300], Step [192/225], Training Accuracy: 45.6299%, Training Loss: 1.1037%\n",
      "Epoch [11/300], Step [193/225], Training Accuracy: 45.6282%, Training Loss: 1.1034%\n",
      "Epoch [11/300], Step [194/225], Training Accuracy: 45.6508%, Training Loss: 1.1036%\n",
      "Epoch [11/300], Step [195/225], Training Accuracy: 45.6571%, Training Loss: 1.1032%\n",
      "Epoch [11/300], Step [196/225], Training Accuracy: 45.6712%, Training Loss: 1.1031%\n",
      "Epoch [11/300], Step [197/225], Training Accuracy: 45.6773%, Training Loss: 1.1028%\n",
      "Epoch [11/300], Step [198/225], Training Accuracy: 45.6755%, Training Loss: 1.1022%\n",
      "Epoch [11/300], Step [199/225], Training Accuracy: 45.6972%, Training Loss: 1.1019%\n",
      "Epoch [11/300], Step [200/225], Training Accuracy: 45.6797%, Training Loss: 1.1023%\n",
      "Epoch [11/300], Step [201/225], Training Accuracy: 45.6545%, Training Loss: 1.1024%\n",
      "Epoch [11/300], Step [202/225], Training Accuracy: 45.6993%, Training Loss: 1.1020%\n",
      "Epoch [11/300], Step [203/225], Training Accuracy: 45.7050%, Training Loss: 1.1024%\n",
      "Epoch [11/300], Step [204/225], Training Accuracy: 45.7338%, Training Loss: 1.1021%\n",
      "Epoch [11/300], Step [205/225], Training Accuracy: 45.7317%, Training Loss: 1.1023%\n",
      "Epoch [11/300], Step [206/225], Training Accuracy: 45.7676%, Training Loss: 1.1027%\n",
      "Epoch [11/300], Step [207/225], Training Accuracy: 45.7352%, Training Loss: 1.1030%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Step [208/225], Training Accuracy: 45.7782%, Training Loss: 1.1025%\n",
      "Epoch [11/300], Step [209/225], Training Accuracy: 45.7461%, Training Loss: 1.1025%\n",
      "Epoch [11/300], Step [210/225], Training Accuracy: 45.7217%, Training Loss: 1.1022%\n",
      "Epoch [11/300], Step [211/225], Training Accuracy: 45.7346%, Training Loss: 1.1020%\n",
      "Epoch [11/300], Step [212/225], Training Accuracy: 45.6810%, Training Loss: 1.1025%\n",
      "Epoch [11/300], Step [213/225], Training Accuracy: 45.6279%, Training Loss: 1.1031%\n",
      "Epoch [11/300], Step [214/225], Training Accuracy: 45.6484%, Training Loss: 1.1028%\n",
      "Epoch [11/300], Step [215/225], Training Accuracy: 45.6759%, Training Loss: 1.1026%\n",
      "Epoch [11/300], Step [216/225], Training Accuracy: 45.6742%, Training Loss: 1.1028%\n",
      "Epoch [11/300], Step [217/225], Training Accuracy: 45.6581%, Training Loss: 1.1028%\n",
      "Epoch [11/300], Step [218/225], Training Accuracy: 45.6064%, Training Loss: 1.1034%\n",
      "Epoch [11/300], Step [219/225], Training Accuracy: 45.5693%, Training Loss: 1.1034%\n",
      "Epoch [11/300], Step [220/225], Training Accuracy: 45.6250%, Training Loss: 1.1031%\n",
      "Epoch [11/300], Step [221/225], Training Accuracy: 45.5953%, Training Loss: 1.1032%\n",
      "Epoch [11/300], Step [222/225], Training Accuracy: 45.6222%, Training Loss: 1.1031%\n",
      "Epoch [11/300], Step [223/225], Training Accuracy: 45.5577%, Training Loss: 1.1037%\n",
      "Epoch [11/300], Step [224/225], Training Accuracy: 45.5287%, Training Loss: 1.1038%\n",
      "Epoch [11/300], Step [225/225], Training Accuracy: 45.5183%, Training Loss: 1.1043%\n",
      "Epoch [12/300], Step [1/225], Training Accuracy: 56.2500%, Training Loss: 1.0296%\n",
      "Epoch [12/300], Step [2/225], Training Accuracy: 45.3125%, Training Loss: 1.1366%\n",
      "Epoch [12/300], Step [3/225], Training Accuracy: 42.7083%, Training Loss: 1.1712%\n",
      "Epoch [12/300], Step [4/225], Training Accuracy: 43.3594%, Training Loss: 1.1432%\n",
      "Epoch [12/300], Step [5/225], Training Accuracy: 45.0000%, Training Loss: 1.1210%\n",
      "Epoch [12/300], Step [6/225], Training Accuracy: 45.0521%, Training Loss: 1.1422%\n",
      "Epoch [12/300], Step [7/225], Training Accuracy: 45.5357%, Training Loss: 1.1339%\n",
      "Epoch [12/300], Step [8/225], Training Accuracy: 46.0938%, Training Loss: 1.1347%\n",
      "Epoch [12/300], Step [9/225], Training Accuracy: 45.3125%, Training Loss: 1.1336%\n",
      "Epoch [12/300], Step [10/225], Training Accuracy: 45.3125%, Training Loss: 1.1220%\n",
      "Epoch [12/300], Step [11/225], Training Accuracy: 46.1648%, Training Loss: 1.1167%\n",
      "Epoch [12/300], Step [12/225], Training Accuracy: 46.4844%, Training Loss: 1.1182%\n",
      "Epoch [12/300], Step [13/225], Training Accuracy: 47.2356%, Training Loss: 1.1178%\n",
      "Epoch [12/300], Step [14/225], Training Accuracy: 46.3170%, Training Loss: 1.1285%\n",
      "Epoch [12/300], Step [15/225], Training Accuracy: 46.3542%, Training Loss: 1.1338%\n",
      "Epoch [12/300], Step [16/225], Training Accuracy: 46.6797%, Training Loss: 1.1322%\n",
      "Epoch [12/300], Step [17/225], Training Accuracy: 46.8750%, Training Loss: 1.1271%\n",
      "Epoch [12/300], Step [18/225], Training Accuracy: 46.2674%, Training Loss: 1.1238%\n",
      "Epoch [12/300], Step [19/225], Training Accuracy: 45.8059%, Training Loss: 1.1245%\n",
      "Epoch [12/300], Step [20/225], Training Accuracy: 45.7812%, Training Loss: 1.1207%\n",
      "Epoch [12/300], Step [21/225], Training Accuracy: 46.0565%, Training Loss: 1.1135%\n",
      "Epoch [12/300], Step [22/225], Training Accuracy: 45.8807%, Training Loss: 1.1151%\n",
      "Epoch [12/300], Step [23/225], Training Accuracy: 45.9239%, Training Loss: 1.1111%\n",
      "Epoch [12/300], Step [24/225], Training Accuracy: 45.8333%, Training Loss: 1.1147%\n",
      "Epoch [12/300], Step [25/225], Training Accuracy: 46.0625%, Training Loss: 1.1107%\n",
      "Epoch [12/300], Step [26/225], Training Accuracy: 45.7332%, Training Loss: 1.1126%\n",
      "Epoch [12/300], Step [27/225], Training Accuracy: 45.8333%, Training Loss: 1.1123%\n",
      "Epoch [12/300], Step [28/225], Training Accuracy: 45.9263%, Training Loss: 1.1101%\n",
      "Epoch [12/300], Step [29/225], Training Accuracy: 45.9052%, Training Loss: 1.1069%\n",
      "Epoch [12/300], Step [30/225], Training Accuracy: 45.7812%, Training Loss: 1.1055%\n",
      "Epoch [12/300], Step [31/225], Training Accuracy: 45.8165%, Training Loss: 1.1053%\n",
      "Epoch [12/300], Step [32/225], Training Accuracy: 45.9961%, Training Loss: 1.1046%\n",
      "Epoch [12/300], Step [33/225], Training Accuracy: 46.2121%, Training Loss: 1.1016%\n",
      "Epoch [12/300], Step [34/225], Training Accuracy: 46.2776%, Training Loss: 1.1041%\n",
      "Epoch [12/300], Step [35/225], Training Accuracy: 46.2054%, Training Loss: 1.1031%\n",
      "Epoch [12/300], Step [36/225], Training Accuracy: 46.2674%, Training Loss: 1.1030%\n",
      "Epoch [12/300], Step [37/225], Training Accuracy: 46.2416%, Training Loss: 1.1011%\n",
      "Epoch [12/300], Step [38/225], Training Accuracy: 46.4227%, Training Loss: 1.0994%\n",
      "Epoch [12/300], Step [39/225], Training Accuracy: 46.3942%, Training Loss: 1.0977%\n",
      "Epoch [12/300], Step [40/225], Training Accuracy: 46.4453%, Training Loss: 1.0978%\n",
      "Epoch [12/300], Step [41/225], Training Accuracy: 46.2652%, Training Loss: 1.0989%\n",
      "Epoch [12/300], Step [42/225], Training Accuracy: 46.3542%, Training Loss: 1.0976%\n",
      "Epoch [12/300], Step [43/225], Training Accuracy: 46.3299%, Training Loss: 1.0975%\n",
      "Epoch [12/300], Step [44/225], Training Accuracy: 46.4134%, Training Loss: 1.0953%\n",
      "Epoch [12/300], Step [45/225], Training Accuracy: 46.4236%, Training Loss: 1.0941%\n",
      "Epoch [12/300], Step [46/225], Training Accuracy: 46.7052%, Training Loss: 1.0903%\n",
      "Epoch [12/300], Step [47/225], Training Accuracy: 46.7088%, Training Loss: 1.0905%\n",
      "Epoch [12/300], Step [48/225], Training Accuracy: 46.6471%, Training Loss: 1.0901%\n",
      "Epoch [12/300], Step [49/225], Training Accuracy: 46.6199%, Training Loss: 1.0914%\n",
      "Epoch [12/300], Step [50/225], Training Accuracy: 46.7188%, Training Loss: 1.0906%\n",
      "Epoch [12/300], Step [51/225], Training Accuracy: 46.7525%, Training Loss: 1.0900%\n",
      "Epoch [12/300], Step [52/225], Training Accuracy: 46.8750%, Training Loss: 1.0896%\n",
      "Epoch [12/300], Step [53/225], Training Accuracy: 46.7866%, Training Loss: 1.0897%\n",
      "Epoch [12/300], Step [54/225], Training Accuracy: 46.7014%, Training Loss: 1.0903%\n",
      "Epoch [12/300], Step [55/225], Training Accuracy: 46.5909%, Training Loss: 1.0911%\n",
      "Epoch [12/300], Step [56/225], Training Accuracy: 46.4844%, Training Loss: 1.0916%\n",
      "Epoch [12/300], Step [57/225], Training Accuracy: 46.6831%, Training Loss: 1.0885%\n",
      "Epoch [12/300], Step [58/225], Training Accuracy: 46.6056%, Training Loss: 1.0887%\n",
      "Epoch [12/300], Step [59/225], Training Accuracy: 46.6631%, Training Loss: 1.0882%\n",
      "Epoch [12/300], Step [60/225], Training Accuracy: 46.5885%, Training Loss: 1.0886%\n",
      "Epoch [12/300], Step [61/225], Training Accuracy: 46.5932%, Training Loss: 1.0888%\n",
      "Epoch [12/300], Step [62/225], Training Accuracy: 46.5978%, Training Loss: 1.0885%\n",
      "Epoch [12/300], Step [63/225], Training Accuracy: 46.5030%, Training Loss: 1.0906%\n",
      "Epoch [12/300], Step [64/225], Training Accuracy: 46.4600%, Training Loss: 1.0917%\n",
      "Epoch [12/300], Step [65/225], Training Accuracy: 46.3702%, Training Loss: 1.0915%\n",
      "Epoch [12/300], Step [66/225], Training Accuracy: 46.3778%, Training Loss: 1.0910%\n",
      "Epoch [12/300], Step [67/225], Training Accuracy: 46.2687%, Training Loss: 1.0906%\n",
      "Epoch [12/300], Step [68/225], Training Accuracy: 46.2086%, Training Loss: 1.0907%\n",
      "Epoch [12/300], Step [69/225], Training Accuracy: 46.1277%, Training Loss: 1.0910%\n",
      "Epoch [12/300], Step [70/225], Training Accuracy: 46.0938%, Training Loss: 1.0924%\n",
      "Epoch [12/300], Step [71/225], Training Accuracy: 46.1268%, Training Loss: 1.0916%\n",
      "Epoch [12/300], Step [72/225], Training Accuracy: 46.0069%, Training Loss: 1.0935%\n",
      "Epoch [12/300], Step [73/225], Training Accuracy: 46.0188%, Training Loss: 1.0962%\n",
      "Epoch [12/300], Step [74/225], Training Accuracy: 46.0515%, Training Loss: 1.0942%\n",
      "Epoch [12/300], Step [75/225], Training Accuracy: 46.1042%, Training Loss: 1.0932%\n",
      "Epoch [12/300], Step [76/225], Training Accuracy: 46.1143%, Training Loss: 1.0937%\n",
      "Epoch [12/300], Step [77/225], Training Accuracy: 46.2256%, Training Loss: 1.0933%\n",
      "Epoch [12/300], Step [78/225], Training Accuracy: 46.1739%, Training Loss: 1.0943%\n",
      "Epoch [12/300], Step [79/225], Training Accuracy: 46.0641%, Training Loss: 1.0954%\n",
      "Epoch [12/300], Step [80/225], Training Accuracy: 46.0742%, Training Loss: 1.0951%\n",
      "Epoch [12/300], Step [81/225], Training Accuracy: 46.1420%, Training Loss: 1.0957%\n",
      "Epoch [12/300], Step [82/225], Training Accuracy: 46.1700%, Training Loss: 1.0949%\n",
      "Epoch [12/300], Step [83/225], Training Accuracy: 46.1408%, Training Loss: 1.0948%\n",
      "Epoch [12/300], Step [84/225], Training Accuracy: 46.0938%, Training Loss: 1.0954%\n",
      "Epoch [12/300], Step [85/225], Training Accuracy: 46.1213%, Training Loss: 1.0952%\n",
      "Epoch [12/300], Step [86/225], Training Accuracy: 46.1846%, Training Loss: 1.0954%\n",
      "Epoch [12/300], Step [87/225], Training Accuracy: 46.2284%, Training Loss: 1.0950%\n",
      "Epoch [12/300], Step [88/225], Training Accuracy: 46.1825%, Training Loss: 1.0948%\n",
      "Epoch [12/300], Step [89/225], Training Accuracy: 46.2254%, Training Loss: 1.0956%\n",
      "Epoch [12/300], Step [90/225], Training Accuracy: 46.1806%, Training Loss: 1.0953%\n",
      "Epoch [12/300], Step [91/225], Training Accuracy: 46.2569%, Training Loss: 1.0937%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Step [92/225], Training Accuracy: 46.2126%, Training Loss: 1.0939%\n",
      "Epoch [12/300], Step [93/225], Training Accuracy: 46.2030%, Training Loss: 1.0940%\n",
      "Epoch [12/300], Step [94/225], Training Accuracy: 46.3098%, Training Loss: 1.0929%\n",
      "Epoch [12/300], Step [95/225], Training Accuracy: 46.1842%, Training Loss: 1.0941%\n",
      "Epoch [12/300], Step [96/225], Training Accuracy: 46.2565%, Training Loss: 1.0937%\n",
      "Epoch [12/300], Step [97/225], Training Accuracy: 46.2790%, Training Loss: 1.0933%\n",
      "Epoch [12/300], Step [98/225], Training Accuracy: 46.2691%, Training Loss: 1.0926%\n",
      "Epoch [12/300], Step [99/225], Training Accuracy: 46.2753%, Training Loss: 1.0929%\n",
      "Epoch [12/300], Step [100/225], Training Accuracy: 46.1406%, Training Loss: 1.0930%\n",
      "Epoch [12/300], Step [101/225], Training Accuracy: 46.0860%, Training Loss: 1.0933%\n",
      "Epoch [12/300], Step [102/225], Training Accuracy: 46.0784%, Training Loss: 1.0935%\n",
      "Epoch [12/300], Step [103/225], Training Accuracy: 46.0407%, Training Loss: 1.0943%\n",
      "Epoch [12/300], Step [104/225], Training Accuracy: 46.0487%, Training Loss: 1.0936%\n",
      "Epoch [12/300], Step [105/225], Training Accuracy: 46.0863%, Training Loss: 1.0930%\n",
      "Epoch [12/300], Step [106/225], Training Accuracy: 46.0053%, Training Loss: 1.0935%\n",
      "Epoch [12/300], Step [107/225], Training Accuracy: 45.9988%, Training Loss: 1.0932%\n",
      "Epoch [12/300], Step [108/225], Training Accuracy: 46.0069%, Training Loss: 1.0937%\n",
      "Epoch [12/300], Step [109/225], Training Accuracy: 45.9146%, Training Loss: 1.0939%\n",
      "Epoch [12/300], Step [110/225], Training Accuracy: 45.9091%, Training Loss: 1.0941%\n",
      "Epoch [12/300], Step [111/225], Training Accuracy: 45.9459%, Training Loss: 1.0942%\n",
      "Epoch [12/300], Step [112/225], Training Accuracy: 45.9682%, Training Loss: 1.0939%\n",
      "Epoch [12/300], Step [113/225], Training Accuracy: 45.8933%, Training Loss: 1.0954%\n",
      "Epoch [12/300], Step [114/225], Training Accuracy: 45.8882%, Training Loss: 1.0950%\n",
      "Epoch [12/300], Step [115/225], Training Accuracy: 45.9239%, Training Loss: 1.0945%\n",
      "Epoch [12/300], Step [116/225], Training Accuracy: 46.0399%, Training Loss: 1.0937%\n",
      "Epoch [12/300], Step [117/225], Training Accuracy: 45.9402%, Training Loss: 1.0950%\n",
      "Epoch [12/300], Step [118/225], Training Accuracy: 45.9613%, Training Loss: 1.0951%\n",
      "Epoch [12/300], Step [119/225], Training Accuracy: 45.9165%, Training Loss: 1.0948%\n",
      "Epoch [12/300], Step [120/225], Training Accuracy: 45.9635%, Training Loss: 1.0940%\n",
      "Epoch [12/300], Step [121/225], Training Accuracy: 45.8807%, Training Loss: 1.0941%\n",
      "Epoch [12/300], Step [122/225], Training Accuracy: 45.8632%, Training Loss: 1.0940%\n",
      "Epoch [12/300], Step [123/225], Training Accuracy: 45.8206%, Training Loss: 1.0941%\n",
      "Epoch [12/300], Step [124/225], Training Accuracy: 45.8669%, Training Loss: 1.0936%\n",
      "Epoch [12/300], Step [125/225], Training Accuracy: 45.8375%, Training Loss: 1.0943%\n",
      "Epoch [12/300], Step [126/225], Training Accuracy: 45.8333%, Training Loss: 1.0946%\n",
      "Epoch [12/300], Step [127/225], Training Accuracy: 45.8292%, Training Loss: 1.0944%\n",
      "Epoch [12/300], Step [128/225], Training Accuracy: 45.7764%, Training Loss: 1.0948%\n",
      "Epoch [12/300], Step [129/225], Training Accuracy: 45.7485%, Training Loss: 1.0948%\n",
      "Epoch [12/300], Step [130/225], Training Accuracy: 45.7212%, Training Loss: 1.0952%\n",
      "Epoch [12/300], Step [131/225], Training Accuracy: 45.7180%, Training Loss: 1.0950%\n",
      "Epoch [12/300], Step [132/225], Training Accuracy: 45.7031%, Training Loss: 1.0955%\n",
      "Epoch [12/300], Step [133/225], Training Accuracy: 45.7237%, Training Loss: 1.0949%\n",
      "Epoch [12/300], Step [134/225], Training Accuracy: 45.7556%, Training Loss: 1.0941%\n",
      "Epoch [12/300], Step [135/225], Training Accuracy: 45.7523%, Training Loss: 1.0939%\n",
      "Epoch [12/300], Step [136/225], Training Accuracy: 45.7146%, Training Loss: 1.0933%\n",
      "Epoch [12/300], Step [137/225], Training Accuracy: 45.7459%, Training Loss: 1.0931%\n",
      "Epoch [12/300], Step [138/225], Training Accuracy: 45.7880%, Training Loss: 1.0926%\n",
      "Epoch [12/300], Step [139/225], Training Accuracy: 45.8071%, Training Loss: 1.0928%\n",
      "Epoch [12/300], Step [140/225], Training Accuracy: 45.8594%, Training Loss: 1.0927%\n",
      "Epoch [12/300], Step [141/225], Training Accuracy: 45.9109%, Training Loss: 1.0926%\n",
      "Epoch [12/300], Step [142/225], Training Accuracy: 45.8957%, Training Loss: 1.0926%\n",
      "Epoch [12/300], Step [143/225], Training Accuracy: 45.9790%, Training Loss: 1.0920%\n",
      "Epoch [12/300], Step [144/225], Training Accuracy: 45.9418%, Training Loss: 1.0919%\n",
      "Epoch [12/300], Step [145/225], Training Accuracy: 46.0453%, Training Loss: 1.0909%\n",
      "Epoch [12/300], Step [146/225], Training Accuracy: 46.0295%, Training Loss: 1.0908%\n",
      "Epoch [12/300], Step [147/225], Training Accuracy: 45.9928%, Training Loss: 1.0916%\n",
      "Epoch [12/300], Step [148/225], Training Accuracy: 46.0304%, Training Loss: 1.0915%\n",
      "Epoch [12/300], Step [149/225], Training Accuracy: 45.9627%, Training Loss: 1.0918%\n",
      "Epoch [12/300], Step [150/225], Training Accuracy: 45.9375%, Training Loss: 1.0922%\n",
      "Epoch [12/300], Step [151/225], Training Accuracy: 45.9644%, Training Loss: 1.0915%\n",
      "Epoch [12/300], Step [152/225], Training Accuracy: 45.9190%, Training Loss: 1.0916%\n",
      "Epoch [12/300], Step [153/225], Training Accuracy: 45.9763%, Training Loss: 1.0908%\n",
      "Epoch [12/300], Step [154/225], Training Accuracy: 45.9111%, Training Loss: 1.0910%\n",
      "Epoch [12/300], Step [155/225], Training Accuracy: 45.9274%, Training Loss: 1.0912%\n",
      "Epoch [12/300], Step [156/225], Training Accuracy: 45.9435%, Training Loss: 1.0910%\n",
      "Epoch [12/300], Step [157/225], Training Accuracy: 45.9594%, Training Loss: 1.0915%\n",
      "Epoch [12/300], Step [158/225], Training Accuracy: 46.0047%, Training Loss: 1.0913%\n",
      "Epoch [12/300], Step [159/225], Training Accuracy: 46.0397%, Training Loss: 1.0909%\n",
      "Epoch [12/300], Step [160/225], Training Accuracy: 46.0059%, Training Loss: 1.0913%\n",
      "Epoch [12/300], Step [161/225], Training Accuracy: 46.0792%, Training Loss: 1.0903%\n",
      "Epoch [12/300], Step [162/225], Training Accuracy: 46.0938%, Training Loss: 1.0902%\n",
      "Epoch [12/300], Step [163/225], Training Accuracy: 46.0602%, Training Loss: 1.0899%\n",
      "Epoch [12/300], Step [164/225], Training Accuracy: 46.1033%, Training Loss: 1.0895%\n",
      "Epoch [12/300], Step [165/225], Training Accuracy: 46.1648%, Training Loss: 1.0894%\n",
      "Epoch [12/300], Step [166/225], Training Accuracy: 46.1785%, Training Loss: 1.0897%\n",
      "Epoch [12/300], Step [167/225], Training Accuracy: 46.2013%, Training Loss: 1.0891%\n",
      "Epoch [12/300], Step [168/225], Training Accuracy: 46.2054%, Training Loss: 1.0893%\n",
      "Epoch [12/300], Step [169/225], Training Accuracy: 46.2186%, Training Loss: 1.0892%\n",
      "Epoch [12/300], Step [170/225], Training Accuracy: 46.2132%, Training Loss: 1.0898%\n",
      "Epoch [12/300], Step [171/225], Training Accuracy: 46.1897%, Training Loss: 1.0895%\n",
      "Epoch [12/300], Step [172/225], Training Accuracy: 46.1573%, Training Loss: 1.0900%\n",
      "Epoch [12/300], Step [173/225], Training Accuracy: 46.1525%, Training Loss: 1.0899%\n",
      "Epoch [12/300], Step [174/225], Training Accuracy: 46.1925%, Training Loss: 1.0897%\n",
      "Epoch [12/300], Step [175/225], Training Accuracy: 46.1875%, Training Loss: 1.0897%\n",
      "Epoch [12/300], Step [176/225], Training Accuracy: 46.1559%, Training Loss: 1.0898%\n",
      "Epoch [12/300], Step [177/225], Training Accuracy: 46.0982%, Training Loss: 1.0895%\n",
      "Epoch [12/300], Step [178/225], Training Accuracy: 46.0586%, Training Loss: 1.0895%\n",
      "Epoch [12/300], Step [179/225], Training Accuracy: 46.0807%, Training Loss: 1.0898%\n",
      "Epoch [12/300], Step [180/225], Training Accuracy: 46.1979%, Training Loss: 1.0885%\n",
      "Epoch [12/300], Step [181/225], Training Accuracy: 46.1412%, Training Loss: 1.0893%\n",
      "Epoch [12/300], Step [182/225], Training Accuracy: 46.1367%, Training Loss: 1.0895%\n",
      "Epoch [12/300], Step [183/225], Training Accuracy: 46.1236%, Training Loss: 1.0892%\n",
      "Epoch [12/300], Step [184/225], Training Accuracy: 46.1277%, Training Loss: 1.0892%\n",
      "Epoch [12/300], Step [185/225], Training Accuracy: 46.1402%, Training Loss: 1.0890%\n",
      "Epoch [12/300], Step [186/225], Training Accuracy: 46.1694%, Training Loss: 1.0888%\n",
      "Epoch [12/300], Step [187/225], Training Accuracy: 46.2149%, Training Loss: 1.0885%\n",
      "Epoch [12/300], Step [188/225], Training Accuracy: 46.2683%, Training Loss: 1.0881%\n",
      "Epoch [12/300], Step [189/225], Training Accuracy: 46.3294%, Training Loss: 1.0876%\n",
      "Epoch [12/300], Step [190/225], Training Accuracy: 46.3240%, Training Loss: 1.0880%\n",
      "Epoch [12/300], Step [191/225], Training Accuracy: 46.3187%, Training Loss: 1.0880%\n",
      "Epoch [12/300], Step [192/225], Training Accuracy: 46.3298%, Training Loss: 1.0881%\n",
      "Epoch [12/300], Step [193/225], Training Accuracy: 46.3002%, Training Loss: 1.0878%\n",
      "Epoch [12/300], Step [194/225], Training Accuracy: 46.3032%, Training Loss: 1.0880%\n",
      "Epoch [12/300], Step [195/225], Training Accuracy: 46.3061%, Training Loss: 1.0876%\n",
      "Epoch [12/300], Step [196/225], Training Accuracy: 46.3249%, Training Loss: 1.0876%\n",
      "Epoch [12/300], Step [197/225], Training Accuracy: 46.3198%, Training Loss: 1.0872%\n",
      "Epoch [12/300], Step [198/225], Training Accuracy: 46.3068%, Training Loss: 1.0866%\n",
      "Epoch [12/300], Step [199/225], Training Accuracy: 46.3332%, Training Loss: 1.0863%\n",
      "Epoch [12/300], Step [200/225], Training Accuracy: 46.3047%, Training Loss: 1.0866%\n",
      "Epoch [12/300], Step [201/225], Training Accuracy: 46.2920%, Training Loss: 1.0868%\n",
      "Epoch [12/300], Step [202/225], Training Accuracy: 46.3258%, Training Loss: 1.0865%\n",
      "Epoch [12/300], Step [203/225], Training Accuracy: 46.3285%, Training Loss: 1.0868%\n",
      "Epoch [12/300], Step [204/225], Training Accuracy: 46.3618%, Training Loss: 1.0866%\n",
      "Epoch [12/300], Step [205/225], Training Accuracy: 46.3643%, Training Loss: 1.0868%\n",
      "Epoch [12/300], Step [206/225], Training Accuracy: 46.3896%, Training Loss: 1.0871%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Step [207/225], Training Accuracy: 46.3466%, Training Loss: 1.0875%\n",
      "Epoch [12/300], Step [208/225], Training Accuracy: 46.3942%, Training Loss: 1.0870%\n",
      "Epoch [12/300], Step [209/225], Training Accuracy: 46.3367%, Training Loss: 1.0870%\n",
      "Epoch [12/300], Step [210/225], Training Accuracy: 46.3244%, Training Loss: 1.0867%\n",
      "Epoch [12/300], Step [211/225], Training Accuracy: 46.3270%, Training Loss: 1.0866%\n",
      "Epoch [12/300], Step [212/225], Training Accuracy: 46.2854%, Training Loss: 1.0871%\n",
      "Epoch [12/300], Step [213/225], Training Accuracy: 46.2295%, Training Loss: 1.0877%\n",
      "Epoch [12/300], Step [214/225], Training Accuracy: 46.2471%, Training Loss: 1.0874%\n",
      "Epoch [12/300], Step [215/225], Training Accuracy: 46.2791%, Training Loss: 1.0873%\n",
      "Epoch [12/300], Step [216/225], Training Accuracy: 46.2674%, Training Loss: 1.0874%\n",
      "Epoch [12/300], Step [217/225], Training Accuracy: 46.2486%, Training Loss: 1.0874%\n",
      "Epoch [12/300], Step [218/225], Training Accuracy: 46.2013%, Training Loss: 1.0880%\n",
      "Epoch [12/300], Step [219/225], Training Accuracy: 46.1473%, Training Loss: 1.0881%\n",
      "Epoch [12/300], Step [220/225], Training Accuracy: 46.1932%, Training Loss: 1.0878%\n",
      "Epoch [12/300], Step [221/225], Training Accuracy: 46.1680%, Training Loss: 1.0879%\n",
      "Epoch [12/300], Step [222/225], Training Accuracy: 46.1923%, Training Loss: 1.0878%\n",
      "Epoch [12/300], Step [223/225], Training Accuracy: 46.1113%, Training Loss: 1.0884%\n",
      "Epoch [12/300], Step [224/225], Training Accuracy: 46.0728%, Training Loss: 1.0885%\n",
      "Epoch [12/300], Step [225/225], Training Accuracy: 46.0742%, Training Loss: 1.0890%\n",
      "Epoch [13/300], Step [1/225], Training Accuracy: 57.8125%, Training Loss: 0.9994%\n",
      "Epoch [13/300], Step [2/225], Training Accuracy: 50.0000%, Training Loss: 1.1115%\n",
      "Epoch [13/300], Step [3/225], Training Accuracy: 45.3125%, Training Loss: 1.1520%\n",
      "Epoch [13/300], Step [4/225], Training Accuracy: 45.3125%, Training Loss: 1.1260%\n",
      "Epoch [13/300], Step [5/225], Training Accuracy: 46.5625%, Training Loss: 1.1020%\n",
      "Epoch [13/300], Step [6/225], Training Accuracy: 46.6146%, Training Loss: 1.1237%\n",
      "Epoch [13/300], Step [7/225], Training Accuracy: 46.8750%, Training Loss: 1.1158%\n",
      "Epoch [13/300], Step [8/225], Training Accuracy: 47.2656%, Training Loss: 1.1169%\n",
      "Epoch [13/300], Step [9/225], Training Accuracy: 46.1806%, Training Loss: 1.1155%\n",
      "Epoch [13/300], Step [10/225], Training Accuracy: 46.0938%, Training Loss: 1.1049%\n",
      "Epoch [13/300], Step [11/225], Training Accuracy: 46.8750%, Training Loss: 1.0995%\n",
      "Epoch [13/300], Step [12/225], Training Accuracy: 47.1354%, Training Loss: 1.1021%\n",
      "Epoch [13/300], Step [13/225], Training Accuracy: 47.8365%, Training Loss: 1.1023%\n",
      "Epoch [13/300], Step [14/225], Training Accuracy: 46.9866%, Training Loss: 1.1132%\n",
      "Epoch [13/300], Step [15/225], Training Accuracy: 46.8750%, Training Loss: 1.1188%\n",
      "Epoch [13/300], Step [16/225], Training Accuracy: 47.1680%, Training Loss: 1.1178%\n",
      "Epoch [13/300], Step [17/225], Training Accuracy: 47.2426%, Training Loss: 1.1121%\n",
      "Epoch [13/300], Step [18/225], Training Accuracy: 46.7014%, Training Loss: 1.1089%\n",
      "Epoch [13/300], Step [19/225], Training Accuracy: 46.2993%, Training Loss: 1.1096%\n",
      "Epoch [13/300], Step [20/225], Training Accuracy: 46.2500%, Training Loss: 1.1057%\n",
      "Epoch [13/300], Step [21/225], Training Accuracy: 46.5030%, Training Loss: 1.0987%\n",
      "Epoch [13/300], Step [22/225], Training Accuracy: 46.4489%, Training Loss: 1.1004%\n",
      "Epoch [13/300], Step [23/225], Training Accuracy: 46.4674%, Training Loss: 1.0965%\n",
      "Epoch [13/300], Step [24/225], Training Accuracy: 46.4193%, Training Loss: 1.1006%\n",
      "Epoch [13/300], Step [25/225], Training Accuracy: 46.6250%, Training Loss: 1.0966%\n",
      "Epoch [13/300], Step [26/225], Training Accuracy: 46.2740%, Training Loss: 1.0984%\n",
      "Epoch [13/300], Step [27/225], Training Accuracy: 46.2963%, Training Loss: 1.0981%\n",
      "Epoch [13/300], Step [28/225], Training Accuracy: 46.4286%, Training Loss: 1.0957%\n",
      "Epoch [13/300], Step [29/225], Training Accuracy: 46.4440%, Training Loss: 1.0922%\n",
      "Epoch [13/300], Step [30/225], Training Accuracy: 46.3021%, Training Loss: 1.0910%\n",
      "Epoch [13/300], Step [31/225], Training Accuracy: 46.3710%, Training Loss: 1.0908%\n",
      "Epoch [13/300], Step [32/225], Training Accuracy: 46.5332%, Training Loss: 1.0901%\n",
      "Epoch [13/300], Step [33/225], Training Accuracy: 46.8277%, Training Loss: 1.0872%\n",
      "Epoch [13/300], Step [34/225], Training Accuracy: 46.8750%, Training Loss: 1.0897%\n",
      "Epoch [13/300], Step [35/225], Training Accuracy: 46.7857%, Training Loss: 1.0887%\n",
      "Epoch [13/300], Step [36/225], Training Accuracy: 46.7882%, Training Loss: 1.0885%\n",
      "Epoch [13/300], Step [37/225], Training Accuracy: 46.7483%, Training Loss: 1.0863%\n",
      "Epoch [13/300], Step [38/225], Training Accuracy: 46.9984%, Training Loss: 1.0846%\n",
      "Epoch [13/300], Step [39/225], Training Accuracy: 46.9952%, Training Loss: 1.0830%\n",
      "Epoch [13/300], Step [40/225], Training Accuracy: 47.0312%, Training Loss: 1.0832%\n",
      "Epoch [13/300], Step [41/225], Training Accuracy: 46.8750%, Training Loss: 1.0843%\n",
      "Epoch [13/300], Step [42/225], Training Accuracy: 46.9866%, Training Loss: 1.0829%\n",
      "Epoch [13/300], Step [43/225], Training Accuracy: 47.0203%, Training Loss: 1.0828%\n",
      "Epoch [13/300], Step [44/225], Training Accuracy: 47.1236%, Training Loss: 1.0806%\n",
      "Epoch [13/300], Step [45/225], Training Accuracy: 47.0833%, Training Loss: 1.0795%\n",
      "Epoch [13/300], Step [46/225], Training Accuracy: 47.3505%, Training Loss: 1.0755%\n",
      "Epoch [13/300], Step [47/225], Training Accuracy: 47.3404%, Training Loss: 1.0759%\n",
      "Epoch [13/300], Step [48/225], Training Accuracy: 47.2982%, Training Loss: 1.0756%\n",
      "Epoch [13/300], Step [49/225], Training Accuracy: 47.2577%, Training Loss: 1.0767%\n",
      "Epoch [13/300], Step [50/225], Training Accuracy: 47.3438%, Training Loss: 1.0757%\n",
      "Epoch [13/300], Step [51/225], Training Accuracy: 47.3958%, Training Loss: 1.0750%\n",
      "Epoch [13/300], Step [52/225], Training Accuracy: 47.5060%, Training Loss: 1.0745%\n",
      "Epoch [13/300], Step [53/225], Training Accuracy: 47.4057%, Training Loss: 1.0746%\n",
      "Epoch [13/300], Step [54/225], Training Accuracy: 47.3090%, Training Loss: 1.0753%\n",
      "Epoch [13/300], Step [55/225], Training Accuracy: 47.1875%, Training Loss: 1.0762%\n",
      "Epoch [13/300], Step [56/225], Training Accuracy: 47.0982%, Training Loss: 1.0769%\n",
      "Epoch [13/300], Step [57/225], Training Accuracy: 47.3136%, Training Loss: 1.0737%\n",
      "Epoch [13/300], Step [58/225], Training Accuracy: 47.2522%, Training Loss: 1.0740%\n",
      "Epoch [13/300], Step [59/225], Training Accuracy: 47.3252%, Training Loss: 1.0736%\n",
      "Epoch [13/300], Step [60/225], Training Accuracy: 47.2656%, Training Loss: 1.0740%\n",
      "Epoch [13/300], Step [61/225], Training Accuracy: 47.3361%, Training Loss: 1.0740%\n",
      "Epoch [13/300], Step [62/225], Training Accuracy: 47.3034%, Training Loss: 1.0737%\n",
      "Epoch [13/300], Step [63/225], Training Accuracy: 47.1974%, Training Loss: 1.0756%\n",
      "Epoch [13/300], Step [64/225], Training Accuracy: 47.1436%, Training Loss: 1.0766%\n",
      "Epoch [13/300], Step [65/225], Training Accuracy: 47.0673%, Training Loss: 1.0766%\n",
      "Epoch [13/300], Step [66/225], Training Accuracy: 47.0881%, Training Loss: 1.0760%\n",
      "Epoch [13/300], Step [67/225], Training Accuracy: 46.9450%, Training Loss: 1.0756%\n",
      "Epoch [13/300], Step [68/225], Training Accuracy: 46.8520%, Training Loss: 1.0757%\n",
      "Epoch [13/300], Step [69/225], Training Accuracy: 46.7618%, Training Loss: 1.0760%\n",
      "Epoch [13/300], Step [70/225], Training Accuracy: 46.6964%, Training Loss: 1.0774%\n",
      "Epoch [13/300], Step [71/225], Training Accuracy: 46.7430%, Training Loss: 1.0764%\n",
      "Epoch [13/300], Step [72/225], Training Accuracy: 46.6146%, Training Loss: 1.0783%\n",
      "Epoch [13/300], Step [73/225], Training Accuracy: 46.5967%, Training Loss: 1.0810%\n",
      "Epoch [13/300], Step [74/225], Training Accuracy: 46.6850%, Training Loss: 1.0790%\n",
      "Epoch [13/300], Step [75/225], Training Accuracy: 46.7292%, Training Loss: 1.0780%\n",
      "Epoch [13/300], Step [76/225], Training Accuracy: 46.7311%, Training Loss: 1.0783%\n",
      "Epoch [13/300], Step [77/225], Training Accuracy: 46.8750%, Training Loss: 1.0779%\n",
      "Epoch [13/300], Step [78/225], Training Accuracy: 46.8349%, Training Loss: 1.0790%\n",
      "Epoch [13/300], Step [79/225], Training Accuracy: 46.7761%, Training Loss: 1.0799%\n",
      "Epoch [13/300], Step [80/225], Training Accuracy: 46.7773%, Training Loss: 1.0798%\n",
      "Epoch [13/300], Step [81/225], Training Accuracy: 46.8750%, Training Loss: 1.0804%\n",
      "Epoch [13/300], Step [82/225], Training Accuracy: 46.9322%, Training Loss: 1.0795%\n",
      "Epoch [13/300], Step [83/225], Training Accuracy: 46.9503%, Training Loss: 1.0795%\n",
      "Epoch [13/300], Step [84/225], Training Accuracy: 46.8936%, Training Loss: 1.0801%\n",
      "Epoch [13/300], Step [85/225], Training Accuracy: 46.9301%, Training Loss: 1.0798%\n",
      "Epoch [13/300], Step [86/225], Training Accuracy: 46.9658%, Training Loss: 1.0800%\n",
      "Epoch [13/300], Step [87/225], Training Accuracy: 47.0366%, Training Loss: 1.0796%\n",
      "Epoch [13/300], Step [88/225], Training Accuracy: 46.9283%, Training Loss: 1.0795%\n",
      "Epoch [13/300], Step [89/225], Training Accuracy: 46.9628%, Training Loss: 1.0804%\n",
      "Epoch [13/300], Step [90/225], Training Accuracy: 46.9271%, Training Loss: 1.0802%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Step [91/225], Training Accuracy: 47.0295%, Training Loss: 1.0786%\n",
      "Epoch [13/300], Step [92/225], Training Accuracy: 46.9939%, Training Loss: 1.0787%\n",
      "Epoch [13/300], Step [93/225], Training Accuracy: 46.9422%, Training Loss: 1.0788%\n",
      "Epoch [13/300], Step [94/225], Training Accuracy: 47.0745%, Training Loss: 1.0776%\n",
      "Epoch [13/300], Step [95/225], Training Accuracy: 46.9243%, Training Loss: 1.0788%\n",
      "Epoch [13/300], Step [96/225], Training Accuracy: 46.9889%, Training Loss: 1.0784%\n",
      "Epoch [13/300], Step [97/225], Training Accuracy: 47.0039%, Training Loss: 1.0779%\n",
      "Epoch [13/300], Step [98/225], Training Accuracy: 46.9707%, Training Loss: 1.0772%\n",
      "Epoch [13/300], Step [99/225], Training Accuracy: 46.9855%, Training Loss: 1.0774%\n",
      "Epoch [13/300], Step [100/225], Training Accuracy: 46.8750%, Training Loss: 1.0775%\n",
      "Epoch [13/300], Step [101/225], Training Accuracy: 46.8441%, Training Loss: 1.0778%\n",
      "Epoch [13/300], Step [102/225], Training Accuracy: 46.8444%, Training Loss: 1.0781%\n",
      "Epoch [13/300], Step [103/225], Training Accuracy: 46.7688%, Training Loss: 1.0791%\n",
      "Epoch [13/300], Step [104/225], Training Accuracy: 46.7398%, Training Loss: 1.0784%\n",
      "Epoch [13/300], Step [105/225], Training Accuracy: 46.7411%, Training Loss: 1.0777%\n",
      "Epoch [13/300], Step [106/225], Training Accuracy: 46.6392%, Training Loss: 1.0784%\n",
      "Epoch [13/300], Step [107/225], Training Accuracy: 46.6414%, Training Loss: 1.0780%\n",
      "Epoch [13/300], Step [108/225], Training Accuracy: 46.6725%, Training Loss: 1.0785%\n",
      "Epoch [13/300], Step [109/225], Training Accuracy: 46.6026%, Training Loss: 1.0787%\n",
      "Epoch [13/300], Step [110/225], Training Accuracy: 46.5909%, Training Loss: 1.0788%\n",
      "Epoch [13/300], Step [111/225], Training Accuracy: 46.6216%, Training Loss: 1.0788%\n",
      "Epoch [13/300], Step [112/225], Training Accuracy: 46.6378%, Training Loss: 1.0785%\n",
      "Epoch [13/300], Step [113/225], Training Accuracy: 46.5570%, Training Loss: 1.0801%\n",
      "Epoch [13/300], Step [114/225], Training Accuracy: 46.5598%, Training Loss: 1.0798%\n",
      "Epoch [13/300], Step [115/225], Training Accuracy: 46.6304%, Training Loss: 1.0793%\n",
      "Epoch [13/300], Step [116/225], Training Accuracy: 46.7134%, Training Loss: 1.0786%\n",
      "Epoch [13/300], Step [117/225], Training Accuracy: 46.6213%, Training Loss: 1.0800%\n",
      "Epoch [13/300], Step [118/225], Training Accuracy: 46.6367%, Training Loss: 1.0800%\n",
      "Epoch [13/300], Step [119/225], Training Accuracy: 46.6124%, Training Loss: 1.0798%\n",
      "Epoch [13/300], Step [120/225], Training Accuracy: 46.6797%, Training Loss: 1.0789%\n",
      "Epoch [13/300], Step [121/225], Training Accuracy: 46.6167%, Training Loss: 1.0791%\n",
      "Epoch [13/300], Step [122/225], Training Accuracy: 46.5932%, Training Loss: 1.0790%\n",
      "Epoch [13/300], Step [123/225], Training Accuracy: 46.5447%, Training Loss: 1.0791%\n",
      "Epoch [13/300], Step [124/225], Training Accuracy: 46.5852%, Training Loss: 1.0786%\n",
      "Epoch [13/300], Step [125/225], Training Accuracy: 46.5500%, Training Loss: 1.0795%\n",
      "Epoch [13/300], Step [126/225], Training Accuracy: 46.5402%, Training Loss: 1.0798%\n",
      "Epoch [13/300], Step [127/225], Training Accuracy: 46.5428%, Training Loss: 1.0796%\n",
      "Epoch [13/300], Step [128/225], Training Accuracy: 46.4844%, Training Loss: 1.0800%\n",
      "Epoch [13/300], Step [129/225], Training Accuracy: 46.4632%, Training Loss: 1.0800%\n",
      "Epoch [13/300], Step [130/225], Training Accuracy: 46.4543%, Training Loss: 1.0805%\n",
      "Epoch [13/300], Step [131/225], Training Accuracy: 46.4575%, Training Loss: 1.0802%\n",
      "Epoch [13/300], Step [132/225], Training Accuracy: 46.4489%, Training Loss: 1.0807%\n",
      "Epoch [13/300], Step [133/225], Training Accuracy: 46.4638%, Training Loss: 1.0801%\n",
      "Epoch [13/300], Step [134/225], Training Accuracy: 46.4902%, Training Loss: 1.0793%\n",
      "Epoch [13/300], Step [135/225], Training Accuracy: 46.4931%, Training Loss: 1.0792%\n",
      "Epoch [13/300], Step [136/225], Training Accuracy: 46.4614%, Training Loss: 1.0786%\n",
      "Epoch [13/300], Step [137/225], Training Accuracy: 46.4872%, Training Loss: 1.0784%\n",
      "Epoch [13/300], Step [138/225], Training Accuracy: 46.5127%, Training Loss: 1.0779%\n",
      "Epoch [13/300], Step [139/225], Training Accuracy: 46.5490%, Training Loss: 1.0782%\n",
      "Epoch [13/300], Step [140/225], Training Accuracy: 46.5848%, Training Loss: 1.0780%\n",
      "Epoch [13/300], Step [141/225], Training Accuracy: 46.6312%, Training Loss: 1.0779%\n",
      "Epoch [13/300], Step [142/225], Training Accuracy: 46.6109%, Training Loss: 1.0780%\n",
      "Epoch [13/300], Step [143/225], Training Accuracy: 46.6892%, Training Loss: 1.0774%\n",
      "Epoch [13/300], Step [144/225], Training Accuracy: 46.6471%, Training Loss: 1.0773%\n",
      "Epoch [13/300], Step [145/225], Training Accuracy: 46.7565%, Training Loss: 1.0763%\n",
      "Epoch [13/300], Step [146/225], Training Accuracy: 46.7573%, Training Loss: 1.0762%\n",
      "Epoch [13/300], Step [147/225], Training Accuracy: 46.7262%, Training Loss: 1.0771%\n",
      "Epoch [13/300], Step [148/225], Training Accuracy: 46.7694%, Training Loss: 1.0770%\n",
      "Epoch [13/300], Step [149/225], Training Accuracy: 46.6967%, Training Loss: 1.0773%\n",
      "Epoch [13/300], Step [150/225], Training Accuracy: 46.6667%, Training Loss: 1.0778%\n",
      "Epoch [13/300], Step [151/225], Training Accuracy: 46.6784%, Training Loss: 1.0771%\n",
      "Epoch [13/300], Step [152/225], Training Accuracy: 46.6488%, Training Loss: 1.0771%\n",
      "Epoch [13/300], Step [153/225], Training Accuracy: 46.7116%, Training Loss: 1.0764%\n",
      "Epoch [13/300], Step [154/225], Training Accuracy: 46.6518%, Training Loss: 1.0764%\n",
      "Epoch [13/300], Step [155/225], Training Accuracy: 46.6532%, Training Loss: 1.0768%\n",
      "Epoch [13/300], Step [156/225], Training Accuracy: 46.6647%, Training Loss: 1.0765%\n",
      "Epoch [13/300], Step [157/225], Training Accuracy: 46.6760%, Training Loss: 1.0771%\n",
      "Epoch [13/300], Step [158/225], Training Accuracy: 46.6970%, Training Loss: 1.0769%\n",
      "Epoch [13/300], Step [159/225], Training Accuracy: 46.7178%, Training Loss: 1.0765%\n",
      "Epoch [13/300], Step [160/225], Training Accuracy: 46.6699%, Training Loss: 1.0769%\n",
      "Epoch [13/300], Step [161/225], Training Accuracy: 46.7294%, Training Loss: 1.0759%\n",
      "Epoch [13/300], Step [162/225], Training Accuracy: 46.7400%, Training Loss: 1.0759%\n",
      "Epoch [13/300], Step [163/225], Training Accuracy: 46.7025%, Training Loss: 1.0756%\n",
      "Epoch [13/300], Step [164/225], Training Accuracy: 46.7416%, Training Loss: 1.0751%\n",
      "Epoch [13/300], Step [165/225], Training Accuracy: 46.7898%, Training Loss: 1.0750%\n",
      "Epoch [13/300], Step [166/225], Training Accuracy: 46.8373%, Training Loss: 1.0753%\n",
      "Epoch [13/300], Step [167/225], Training Accuracy: 46.8656%, Training Loss: 1.0746%\n",
      "Epoch [13/300], Step [168/225], Training Accuracy: 46.8564%, Training Loss: 1.0749%\n",
      "Epoch [13/300], Step [169/225], Training Accuracy: 46.8658%, Training Loss: 1.0748%\n",
      "Epoch [13/300], Step [170/225], Training Accuracy: 46.8474%, Training Loss: 1.0754%\n",
      "Epoch [13/300], Step [171/225], Training Accuracy: 46.8110%, Training Loss: 1.0751%\n",
      "Epoch [13/300], Step [172/225], Training Accuracy: 46.7569%, Training Loss: 1.0756%\n",
      "Epoch [13/300], Step [173/225], Training Accuracy: 46.7486%, Training Loss: 1.0755%\n",
      "Epoch [13/300], Step [174/225], Training Accuracy: 46.7942%, Training Loss: 1.0754%\n",
      "Epoch [13/300], Step [175/225], Training Accuracy: 46.7857%, Training Loss: 1.0754%\n",
      "Epoch [13/300], Step [176/225], Training Accuracy: 46.7596%, Training Loss: 1.0754%\n",
      "Epoch [13/300], Step [177/225], Training Accuracy: 46.6984%, Training Loss: 1.0751%\n",
      "Epoch [13/300], Step [178/225], Training Accuracy: 46.6555%, Training Loss: 1.0751%\n",
      "Epoch [13/300], Step [179/225], Training Accuracy: 46.6830%, Training Loss: 1.0753%\n",
      "Epoch [13/300], Step [180/225], Training Accuracy: 46.8056%, Training Loss: 1.0740%\n",
      "Epoch [13/300], Step [181/225], Training Accuracy: 46.7541%, Training Loss: 1.0748%\n",
      "Epoch [13/300], Step [182/225], Training Accuracy: 46.7376%, Training Loss: 1.0751%\n",
      "Epoch [13/300], Step [183/225], Training Accuracy: 46.7213%, Training Loss: 1.0747%\n",
      "Epoch [13/300], Step [184/225], Training Accuracy: 46.7221%, Training Loss: 1.0747%\n",
      "Epoch [13/300], Step [185/225], Training Accuracy: 46.7399%, Training Loss: 1.0746%\n",
      "Epoch [13/300], Step [186/225], Training Accuracy: 46.7742%, Training Loss: 1.0744%\n",
      "Epoch [13/300], Step [187/225], Training Accuracy: 46.8082%, Training Loss: 1.0740%\n",
      "Epoch [13/300], Step [188/225], Training Accuracy: 46.8584%, Training Loss: 1.0736%\n",
      "Epoch [13/300], Step [189/225], Training Accuracy: 46.8915%, Training Loss: 1.0732%\n",
      "Epoch [13/300], Step [190/225], Training Accuracy: 46.8914%, Training Loss: 1.0735%\n",
      "Epoch [13/300], Step [191/225], Training Accuracy: 46.8832%, Training Loss: 1.0736%\n",
      "Epoch [13/300], Step [192/225], Training Accuracy: 46.8913%, Training Loss: 1.0736%\n",
      "Epoch [13/300], Step [193/225], Training Accuracy: 46.8588%, Training Loss: 1.0733%\n",
      "Epoch [13/300], Step [194/225], Training Accuracy: 46.8669%, Training Loss: 1.0736%\n",
      "Epoch [13/300], Step [195/225], Training Accuracy: 46.8750%, Training Loss: 1.0731%\n",
      "Epoch [13/300], Step [196/225], Training Accuracy: 46.9149%, Training Loss: 1.0731%\n",
      "Epoch [13/300], Step [197/225], Training Accuracy: 46.8988%, Training Loss: 1.0727%\n",
      "Epoch [13/300], Step [198/225], Training Accuracy: 46.8987%, Training Loss: 1.0721%\n",
      "Epoch [13/300], Step [199/225], Training Accuracy: 46.9300%, Training Loss: 1.0718%\n",
      "Epoch [13/300], Step [200/225], Training Accuracy: 46.9219%, Training Loss: 1.0721%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Step [201/225], Training Accuracy: 46.9139%, Training Loss: 1.0723%\n",
      "Epoch [13/300], Step [202/225], Training Accuracy: 46.9446%, Training Loss: 1.0720%\n",
      "Epoch [13/300], Step [203/225], Training Accuracy: 46.9443%, Training Loss: 1.0723%\n",
      "Epoch [13/300], Step [204/225], Training Accuracy: 46.9899%, Training Loss: 1.0721%\n",
      "Epoch [13/300], Step [205/225], Training Accuracy: 47.0046%, Training Loss: 1.0723%\n",
      "Epoch [13/300], Step [206/225], Training Accuracy: 47.0419%, Training Loss: 1.0726%\n",
      "Epoch [13/300], Step [207/225], Training Accuracy: 47.0033%, Training Loss: 1.0730%\n",
      "Epoch [13/300], Step [208/225], Training Accuracy: 47.0628%, Training Loss: 1.0725%\n",
      "Epoch [13/300], Step [209/225], Training Accuracy: 47.0320%, Training Loss: 1.0726%\n",
      "Epoch [13/300], Step [210/225], Training Accuracy: 47.0089%, Training Loss: 1.0723%\n",
      "Epoch [13/300], Step [211/225], Training Accuracy: 47.0157%, Training Loss: 1.0722%\n",
      "Epoch [13/300], Step [212/225], Training Accuracy: 46.9708%, Training Loss: 1.0727%\n",
      "Epoch [13/300], Step [213/225], Training Accuracy: 46.9263%, Training Loss: 1.0733%\n",
      "Epoch [13/300], Step [214/225], Training Accuracy: 46.9407%, Training Loss: 1.0731%\n",
      "Epoch [13/300], Step [215/225], Training Accuracy: 46.9695%, Training Loss: 1.0730%\n",
      "Epoch [13/300], Step [216/225], Training Accuracy: 46.9690%, Training Loss: 1.0731%\n",
      "Epoch [13/300], Step [217/225], Training Accuracy: 46.9542%, Training Loss: 1.0731%\n",
      "Epoch [13/300], Step [218/225], Training Accuracy: 46.9180%, Training Loss: 1.0738%\n",
      "Epoch [13/300], Step [219/225], Training Accuracy: 46.8536%, Training Loss: 1.0738%\n",
      "Epoch [13/300], Step [220/225], Training Accuracy: 46.8963%, Training Loss: 1.0735%\n",
      "Epoch [13/300], Step [221/225], Training Accuracy: 46.8750%, Training Loss: 1.0737%\n",
      "Epoch [13/300], Step [222/225], Training Accuracy: 46.8961%, Training Loss: 1.0737%\n",
      "Epoch [13/300], Step [223/225], Training Accuracy: 46.7979%, Training Loss: 1.0743%\n",
      "Epoch [13/300], Step [224/225], Training Accuracy: 46.7704%, Training Loss: 1.0744%\n",
      "Epoch [13/300], Step [225/225], Training Accuracy: 46.7760%, Training Loss: 1.0748%\n",
      "Epoch [14/300], Step [1/225], Training Accuracy: 62.5000%, Training Loss: 0.9733%\n",
      "Epoch [14/300], Step [2/225], Training Accuracy: 53.1250%, Training Loss: 1.0897%\n",
      "Epoch [14/300], Step [3/225], Training Accuracy: 48.4375%, Training Loss: 1.1349%\n",
      "Epoch [14/300], Step [4/225], Training Accuracy: 46.8750%, Training Loss: 1.1116%\n",
      "Epoch [14/300], Step [5/225], Training Accuracy: 47.5000%, Training Loss: 1.0854%\n",
      "Epoch [14/300], Step [6/225], Training Accuracy: 47.6562%, Training Loss: 1.1072%\n",
      "Epoch [14/300], Step [7/225], Training Accuracy: 47.3214%, Training Loss: 1.0998%\n",
      "Epoch [14/300], Step [8/225], Training Accuracy: 47.8516%, Training Loss: 1.1011%\n",
      "Epoch [14/300], Step [9/225], Training Accuracy: 47.0486%, Training Loss: 1.0987%\n",
      "Epoch [14/300], Step [10/225], Training Accuracy: 46.8750%, Training Loss: 1.0890%\n",
      "Epoch [14/300], Step [11/225], Training Accuracy: 47.5852%, Training Loss: 1.0834%\n",
      "Epoch [14/300], Step [12/225], Training Accuracy: 47.9167%, Training Loss: 1.0867%\n",
      "Epoch [14/300], Step [13/225], Training Accuracy: 48.3173%, Training Loss: 1.0873%\n",
      "Epoch [14/300], Step [14/225], Training Accuracy: 47.5446%, Training Loss: 1.0978%\n",
      "Epoch [14/300], Step [15/225], Training Accuracy: 47.2917%, Training Loss: 1.1038%\n",
      "Epoch [14/300], Step [16/225], Training Accuracy: 47.2656%, Training Loss: 1.1033%\n",
      "Epoch [14/300], Step [17/225], Training Accuracy: 47.4265%, Training Loss: 1.0973%\n",
      "Epoch [14/300], Step [18/225], Training Accuracy: 46.7882%, Training Loss: 1.0943%\n",
      "Epoch [14/300], Step [19/225], Training Accuracy: 46.4638%, Training Loss: 1.0951%\n",
      "Epoch [14/300], Step [20/225], Training Accuracy: 46.4844%, Training Loss: 1.0912%\n",
      "Epoch [14/300], Step [21/225], Training Accuracy: 46.7262%, Training Loss: 1.0844%\n",
      "Epoch [14/300], Step [22/225], Training Accuracy: 46.6619%, Training Loss: 1.0863%\n",
      "Epoch [14/300], Step [23/225], Training Accuracy: 46.6033%, Training Loss: 1.0826%\n",
      "Epoch [14/300], Step [24/225], Training Accuracy: 46.6146%, Training Loss: 1.0869%\n",
      "Epoch [14/300], Step [25/225], Training Accuracy: 46.8750%, Training Loss: 1.0830%\n",
      "Epoch [14/300], Step [26/225], Training Accuracy: 46.5745%, Training Loss: 1.0849%\n",
      "Epoch [14/300], Step [27/225], Training Accuracy: 46.5278%, Training Loss: 1.0845%\n",
      "Epoch [14/300], Step [28/225], Training Accuracy: 46.7076%, Training Loss: 1.0819%\n",
      "Epoch [14/300], Step [29/225], Training Accuracy: 46.7672%, Training Loss: 1.0782%\n",
      "Epoch [14/300], Step [30/225], Training Accuracy: 46.6667%, Training Loss: 1.0770%\n",
      "Epoch [14/300], Step [31/225], Training Accuracy: 46.6734%, Training Loss: 1.0768%\n",
      "Epoch [14/300], Step [32/225], Training Accuracy: 46.8262%, Training Loss: 1.0759%\n",
      "Epoch [14/300], Step [33/225], Training Accuracy: 47.0170%, Training Loss: 1.0733%\n",
      "Epoch [14/300], Step [34/225], Training Accuracy: 47.0588%, Training Loss: 1.0759%\n",
      "Epoch [14/300], Step [35/225], Training Accuracy: 46.9643%, Training Loss: 1.0749%\n",
      "Epoch [14/300], Step [36/225], Training Accuracy: 46.9618%, Training Loss: 1.0746%\n",
      "Epoch [14/300], Step [37/225], Training Accuracy: 47.0017%, Training Loss: 1.0721%\n",
      "Epoch [14/300], Step [38/225], Training Accuracy: 47.2862%, Training Loss: 1.0704%\n",
      "Epoch [14/300], Step [39/225], Training Accuracy: 47.2356%, Training Loss: 1.0690%\n",
      "Epoch [14/300], Step [40/225], Training Accuracy: 47.2656%, Training Loss: 1.0693%\n",
      "Epoch [14/300], Step [41/225], Training Accuracy: 47.0655%, Training Loss: 1.0705%\n",
      "Epoch [14/300], Step [42/225], Training Accuracy: 47.3214%, Training Loss: 1.0689%\n",
      "Epoch [14/300], Step [43/225], Training Accuracy: 47.3837%, Training Loss: 1.0688%\n",
      "Epoch [14/300], Step [44/225], Training Accuracy: 47.5142%, Training Loss: 1.0666%\n",
      "Epoch [14/300], Step [45/225], Training Accuracy: 47.4653%, Training Loss: 1.0656%\n",
      "Epoch [14/300], Step [46/225], Training Accuracy: 47.7921%, Training Loss: 1.0617%\n",
      "Epoch [14/300], Step [47/225], Training Accuracy: 47.8059%, Training Loss: 1.0621%\n",
      "Epoch [14/300], Step [48/225], Training Accuracy: 47.8516%, Training Loss: 1.0619%\n",
      "Epoch [14/300], Step [49/225], Training Accuracy: 47.7997%, Training Loss: 1.0631%\n",
      "Epoch [14/300], Step [50/225], Training Accuracy: 47.8750%, Training Loss: 1.0619%\n",
      "Epoch [14/300], Step [51/225], Training Accuracy: 47.9473%, Training Loss: 1.0612%\n",
      "Epoch [14/300], Step [52/225], Training Accuracy: 48.0168%, Training Loss: 1.0605%\n",
      "Epoch [14/300], Step [53/225], Training Accuracy: 47.8774%, Training Loss: 1.0606%\n",
      "Epoch [14/300], Step [54/225], Training Accuracy: 47.7431%, Training Loss: 1.0614%\n",
      "Epoch [14/300], Step [55/225], Training Accuracy: 47.6136%, Training Loss: 1.0625%\n",
      "Epoch [14/300], Step [56/225], Training Accuracy: 47.5725%, Training Loss: 1.0632%\n",
      "Epoch [14/300], Step [57/225], Training Accuracy: 47.8070%, Training Loss: 1.0600%\n",
      "Epoch [14/300], Step [58/225], Training Accuracy: 47.7371%, Training Loss: 1.0603%\n",
      "Epoch [14/300], Step [59/225], Training Accuracy: 47.8284%, Training Loss: 1.0601%\n",
      "Epoch [14/300], Step [60/225], Training Accuracy: 47.7604%, Training Loss: 1.0605%\n",
      "Epoch [14/300], Step [61/225], Training Accuracy: 47.8227%, Training Loss: 1.0604%\n",
      "Epoch [14/300], Step [62/225], Training Accuracy: 47.7571%, Training Loss: 1.0601%\n",
      "Epoch [14/300], Step [63/225], Training Accuracy: 47.6687%, Training Loss: 1.0619%\n",
      "Epoch [14/300], Step [64/225], Training Accuracy: 47.6318%, Training Loss: 1.0628%\n",
      "Epoch [14/300], Step [65/225], Training Accuracy: 47.5481%, Training Loss: 1.0629%\n",
      "Epoch [14/300], Step [66/225], Training Accuracy: 47.5616%, Training Loss: 1.0621%\n",
      "Epoch [14/300], Step [67/225], Training Accuracy: 47.4813%, Training Loss: 1.0618%\n",
      "Epoch [14/300], Step [68/225], Training Accuracy: 47.3805%, Training Loss: 1.0619%\n",
      "Epoch [14/300], Step [69/225], Training Accuracy: 47.2826%, Training Loss: 1.0621%\n",
      "Epoch [14/300], Step [70/225], Training Accuracy: 47.2098%, Training Loss: 1.0637%\n",
      "Epoch [14/300], Step [71/225], Training Accuracy: 47.2711%, Training Loss: 1.0625%\n",
      "Epoch [14/300], Step [72/225], Training Accuracy: 47.1137%, Training Loss: 1.0644%\n",
      "Epoch [14/300], Step [73/225], Training Accuracy: 47.0462%, Training Loss: 1.0672%\n",
      "Epoch [14/300], Step [74/225], Training Accuracy: 47.1284%, Training Loss: 1.0651%\n",
      "Epoch [14/300], Step [75/225], Training Accuracy: 47.1458%, Training Loss: 1.0641%\n",
      "Epoch [14/300], Step [76/225], Training Accuracy: 47.1423%, Training Loss: 1.0644%\n",
      "Epoch [14/300], Step [77/225], Training Accuracy: 47.2808%, Training Loss: 1.0639%\n",
      "Epoch [14/300], Step [78/225], Training Accuracy: 47.2556%, Training Loss: 1.0651%\n",
      "Epoch [14/300], Step [79/225], Training Accuracy: 47.2508%, Training Loss: 1.0660%\n",
      "Epoch [14/300], Step [80/225], Training Accuracy: 47.2266%, Training Loss: 1.0659%\n",
      "Epoch [14/300], Step [81/225], Training Accuracy: 47.3187%, Training Loss: 1.0666%\n",
      "Epoch [14/300], Step [82/225], Training Accuracy: 47.4085%, Training Loss: 1.0657%\n",
      "Epoch [14/300], Step [83/225], Training Accuracy: 47.4021%, Training Loss: 1.0657%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300], Step [84/225], Training Accuracy: 47.3586%, Training Loss: 1.0664%\n",
      "Epoch [14/300], Step [85/225], Training Accuracy: 47.3897%, Training Loss: 1.0659%\n",
      "Epoch [14/300], Step [86/225], Training Accuracy: 47.4201%, Training Loss: 1.0663%\n",
      "Epoch [14/300], Step [87/225], Training Accuracy: 47.4856%, Training Loss: 1.0659%\n",
      "Epoch [14/300], Step [88/225], Training Accuracy: 47.3899%, Training Loss: 1.0658%\n",
      "Epoch [14/300], Step [89/225], Training Accuracy: 47.4192%, Training Loss: 1.0668%\n",
      "Epoch [14/300], Step [90/225], Training Accuracy: 47.3438%, Training Loss: 1.0669%\n",
      "Epoch [14/300], Step [91/225], Training Accuracy: 47.4416%, Training Loss: 1.0652%\n",
      "Epoch [14/300], Step [92/225], Training Accuracy: 47.4015%, Training Loss: 1.0652%\n",
      "Epoch [14/300], Step [93/225], Training Accuracy: 47.3790%, Training Loss: 1.0654%\n",
      "Epoch [14/300], Step [94/225], Training Accuracy: 47.4900%, Training Loss: 1.0641%\n",
      "Epoch [14/300], Step [95/225], Training Accuracy: 47.3520%, Training Loss: 1.0653%\n",
      "Epoch [14/300], Step [96/225], Training Accuracy: 47.3958%, Training Loss: 1.0649%\n",
      "Epoch [14/300], Step [97/225], Training Accuracy: 47.4227%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [98/225], Training Accuracy: 47.3852%, Training Loss: 1.0635%\n",
      "Epoch [14/300], Step [99/225], Training Accuracy: 47.4116%, Training Loss: 1.0638%\n",
      "Epoch [14/300], Step [100/225], Training Accuracy: 47.2969%, Training Loss: 1.0639%\n",
      "Epoch [14/300], Step [101/225], Training Accuracy: 47.2618%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [102/225], Training Accuracy: 47.2426%, Training Loss: 1.0647%\n",
      "Epoch [14/300], Step [103/225], Training Accuracy: 47.1481%, Training Loss: 1.0657%\n",
      "Epoch [14/300], Step [104/225], Training Accuracy: 47.1605%, Training Loss: 1.0651%\n",
      "Epoch [14/300], Step [105/225], Training Accuracy: 47.1726%, Training Loss: 1.0643%\n",
      "Epoch [14/300], Step [106/225], Training Accuracy: 47.0666%, Training Loss: 1.0650%\n",
      "Epoch [14/300], Step [107/225], Training Accuracy: 47.0794%, Training Loss: 1.0647%\n",
      "Epoch [14/300], Step [108/225], Training Accuracy: 47.0920%, Training Loss: 1.0652%\n",
      "Epoch [14/300], Step [109/225], Training Accuracy: 47.0183%, Training Loss: 1.0653%\n",
      "Epoch [14/300], Step [110/225], Training Accuracy: 47.0028%, Training Loss: 1.0652%\n",
      "Epoch [14/300], Step [111/225], Training Accuracy: 47.0158%, Training Loss: 1.0654%\n",
      "Epoch [14/300], Step [112/225], Training Accuracy: 47.0285%, Training Loss: 1.0651%\n",
      "Epoch [14/300], Step [113/225], Training Accuracy: 46.9441%, Training Loss: 1.0667%\n",
      "Epoch [14/300], Step [114/225], Training Accuracy: 46.9572%, Training Loss: 1.0664%\n",
      "Epoch [14/300], Step [115/225], Training Accuracy: 46.9973%, Training Loss: 1.0658%\n",
      "Epoch [14/300], Step [116/225], Training Accuracy: 47.0770%, Training Loss: 1.0653%\n",
      "Epoch [14/300], Step [117/225], Training Accuracy: 46.9818%, Training Loss: 1.0667%\n",
      "Epoch [14/300], Step [118/225], Training Accuracy: 47.0074%, Training Loss: 1.0666%\n",
      "Epoch [14/300], Step [119/225], Training Accuracy: 46.9669%, Training Loss: 1.0666%\n",
      "Epoch [14/300], Step [120/225], Training Accuracy: 47.0443%, Training Loss: 1.0657%\n",
      "Epoch [14/300], Step [121/225], Training Accuracy: 46.9783%, Training Loss: 1.0658%\n",
      "Epoch [14/300], Step [122/225], Training Accuracy: 46.9647%, Training Loss: 1.0657%\n",
      "Epoch [14/300], Step [123/225], Training Accuracy: 46.8877%, Training Loss: 1.0658%\n",
      "Epoch [14/300], Step [124/225], Training Accuracy: 46.9380%, Training Loss: 1.0654%\n",
      "Epoch [14/300], Step [125/225], Training Accuracy: 46.9000%, Training Loss: 1.0663%\n",
      "Epoch [14/300], Step [126/225], Training Accuracy: 46.8750%, Training Loss: 1.0667%\n",
      "Epoch [14/300], Step [127/225], Training Accuracy: 46.8750%, Training Loss: 1.0664%\n",
      "Epoch [14/300], Step [128/225], Training Accuracy: 46.8140%, Training Loss: 1.0668%\n",
      "Epoch [14/300], Step [129/225], Training Accuracy: 46.8023%, Training Loss: 1.0669%\n",
      "Epoch [14/300], Step [130/225], Training Accuracy: 46.7909%, Training Loss: 1.0674%\n",
      "Epoch [14/300], Step [131/225], Training Accuracy: 46.8034%, Training Loss: 1.0671%\n",
      "Epoch [14/300], Step [132/225], Training Accuracy: 46.7921%, Training Loss: 1.0676%\n",
      "Epoch [14/300], Step [133/225], Training Accuracy: 46.8163%, Training Loss: 1.0670%\n",
      "Epoch [14/300], Step [134/225], Training Accuracy: 46.8517%, Training Loss: 1.0662%\n",
      "Epoch [14/300], Step [135/225], Training Accuracy: 46.8403%, Training Loss: 1.0661%\n",
      "Epoch [14/300], Step [136/225], Training Accuracy: 46.8176%, Training Loss: 1.0655%\n",
      "Epoch [14/300], Step [137/225], Training Accuracy: 46.8408%, Training Loss: 1.0654%\n",
      "Epoch [14/300], Step [138/225], Training Accuracy: 46.8750%, Training Loss: 1.0648%\n",
      "Epoch [14/300], Step [139/225], Training Accuracy: 46.9087%, Training Loss: 1.0651%\n",
      "Epoch [14/300], Step [140/225], Training Accuracy: 46.9420%, Training Loss: 1.0649%\n",
      "Epoch [14/300], Step [141/225], Training Accuracy: 46.9969%, Training Loss: 1.0649%\n",
      "Epoch [14/300], Step [142/225], Training Accuracy: 46.9850%, Training Loss: 1.0650%\n",
      "Epoch [14/300], Step [143/225], Training Accuracy: 47.0717%, Training Loss: 1.0644%\n",
      "Epoch [14/300], Step [144/225], Training Accuracy: 47.0486%, Training Loss: 1.0643%\n",
      "Epoch [14/300], Step [145/225], Training Accuracy: 47.1659%, Training Loss: 1.0633%\n",
      "Epoch [14/300], Step [146/225], Training Accuracy: 47.1747%, Training Loss: 1.0633%\n",
      "Epoch [14/300], Step [147/225], Training Accuracy: 47.1301%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [148/225], Training Accuracy: 47.2128%, Training Loss: 1.0641%\n",
      "Epoch [14/300], Step [149/225], Training Accuracy: 47.1477%, Training Loss: 1.0644%\n",
      "Epoch [14/300], Step [150/225], Training Accuracy: 47.1250%, Training Loss: 1.0649%\n",
      "Epoch [14/300], Step [151/225], Training Accuracy: 47.1233%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [152/225], Training Accuracy: 47.0806%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [153/225], Training Accuracy: 47.1405%, Training Loss: 1.0635%\n",
      "Epoch [14/300], Step [154/225], Training Accuracy: 47.0678%, Training Loss: 1.0635%\n",
      "Epoch [14/300], Step [155/225], Training Accuracy: 47.0565%, Training Loss: 1.0639%\n",
      "Epoch [14/300], Step [156/225], Training Accuracy: 47.0653%, Training Loss: 1.0637%\n",
      "Epoch [14/300], Step [157/225], Training Accuracy: 47.0740%, Training Loss: 1.0643%\n",
      "Epoch [14/300], Step [158/225], Training Accuracy: 47.1025%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [159/225], Training Accuracy: 47.1403%, Training Loss: 1.0638%\n",
      "Epoch [14/300], Step [160/225], Training Accuracy: 47.0898%, Training Loss: 1.0642%\n",
      "Epoch [14/300], Step [161/225], Training Accuracy: 47.1564%, Training Loss: 1.0631%\n",
      "Epoch [14/300], Step [162/225], Training Accuracy: 47.1644%, Training Loss: 1.0631%\n",
      "Epoch [14/300], Step [163/225], Training Accuracy: 47.1146%, Training Loss: 1.0628%\n",
      "Epoch [14/300], Step [164/225], Training Accuracy: 47.1704%, Training Loss: 1.0622%\n",
      "Epoch [14/300], Step [165/225], Training Accuracy: 47.2159%, Training Loss: 1.0621%\n",
      "Epoch [14/300], Step [166/225], Training Accuracy: 47.2609%, Training Loss: 1.0624%\n",
      "Epoch [14/300], Step [167/225], Training Accuracy: 47.2960%, Training Loss: 1.0617%\n",
      "Epoch [14/300], Step [168/225], Training Accuracy: 47.2935%, Training Loss: 1.0620%\n",
      "Epoch [14/300], Step [169/225], Training Accuracy: 47.3003%, Training Loss: 1.0619%\n",
      "Epoch [14/300], Step [170/225], Training Accuracy: 47.2702%, Training Loss: 1.0625%\n",
      "Epoch [14/300], Step [171/225], Training Accuracy: 47.2496%, Training Loss: 1.0623%\n",
      "Epoch [14/300], Step [172/225], Training Accuracy: 47.1930%, Training Loss: 1.0628%\n",
      "Epoch [14/300], Step [173/225], Training Accuracy: 47.2001%, Training Loss: 1.0626%\n",
      "Epoch [14/300], Step [174/225], Training Accuracy: 47.2432%, Training Loss: 1.0625%\n",
      "Epoch [14/300], Step [175/225], Training Accuracy: 47.2500%, Training Loss: 1.0625%\n",
      "Epoch [14/300], Step [176/225], Training Accuracy: 47.2390%, Training Loss: 1.0626%\n",
      "Epoch [14/300], Step [177/225], Training Accuracy: 47.2016%, Training Loss: 1.0623%\n",
      "Epoch [14/300], Step [178/225], Training Accuracy: 47.1647%, Training Loss: 1.0623%\n",
      "Epoch [14/300], Step [179/225], Training Accuracy: 47.1805%, Training Loss: 1.0624%\n",
      "Epoch [14/300], Step [180/225], Training Accuracy: 47.3090%, Training Loss: 1.0611%\n",
      "Epoch [14/300], Step [181/225], Training Accuracy: 47.2721%, Training Loss: 1.0619%\n",
      "Epoch [14/300], Step [182/225], Training Accuracy: 47.2356%, Training Loss: 1.0622%\n",
      "Epoch [14/300], Step [183/225], Training Accuracy: 47.2165%, Training Loss: 1.0618%\n",
      "Epoch [14/300], Step [184/225], Training Accuracy: 47.2147%, Training Loss: 1.0618%\n",
      "Epoch [14/300], Step [185/225], Training Accuracy: 47.2213%, Training Loss: 1.0617%\n",
      "Epoch [14/300], Step [186/225], Training Accuracy: 47.2614%, Training Loss: 1.0615%\n",
      "Epoch [14/300], Step [187/225], Training Accuracy: 47.2928%, Training Loss: 1.0611%\n",
      "Epoch [14/300], Step [188/225], Training Accuracy: 47.3404%, Training Loss: 1.0607%\n",
      "Epoch [14/300], Step [189/225], Training Accuracy: 47.3710%, Training Loss: 1.0602%\n",
      "Epoch [14/300], Step [190/225], Training Accuracy: 47.3684%, Training Loss: 1.0605%\n",
      "Epoch [14/300], Step [191/225], Training Accuracy: 47.3658%, Training Loss: 1.0606%\n",
      "Epoch [14/300], Step [192/225], Training Accuracy: 47.3796%, Training Loss: 1.0605%\n",
      "Epoch [14/300], Step [193/225], Training Accuracy: 47.3446%, Training Loss: 1.0604%\n",
      "Epoch [14/300], Step [194/225], Training Accuracy: 47.3502%, Training Loss: 1.0606%\n",
      "Epoch [14/300], Step [195/225], Training Accuracy: 47.3558%, Training Loss: 1.0601%\n",
      "Epoch [14/300], Step [196/225], Training Accuracy: 47.4011%, Training Loss: 1.0602%\n",
      "Epoch [14/300], Step [197/225], Training Accuracy: 47.3826%, Training Loss: 1.0597%\n",
      "Epoch [14/300], Step [198/225], Training Accuracy: 47.3801%, Training Loss: 1.0591%\n",
      "Epoch [14/300], Step [199/225], Training Accuracy: 47.4089%, Training Loss: 1.0588%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/300], Step [200/225], Training Accuracy: 47.4062%, Training Loss: 1.0590%\n",
      "Epoch [14/300], Step [201/225], Training Accuracy: 47.4114%, Training Loss: 1.0593%\n",
      "Epoch [14/300], Step [202/225], Training Accuracy: 47.4551%, Training Loss: 1.0590%\n",
      "Epoch [14/300], Step [203/225], Training Accuracy: 47.4446%, Training Loss: 1.0593%\n",
      "Epoch [14/300], Step [204/225], Training Accuracy: 47.4954%, Training Loss: 1.0592%\n",
      "Epoch [14/300], Step [205/225], Training Accuracy: 47.5000%, Training Loss: 1.0592%\n",
      "Epoch [14/300], Step [206/225], Training Accuracy: 47.5501%, Training Loss: 1.0597%\n",
      "Epoch [14/300], Step [207/225], Training Accuracy: 47.5242%, Training Loss: 1.0601%\n",
      "Epoch [14/300], Step [208/225], Training Accuracy: 47.5886%, Training Loss: 1.0596%\n",
      "Epoch [14/300], Step [209/225], Training Accuracy: 47.5628%, Training Loss: 1.0596%\n",
      "Epoch [14/300], Step [210/225], Training Accuracy: 47.5521%, Training Loss: 1.0594%\n",
      "Epoch [14/300], Step [211/225], Training Accuracy: 47.5711%, Training Loss: 1.0592%\n",
      "Epoch [14/300], Step [212/225], Training Accuracy: 47.5310%, Training Loss: 1.0598%\n",
      "Epoch [14/300], Step [213/225], Training Accuracy: 47.4765%, Training Loss: 1.0604%\n",
      "Epoch [14/300], Step [214/225], Training Accuracy: 47.4883%, Training Loss: 1.0602%\n",
      "Epoch [14/300], Step [215/225], Training Accuracy: 47.5145%, Training Loss: 1.0601%\n",
      "Epoch [14/300], Step [216/225], Training Accuracy: 47.5405%, Training Loss: 1.0602%\n",
      "Epoch [14/300], Step [217/225], Training Accuracy: 47.5230%, Training Loss: 1.0602%\n",
      "Epoch [14/300], Step [218/225], Training Accuracy: 47.4914%, Training Loss: 1.0609%\n",
      "Epoch [14/300], Step [219/225], Training Accuracy: 47.4315%, Training Loss: 1.0611%\n",
      "Epoch [14/300], Step [220/225], Training Accuracy: 47.4716%, Training Loss: 1.0608%\n",
      "Epoch [14/300], Step [221/225], Training Accuracy: 47.4406%, Training Loss: 1.0609%\n",
      "Epoch [14/300], Step [222/225], Training Accuracy: 47.4662%, Training Loss: 1.0609%\n",
      "Epoch [14/300], Step [223/225], Training Accuracy: 47.4005%, Training Loss: 1.0615%\n",
      "Epoch [14/300], Step [224/225], Training Accuracy: 47.3842%, Training Loss: 1.0616%\n",
      "Epoch [14/300], Step [225/225], Training Accuracy: 47.3805%, Training Loss: 1.0619%\n",
      "Epoch [15/300], Step [1/225], Training Accuracy: 62.5000%, Training Loss: 0.9448%\n",
      "Epoch [15/300], Step [2/225], Training Accuracy: 53.1250%, Training Loss: 1.0634%\n",
      "Epoch [15/300], Step [3/225], Training Accuracy: 47.9167%, Training Loss: 1.1126%\n",
      "Epoch [15/300], Step [4/225], Training Accuracy: 46.4844%, Training Loss: 1.0935%\n",
      "Epoch [15/300], Step [5/225], Training Accuracy: 47.8125%, Training Loss: 1.0668%\n",
      "Epoch [15/300], Step [6/225], Training Accuracy: 47.9167%, Training Loss: 1.0890%\n",
      "Epoch [15/300], Step [7/225], Training Accuracy: 47.7679%, Training Loss: 1.0822%\n",
      "Epoch [15/300], Step [8/225], Training Accuracy: 48.0469%, Training Loss: 1.0841%\n",
      "Epoch [15/300], Step [9/225], Training Accuracy: 47.3958%, Training Loss: 1.0808%\n",
      "Epoch [15/300], Step [10/225], Training Accuracy: 47.5000%, Training Loss: 1.0720%\n",
      "Epoch [15/300], Step [11/225], Training Accuracy: 48.2955%, Training Loss: 1.0662%\n",
      "Epoch [15/300], Step [12/225], Training Accuracy: 48.4375%, Training Loss: 1.0703%\n",
      "Epoch [15/300], Step [13/225], Training Accuracy: 48.9183%, Training Loss: 1.0713%\n",
      "Epoch [15/300], Step [14/225], Training Accuracy: 48.1027%, Training Loss: 1.0811%\n",
      "Epoch [15/300], Step [15/225], Training Accuracy: 47.9167%, Training Loss: 1.0877%\n",
      "Epoch [15/300], Step [16/225], Training Accuracy: 47.8516%, Training Loss: 1.0879%\n",
      "Epoch [15/300], Step [17/225], Training Accuracy: 47.9779%, Training Loss: 1.0816%\n",
      "Epoch [15/300], Step [18/225], Training Accuracy: 47.6562%, Training Loss: 1.0790%\n",
      "Epoch [15/300], Step [19/225], Training Accuracy: 47.3684%, Training Loss: 1.0800%\n",
      "Epoch [15/300], Step [20/225], Training Accuracy: 47.3438%, Training Loss: 1.0763%\n",
      "Epoch [15/300], Step [21/225], Training Accuracy: 47.6190%, Training Loss: 1.0698%\n",
      "Epoch [15/300], Step [22/225], Training Accuracy: 47.5142%, Training Loss: 1.0717%\n",
      "Epoch [15/300], Step [23/225], Training Accuracy: 47.5543%, Training Loss: 1.0682%\n",
      "Epoch [15/300], Step [24/225], Training Accuracy: 47.4609%, Training Loss: 1.0727%\n",
      "Epoch [15/300], Step [25/225], Training Accuracy: 47.6875%, Training Loss: 1.0688%\n",
      "Epoch [15/300], Step [26/225], Training Accuracy: 47.3558%, Training Loss: 1.0709%\n",
      "Epoch [15/300], Step [27/225], Training Accuracy: 47.3380%, Training Loss: 1.0706%\n",
      "Epoch [15/300], Step [28/225], Training Accuracy: 47.5446%, Training Loss: 1.0679%\n",
      "Epoch [15/300], Step [29/225], Training Accuracy: 47.6832%, Training Loss: 1.0640%\n",
      "Epoch [15/300], Step [30/225], Training Accuracy: 47.6042%, Training Loss: 1.0628%\n",
      "Epoch [15/300], Step [31/225], Training Accuracy: 47.6815%, Training Loss: 1.0627%\n",
      "Epoch [15/300], Step [32/225], Training Accuracy: 47.9004%, Training Loss: 1.0616%\n",
      "Epoch [15/300], Step [33/225], Training Accuracy: 48.0587%, Training Loss: 1.0592%\n",
      "Epoch [15/300], Step [34/225], Training Accuracy: 48.1158%, Training Loss: 1.0620%\n",
      "Epoch [15/300], Step [35/225], Training Accuracy: 48.0357%, Training Loss: 1.0610%\n",
      "Epoch [15/300], Step [36/225], Training Accuracy: 48.0035%, Training Loss: 1.0607%\n",
      "Epoch [15/300], Step [37/225], Training Accuracy: 48.0574%, Training Loss: 1.0581%\n",
      "Epoch [15/300], Step [38/225], Training Accuracy: 48.2730%, Training Loss: 1.0564%\n",
      "Epoch [15/300], Step [39/225], Training Accuracy: 48.1571%, Training Loss: 1.0551%\n",
      "Epoch [15/300], Step [40/225], Training Accuracy: 48.1641%, Training Loss: 1.0555%\n",
      "Epoch [15/300], Step [41/225], Training Accuracy: 47.9421%, Training Loss: 1.0567%\n",
      "Epoch [15/300], Step [42/225], Training Accuracy: 48.1771%, Training Loss: 1.0550%\n",
      "Epoch [15/300], Step [43/225], Training Accuracy: 48.2195%, Training Loss: 1.0549%\n",
      "Epoch [15/300], Step [44/225], Training Accuracy: 48.3665%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [45/225], Training Accuracy: 48.3333%, Training Loss: 1.0520%\n",
      "Epoch [15/300], Step [46/225], Training Accuracy: 48.6413%, Training Loss: 1.0481%\n",
      "Epoch [15/300], Step [47/225], Training Accuracy: 48.6702%, Training Loss: 1.0487%\n",
      "Epoch [15/300], Step [48/225], Training Accuracy: 48.7956%, Training Loss: 1.0486%\n",
      "Epoch [15/300], Step [49/225], Training Accuracy: 48.6288%, Training Loss: 1.0497%\n",
      "Epoch [15/300], Step [50/225], Training Accuracy: 48.7812%, Training Loss: 1.0484%\n",
      "Epoch [15/300], Step [51/225], Training Accuracy: 48.8051%, Training Loss: 1.0477%\n",
      "Epoch [15/300], Step [52/225], Training Accuracy: 48.8582%, Training Loss: 1.0468%\n",
      "Epoch [15/300], Step [53/225], Training Accuracy: 48.7028%, Training Loss: 1.0469%\n",
      "Epoch [15/300], Step [54/225], Training Accuracy: 48.5243%, Training Loss: 1.0478%\n",
      "Epoch [15/300], Step [55/225], Training Accuracy: 48.3807%, Training Loss: 1.0491%\n",
      "Epoch [15/300], Step [56/225], Training Accuracy: 48.3259%, Training Loss: 1.0499%\n",
      "Epoch [15/300], Step [57/225], Training Accuracy: 48.5471%, Training Loss: 1.0467%\n",
      "Epoch [15/300], Step [58/225], Training Accuracy: 48.4644%, Training Loss: 1.0471%\n",
      "Epoch [15/300], Step [59/225], Training Accuracy: 48.5169%, Training Loss: 1.0469%\n",
      "Epoch [15/300], Step [60/225], Training Accuracy: 48.4375%, Training Loss: 1.0474%\n",
      "Epoch [15/300], Step [61/225], Training Accuracy: 48.4887%, Training Loss: 1.0473%\n",
      "Epoch [15/300], Step [62/225], Training Accuracy: 48.4123%, Training Loss: 1.0470%\n",
      "Epoch [15/300], Step [63/225], Training Accuracy: 48.3383%, Training Loss: 1.0487%\n",
      "Epoch [15/300], Step [64/225], Training Accuracy: 48.3154%, Training Loss: 1.0495%\n",
      "Epoch [15/300], Step [65/225], Training Accuracy: 48.2212%, Training Loss: 1.0497%\n",
      "Epoch [15/300], Step [66/225], Training Accuracy: 48.2718%, Training Loss: 1.0487%\n",
      "Epoch [15/300], Step [67/225], Training Accuracy: 48.2043%, Training Loss: 1.0485%\n",
      "Epoch [15/300], Step [68/225], Training Accuracy: 48.1158%, Training Loss: 1.0486%\n",
      "Epoch [15/300], Step [69/225], Training Accuracy: 47.9846%, Training Loss: 1.0488%\n",
      "Epoch [15/300], Step [70/225], Training Accuracy: 47.8795%, Training Loss: 1.0504%\n",
      "Epoch [15/300], Step [71/225], Training Accuracy: 47.9313%, Training Loss: 1.0491%\n",
      "Epoch [15/300], Step [72/225], Training Accuracy: 47.7865%, Training Loss: 1.0511%\n",
      "Epoch [15/300], Step [73/225], Training Accuracy: 47.7098%, Training Loss: 1.0539%\n",
      "Epoch [15/300], Step [74/225], Training Accuracy: 47.7618%, Training Loss: 1.0518%\n",
      "Epoch [15/300], Step [75/225], Training Accuracy: 47.7708%, Training Loss: 1.0509%\n",
      "Epoch [15/300], Step [76/225], Training Accuracy: 47.7796%, Training Loss: 1.0510%\n",
      "Epoch [15/300], Step [77/225], Training Accuracy: 47.8693%, Training Loss: 1.0506%\n",
      "Epoch [15/300], Step [78/225], Training Accuracy: 47.8566%, Training Loss: 1.0518%\n",
      "Epoch [15/300], Step [79/225], Training Accuracy: 47.9035%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [80/225], Training Accuracy: 47.8711%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [81/225], Training Accuracy: 47.9360%, Training Loss: 1.0535%\n",
      "Epoch [15/300], Step [82/225], Training Accuracy: 48.0183%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [83/225], Training Accuracy: 48.0045%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [84/225], Training Accuracy: 47.9539%, Training Loss: 1.0534%\n",
      "Epoch [15/300], Step [85/225], Training Accuracy: 47.9963%, Training Loss: 1.0529%\n",
      "Epoch [15/300], Step [86/225], Training Accuracy: 48.0741%, Training Loss: 1.0532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Step [87/225], Training Accuracy: 48.1322%, Training Loss: 1.0529%\n",
      "Epoch [15/300], Step [88/225], Training Accuracy: 48.0114%, Training Loss: 1.0530%\n",
      "Epoch [15/300], Step [89/225], Training Accuracy: 48.0337%, Training Loss: 1.0541%\n",
      "Epoch [15/300], Step [90/225], Training Accuracy: 47.9340%, Training Loss: 1.0543%\n",
      "Epoch [15/300], Step [91/225], Training Accuracy: 48.0254%, Training Loss: 1.0525%\n",
      "Epoch [15/300], Step [92/225], Training Accuracy: 48.0129%, Training Loss: 1.0525%\n",
      "Epoch [15/300], Step [93/225], Training Accuracy: 48.0007%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [94/225], Training Accuracy: 48.0884%, Training Loss: 1.0514%\n",
      "Epoch [15/300], Step [95/225], Training Accuracy: 47.9770%, Training Loss: 1.0525%\n",
      "Epoch [15/300], Step [96/225], Training Accuracy: 48.0143%, Training Loss: 1.0521%\n",
      "Epoch [15/300], Step [97/225], Training Accuracy: 48.0509%, Training Loss: 1.0514%\n",
      "Epoch [15/300], Step [98/225], Training Accuracy: 48.0389%, Training Loss: 1.0507%\n",
      "Epoch [15/300], Step [99/225], Training Accuracy: 48.0745%, Training Loss: 1.0509%\n",
      "Epoch [15/300], Step [100/225], Training Accuracy: 47.9688%, Training Loss: 1.0511%\n",
      "Epoch [15/300], Step [101/225], Training Accuracy: 47.9115%, Training Loss: 1.0515%\n",
      "Epoch [15/300], Step [102/225], Training Accuracy: 47.9013%, Training Loss: 1.0521%\n",
      "Epoch [15/300], Step [103/225], Training Accuracy: 47.8004%, Training Loss: 1.0531%\n",
      "Epoch [15/300], Step [104/225], Training Accuracy: 47.8065%, Training Loss: 1.0525%\n",
      "Epoch [15/300], Step [105/225], Training Accuracy: 47.7827%, Training Loss: 1.0517%\n",
      "Epoch [15/300], Step [106/225], Training Accuracy: 47.6710%, Training Loss: 1.0525%\n",
      "Epoch [15/300], Step [107/225], Training Accuracy: 47.6489%, Training Loss: 1.0522%\n",
      "Epoch [15/300], Step [108/225], Training Accuracy: 47.6562%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [109/225], Training Accuracy: 47.5917%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [110/225], Training Accuracy: 47.6562%, Training Loss: 1.0526%\n",
      "Epoch [15/300], Step [111/225], Training Accuracy: 47.6633%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [112/225], Training Accuracy: 47.6842%, Training Loss: 1.0525%\n",
      "Epoch [15/300], Step [113/225], Training Accuracy: 47.6079%, Training Loss: 1.0541%\n",
      "Epoch [15/300], Step [114/225], Training Accuracy: 47.6151%, Training Loss: 1.0539%\n",
      "Epoch [15/300], Step [115/225], Training Accuracy: 47.6359%, Training Loss: 1.0532%\n",
      "Epoch [15/300], Step [116/225], Training Accuracy: 47.7101%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [117/225], Training Accuracy: 47.6362%, Training Loss: 1.0542%\n",
      "Epoch [15/300], Step [118/225], Training Accuracy: 47.6562%, Training Loss: 1.0541%\n",
      "Epoch [15/300], Step [119/225], Training Accuracy: 47.6103%, Training Loss: 1.0542%\n",
      "Epoch [15/300], Step [120/225], Training Accuracy: 47.6953%, Training Loss: 1.0533%\n",
      "Epoch [15/300], Step [121/225], Training Accuracy: 47.6369%, Training Loss: 1.0534%\n",
      "Epoch [15/300], Step [122/225], Training Accuracy: 47.6178%, Training Loss: 1.0533%\n",
      "Epoch [15/300], Step [123/225], Training Accuracy: 47.5229%, Training Loss: 1.0535%\n",
      "Epoch [15/300], Step [124/225], Training Accuracy: 47.5680%, Training Loss: 1.0531%\n",
      "Epoch [15/300], Step [125/225], Training Accuracy: 47.5250%, Training Loss: 1.0540%\n",
      "Epoch [15/300], Step [126/225], Training Accuracy: 47.4950%, Training Loss: 1.0544%\n",
      "Epoch [15/300], Step [127/225], Training Accuracy: 47.5025%, Training Loss: 1.0541%\n",
      "Epoch [15/300], Step [128/225], Training Accuracy: 47.4487%, Training Loss: 1.0546%\n",
      "Epoch [15/300], Step [129/225], Training Accuracy: 47.4443%, Training Loss: 1.0547%\n",
      "Epoch [15/300], Step [130/225], Training Accuracy: 47.4399%, Training Loss: 1.0552%\n",
      "Epoch [15/300], Step [131/225], Training Accuracy: 47.4475%, Training Loss: 1.0549%\n",
      "Epoch [15/300], Step [132/225], Training Accuracy: 47.4313%, Training Loss: 1.0554%\n",
      "Epoch [15/300], Step [133/225], Training Accuracy: 47.4507%, Training Loss: 1.0548%\n",
      "Epoch [15/300], Step [134/225], Training Accuracy: 47.4930%, Training Loss: 1.0541%\n",
      "Epoch [15/300], Step [135/225], Training Accuracy: 47.4884%, Training Loss: 1.0539%\n",
      "Epoch [15/300], Step [136/225], Training Accuracy: 47.4494%, Training Loss: 1.0533%\n",
      "Epoch [15/300], Step [137/225], Training Accuracy: 47.4681%, Training Loss: 1.0532%\n",
      "Epoch [15/300], Step [138/225], Training Accuracy: 47.4977%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [139/225], Training Accuracy: 47.5382%, Training Loss: 1.0529%\n",
      "Epoch [15/300], Step [140/225], Training Accuracy: 47.5893%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [141/225], Training Accuracy: 47.6396%, Training Loss: 1.0527%\n",
      "Epoch [15/300], Step [142/225], Training Accuracy: 47.6232%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [143/225], Training Accuracy: 47.6836%, Training Loss: 1.0523%\n",
      "Epoch [15/300], Step [144/225], Training Accuracy: 47.6671%, Training Loss: 1.0521%\n",
      "Epoch [15/300], Step [145/225], Training Accuracy: 47.7802%, Training Loss: 1.0512%\n",
      "Epoch [15/300], Step [146/225], Training Accuracy: 47.7740%, Training Loss: 1.0512%\n",
      "Epoch [15/300], Step [147/225], Training Accuracy: 47.7360%, Training Loss: 1.0522%\n",
      "Epoch [15/300], Step [148/225], Training Accuracy: 47.8146%, Training Loss: 1.0520%\n",
      "Epoch [15/300], Step [149/225], Training Accuracy: 47.7664%, Training Loss: 1.0523%\n",
      "Epoch [15/300], Step [150/225], Training Accuracy: 47.7500%, Training Loss: 1.0528%\n",
      "Epoch [15/300], Step [151/225], Training Accuracy: 47.7442%, Training Loss: 1.0522%\n",
      "Epoch [15/300], Step [152/225], Training Accuracy: 47.7282%, Training Loss: 1.0522%\n",
      "Epoch [15/300], Step [153/225], Training Accuracy: 47.7737%, Training Loss: 1.0515%\n",
      "Epoch [15/300], Step [154/225], Training Accuracy: 47.7070%, Training Loss: 1.0515%\n",
      "Epoch [15/300], Step [155/225], Training Accuracy: 47.7016%, Training Loss: 1.0519%\n",
      "Epoch [15/300], Step [156/225], Training Accuracy: 47.6963%, Training Loss: 1.0517%\n",
      "Epoch [15/300], Step [157/225], Training Accuracy: 47.6811%, Training Loss: 1.0523%\n",
      "Epoch [15/300], Step [158/225], Training Accuracy: 47.7156%, Training Loss: 1.0522%\n",
      "Epoch [15/300], Step [159/225], Training Accuracy: 47.7398%, Training Loss: 1.0519%\n",
      "Epoch [15/300], Step [160/225], Training Accuracy: 47.6953%, Training Loss: 1.0522%\n",
      "Epoch [15/300], Step [161/225], Training Accuracy: 47.7679%, Training Loss: 1.0512%\n",
      "Epoch [15/300], Step [162/225], Training Accuracy: 47.7720%, Training Loss: 1.0512%\n",
      "Epoch [15/300], Step [163/225], Training Accuracy: 47.7090%, Training Loss: 1.0509%\n",
      "Epoch [15/300], Step [164/225], Training Accuracy: 47.7611%, Training Loss: 1.0502%\n",
      "Epoch [15/300], Step [165/225], Training Accuracy: 47.8220%, Training Loss: 1.0501%\n",
      "Epoch [15/300], Step [166/225], Training Accuracy: 47.8539%, Training Loss: 1.0504%\n",
      "Epoch [15/300], Step [167/225], Training Accuracy: 47.8855%, Training Loss: 1.0498%\n",
      "Epoch [15/300], Step [168/225], Training Accuracy: 47.8795%, Training Loss: 1.0500%\n",
      "Epoch [15/300], Step [169/225], Training Accuracy: 47.8828%, Training Loss: 1.0499%\n",
      "Epoch [15/300], Step [170/225], Training Accuracy: 47.8768%, Training Loss: 1.0505%\n",
      "Epoch [15/300], Step [171/225], Training Accuracy: 47.8527%, Training Loss: 1.0504%\n",
      "Epoch [15/300], Step [172/225], Training Accuracy: 47.7925%, Training Loss: 1.0509%\n",
      "Epoch [15/300], Step [173/225], Training Accuracy: 47.7962%, Training Loss: 1.0507%\n",
      "Epoch [15/300], Step [174/225], Training Accuracy: 47.8269%, Training Loss: 1.0506%\n",
      "Epoch [15/300], Step [175/225], Training Accuracy: 47.8393%, Training Loss: 1.0506%\n",
      "Epoch [15/300], Step [176/225], Training Accuracy: 47.8072%, Training Loss: 1.0506%\n",
      "Epoch [15/300], Step [177/225], Training Accuracy: 47.7754%, Training Loss: 1.0503%\n",
      "Epoch [15/300], Step [178/225], Training Accuracy: 47.7353%, Training Loss: 1.0503%\n",
      "Epoch [15/300], Step [179/225], Training Accuracy: 47.7566%, Training Loss: 1.0504%\n",
      "Epoch [15/300], Step [180/225], Training Accuracy: 47.8733%, Training Loss: 1.0491%\n",
      "Epoch [15/300], Step [181/225], Training Accuracy: 47.8505%, Training Loss: 1.0499%\n",
      "Epoch [15/300], Step [182/225], Training Accuracy: 47.8194%, Training Loss: 1.0502%\n",
      "Epoch [15/300], Step [183/225], Training Accuracy: 47.8142%, Training Loss: 1.0498%\n",
      "Epoch [15/300], Step [184/225], Training Accuracy: 47.8176%, Training Loss: 1.0498%\n",
      "Epoch [15/300], Step [185/225], Training Accuracy: 47.8294%, Training Loss: 1.0497%\n",
      "Epoch [15/300], Step [186/225], Training Accuracy: 47.8663%, Training Loss: 1.0495%\n",
      "Epoch [15/300], Step [187/225], Training Accuracy: 47.8944%, Training Loss: 1.0490%\n",
      "Epoch [15/300], Step [188/225], Training Accuracy: 47.9471%, Training Loss: 1.0485%\n",
      "Epoch [15/300], Step [189/225], Training Accuracy: 47.9993%, Training Loss: 1.0481%\n",
      "Epoch [15/300], Step [190/225], Training Accuracy: 47.9852%, Training Loss: 1.0484%\n",
      "Epoch [15/300], Step [191/225], Training Accuracy: 47.9794%, Training Loss: 1.0485%\n",
      "Epoch [15/300], Step [192/225], Training Accuracy: 48.0143%, Training Loss: 1.0484%\n",
      "Epoch [15/300], Step [193/225], Training Accuracy: 47.9841%, Training Loss: 1.0482%\n",
      "Epoch [15/300], Step [194/225], Training Accuracy: 48.0187%, Training Loss: 1.0484%\n",
      "Epoch [15/300], Step [195/225], Training Accuracy: 48.0208%, Training Loss: 1.0479%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Step [196/225], Training Accuracy: 48.0628%, Training Loss: 1.0480%\n",
      "Epoch [15/300], Step [197/225], Training Accuracy: 48.0489%, Training Loss: 1.0475%\n",
      "Epoch [15/300], Step [198/225], Training Accuracy: 48.0508%, Training Loss: 1.0469%\n",
      "Epoch [15/300], Step [199/225], Training Accuracy: 48.0528%, Training Loss: 1.0466%\n",
      "Epoch [15/300], Step [200/225], Training Accuracy: 48.0703%, Training Loss: 1.0468%\n",
      "Epoch [15/300], Step [201/225], Training Accuracy: 48.0721%, Training Loss: 1.0472%\n",
      "Epoch [15/300], Step [202/225], Training Accuracy: 48.1126%, Training Loss: 1.0468%\n",
      "Epoch [15/300], Step [203/225], Training Accuracy: 48.0911%, Training Loss: 1.0471%\n",
      "Epoch [15/300], Step [204/225], Training Accuracy: 48.1235%, Training Loss: 1.0470%\n",
      "Epoch [15/300], Step [205/225], Training Accuracy: 48.1174%, Training Loss: 1.0471%\n",
      "Epoch [15/300], Step [206/225], Training Accuracy: 48.1644%, Training Loss: 1.0475%\n",
      "Epoch [15/300], Step [207/225], Training Accuracy: 48.1431%, Training Loss: 1.0479%\n",
      "Epoch [15/300], Step [208/225], Training Accuracy: 48.2121%, Training Loss: 1.0474%\n",
      "Epoch [15/300], Step [209/225], Training Accuracy: 48.1833%, Training Loss: 1.0475%\n",
      "Epoch [15/300], Step [210/225], Training Accuracy: 48.1622%, Training Loss: 1.0473%\n",
      "Epoch [15/300], Step [211/225], Training Accuracy: 48.1783%, Training Loss: 1.0471%\n",
      "Epoch [15/300], Step [212/225], Training Accuracy: 48.1279%, Training Loss: 1.0476%\n",
      "Epoch [15/300], Step [213/225], Training Accuracy: 48.0707%, Training Loss: 1.0483%\n",
      "Epoch [15/300], Step [214/225], Training Accuracy: 48.0724%, Training Loss: 1.0481%\n",
      "Epoch [15/300], Step [215/225], Training Accuracy: 48.0959%, Training Loss: 1.0480%\n",
      "Epoch [15/300], Step [216/225], Training Accuracy: 48.1192%, Training Loss: 1.0481%\n",
      "Epoch [15/300], Step [217/225], Training Accuracy: 48.0991%, Training Loss: 1.0482%\n",
      "Epoch [15/300], Step [218/225], Training Accuracy: 48.0576%, Training Loss: 1.0489%\n",
      "Epoch [15/300], Step [219/225], Training Accuracy: 48.0023%, Training Loss: 1.0491%\n",
      "Epoch [15/300], Step [220/225], Training Accuracy: 48.0398%, Training Loss: 1.0487%\n",
      "Epoch [15/300], Step [221/225], Training Accuracy: 48.0062%, Training Loss: 1.0489%\n",
      "Epoch [15/300], Step [222/225], Training Accuracy: 48.0222%, Training Loss: 1.0489%\n",
      "Epoch [15/300], Step [223/225], Training Accuracy: 47.9540%, Training Loss: 1.0495%\n",
      "Epoch [15/300], Step [224/225], Training Accuracy: 47.9353%, Training Loss: 1.0496%\n",
      "Epoch [15/300], Step [225/225], Training Accuracy: 47.9294%, Training Loss: 1.0499%\n",
      "Epoch [16/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.9209%\n",
      "Epoch [16/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 1.0427%\n",
      "Epoch [16/300], Step [3/225], Training Accuracy: 49.4792%, Training Loss: 1.0939%\n",
      "Epoch [16/300], Step [4/225], Training Accuracy: 47.6562%, Training Loss: 1.0785%\n",
      "Epoch [16/300], Step [5/225], Training Accuracy: 48.4375%, Training Loss: 1.0512%\n",
      "Epoch [16/300], Step [6/225], Training Accuracy: 48.4375%, Training Loss: 1.0733%\n",
      "Epoch [16/300], Step [7/225], Training Accuracy: 48.6607%, Training Loss: 1.0670%\n",
      "Epoch [16/300], Step [8/225], Training Accuracy: 49.2188%, Training Loss: 1.0694%\n",
      "Epoch [16/300], Step [9/225], Training Accuracy: 48.7847%, Training Loss: 1.0655%\n",
      "Epoch [16/300], Step [10/225], Training Accuracy: 48.9062%, Training Loss: 1.0572%\n",
      "Epoch [16/300], Step [11/225], Training Accuracy: 49.7159%, Training Loss: 1.0514%\n",
      "Epoch [16/300], Step [12/225], Training Accuracy: 49.8698%, Training Loss: 1.0562%\n",
      "Epoch [16/300], Step [13/225], Training Accuracy: 50.1202%, Training Loss: 1.0574%\n",
      "Epoch [16/300], Step [14/225], Training Accuracy: 49.4420%, Training Loss: 1.0666%\n",
      "Epoch [16/300], Step [15/225], Training Accuracy: 49.2708%, Training Loss: 1.0737%\n",
      "Epoch [16/300], Step [16/225], Training Accuracy: 49.1211%, Training Loss: 1.0744%\n",
      "Epoch [16/300], Step [17/225], Training Accuracy: 49.1728%, Training Loss: 1.0682%\n",
      "Epoch [16/300], Step [18/225], Training Accuracy: 48.8715%, Training Loss: 1.0659%\n",
      "Epoch [16/300], Step [19/225], Training Accuracy: 48.3553%, Training Loss: 1.0669%\n",
      "Epoch [16/300], Step [20/225], Training Accuracy: 48.3594%, Training Loss: 1.0634%\n",
      "Epoch [16/300], Step [21/225], Training Accuracy: 48.6607%, Training Loss: 1.0573%\n",
      "Epoch [16/300], Step [22/225], Training Accuracy: 48.5795%, Training Loss: 1.0591%\n",
      "Epoch [16/300], Step [23/225], Training Accuracy: 48.5734%, Training Loss: 1.0557%\n",
      "Epoch [16/300], Step [24/225], Training Accuracy: 48.4375%, Training Loss: 1.0603%\n",
      "Epoch [16/300], Step [25/225], Training Accuracy: 48.6250%, Training Loss: 1.0565%\n",
      "Epoch [16/300], Step [26/225], Training Accuracy: 48.3173%, Training Loss: 1.0587%\n",
      "Epoch [16/300], Step [27/225], Training Accuracy: 48.2639%, Training Loss: 1.0586%\n",
      "Epoch [16/300], Step [28/225], Training Accuracy: 48.4933%, Training Loss: 1.0557%\n",
      "Epoch [16/300], Step [29/225], Training Accuracy: 48.7608%, Training Loss: 1.0517%\n",
      "Epoch [16/300], Step [30/225], Training Accuracy: 48.6979%, Training Loss: 1.0505%\n",
      "Epoch [16/300], Step [31/225], Training Accuracy: 48.7903%, Training Loss: 1.0504%\n",
      "Epoch [16/300], Step [32/225], Training Accuracy: 48.9746%, Training Loss: 1.0490%\n",
      "Epoch [16/300], Step [33/225], Training Accuracy: 49.1004%, Training Loss: 1.0468%\n",
      "Epoch [16/300], Step [34/225], Training Accuracy: 49.1268%, Training Loss: 1.0498%\n",
      "Epoch [16/300], Step [35/225], Training Accuracy: 48.9732%, Training Loss: 1.0490%\n",
      "Epoch [16/300], Step [36/225], Training Accuracy: 49.0017%, Training Loss: 1.0486%\n",
      "Epoch [16/300], Step [37/225], Training Accuracy: 49.1132%, Training Loss: 1.0459%\n",
      "Epoch [16/300], Step [38/225], Training Accuracy: 49.3421%, Training Loss: 1.0441%\n",
      "Epoch [16/300], Step [39/225], Training Accuracy: 49.1987%, Training Loss: 1.0429%\n",
      "Epoch [16/300], Step [40/225], Training Accuracy: 49.1797%, Training Loss: 1.0433%\n",
      "Epoch [16/300], Step [41/225], Training Accuracy: 48.8948%, Training Loss: 1.0446%\n",
      "Epoch [16/300], Step [42/225], Training Accuracy: 49.1071%, Training Loss: 1.0428%\n",
      "Epoch [16/300], Step [43/225], Training Accuracy: 49.1642%, Training Loss: 1.0428%\n",
      "Epoch [16/300], Step [44/225], Training Accuracy: 49.3608%, Training Loss: 1.0408%\n",
      "Epoch [16/300], Step [45/225], Training Accuracy: 49.3056%, Training Loss: 1.0400%\n",
      "Epoch [16/300], Step [46/225], Training Accuracy: 49.5924%, Training Loss: 1.0362%\n",
      "Epoch [16/300], Step [47/225], Training Accuracy: 49.6011%, Training Loss: 1.0369%\n",
      "Epoch [16/300], Step [48/225], Training Accuracy: 49.7070%, Training Loss: 1.0369%\n",
      "Epoch [16/300], Step [49/225], Training Accuracy: 49.5217%, Training Loss: 1.0382%\n",
      "Epoch [16/300], Step [50/225], Training Accuracy: 49.6250%, Training Loss: 1.0368%\n",
      "Epoch [16/300], Step [51/225], Training Accuracy: 49.6324%, Training Loss: 1.0358%\n",
      "Epoch [16/300], Step [52/225], Training Accuracy: 49.6995%, Training Loss: 1.0348%\n",
      "Epoch [16/300], Step [53/225], Training Accuracy: 49.5578%, Training Loss: 1.0349%\n",
      "Epoch [16/300], Step [54/225], Training Accuracy: 49.3345%, Training Loss: 1.0360%\n",
      "Epoch [16/300], Step [55/225], Training Accuracy: 49.2330%, Training Loss: 1.0373%\n",
      "Epoch [16/300], Step [56/225], Training Accuracy: 49.1908%, Training Loss: 1.0382%\n",
      "Epoch [16/300], Step [57/225], Training Accuracy: 49.4243%, Training Loss: 1.0351%\n",
      "Epoch [16/300], Step [58/225], Training Accuracy: 49.3265%, Training Loss: 1.0355%\n",
      "Epoch [16/300], Step [59/225], Training Accuracy: 49.3909%, Training Loss: 1.0353%\n",
      "Epoch [16/300], Step [60/225], Training Accuracy: 49.2969%, Training Loss: 1.0359%\n",
      "Epoch [16/300], Step [61/225], Training Accuracy: 49.3340%, Training Loss: 1.0357%\n",
      "Epoch [16/300], Step [62/225], Training Accuracy: 49.2440%, Training Loss: 1.0355%\n",
      "Epoch [16/300], Step [63/225], Training Accuracy: 49.1815%, Training Loss: 1.0372%\n",
      "Epoch [16/300], Step [64/225], Training Accuracy: 49.1455%, Training Loss: 1.0378%\n",
      "Epoch [16/300], Step [65/225], Training Accuracy: 49.0625%, Training Loss: 1.0380%\n",
      "Epoch [16/300], Step [66/225], Training Accuracy: 49.1951%, Training Loss: 1.0368%\n",
      "Epoch [16/300], Step [67/225], Training Accuracy: 49.1138%, Training Loss: 1.0367%\n",
      "Epoch [16/300], Step [68/225], Training Accuracy: 49.0349%, Training Loss: 1.0368%\n",
      "Epoch [16/300], Step [69/225], Training Accuracy: 48.8904%, Training Loss: 1.0370%\n",
      "Epoch [16/300], Step [70/225], Training Accuracy: 48.7500%, Training Loss: 1.0386%\n",
      "Epoch [16/300], Step [71/225], Training Accuracy: 48.8116%, Training Loss: 1.0373%\n",
      "Epoch [16/300], Step [72/225], Training Accuracy: 48.6545%, Training Loss: 1.0393%\n",
      "Epoch [16/300], Step [73/225], Training Accuracy: 48.5659%, Training Loss: 1.0421%\n",
      "Epoch [16/300], Step [74/225], Training Accuracy: 48.6275%, Training Loss: 1.0400%\n",
      "Epoch [16/300], Step [75/225], Training Accuracy: 48.6458%, Training Loss: 1.0392%\n",
      "Epoch [16/300], Step [76/225], Training Accuracy: 48.7048%, Training Loss: 1.0393%\n",
      "Epoch [16/300], Step [77/225], Training Accuracy: 48.8028%, Training Loss: 1.0389%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Step [78/225], Training Accuracy: 48.7981%, Training Loss: 1.0402%\n",
      "Epoch [16/300], Step [79/225], Training Accuracy: 48.8133%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [80/225], Training Accuracy: 48.7695%, Training Loss: 1.0412%\n",
      "Epoch [16/300], Step [81/225], Training Accuracy: 48.8040%, Training Loss: 1.0420%\n",
      "Epoch [16/300], Step [82/225], Training Accuracy: 48.8758%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [83/225], Training Accuracy: 48.8517%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [84/225], Training Accuracy: 48.8281%, Training Loss: 1.0418%\n",
      "Epoch [16/300], Step [85/225], Training Accuracy: 48.8603%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [86/225], Training Accuracy: 48.9099%, Training Loss: 1.0416%\n",
      "Epoch [16/300], Step [87/225], Training Accuracy: 48.9224%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [88/225], Training Accuracy: 48.7926%, Training Loss: 1.0415%\n",
      "Epoch [16/300], Step [89/225], Training Accuracy: 48.8062%, Training Loss: 1.0427%\n",
      "Epoch [16/300], Step [90/225], Training Accuracy: 48.6979%, Training Loss: 1.0430%\n",
      "Epoch [16/300], Step [91/225], Training Accuracy: 48.7809%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [92/225], Training Accuracy: 48.7772%, Training Loss: 1.0412%\n",
      "Epoch [16/300], Step [93/225], Training Accuracy: 48.7735%, Training Loss: 1.0414%\n",
      "Epoch [16/300], Step [94/225], Training Accuracy: 48.8697%, Training Loss: 1.0400%\n",
      "Epoch [16/300], Step [95/225], Training Accuracy: 48.7664%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [96/225], Training Accuracy: 48.7793%, Training Loss: 1.0406%\n",
      "Epoch [16/300], Step [97/225], Training Accuracy: 48.8080%, Training Loss: 1.0399%\n",
      "Epoch [16/300], Step [98/225], Training Accuracy: 48.8042%, Training Loss: 1.0392%\n",
      "Epoch [16/300], Step [99/225], Training Accuracy: 48.8321%, Training Loss: 1.0395%\n",
      "Epoch [16/300], Step [100/225], Training Accuracy: 48.7188%, Training Loss: 1.0397%\n",
      "Epoch [16/300], Step [101/225], Training Accuracy: 48.6541%, Training Loss: 1.0401%\n",
      "Epoch [16/300], Step [102/225], Training Accuracy: 48.6366%, Training Loss: 1.0408%\n",
      "Epoch [16/300], Step [103/225], Training Accuracy: 48.5285%, Training Loss: 1.0418%\n",
      "Epoch [16/300], Step [104/225], Training Accuracy: 48.5276%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [105/225], Training Accuracy: 48.5119%, Training Loss: 1.0405%\n",
      "Epoch [16/300], Step [106/225], Training Accuracy: 48.3933%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [107/225], Training Accuracy: 48.3937%, Training Loss: 1.0410%\n",
      "Epoch [16/300], Step [108/225], Training Accuracy: 48.4375%, Training Loss: 1.0415%\n",
      "Epoch [16/300], Step [109/225], Training Accuracy: 48.3945%, Training Loss: 1.0416%\n",
      "Epoch [16/300], Step [110/225], Training Accuracy: 48.4517%, Training Loss: 1.0414%\n",
      "Epoch [16/300], Step [111/225], Training Accuracy: 48.4797%, Training Loss: 1.0416%\n",
      "Epoch [16/300], Step [112/225], Training Accuracy: 48.4654%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [113/225], Training Accuracy: 48.3822%, Training Loss: 1.0428%\n",
      "Epoch [16/300], Step [114/225], Training Accuracy: 48.3690%, Training Loss: 1.0426%\n",
      "Epoch [16/300], Step [115/225], Training Accuracy: 48.3967%, Training Loss: 1.0419%\n",
      "Epoch [16/300], Step [116/225], Training Accuracy: 48.4375%, Training Loss: 1.0416%\n",
      "Epoch [16/300], Step [117/225], Training Accuracy: 48.3574%, Training Loss: 1.0430%\n",
      "Epoch [16/300], Step [118/225], Training Accuracy: 48.3845%, Training Loss: 1.0429%\n",
      "Epoch [16/300], Step [119/225], Training Accuracy: 48.3325%, Training Loss: 1.0430%\n",
      "Epoch [16/300], Step [120/225], Training Accuracy: 48.4115%, Training Loss: 1.0421%\n",
      "Epoch [16/300], Step [121/225], Training Accuracy: 48.3471%, Training Loss: 1.0422%\n",
      "Epoch [16/300], Step [122/225], Training Accuracy: 48.3094%, Training Loss: 1.0421%\n",
      "Epoch [16/300], Step [123/225], Training Accuracy: 48.2215%, Training Loss: 1.0422%\n",
      "Epoch [16/300], Step [124/225], Training Accuracy: 48.2611%, Training Loss: 1.0418%\n",
      "Epoch [16/300], Step [125/225], Training Accuracy: 48.2125%, Training Loss: 1.0428%\n",
      "Epoch [16/300], Step [126/225], Training Accuracy: 48.1771%, Training Loss: 1.0433%\n",
      "Epoch [16/300], Step [127/225], Training Accuracy: 48.1791%, Training Loss: 1.0430%\n",
      "Epoch [16/300], Step [128/225], Training Accuracy: 48.1079%, Training Loss: 1.0434%\n",
      "Epoch [16/300], Step [129/225], Training Accuracy: 48.0984%, Training Loss: 1.0435%\n",
      "Epoch [16/300], Step [130/225], Training Accuracy: 48.0649%, Training Loss: 1.0441%\n",
      "Epoch [16/300], Step [131/225], Training Accuracy: 48.0677%, Training Loss: 1.0438%\n",
      "Epoch [16/300], Step [132/225], Training Accuracy: 48.0587%, Training Loss: 1.0443%\n",
      "Epoch [16/300], Step [133/225], Training Accuracy: 48.0733%, Training Loss: 1.0437%\n",
      "Epoch [16/300], Step [134/225], Training Accuracy: 48.0993%, Training Loss: 1.0430%\n",
      "Epoch [16/300], Step [135/225], Training Accuracy: 48.0903%, Training Loss: 1.0428%\n",
      "Epoch [16/300], Step [136/225], Training Accuracy: 48.0584%, Training Loss: 1.0422%\n",
      "Epoch [16/300], Step [137/225], Training Accuracy: 48.0611%, Training Loss: 1.0421%\n",
      "Epoch [16/300], Step [138/225], Training Accuracy: 48.0865%, Training Loss: 1.0415%\n",
      "Epoch [16/300], Step [139/225], Training Accuracy: 48.1115%, Training Loss: 1.0418%\n",
      "Epoch [16/300], Step [140/225], Training Accuracy: 48.1696%, Training Loss: 1.0416%\n",
      "Epoch [16/300], Step [141/225], Training Accuracy: 48.2270%, Training Loss: 1.0416%\n",
      "Epoch [16/300], Step [142/225], Training Accuracy: 48.2394%, Training Loss: 1.0417%\n",
      "Epoch [16/300], Step [143/225], Training Accuracy: 48.2955%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [144/225], Training Accuracy: 48.2530%, Training Loss: 1.0410%\n",
      "Epoch [16/300], Step [145/225], Training Accuracy: 48.3728%, Training Loss: 1.0401%\n",
      "Epoch [16/300], Step [146/225], Training Accuracy: 48.3519%, Training Loss: 1.0401%\n",
      "Epoch [16/300], Step [147/225], Training Accuracy: 48.2993%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [148/225], Training Accuracy: 48.3742%, Training Loss: 1.0409%\n",
      "Epoch [16/300], Step [149/225], Training Accuracy: 48.3221%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [150/225], Training Accuracy: 48.3229%, Training Loss: 1.0418%\n",
      "Epoch [16/300], Step [151/225], Training Accuracy: 48.3133%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [152/225], Training Accuracy: 48.2936%, Training Loss: 1.0411%\n",
      "Epoch [16/300], Step [153/225], Training Accuracy: 48.3252%, Training Loss: 1.0404%\n",
      "Epoch [16/300], Step [154/225], Training Accuracy: 48.2650%, Training Loss: 1.0404%\n",
      "Epoch [16/300], Step [155/225], Training Accuracy: 48.2560%, Training Loss: 1.0409%\n",
      "Epoch [16/300], Step [156/225], Training Accuracy: 48.2472%, Training Loss: 1.0407%\n",
      "Epoch [16/300], Step [157/225], Training Accuracy: 48.2186%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [158/225], Training Accuracy: 48.2595%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [159/225], Training Accuracy: 48.2803%, Training Loss: 1.0410%\n",
      "Epoch [16/300], Step [160/225], Training Accuracy: 48.2324%, Training Loss: 1.0413%\n",
      "Epoch [16/300], Step [161/225], Training Accuracy: 48.3307%, Training Loss: 1.0402%\n",
      "Epoch [16/300], Step [162/225], Training Accuracy: 48.3410%, Training Loss: 1.0403%\n",
      "Epoch [16/300], Step [163/225], Training Accuracy: 48.2841%, Training Loss: 1.0399%\n",
      "Epoch [16/300], Step [164/225], Training Accuracy: 48.3422%, Training Loss: 1.0392%\n",
      "Epoch [16/300], Step [165/225], Training Accuracy: 48.3996%, Training Loss: 1.0391%\n",
      "Epoch [16/300], Step [166/225], Training Accuracy: 48.4187%, Training Loss: 1.0394%\n",
      "Epoch [16/300], Step [167/225], Training Accuracy: 48.4469%, Training Loss: 1.0387%\n",
      "Epoch [16/300], Step [168/225], Training Accuracy: 48.4375%, Training Loss: 1.0390%\n",
      "Epoch [16/300], Step [169/225], Training Accuracy: 48.4467%, Training Loss: 1.0389%\n",
      "Epoch [16/300], Step [170/225], Training Accuracy: 48.4283%, Training Loss: 1.0395%\n",
      "Epoch [16/300], Step [171/225], Training Accuracy: 48.4010%, Training Loss: 1.0394%\n",
      "Epoch [16/300], Step [172/225], Training Accuracy: 48.3467%, Training Loss: 1.0399%\n",
      "Epoch [16/300], Step [173/225], Training Accuracy: 48.3562%, Training Loss: 1.0397%\n",
      "Epoch [16/300], Step [174/225], Training Accuracy: 48.4016%, Training Loss: 1.0396%\n",
      "Epoch [16/300], Step [175/225], Training Accuracy: 48.4107%, Training Loss: 1.0396%\n",
      "Epoch [16/300], Step [176/225], Training Accuracy: 48.3842%, Training Loss: 1.0396%\n",
      "Epoch [16/300], Step [177/225], Training Accuracy: 48.3669%, Training Loss: 1.0393%\n",
      "Epoch [16/300], Step [178/225], Training Accuracy: 48.3322%, Training Loss: 1.0392%\n",
      "Epoch [16/300], Step [179/225], Training Accuracy: 48.3502%, Training Loss: 1.0394%\n",
      "Epoch [16/300], Step [180/225], Training Accuracy: 48.4635%, Training Loss: 1.0381%\n",
      "Epoch [16/300], Step [181/225], Training Accuracy: 48.4202%, Training Loss: 1.0388%\n",
      "Epoch [16/300], Step [182/225], Training Accuracy: 48.3860%, Training Loss: 1.0392%\n",
      "Epoch [16/300], Step [183/225], Training Accuracy: 48.3948%, Training Loss: 1.0388%\n",
      "Epoch [16/300], Step [184/225], Training Accuracy: 48.3865%, Training Loss: 1.0388%\n",
      "Epoch [16/300], Step [185/225], Training Accuracy: 48.3953%, Training Loss: 1.0387%\n",
      "Epoch [16/300], Step [186/225], Training Accuracy: 48.4375%, Training Loss: 1.0384%\n",
      "Epoch [16/300], Step [187/225], Training Accuracy: 48.4709%, Training Loss: 1.0379%\n",
      "Epoch [16/300], Step [188/225], Training Accuracy: 48.5289%, Training Loss: 1.0374%\n",
      "Epoch [16/300], Step [189/225], Training Accuracy: 48.5863%, Training Loss: 1.0370%\n",
      "Epoch [16/300], Step [190/225], Training Accuracy: 48.5855%, Training Loss: 1.0373%\n",
      "Epoch [16/300], Step [191/225], Training Accuracy: 48.5848%, Training Loss: 1.0374%\n",
      "Epoch [16/300], Step [192/225], Training Accuracy: 48.6084%, Training Loss: 1.0372%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Step [193/225], Training Accuracy: 48.5832%, Training Loss: 1.0371%\n",
      "Epoch [16/300], Step [194/225], Training Accuracy: 48.6227%, Training Loss: 1.0373%\n",
      "Epoch [16/300], Step [195/225], Training Accuracy: 48.6298%, Training Loss: 1.0368%\n",
      "Epoch [16/300], Step [196/225], Training Accuracy: 48.6767%, Training Loss: 1.0369%\n",
      "Epoch [16/300], Step [197/225], Training Accuracy: 48.6675%, Training Loss: 1.0364%\n",
      "Epoch [16/300], Step [198/225], Training Accuracy: 48.6664%, Training Loss: 1.0358%\n",
      "Epoch [16/300], Step [199/225], Training Accuracy: 48.6652%, Training Loss: 1.0355%\n",
      "Epoch [16/300], Step [200/225], Training Accuracy: 48.6875%, Training Loss: 1.0357%\n",
      "Epoch [16/300], Step [201/225], Training Accuracy: 48.7018%, Training Loss: 1.0360%\n",
      "Epoch [16/300], Step [202/225], Training Accuracy: 48.7237%, Training Loss: 1.0356%\n",
      "Epoch [16/300], Step [203/225], Training Accuracy: 48.6992%, Training Loss: 1.0360%\n",
      "Epoch [16/300], Step [204/225], Training Accuracy: 48.7286%, Training Loss: 1.0359%\n",
      "Epoch [16/300], Step [205/225], Training Accuracy: 48.7195%, Training Loss: 1.0359%\n",
      "Epoch [16/300], Step [206/225], Training Accuracy: 48.7637%, Training Loss: 1.0364%\n",
      "Epoch [16/300], Step [207/225], Training Accuracy: 48.7470%, Training Loss: 1.0368%\n",
      "Epoch [16/300], Step [208/225], Training Accuracy: 48.8131%, Training Loss: 1.0363%\n",
      "Epoch [16/300], Step [209/225], Training Accuracy: 48.7964%, Training Loss: 1.0364%\n",
      "Epoch [16/300], Step [210/225], Training Accuracy: 48.7723%, Training Loss: 1.0362%\n",
      "Epoch [16/300], Step [211/225], Training Accuracy: 48.7781%, Training Loss: 1.0360%\n",
      "Epoch [16/300], Step [212/225], Training Accuracy: 48.7397%, Training Loss: 1.0365%\n",
      "Epoch [16/300], Step [213/225], Training Accuracy: 48.6942%, Training Loss: 1.0371%\n",
      "Epoch [16/300], Step [214/225], Training Accuracy: 48.7004%, Training Loss: 1.0369%\n",
      "Epoch [16/300], Step [215/225], Training Accuracy: 48.7209%, Training Loss: 1.0369%\n",
      "Epoch [16/300], Step [216/225], Training Accuracy: 48.7341%, Training Loss: 1.0370%\n",
      "Epoch [16/300], Step [217/225], Training Accuracy: 48.7039%, Training Loss: 1.0371%\n",
      "Epoch [16/300], Step [218/225], Training Accuracy: 48.6597%, Training Loss: 1.0378%\n",
      "Epoch [16/300], Step [219/225], Training Accuracy: 48.5945%, Training Loss: 1.0381%\n",
      "Epoch [16/300], Step [220/225], Training Accuracy: 48.6293%, Training Loss: 1.0377%\n",
      "Epoch [16/300], Step [221/225], Training Accuracy: 48.6072%, Training Loss: 1.0379%\n",
      "Epoch [16/300], Step [222/225], Training Accuracy: 48.6205%, Training Loss: 1.0378%\n",
      "Epoch [16/300], Step [223/225], Training Accuracy: 48.5496%, Training Loss: 1.0385%\n",
      "Epoch [16/300], Step [224/225], Training Accuracy: 48.5282%, Training Loss: 1.0385%\n",
      "Epoch [16/300], Step [225/225], Training Accuracy: 48.5131%, Training Loss: 1.0388%\n",
      "Epoch [17/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8989%\n",
      "Epoch [17/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 1.0245%\n",
      "Epoch [17/300], Step [3/225], Training Accuracy: 49.4792%, Training Loss: 1.0771%\n",
      "Epoch [17/300], Step [4/225], Training Accuracy: 48.0469%, Training Loss: 1.0651%\n",
      "Epoch [17/300], Step [5/225], Training Accuracy: 49.0625%, Training Loss: 1.0373%\n",
      "Epoch [17/300], Step [6/225], Training Accuracy: 48.6979%, Training Loss: 1.0594%\n",
      "Epoch [17/300], Step [7/225], Training Accuracy: 48.6607%, Training Loss: 1.0536%\n",
      "Epoch [17/300], Step [8/225], Training Accuracy: 49.2188%, Training Loss: 1.0567%\n",
      "Epoch [17/300], Step [9/225], Training Accuracy: 48.9583%, Training Loss: 1.0523%\n",
      "Epoch [17/300], Step [10/225], Training Accuracy: 49.2188%, Training Loss: 1.0449%\n",
      "Epoch [17/300], Step [11/225], Training Accuracy: 50.1420%, Training Loss: 1.0390%\n",
      "Epoch [17/300], Step [12/225], Training Accuracy: 50.3906%, Training Loss: 1.0442%\n",
      "Epoch [17/300], Step [13/225], Training Accuracy: 50.7212%, Training Loss: 1.0452%\n",
      "Epoch [17/300], Step [14/225], Training Accuracy: 50.1116%, Training Loss: 1.0534%\n",
      "Epoch [17/300], Step [15/225], Training Accuracy: 49.8958%, Training Loss: 1.0609%\n",
      "Epoch [17/300], Step [16/225], Training Accuracy: 49.8047%, Training Loss: 1.0621%\n",
      "Epoch [17/300], Step [17/225], Training Accuracy: 49.8162%, Training Loss: 1.0562%\n",
      "Epoch [17/300], Step [18/225], Training Accuracy: 49.4792%, Training Loss: 1.0541%\n",
      "Epoch [17/300], Step [19/225], Training Accuracy: 49.0132%, Training Loss: 1.0553%\n",
      "Epoch [17/300], Step [20/225], Training Accuracy: 49.0625%, Training Loss: 1.0519%\n",
      "Epoch [17/300], Step [21/225], Training Accuracy: 49.3304%, Training Loss: 1.0462%\n",
      "Epoch [17/300], Step [22/225], Training Accuracy: 49.0767%, Training Loss: 1.0479%\n",
      "Epoch [17/300], Step [23/225], Training Accuracy: 48.9810%, Training Loss: 1.0446%\n",
      "Epoch [17/300], Step [24/225], Training Accuracy: 48.7630%, Training Loss: 1.0491%\n",
      "Epoch [17/300], Step [25/225], Training Accuracy: 48.9375%, Training Loss: 1.0455%\n",
      "Epoch [17/300], Step [26/225], Training Accuracy: 48.6178%, Training Loss: 1.0478%\n",
      "Epoch [17/300], Step [27/225], Training Accuracy: 48.4954%, Training Loss: 1.0478%\n",
      "Epoch [17/300], Step [28/225], Training Accuracy: 48.7165%, Training Loss: 1.0449%\n",
      "Epoch [17/300], Step [29/225], Training Accuracy: 49.0302%, Training Loss: 1.0408%\n",
      "Epoch [17/300], Step [30/225], Training Accuracy: 48.9062%, Training Loss: 1.0395%\n",
      "Epoch [17/300], Step [31/225], Training Accuracy: 48.9415%, Training Loss: 1.0393%\n",
      "Epoch [17/300], Step [32/225], Training Accuracy: 49.0723%, Training Loss: 1.0376%\n",
      "Epoch [17/300], Step [33/225], Training Accuracy: 49.1951%, Training Loss: 1.0356%\n",
      "Epoch [17/300], Step [34/225], Training Accuracy: 49.2188%, Training Loss: 1.0387%\n",
      "Epoch [17/300], Step [35/225], Training Accuracy: 49.1071%, Training Loss: 1.0378%\n",
      "Epoch [17/300], Step [36/225], Training Accuracy: 49.1319%, Training Loss: 1.0375%\n",
      "Epoch [17/300], Step [37/225], Training Accuracy: 49.2399%, Training Loss: 1.0347%\n",
      "Epoch [17/300], Step [38/225], Training Accuracy: 49.3832%, Training Loss: 1.0329%\n",
      "Epoch [17/300], Step [39/225], Training Accuracy: 49.2388%, Training Loss: 1.0318%\n",
      "Epoch [17/300], Step [40/225], Training Accuracy: 49.2188%, Training Loss: 1.0322%\n",
      "Epoch [17/300], Step [41/225], Training Accuracy: 48.9329%, Training Loss: 1.0335%\n",
      "Epoch [17/300], Step [42/225], Training Accuracy: 49.1071%, Training Loss: 1.0316%\n",
      "Epoch [17/300], Step [43/225], Training Accuracy: 49.1279%, Training Loss: 1.0317%\n",
      "Epoch [17/300], Step [44/225], Training Accuracy: 49.3253%, Training Loss: 1.0297%\n",
      "Epoch [17/300], Step [45/225], Training Accuracy: 49.3403%, Training Loss: 1.0291%\n",
      "Epoch [17/300], Step [46/225], Training Accuracy: 49.6264%, Training Loss: 1.0253%\n",
      "Epoch [17/300], Step [47/225], Training Accuracy: 49.5678%, Training Loss: 1.0262%\n",
      "Epoch [17/300], Step [48/225], Training Accuracy: 49.6745%, Training Loss: 1.0263%\n",
      "Epoch [17/300], Step [49/225], Training Accuracy: 49.4898%, Training Loss: 1.0276%\n",
      "Epoch [17/300], Step [50/225], Training Accuracy: 49.5938%, Training Loss: 1.0261%\n",
      "Epoch [17/300], Step [51/225], Training Accuracy: 49.6017%, Training Loss: 1.0250%\n",
      "Epoch [17/300], Step [52/225], Training Accuracy: 49.6995%, Training Loss: 1.0237%\n",
      "Epoch [17/300], Step [53/225], Training Accuracy: 49.5283%, Training Loss: 1.0238%\n",
      "Epoch [17/300], Step [54/225], Training Accuracy: 49.3345%, Training Loss: 1.0250%\n",
      "Epoch [17/300], Step [55/225], Training Accuracy: 49.2330%, Training Loss: 1.0264%\n",
      "Epoch [17/300], Step [56/225], Training Accuracy: 49.2188%, Training Loss: 1.0274%\n",
      "Epoch [17/300], Step [57/225], Training Accuracy: 49.4518%, Training Loss: 1.0242%\n",
      "Epoch [17/300], Step [58/225], Training Accuracy: 49.3265%, Training Loss: 1.0247%\n",
      "Epoch [17/300], Step [59/225], Training Accuracy: 49.3909%, Training Loss: 1.0245%\n",
      "Epoch [17/300], Step [60/225], Training Accuracy: 49.2969%, Training Loss: 1.0251%\n",
      "Epoch [17/300], Step [61/225], Training Accuracy: 49.3084%, Training Loss: 1.0249%\n",
      "Epoch [17/300], Step [62/225], Training Accuracy: 49.2440%, Training Loss: 1.0248%\n",
      "Epoch [17/300], Step [63/225], Training Accuracy: 49.1567%, Training Loss: 1.0264%\n",
      "Epoch [17/300], Step [64/225], Training Accuracy: 49.1455%, Training Loss: 1.0268%\n",
      "Epoch [17/300], Step [65/225], Training Accuracy: 49.0865%, Training Loss: 1.0270%\n",
      "Epoch [17/300], Step [66/225], Training Accuracy: 49.2424%, Training Loss: 1.0257%\n",
      "Epoch [17/300], Step [67/225], Training Accuracy: 49.1604%, Training Loss: 1.0257%\n",
      "Epoch [17/300], Step [68/225], Training Accuracy: 49.0809%, Training Loss: 1.0257%\n",
      "Epoch [17/300], Step [69/225], Training Accuracy: 48.9357%, Training Loss: 1.0259%\n",
      "Epoch [17/300], Step [70/225], Training Accuracy: 48.7946%, Training Loss: 1.0275%\n",
      "Epoch [17/300], Step [71/225], Training Accuracy: 48.8556%, Training Loss: 1.0262%\n",
      "Epoch [17/300], Step [72/225], Training Accuracy: 48.7413%, Training Loss: 1.0283%\n",
      "Epoch [17/300], Step [73/225], Training Accuracy: 48.6943%, Training Loss: 1.0311%\n",
      "Epoch [17/300], Step [74/225], Training Accuracy: 48.7965%, Training Loss: 1.0291%\n",
      "Epoch [17/300], Step [75/225], Training Accuracy: 48.8125%, Training Loss: 1.0283%\n",
      "Epoch [17/300], Step [76/225], Training Accuracy: 48.8487%, Training Loss: 1.0285%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Step [77/225], Training Accuracy: 48.9651%, Training Loss: 1.0280%\n",
      "Epoch [17/300], Step [78/225], Training Accuracy: 48.9183%, Training Loss: 1.0293%\n",
      "Epoch [17/300], Step [79/225], Training Accuracy: 48.9320%, Training Loss: 1.0303%\n",
      "Epoch [17/300], Step [80/225], Training Accuracy: 48.8672%, Training Loss: 1.0304%\n",
      "Epoch [17/300], Step [81/225], Training Accuracy: 48.8812%, Training Loss: 1.0312%\n",
      "Epoch [17/300], Step [82/225], Training Accuracy: 48.9329%, Training Loss: 1.0303%\n",
      "Epoch [17/300], Step [83/225], Training Accuracy: 48.9081%, Training Loss: 1.0303%\n",
      "Epoch [17/300], Step [84/225], Training Accuracy: 48.9025%, Training Loss: 1.0310%\n",
      "Epoch [17/300], Step [85/225], Training Accuracy: 48.9338%, Training Loss: 1.0304%\n",
      "Epoch [17/300], Step [86/225], Training Accuracy: 48.9826%, Training Loss: 1.0307%\n",
      "Epoch [17/300], Step [87/225], Training Accuracy: 49.0122%, Training Loss: 1.0305%\n",
      "Epoch [17/300], Step [88/225], Training Accuracy: 48.8991%, Training Loss: 1.0307%\n",
      "Epoch [17/300], Step [89/225], Training Accuracy: 48.9115%, Training Loss: 1.0320%\n",
      "Epoch [17/300], Step [90/225], Training Accuracy: 48.8021%, Training Loss: 1.0324%\n",
      "Epoch [17/300], Step [91/225], Training Accuracy: 48.9011%, Training Loss: 1.0307%\n",
      "Epoch [17/300], Step [92/225], Training Accuracy: 48.8791%, Training Loss: 1.0306%\n",
      "Epoch [17/300], Step [93/225], Training Accuracy: 48.8743%, Training Loss: 1.0308%\n",
      "Epoch [17/300], Step [94/225], Training Accuracy: 48.9694%, Training Loss: 1.0294%\n",
      "Epoch [17/300], Step [95/225], Training Accuracy: 48.8651%, Training Loss: 1.0304%\n",
      "Epoch [17/300], Step [96/225], Training Accuracy: 48.9095%, Training Loss: 1.0299%\n",
      "Epoch [17/300], Step [97/225], Training Accuracy: 48.9046%, Training Loss: 1.0292%\n",
      "Epoch [17/300], Step [98/225], Training Accuracy: 48.9318%, Training Loss: 1.0285%\n",
      "Epoch [17/300], Step [99/225], Training Accuracy: 48.9583%, Training Loss: 1.0287%\n",
      "Epoch [17/300], Step [100/225], Training Accuracy: 48.8438%, Training Loss: 1.0289%\n",
      "Epoch [17/300], Step [101/225], Training Accuracy: 48.7778%, Training Loss: 1.0294%\n",
      "Epoch [17/300], Step [102/225], Training Accuracy: 48.7439%, Training Loss: 1.0302%\n",
      "Epoch [17/300], Step [103/225], Training Accuracy: 48.6195%, Training Loss: 1.0313%\n",
      "Epoch [17/300], Step [104/225], Training Accuracy: 48.6328%, Training Loss: 1.0308%\n",
      "Epoch [17/300], Step [105/225], Training Accuracy: 48.6161%, Training Loss: 1.0300%\n",
      "Epoch [17/300], Step [106/225], Training Accuracy: 48.4965%, Training Loss: 1.0308%\n",
      "Epoch [17/300], Step [107/225], Training Accuracy: 48.5105%, Training Loss: 1.0305%\n",
      "Epoch [17/300], Step [108/225], Training Accuracy: 48.5532%, Training Loss: 1.0312%\n",
      "Epoch [17/300], Step [109/225], Training Accuracy: 48.5235%, Training Loss: 1.0311%\n",
      "Epoch [17/300], Step [110/225], Training Accuracy: 48.5653%, Training Loss: 1.0309%\n",
      "Epoch [17/300], Step [111/225], Training Accuracy: 48.6205%, Training Loss: 1.0312%\n",
      "Epoch [17/300], Step [112/225], Training Accuracy: 48.6189%, Training Loss: 1.0309%\n",
      "Epoch [17/300], Step [113/225], Training Accuracy: 48.5343%, Training Loss: 1.0323%\n",
      "Epoch [17/300], Step [114/225], Training Accuracy: 48.5060%, Training Loss: 1.0322%\n",
      "Epoch [17/300], Step [115/225], Training Accuracy: 48.5190%, Training Loss: 1.0314%\n",
      "Epoch [17/300], Step [116/225], Training Accuracy: 48.5722%, Training Loss: 1.0311%\n",
      "Epoch [17/300], Step [117/225], Training Accuracy: 48.4642%, Training Loss: 1.0325%\n",
      "Epoch [17/300], Step [118/225], Training Accuracy: 48.4772%, Training Loss: 1.0324%\n",
      "Epoch [17/300], Step [119/225], Training Accuracy: 48.4375%, Training Loss: 1.0326%\n",
      "Epoch [17/300], Step [120/225], Training Accuracy: 48.5417%, Training Loss: 1.0317%\n",
      "Epoch [17/300], Step [121/225], Training Accuracy: 48.4892%, Training Loss: 1.0318%\n",
      "Epoch [17/300], Step [122/225], Training Accuracy: 48.4631%, Training Loss: 1.0317%\n",
      "Epoch [17/300], Step [123/225], Training Accuracy: 48.3613%, Training Loss: 1.0318%\n",
      "Epoch [17/300], Step [124/225], Training Accuracy: 48.3997%, Training Loss: 1.0314%\n",
      "Epoch [17/300], Step [125/225], Training Accuracy: 48.3500%, Training Loss: 1.0324%\n",
      "Epoch [17/300], Step [126/225], Training Accuracy: 48.3135%, Training Loss: 1.0329%\n",
      "Epoch [17/300], Step [127/225], Training Accuracy: 48.3268%, Training Loss: 1.0326%\n",
      "Epoch [17/300], Step [128/225], Training Accuracy: 48.2666%, Training Loss: 1.0330%\n",
      "Epoch [17/300], Step [129/225], Training Accuracy: 48.2558%, Training Loss: 1.0332%\n",
      "Epoch [17/300], Step [130/225], Training Accuracy: 48.2452%, Training Loss: 1.0338%\n",
      "Epoch [17/300], Step [131/225], Training Accuracy: 48.2586%, Training Loss: 1.0335%\n",
      "Epoch [17/300], Step [132/225], Training Accuracy: 48.2481%, Training Loss: 1.0340%\n",
      "Epoch [17/300], Step [133/225], Training Accuracy: 48.2730%, Training Loss: 1.0333%\n",
      "Epoch [17/300], Step [134/225], Training Accuracy: 48.2976%, Training Loss: 1.0326%\n",
      "Epoch [17/300], Step [135/225], Training Accuracy: 48.2755%, Training Loss: 1.0325%\n",
      "Epoch [17/300], Step [136/225], Training Accuracy: 48.2422%, Training Loss: 1.0318%\n",
      "Epoch [17/300], Step [137/225], Training Accuracy: 48.2322%, Training Loss: 1.0317%\n",
      "Epoch [17/300], Step [138/225], Training Accuracy: 48.2563%, Training Loss: 1.0311%\n",
      "Epoch [17/300], Step [139/225], Training Accuracy: 48.2689%, Training Loss: 1.0314%\n",
      "Epoch [17/300], Step [140/225], Training Accuracy: 48.3371%, Training Loss: 1.0312%\n",
      "Epoch [17/300], Step [141/225], Training Accuracy: 48.3932%, Training Loss: 1.0312%\n",
      "Epoch [17/300], Step [142/225], Training Accuracy: 48.4045%, Training Loss: 1.0313%\n",
      "Epoch [17/300], Step [143/225], Training Accuracy: 48.4594%, Training Loss: 1.0308%\n",
      "Epoch [17/300], Step [144/225], Training Accuracy: 48.4158%, Training Loss: 1.0306%\n",
      "Epoch [17/300], Step [145/225], Training Accuracy: 48.5345%, Training Loss: 1.0297%\n",
      "Epoch [17/300], Step [146/225], Training Accuracy: 48.5124%, Training Loss: 1.0298%\n",
      "Epoch [17/300], Step [147/225], Training Accuracy: 48.4694%, Training Loss: 1.0307%\n",
      "Epoch [17/300], Step [148/225], Training Accuracy: 48.5431%, Training Loss: 1.0305%\n",
      "Epoch [17/300], Step [149/225], Training Accuracy: 48.5004%, Training Loss: 1.0308%\n",
      "Epoch [17/300], Step [150/225], Training Accuracy: 48.4896%, Training Loss: 1.0314%\n",
      "Epoch [17/300], Step [151/225], Training Accuracy: 48.4892%, Training Loss: 1.0306%\n",
      "Epoch [17/300], Step [152/225], Training Accuracy: 48.4889%, Training Loss: 1.0307%\n",
      "Epoch [17/300], Step [153/225], Training Accuracy: 48.5192%, Training Loss: 1.0300%\n",
      "Epoch [17/300], Step [154/225], Training Accuracy: 48.4476%, Training Loss: 1.0299%\n",
      "Epoch [17/300], Step [155/225], Training Accuracy: 48.4274%, Training Loss: 1.0305%\n",
      "Epoch [17/300], Step [156/225], Training Accuracy: 48.4175%, Training Loss: 1.0304%\n",
      "Epoch [17/300], Step [157/225], Training Accuracy: 48.4076%, Training Loss: 1.0310%\n",
      "Epoch [17/300], Step [158/225], Training Accuracy: 48.4573%, Training Loss: 1.0310%\n",
      "Epoch [17/300], Step [159/225], Training Accuracy: 48.4768%, Training Loss: 1.0307%\n",
      "Epoch [17/300], Step [160/225], Training Accuracy: 48.4375%, Training Loss: 1.0309%\n",
      "Epoch [17/300], Step [161/225], Training Accuracy: 48.5443%, Training Loss: 1.0299%\n",
      "Epoch [17/300], Step [162/225], Training Accuracy: 48.5532%, Training Loss: 1.0300%\n",
      "Epoch [17/300], Step [163/225], Training Accuracy: 48.5046%, Training Loss: 1.0296%\n",
      "Epoch [17/300], Step [164/225], Training Accuracy: 48.5709%, Training Loss: 1.0289%\n",
      "Epoch [17/300], Step [165/225], Training Accuracy: 48.6080%, Training Loss: 1.0287%\n",
      "Epoch [17/300], Step [166/225], Training Accuracy: 48.6352%, Training Loss: 1.0290%\n",
      "Epoch [17/300], Step [167/225], Training Accuracy: 48.6621%, Training Loss: 1.0284%\n",
      "Epoch [17/300], Step [168/225], Training Accuracy: 48.6514%, Training Loss: 1.0286%\n",
      "Epoch [17/300], Step [169/225], Training Accuracy: 48.6594%, Training Loss: 1.0286%\n",
      "Epoch [17/300], Step [170/225], Training Accuracy: 48.6213%, Training Loss: 1.0292%\n",
      "Epoch [17/300], Step [171/225], Training Accuracy: 48.6111%, Training Loss: 1.0290%\n",
      "Epoch [17/300], Step [172/225], Training Accuracy: 48.5556%, Training Loss: 1.0296%\n",
      "Epoch [17/300], Step [173/225], Training Accuracy: 48.5639%, Training Loss: 1.0293%\n",
      "Epoch [17/300], Step [174/225], Training Accuracy: 48.5812%, Training Loss: 1.0292%\n",
      "Epoch [17/300], Step [175/225], Training Accuracy: 48.5804%, Training Loss: 1.0292%\n",
      "Epoch [17/300], Step [176/225], Training Accuracy: 48.5707%, Training Loss: 1.0292%\n",
      "Epoch [17/300], Step [177/225], Training Accuracy: 48.5699%, Training Loss: 1.0289%\n",
      "Epoch [17/300], Step [178/225], Training Accuracy: 48.5341%, Training Loss: 1.0288%\n",
      "Epoch [17/300], Step [179/225], Training Accuracy: 48.5510%, Training Loss: 1.0290%\n",
      "Epoch [17/300], Step [180/225], Training Accuracy: 48.6632%, Training Loss: 1.0277%\n",
      "Epoch [17/300], Step [181/225], Training Accuracy: 48.6274%, Training Loss: 1.0284%\n",
      "Epoch [17/300], Step [182/225], Training Accuracy: 48.5749%, Training Loss: 1.0288%\n",
      "Epoch [17/300], Step [183/225], Training Accuracy: 48.5912%, Training Loss: 1.0284%\n",
      "Epoch [17/300], Step [184/225], Training Accuracy: 48.5904%, Training Loss: 1.0284%\n",
      "Epoch [17/300], Step [185/225], Training Accuracy: 48.5980%, Training Loss: 1.0283%\n",
      "Epoch [17/300], Step [186/225], Training Accuracy: 48.6307%, Training Loss: 1.0281%\n",
      "Epoch [17/300], Step [187/225], Training Accuracy: 48.6631%, Training Loss: 1.0275%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Step [188/225], Training Accuracy: 48.7284%, Training Loss: 1.0270%\n",
      "Epoch [17/300], Step [189/225], Training Accuracy: 48.7930%, Training Loss: 1.0265%\n",
      "Epoch [17/300], Step [190/225], Training Accuracy: 48.7993%, Training Loss: 1.0269%\n",
      "Epoch [17/300], Step [191/225], Training Accuracy: 48.7974%, Training Loss: 1.0270%\n",
      "Epoch [17/300], Step [192/225], Training Accuracy: 48.8363%, Training Loss: 1.0267%\n",
      "Epoch [17/300], Step [193/225], Training Accuracy: 48.8099%, Training Loss: 1.0267%\n",
      "Epoch [17/300], Step [194/225], Training Accuracy: 48.8563%, Training Loss: 1.0268%\n",
      "Epoch [17/300], Step [195/225], Training Accuracy: 48.8622%, Training Loss: 1.0263%\n",
      "Epoch [17/300], Step [196/225], Training Accuracy: 48.9078%, Training Loss: 1.0264%\n",
      "Epoch [17/300], Step [197/225], Training Accuracy: 48.9134%, Training Loss: 1.0259%\n",
      "Epoch [17/300], Step [198/225], Training Accuracy: 48.9189%, Training Loss: 1.0253%\n",
      "Epoch [17/300], Step [199/225], Training Accuracy: 48.9165%, Training Loss: 1.0250%\n",
      "Epoch [17/300], Step [200/225], Training Accuracy: 48.9531%, Training Loss: 1.0252%\n",
      "Epoch [17/300], Step [201/225], Training Accuracy: 48.9661%, Training Loss: 1.0255%\n",
      "Epoch [17/300], Step [202/225], Training Accuracy: 48.9867%, Training Loss: 1.0252%\n",
      "Epoch [17/300], Step [203/225], Training Accuracy: 48.9532%, Training Loss: 1.0255%\n",
      "Epoch [17/300], Step [204/225], Training Accuracy: 48.9737%, Training Loss: 1.0254%\n",
      "Epoch [17/300], Step [205/225], Training Accuracy: 48.9634%, Training Loss: 1.0254%\n",
      "Epoch [17/300], Step [206/225], Training Accuracy: 49.0140%, Training Loss: 1.0259%\n",
      "Epoch [17/300], Step [207/225], Training Accuracy: 49.0036%, Training Loss: 1.0263%\n",
      "Epoch [17/300], Step [208/225], Training Accuracy: 49.0760%, Training Loss: 1.0258%\n",
      "Epoch [17/300], Step [209/225], Training Accuracy: 49.0580%, Training Loss: 1.0259%\n",
      "Epoch [17/300], Step [210/225], Training Accuracy: 49.0476%, Training Loss: 1.0257%\n",
      "Epoch [17/300], Step [211/225], Training Accuracy: 49.0521%, Training Loss: 1.0255%\n",
      "Epoch [17/300], Step [212/225], Training Accuracy: 49.0198%, Training Loss: 1.0260%\n",
      "Epoch [17/300], Step [213/225], Training Accuracy: 48.9730%, Training Loss: 1.0266%\n",
      "Epoch [17/300], Step [214/225], Training Accuracy: 48.9778%, Training Loss: 1.0265%\n",
      "Epoch [17/300], Step [215/225], Training Accuracy: 48.9971%, Training Loss: 1.0264%\n",
      "Epoch [17/300], Step [216/225], Training Accuracy: 49.0017%, Training Loss: 1.0265%\n",
      "Epoch [17/300], Step [217/225], Training Accuracy: 48.9775%, Training Loss: 1.0266%\n",
      "Epoch [17/300], Step [218/225], Training Accuracy: 48.9249%, Training Loss: 1.0274%\n",
      "Epoch [17/300], Step [219/225], Training Accuracy: 48.8584%, Training Loss: 1.0276%\n",
      "Epoch [17/300], Step [220/225], Training Accuracy: 48.9205%, Training Loss: 1.0273%\n",
      "Epoch [17/300], Step [221/225], Training Accuracy: 48.9183%, Training Loss: 1.0274%\n",
      "Epoch [17/300], Step [222/225], Training Accuracy: 48.9231%, Training Loss: 1.0274%\n",
      "Epoch [17/300], Step [223/225], Training Accuracy: 48.8509%, Training Loss: 1.0281%\n",
      "Epoch [17/300], Step [224/225], Training Accuracy: 48.8281%, Training Loss: 1.0281%\n",
      "Epoch [17/300], Step [225/225], Training Accuracy: 48.8257%, Training Loss: 1.0283%\n",
      "Epoch [18/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8811%\n",
      "Epoch [18/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 1.0122%\n",
      "Epoch [18/300], Step [3/225], Training Accuracy: 49.4792%, Training Loss: 1.0655%\n",
      "Epoch [18/300], Step [4/225], Training Accuracy: 48.4375%, Training Loss: 1.0546%\n",
      "Epoch [18/300], Step [5/225], Training Accuracy: 49.3750%, Training Loss: 1.0259%\n",
      "Epoch [18/300], Step [6/225], Training Accuracy: 48.9583%, Training Loss: 1.0477%\n",
      "Epoch [18/300], Step [7/225], Training Accuracy: 49.1071%, Training Loss: 1.0420%\n",
      "Epoch [18/300], Step [8/225], Training Accuracy: 49.8047%, Training Loss: 1.0456%\n",
      "Epoch [18/300], Step [9/225], Training Accuracy: 49.4792%, Training Loss: 1.0411%\n",
      "Epoch [18/300], Step [10/225], Training Accuracy: 49.6875%, Training Loss: 1.0342%\n",
      "Epoch [18/300], Step [11/225], Training Accuracy: 50.5682%, Training Loss: 1.0284%\n",
      "Epoch [18/300], Step [12/225], Training Accuracy: 50.5208%, Training Loss: 1.0340%\n",
      "Epoch [18/300], Step [13/225], Training Accuracy: 50.7212%, Training Loss: 1.0345%\n",
      "Epoch [18/300], Step [14/225], Training Accuracy: 50.1116%, Training Loss: 1.0420%\n",
      "Epoch [18/300], Step [15/225], Training Accuracy: 49.6875%, Training Loss: 1.0500%\n",
      "Epoch [18/300], Step [16/225], Training Accuracy: 49.7070%, Training Loss: 1.0514%\n",
      "Epoch [18/300], Step [17/225], Training Accuracy: 49.8162%, Training Loss: 1.0457%\n",
      "Epoch [18/300], Step [18/225], Training Accuracy: 49.6528%, Training Loss: 1.0439%\n",
      "Epoch [18/300], Step [19/225], Training Accuracy: 49.0132%, Training Loss: 1.0452%\n",
      "Epoch [18/300], Step [20/225], Training Accuracy: 49.2188%, Training Loss: 1.0420%\n",
      "Epoch [18/300], Step [21/225], Training Accuracy: 49.4048%, Training Loss: 1.0366%\n",
      "Epoch [18/300], Step [22/225], Training Accuracy: 49.2188%, Training Loss: 1.0380%\n",
      "Epoch [18/300], Step [23/225], Training Accuracy: 49.0489%, Training Loss: 1.0347%\n",
      "Epoch [18/300], Step [24/225], Training Accuracy: 48.8281%, Training Loss: 1.0391%\n",
      "Epoch [18/300], Step [25/225], Training Accuracy: 48.9375%, Training Loss: 1.0355%\n",
      "Epoch [18/300], Step [26/225], Training Accuracy: 48.5577%, Training Loss: 1.0378%\n",
      "Epoch [18/300], Step [27/225], Training Accuracy: 48.4954%, Training Loss: 1.0380%\n",
      "Epoch [18/300], Step [28/225], Training Accuracy: 48.7165%, Training Loss: 1.0351%\n",
      "Epoch [18/300], Step [29/225], Training Accuracy: 49.0841%, Training Loss: 1.0310%\n",
      "Epoch [18/300], Step [30/225], Training Accuracy: 49.0625%, Training Loss: 1.0295%\n",
      "Epoch [18/300], Step [31/225], Training Accuracy: 49.1431%, Training Loss: 1.0293%\n",
      "Epoch [18/300], Step [32/225], Training Accuracy: 49.3164%, Training Loss: 1.0274%\n",
      "Epoch [18/300], Step [33/225], Training Accuracy: 49.4318%, Training Loss: 1.0254%\n",
      "Epoch [18/300], Step [34/225], Training Accuracy: 49.4026%, Training Loss: 1.0285%\n",
      "Epoch [18/300], Step [35/225], Training Accuracy: 49.2857%, Training Loss: 1.0278%\n",
      "Epoch [18/300], Step [36/225], Training Accuracy: 49.3056%, Training Loss: 1.0276%\n",
      "Epoch [18/300], Step [37/225], Training Accuracy: 49.4510%, Training Loss: 1.0248%\n",
      "Epoch [18/300], Step [38/225], Training Accuracy: 49.5888%, Training Loss: 1.0229%\n",
      "Epoch [18/300], Step [39/225], Training Accuracy: 49.4391%, Training Loss: 1.0219%\n",
      "Epoch [18/300], Step [40/225], Training Accuracy: 49.4141%, Training Loss: 1.0222%\n",
      "Epoch [18/300], Step [41/225], Training Accuracy: 49.1616%, Training Loss: 1.0235%\n",
      "Epoch [18/300], Step [42/225], Training Accuracy: 49.2932%, Training Loss: 1.0217%\n",
      "Epoch [18/300], Step [43/225], Training Accuracy: 49.3096%, Training Loss: 1.0217%\n",
      "Epoch [18/300], Step [44/225], Training Accuracy: 49.4673%, Training Loss: 1.0198%\n",
      "Epoch [18/300], Step [45/225], Training Accuracy: 49.4792%, Training Loss: 1.0193%\n",
      "Epoch [18/300], Step [46/225], Training Accuracy: 49.7283%, Training Loss: 1.0156%\n",
      "Epoch [18/300], Step [47/225], Training Accuracy: 49.6676%, Training Loss: 1.0165%\n",
      "Epoch [18/300], Step [48/225], Training Accuracy: 49.8047%, Training Loss: 1.0167%\n",
      "Epoch [18/300], Step [49/225], Training Accuracy: 49.6173%, Training Loss: 1.0181%\n",
      "Epoch [18/300], Step [50/225], Training Accuracy: 49.7500%, Training Loss: 1.0165%\n",
      "Epoch [18/300], Step [51/225], Training Accuracy: 49.7549%, Training Loss: 1.0152%\n",
      "Epoch [18/300], Step [52/225], Training Accuracy: 49.9099%, Training Loss: 1.0137%\n",
      "Epoch [18/300], Step [53/225], Training Accuracy: 49.7347%, Training Loss: 1.0138%\n",
      "Epoch [18/300], Step [54/225], Training Accuracy: 49.5370%, Training Loss: 1.0151%\n",
      "Epoch [18/300], Step [55/225], Training Accuracy: 49.4318%, Training Loss: 1.0166%\n",
      "Epoch [18/300], Step [56/225], Training Accuracy: 49.3862%, Training Loss: 1.0176%\n",
      "Epoch [18/300], Step [57/225], Training Accuracy: 49.6436%, Training Loss: 1.0145%\n",
      "Epoch [18/300], Step [58/225], Training Accuracy: 49.5959%, Training Loss: 1.0150%\n",
      "Epoch [18/300], Step [59/225], Training Accuracy: 49.6557%, Training Loss: 1.0148%\n",
      "Epoch [18/300], Step [60/225], Training Accuracy: 49.5573%, Training Loss: 1.0154%\n",
      "Epoch [18/300], Step [61/225], Training Accuracy: 49.5645%, Training Loss: 1.0151%\n",
      "Epoch [18/300], Step [62/225], Training Accuracy: 49.5212%, Training Loss: 1.0152%\n",
      "Epoch [18/300], Step [63/225], Training Accuracy: 49.4296%, Training Loss: 1.0167%\n",
      "Epoch [18/300], Step [64/225], Training Accuracy: 49.3896%, Training Loss: 1.0169%\n",
      "Epoch [18/300], Step [65/225], Training Accuracy: 49.3510%, Training Loss: 1.0172%\n",
      "Epoch [18/300], Step [66/225], Training Accuracy: 49.5028%, Training Loss: 1.0157%\n",
      "Epoch [18/300], Step [67/225], Training Accuracy: 49.3937%, Training Loss: 1.0158%\n",
      "Epoch [18/300], Step [68/225], Training Accuracy: 49.3107%, Training Loss: 1.0157%\n",
      "Epoch [18/300], Step [69/225], Training Accuracy: 49.2074%, Training Loss: 1.0159%\n",
      "Epoch [18/300], Step [70/225], Training Accuracy: 49.0848%, Training Loss: 1.0174%\n",
      "Epoch [18/300], Step [71/225], Training Accuracy: 49.1857%, Training Loss: 1.0162%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Step [72/225], Training Accuracy: 49.0885%, Training Loss: 1.0182%\n",
      "Epoch [18/300], Step [73/225], Training Accuracy: 49.0368%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [74/225], Training Accuracy: 49.1343%, Training Loss: 1.0191%\n",
      "Epoch [18/300], Step [75/225], Training Accuracy: 49.1458%, Training Loss: 1.0184%\n",
      "Epoch [18/300], Step [76/225], Training Accuracy: 49.2188%, Training Loss: 1.0185%\n",
      "Epoch [18/300], Step [77/225], Training Accuracy: 49.3304%, Training Loss: 1.0180%\n",
      "Epoch [18/300], Step [78/225], Training Accuracy: 49.3189%, Training Loss: 1.0193%\n",
      "Epoch [18/300], Step [79/225], Training Accuracy: 49.3473%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [80/225], Training Accuracy: 49.2773%, Training Loss: 1.0206%\n",
      "Epoch [18/300], Step [81/225], Training Accuracy: 49.2670%, Training Loss: 1.0213%\n",
      "Epoch [18/300], Step [82/225], Training Accuracy: 49.3521%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [83/225], Training Accuracy: 49.3411%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [84/225], Training Accuracy: 49.3676%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [85/225], Training Accuracy: 49.4301%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [86/225], Training Accuracy: 49.4731%, Training Loss: 1.0206%\n",
      "Epoch [18/300], Step [87/225], Training Accuracy: 49.4971%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [88/225], Training Accuracy: 49.3963%, Training Loss: 1.0207%\n",
      "Epoch [18/300], Step [89/225], Training Accuracy: 49.4206%, Training Loss: 1.0221%\n",
      "Epoch [18/300], Step [90/225], Training Accuracy: 49.2882%, Training Loss: 1.0226%\n",
      "Epoch [18/300], Step [91/225], Training Accuracy: 49.3647%, Training Loss: 1.0208%\n",
      "Epoch [18/300], Step [92/225], Training Accuracy: 49.3376%, Training Loss: 1.0207%\n",
      "Epoch [18/300], Step [93/225], Training Accuracy: 49.3280%, Training Loss: 1.0209%\n",
      "Epoch [18/300], Step [94/225], Training Accuracy: 49.4348%, Training Loss: 1.0195%\n",
      "Epoch [18/300], Step [95/225], Training Accuracy: 49.3421%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [96/225], Training Accuracy: 49.3815%, Training Loss: 1.0199%\n",
      "Epoch [18/300], Step [97/225], Training Accuracy: 49.3879%, Training Loss: 1.0192%\n",
      "Epoch [18/300], Step [98/225], Training Accuracy: 49.3941%, Training Loss: 1.0185%\n",
      "Epoch [18/300], Step [99/225], Training Accuracy: 49.4318%, Training Loss: 1.0186%\n",
      "Epoch [18/300], Step [100/225], Training Accuracy: 49.3281%, Training Loss: 1.0189%\n",
      "Epoch [18/300], Step [101/225], Training Accuracy: 49.2574%, Training Loss: 1.0194%\n",
      "Epoch [18/300], Step [102/225], Training Accuracy: 49.2034%, Training Loss: 1.0203%\n",
      "Epoch [18/300], Step [103/225], Training Accuracy: 49.0595%, Training Loss: 1.0214%\n",
      "Epoch [18/300], Step [104/225], Training Accuracy: 49.0685%, Training Loss: 1.0210%\n",
      "Epoch [18/300], Step [105/225], Training Accuracy: 49.0327%, Training Loss: 1.0202%\n",
      "Epoch [18/300], Step [106/225], Training Accuracy: 48.9387%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [107/225], Training Accuracy: 48.9632%, Training Loss: 1.0208%\n",
      "Epoch [18/300], Step [108/225], Training Accuracy: 48.9728%, Training Loss: 1.0215%\n",
      "Epoch [18/300], Step [109/225], Training Accuracy: 48.9679%, Training Loss: 1.0214%\n",
      "Epoch [18/300], Step [110/225], Training Accuracy: 49.0057%, Training Loss: 1.0212%\n",
      "Epoch [18/300], Step [111/225], Training Accuracy: 49.0991%, Training Loss: 1.0216%\n",
      "Epoch [18/300], Step [112/225], Training Accuracy: 49.1071%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [113/225], Training Accuracy: 49.0321%, Training Loss: 1.0225%\n",
      "Epoch [18/300], Step [114/225], Training Accuracy: 49.0132%, Training Loss: 1.0224%\n",
      "Epoch [18/300], Step [115/225], Training Accuracy: 49.0217%, Training Loss: 1.0216%\n",
      "Epoch [18/300], Step [116/225], Training Accuracy: 49.0841%, Training Loss: 1.0213%\n",
      "Epoch [18/300], Step [117/225], Training Accuracy: 48.9984%, Training Loss: 1.0227%\n",
      "Epoch [18/300], Step [118/225], Training Accuracy: 49.0201%, Training Loss: 1.0227%\n",
      "Epoch [18/300], Step [119/225], Training Accuracy: 48.9627%, Training Loss: 1.0229%\n",
      "Epoch [18/300], Step [120/225], Training Accuracy: 49.0625%, Training Loss: 1.0220%\n",
      "Epoch [18/300], Step [121/225], Training Accuracy: 49.0186%, Training Loss: 1.0221%\n",
      "Epoch [18/300], Step [122/225], Training Accuracy: 49.0010%, Training Loss: 1.0220%\n",
      "Epoch [18/300], Step [123/225], Training Accuracy: 48.9202%, Training Loss: 1.0221%\n",
      "Epoch [18/300], Step [124/225], Training Accuracy: 48.9415%, Training Loss: 1.0217%\n",
      "Epoch [18/300], Step [125/225], Training Accuracy: 48.9125%, Training Loss: 1.0227%\n",
      "Epoch [18/300], Step [126/225], Training Accuracy: 48.8591%, Training Loss: 1.0233%\n",
      "Epoch [18/300], Step [127/225], Training Accuracy: 48.8804%, Training Loss: 1.0231%\n",
      "Epoch [18/300], Step [128/225], Training Accuracy: 48.8281%, Training Loss: 1.0234%\n",
      "Epoch [18/300], Step [129/225], Training Accuracy: 48.8130%, Training Loss: 1.0236%\n",
      "Epoch [18/300], Step [130/225], Training Accuracy: 48.7981%, Training Loss: 1.0242%\n",
      "Epoch [18/300], Step [131/225], Training Accuracy: 48.8073%, Training Loss: 1.0239%\n",
      "Epoch [18/300], Step [132/225], Training Accuracy: 48.8045%, Training Loss: 1.0244%\n",
      "Epoch [18/300], Step [133/225], Training Accuracy: 48.8487%, Training Loss: 1.0237%\n",
      "Epoch [18/300], Step [134/225], Training Accuracy: 48.8923%, Training Loss: 1.0231%\n",
      "Epoch [18/300], Step [135/225], Training Accuracy: 48.8773%, Training Loss: 1.0229%\n",
      "Epoch [18/300], Step [136/225], Training Accuracy: 48.8511%, Training Loss: 1.0222%\n",
      "Epoch [18/300], Step [137/225], Training Accuracy: 48.8481%, Training Loss: 1.0222%\n",
      "Epoch [18/300], Step [138/225], Training Accuracy: 48.8678%, Training Loss: 1.0214%\n",
      "Epoch [18/300], Step [139/225], Training Accuracy: 48.8759%, Training Loss: 1.0217%\n",
      "Epoch [18/300], Step [140/225], Training Accuracy: 48.9397%, Training Loss: 1.0215%\n",
      "Epoch [18/300], Step [141/225], Training Accuracy: 48.9916%, Training Loss: 1.0216%\n",
      "Epoch [18/300], Step [142/225], Training Accuracy: 48.9987%, Training Loss: 1.0216%\n",
      "Epoch [18/300], Step [143/225], Training Accuracy: 49.0494%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [144/225], Training Accuracy: 49.0017%, Training Loss: 1.0210%\n",
      "Epoch [18/300], Step [145/225], Training Accuracy: 49.1056%, Training Loss: 1.0201%\n",
      "Epoch [18/300], Step [146/225], Training Accuracy: 49.1117%, Training Loss: 1.0202%\n",
      "Epoch [18/300], Step [147/225], Training Accuracy: 49.0540%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [148/225], Training Accuracy: 49.1343%, Training Loss: 1.0209%\n",
      "Epoch [18/300], Step [149/225], Training Accuracy: 49.1086%, Training Loss: 1.0212%\n",
      "Epoch [18/300], Step [150/225], Training Accuracy: 49.0833%, Training Loss: 1.0217%\n",
      "Epoch [18/300], Step [151/225], Training Accuracy: 49.0998%, Training Loss: 1.0210%\n",
      "Epoch [18/300], Step [152/225], Training Accuracy: 49.0954%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [153/225], Training Accuracy: 49.1115%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [154/225], Training Accuracy: 49.0463%, Training Loss: 1.0202%\n",
      "Epoch [18/300], Step [155/225], Training Accuracy: 49.0020%, Training Loss: 1.0209%\n",
      "Epoch [18/300], Step [156/225], Training Accuracy: 48.9784%, Training Loss: 1.0208%\n",
      "Epoch [18/300], Step [157/225], Training Accuracy: 48.9650%, Training Loss: 1.0213%\n",
      "Epoch [18/300], Step [158/225], Training Accuracy: 49.0012%, Training Loss: 1.0214%\n",
      "Epoch [18/300], Step [159/225], Training Accuracy: 48.9976%, Training Loss: 1.0211%\n",
      "Epoch [18/300], Step [160/225], Training Accuracy: 48.9551%, Training Loss: 1.0213%\n",
      "Epoch [18/300], Step [161/225], Training Accuracy: 49.0586%, Training Loss: 1.0202%\n",
      "Epoch [18/300], Step [162/225], Training Accuracy: 49.0644%, Training Loss: 1.0204%\n",
      "Epoch [18/300], Step [163/225], Training Accuracy: 49.0127%, Training Loss: 1.0200%\n",
      "Epoch [18/300], Step [164/225], Training Accuracy: 49.0854%, Training Loss: 1.0192%\n",
      "Epoch [18/300], Step [165/225], Training Accuracy: 49.1098%, Training Loss: 1.0191%\n",
      "Epoch [18/300], Step [166/225], Training Accuracy: 49.1340%, Training Loss: 1.0193%\n",
      "Epoch [18/300], Step [167/225], Training Accuracy: 49.1673%, Training Loss: 1.0187%\n",
      "Epoch [18/300], Step [168/225], Training Accuracy: 49.1629%, Training Loss: 1.0189%\n",
      "Epoch [18/300], Step [169/225], Training Accuracy: 49.1771%, Training Loss: 1.0189%\n",
      "Epoch [18/300], Step [170/225], Training Accuracy: 49.1544%, Training Loss: 1.0195%\n",
      "Epoch [18/300], Step [171/225], Training Accuracy: 49.1411%, Training Loss: 1.0194%\n",
      "Epoch [18/300], Step [172/225], Training Accuracy: 49.0825%, Training Loss: 1.0200%\n",
      "Epoch [18/300], Step [173/225], Training Accuracy: 49.0878%, Training Loss: 1.0196%\n",
      "Epoch [18/300], Step [174/225], Training Accuracy: 49.1110%, Training Loss: 1.0195%\n",
      "Epoch [18/300], Step [175/225], Training Accuracy: 49.0893%, Training Loss: 1.0196%\n",
      "Epoch [18/300], Step [176/225], Training Accuracy: 49.1033%, Training Loss: 1.0195%\n",
      "Epoch [18/300], Step [177/225], Training Accuracy: 49.0907%, Training Loss: 1.0192%\n",
      "Epoch [18/300], Step [178/225], Training Accuracy: 49.0520%, Training Loss: 1.0191%\n",
      "Epoch [18/300], Step [179/225], Training Accuracy: 49.0747%, Training Loss: 1.0192%\n",
      "Epoch [18/300], Step [180/225], Training Accuracy: 49.1840%, Training Loss: 1.0180%\n",
      "Epoch [18/300], Step [181/225], Training Accuracy: 49.1454%, Training Loss: 1.0188%\n",
      "Epoch [18/300], Step [182/225], Training Accuracy: 49.1243%, Training Loss: 1.0191%\n",
      "Epoch [18/300], Step [183/225], Training Accuracy: 49.1376%, Training Loss: 1.0187%\n",
      "Epoch [18/300], Step [184/225], Training Accuracy: 49.1508%, Training Loss: 1.0187%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Step [185/225], Training Accuracy: 49.1470%, Training Loss: 1.0187%\n",
      "Epoch [18/300], Step [186/225], Training Accuracy: 49.1851%, Training Loss: 1.0184%\n",
      "Epoch [18/300], Step [187/225], Training Accuracy: 49.2229%, Training Loss: 1.0178%\n",
      "Epoch [18/300], Step [188/225], Training Accuracy: 49.2852%, Training Loss: 1.0173%\n",
      "Epoch [18/300], Step [189/225], Training Accuracy: 49.3386%, Training Loss: 1.0168%\n",
      "Epoch [18/300], Step [190/225], Training Accuracy: 49.3339%, Training Loss: 1.0172%\n",
      "Epoch [18/300], Step [191/225], Training Accuracy: 49.3210%, Training Loss: 1.0172%\n",
      "Epoch [18/300], Step [192/225], Training Accuracy: 49.3571%, Training Loss: 1.0170%\n",
      "Epoch [18/300], Step [193/225], Training Accuracy: 49.3280%, Training Loss: 1.0169%\n",
      "Epoch [18/300], Step [194/225], Training Accuracy: 49.3718%, Training Loss: 1.0171%\n",
      "Epoch [18/300], Step [195/225], Training Accuracy: 49.3910%, Training Loss: 1.0166%\n",
      "Epoch [18/300], Step [196/225], Training Accuracy: 49.4340%, Training Loss: 1.0167%\n",
      "Epoch [18/300], Step [197/225], Training Accuracy: 49.4448%, Training Loss: 1.0161%\n",
      "Epoch [18/300], Step [198/225], Training Accuracy: 49.4476%, Training Loss: 1.0156%\n",
      "Epoch [18/300], Step [199/225], Training Accuracy: 49.4504%, Training Loss: 1.0152%\n",
      "Epoch [18/300], Step [200/225], Training Accuracy: 49.4766%, Training Loss: 1.0154%\n",
      "Epoch [18/300], Step [201/225], Training Accuracy: 49.4792%, Training Loss: 1.0157%\n",
      "Epoch [18/300], Step [202/225], Training Accuracy: 49.5050%, Training Loss: 1.0154%\n",
      "Epoch [18/300], Step [203/225], Training Accuracy: 49.4766%, Training Loss: 1.0158%\n",
      "Epoch [18/300], Step [204/225], Training Accuracy: 49.4945%, Training Loss: 1.0156%\n",
      "Epoch [18/300], Step [205/225], Training Accuracy: 49.4893%, Training Loss: 1.0156%\n",
      "Epoch [18/300], Step [206/225], Training Accuracy: 49.5297%, Training Loss: 1.0161%\n",
      "Epoch [18/300], Step [207/225], Training Accuracy: 49.5245%, Training Loss: 1.0165%\n",
      "Epoch [18/300], Step [208/225], Training Accuracy: 49.5868%, Training Loss: 1.0160%\n",
      "Epoch [18/300], Step [209/225], Training Accuracy: 49.5813%, Training Loss: 1.0161%\n",
      "Epoch [18/300], Step [210/225], Training Accuracy: 49.5759%, Training Loss: 1.0160%\n",
      "Epoch [18/300], Step [211/225], Training Accuracy: 49.5853%, Training Loss: 1.0158%\n",
      "Epoch [18/300], Step [212/225], Training Accuracy: 49.5578%, Training Loss: 1.0162%\n",
      "Epoch [18/300], Step [213/225], Training Accuracy: 49.5085%, Training Loss: 1.0168%\n",
      "Epoch [18/300], Step [214/225], Training Accuracy: 49.5035%, Training Loss: 1.0167%\n",
      "Epoch [18/300], Step [215/225], Training Accuracy: 49.5131%, Training Loss: 1.0166%\n",
      "Epoch [18/300], Step [216/225], Training Accuracy: 49.5153%, Training Loss: 1.0167%\n",
      "Epoch [18/300], Step [217/225], Training Accuracy: 49.4960%, Training Loss: 1.0169%\n",
      "Epoch [18/300], Step [218/225], Training Accuracy: 49.4409%, Training Loss: 1.0176%\n",
      "Epoch [18/300], Step [219/225], Training Accuracy: 49.3793%, Training Loss: 1.0179%\n",
      "Epoch [18/300], Step [220/225], Training Accuracy: 49.4389%, Training Loss: 1.0176%\n",
      "Epoch [18/300], Step [221/225], Training Accuracy: 49.4415%, Training Loss: 1.0177%\n",
      "Epoch [18/300], Step [222/225], Training Accuracy: 49.4440%, Training Loss: 1.0177%\n",
      "Epoch [18/300], Step [223/225], Training Accuracy: 49.3834%, Training Loss: 1.0183%\n",
      "Epoch [18/300], Step [224/225], Training Accuracy: 49.3722%, Training Loss: 1.0184%\n",
      "Epoch [18/300], Step [225/225], Training Accuracy: 49.3677%, Training Loss: 1.0186%\n",
      "Epoch [19/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8656%\n",
      "Epoch [19/300], Step [2/225], Training Accuracy: 53.9062%, Training Loss: 1.0030%\n",
      "Epoch [19/300], Step [3/225], Training Accuracy: 49.4792%, Training Loss: 1.0559%\n",
      "Epoch [19/300], Step [4/225], Training Accuracy: 48.4375%, Training Loss: 1.0457%\n",
      "Epoch [19/300], Step [5/225], Training Accuracy: 49.3750%, Training Loss: 1.0160%\n",
      "Epoch [19/300], Step [6/225], Training Accuracy: 49.2188%, Training Loss: 1.0377%\n",
      "Epoch [19/300], Step [7/225], Training Accuracy: 48.8839%, Training Loss: 1.0322%\n",
      "Epoch [19/300], Step [8/225], Training Accuracy: 50.0000%, Training Loss: 1.0363%\n",
      "Epoch [19/300], Step [9/225], Training Accuracy: 49.6528%, Training Loss: 1.0314%\n",
      "Epoch [19/300], Step [10/225], Training Accuracy: 49.8438%, Training Loss: 1.0251%\n",
      "Epoch [19/300], Step [11/225], Training Accuracy: 50.7102%, Training Loss: 1.0191%\n",
      "Epoch [19/300], Step [12/225], Training Accuracy: 50.7812%, Training Loss: 1.0249%\n",
      "Epoch [19/300], Step [13/225], Training Accuracy: 50.9615%, Training Loss: 1.0249%\n",
      "Epoch [19/300], Step [14/225], Training Accuracy: 50.3348%, Training Loss: 1.0315%\n",
      "Epoch [19/300], Step [15/225], Training Accuracy: 50.0000%, Training Loss: 1.0401%\n",
      "Epoch [19/300], Step [16/225], Training Accuracy: 50.0977%, Training Loss: 1.0417%\n",
      "Epoch [19/300], Step [17/225], Training Accuracy: 50.0919%, Training Loss: 1.0362%\n",
      "Epoch [19/300], Step [18/225], Training Accuracy: 49.9132%, Training Loss: 1.0347%\n",
      "Epoch [19/300], Step [19/225], Training Accuracy: 49.3421%, Training Loss: 1.0361%\n",
      "Epoch [19/300], Step [20/225], Training Accuracy: 49.5312%, Training Loss: 1.0331%\n",
      "Epoch [19/300], Step [21/225], Training Accuracy: 49.7024%, Training Loss: 1.0279%\n",
      "Epoch [19/300], Step [22/225], Training Accuracy: 49.5739%, Training Loss: 1.0292%\n",
      "Epoch [19/300], Step [23/225], Training Accuracy: 49.4565%, Training Loss: 1.0259%\n",
      "Epoch [19/300], Step [24/225], Training Accuracy: 49.2188%, Training Loss: 1.0300%\n",
      "Epoch [19/300], Step [25/225], Training Accuracy: 49.3125%, Training Loss: 1.0265%\n",
      "Epoch [19/300], Step [26/225], Training Accuracy: 48.9183%, Training Loss: 1.0288%\n",
      "Epoch [19/300], Step [27/225], Training Accuracy: 48.7847%, Training Loss: 1.0292%\n",
      "Epoch [19/300], Step [28/225], Training Accuracy: 49.0513%, Training Loss: 1.0263%\n",
      "Epoch [19/300], Step [29/225], Training Accuracy: 49.4073%, Training Loss: 1.0220%\n",
      "Epoch [19/300], Step [30/225], Training Accuracy: 49.3750%, Training Loss: 1.0205%\n",
      "Epoch [19/300], Step [31/225], Training Accuracy: 49.4456%, Training Loss: 1.0202%\n",
      "Epoch [19/300], Step [32/225], Training Accuracy: 49.6094%, Training Loss: 1.0181%\n",
      "Epoch [19/300], Step [33/225], Training Accuracy: 49.7159%, Training Loss: 1.0161%\n",
      "Epoch [19/300], Step [34/225], Training Accuracy: 49.7243%, Training Loss: 1.0192%\n",
      "Epoch [19/300], Step [35/225], Training Accuracy: 49.6429%, Training Loss: 1.0187%\n",
      "Epoch [19/300], Step [36/225], Training Accuracy: 49.6528%, Training Loss: 1.0185%\n",
      "Epoch [19/300], Step [37/225], Training Accuracy: 49.8311%, Training Loss: 1.0157%\n",
      "Epoch [19/300], Step [38/225], Training Accuracy: 50.0000%, Training Loss: 1.0138%\n",
      "Epoch [19/300], Step [39/225], Training Accuracy: 49.8397%, Training Loss: 1.0128%\n",
      "Epoch [19/300], Step [40/225], Training Accuracy: 49.8047%, Training Loss: 1.0130%\n",
      "Epoch [19/300], Step [41/225], Training Accuracy: 49.5808%, Training Loss: 1.0144%\n",
      "Epoch [19/300], Step [42/225], Training Accuracy: 49.7024%, Training Loss: 1.0126%\n",
      "Epoch [19/300], Step [43/225], Training Accuracy: 49.6366%, Training Loss: 1.0125%\n",
      "Epoch [19/300], Step [44/225], Training Accuracy: 49.7869%, Training Loss: 1.0107%\n",
      "Epoch [19/300], Step [45/225], Training Accuracy: 49.7917%, Training Loss: 1.0102%\n",
      "Epoch [19/300], Step [46/225], Training Accuracy: 50.0340%, Training Loss: 1.0066%\n",
      "Epoch [19/300], Step [47/225], Training Accuracy: 49.9335%, Training Loss: 1.0076%\n",
      "Epoch [19/300], Step [48/225], Training Accuracy: 50.0977%, Training Loss: 1.0080%\n",
      "Epoch [19/300], Step [49/225], Training Accuracy: 49.9362%, Training Loss: 1.0094%\n",
      "Epoch [19/300], Step [50/225], Training Accuracy: 50.0625%, Training Loss: 1.0079%\n",
      "Epoch [19/300], Step [51/225], Training Accuracy: 50.0613%, Training Loss: 1.0063%\n",
      "Epoch [19/300], Step [52/225], Training Accuracy: 50.2103%, Training Loss: 1.0046%\n",
      "Epoch [19/300], Step [53/225], Training Accuracy: 50.0590%, Training Loss: 1.0046%\n",
      "Epoch [19/300], Step [54/225], Training Accuracy: 49.8553%, Training Loss: 1.0061%\n",
      "Epoch [19/300], Step [55/225], Training Accuracy: 49.7159%, Training Loss: 1.0077%\n",
      "Epoch [19/300], Step [56/225], Training Accuracy: 49.6652%, Training Loss: 1.0088%\n",
      "Epoch [19/300], Step [57/225], Training Accuracy: 49.9178%, Training Loss: 1.0057%\n",
      "Epoch [19/300], Step [58/225], Training Accuracy: 49.8922%, Training Loss: 1.0062%\n",
      "Epoch [19/300], Step [59/225], Training Accuracy: 49.9470%, Training Loss: 1.0059%\n",
      "Epoch [19/300], Step [60/225], Training Accuracy: 49.8698%, Training Loss: 1.0064%\n",
      "Epoch [19/300], Step [61/225], Training Accuracy: 49.9232%, Training Loss: 1.0062%\n",
      "Epoch [19/300], Step [62/225], Training Accuracy: 49.8740%, Training Loss: 1.0063%\n",
      "Epoch [19/300], Step [63/225], Training Accuracy: 49.7272%, Training Loss: 1.0077%\n",
      "Epoch [19/300], Step [64/225], Training Accuracy: 49.6826%, Training Loss: 1.0079%\n",
      "Epoch [19/300], Step [65/225], Training Accuracy: 49.6635%, Training Loss: 1.0081%\n",
      "Epoch [19/300], Step [66/225], Training Accuracy: 49.8343%, Training Loss: 1.0065%\n",
      "Epoch [19/300], Step [67/225], Training Accuracy: 49.7435%, Training Loss: 1.0067%\n",
      "Epoch [19/300], Step [68/225], Training Accuracy: 49.6783%, Training Loss: 1.0066%\n",
      "Epoch [19/300], Step [69/225], Training Accuracy: 49.5924%, Training Loss: 1.0067%\n",
      "Epoch [19/300], Step [70/225], Training Accuracy: 49.5312%, Training Loss: 1.0081%\n",
      "Epoch [19/300], Step [71/225], Training Accuracy: 49.6259%, Training Loss: 1.0070%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [72/225], Training Accuracy: 49.5226%, Training Loss: 1.0090%\n",
      "Epoch [19/300], Step [73/225], Training Accuracy: 49.4649%, Training Loss: 1.0118%\n",
      "Epoch [19/300], Step [74/225], Training Accuracy: 49.5566%, Training Loss: 1.0098%\n",
      "Epoch [19/300], Step [75/225], Training Accuracy: 49.5208%, Training Loss: 1.0092%\n",
      "Epoch [19/300], Step [76/225], Training Accuracy: 49.5683%, Training Loss: 1.0092%\n",
      "Epoch [19/300], Step [77/225], Training Accuracy: 49.6753%, Training Loss: 1.0088%\n",
      "Epoch [19/300], Step [78/225], Training Accuracy: 49.6795%, Training Loss: 1.0101%\n",
      "Epoch [19/300], Step [79/225], Training Accuracy: 49.7033%, Training Loss: 1.0112%\n",
      "Epoch [19/300], Step [80/225], Training Accuracy: 49.6289%, Training Loss: 1.0114%\n",
      "Epoch [19/300], Step [81/225], Training Accuracy: 49.6721%, Training Loss: 1.0121%\n",
      "Epoch [19/300], Step [82/225], Training Accuracy: 49.7523%, Training Loss: 1.0112%\n",
      "Epoch [19/300], Step [83/225], Training Accuracy: 49.7364%, Training Loss: 1.0111%\n",
      "Epoch [19/300], Step [84/225], Training Accuracy: 49.7396%, Training Loss: 1.0118%\n",
      "Epoch [19/300], Step [85/225], Training Accuracy: 49.8162%, Training Loss: 1.0110%\n",
      "Epoch [19/300], Step [86/225], Training Accuracy: 49.8728%, Training Loss: 1.0112%\n",
      "Epoch [19/300], Step [87/225], Training Accuracy: 49.8743%, Training Loss: 1.0110%\n",
      "Epoch [19/300], Step [88/225], Training Accuracy: 49.7869%, Training Loss: 1.0113%\n",
      "Epoch [19/300], Step [89/225], Training Accuracy: 49.8069%, Training Loss: 1.0127%\n",
      "Epoch [19/300], Step [90/225], Training Accuracy: 49.7049%, Training Loss: 1.0133%\n",
      "Epoch [19/300], Step [91/225], Training Accuracy: 49.7768%, Training Loss: 1.0115%\n",
      "Epoch [19/300], Step [92/225], Training Accuracy: 49.7452%, Training Loss: 1.0115%\n",
      "Epoch [19/300], Step [93/225], Training Accuracy: 49.7312%, Training Loss: 1.0116%\n",
      "Epoch [19/300], Step [94/225], Training Accuracy: 49.8172%, Training Loss: 1.0102%\n",
      "Epoch [19/300], Step [95/225], Training Accuracy: 49.7204%, Training Loss: 1.0111%\n",
      "Epoch [19/300], Step [96/225], Training Accuracy: 49.7559%, Training Loss: 1.0105%\n",
      "Epoch [19/300], Step [97/225], Training Accuracy: 49.7745%, Training Loss: 1.0098%\n",
      "Epoch [19/300], Step [98/225], Training Accuracy: 49.7768%, Training Loss: 1.0091%\n",
      "Epoch [19/300], Step [99/225], Training Accuracy: 49.8106%, Training Loss: 1.0092%\n",
      "Epoch [19/300], Step [100/225], Training Accuracy: 49.6875%, Training Loss: 1.0096%\n",
      "Epoch [19/300], Step [101/225], Training Accuracy: 49.6287%, Training Loss: 1.0101%\n",
      "Epoch [19/300], Step [102/225], Training Accuracy: 49.5711%, Training Loss: 1.0110%\n",
      "Epoch [19/300], Step [103/225], Training Accuracy: 49.4387%, Training Loss: 1.0123%\n",
      "Epoch [19/300], Step [104/225], Training Accuracy: 49.4441%, Training Loss: 1.0119%\n",
      "Epoch [19/300], Step [105/225], Training Accuracy: 49.4048%, Training Loss: 1.0111%\n",
      "Epoch [19/300], Step [106/225], Training Accuracy: 49.3219%, Training Loss: 1.0120%\n",
      "Epoch [19/300], Step [107/225], Training Accuracy: 49.3721%, Training Loss: 1.0117%\n",
      "Epoch [19/300], Step [108/225], Training Accuracy: 49.3924%, Training Loss: 1.0124%\n",
      "Epoch [19/300], Step [109/225], Training Accuracy: 49.3979%, Training Loss: 1.0124%\n",
      "Epoch [19/300], Step [110/225], Training Accuracy: 49.4318%, Training Loss: 1.0121%\n",
      "Epoch [19/300], Step [111/225], Training Accuracy: 49.5355%, Training Loss: 1.0125%\n",
      "Epoch [19/300], Step [112/225], Training Accuracy: 49.5815%, Training Loss: 1.0120%\n",
      "Epoch [19/300], Step [113/225], Training Accuracy: 49.5299%, Training Loss: 1.0134%\n",
      "Epoch [19/300], Step [114/225], Training Accuracy: 49.5203%, Training Loss: 1.0132%\n",
      "Epoch [19/300], Step [115/225], Training Accuracy: 49.5245%, Training Loss: 1.0124%\n",
      "Epoch [19/300], Step [116/225], Training Accuracy: 49.5690%, Training Loss: 1.0122%\n",
      "Epoch [19/300], Step [117/225], Training Accuracy: 49.4658%, Training Loss: 1.0136%\n",
      "Epoch [19/300], Step [118/225], Training Accuracy: 49.4703%, Training Loss: 1.0136%\n",
      "Epoch [19/300], Step [119/225], Training Accuracy: 49.4354%, Training Loss: 1.0138%\n",
      "Epoch [19/300], Step [120/225], Training Accuracy: 49.5312%, Training Loss: 1.0130%\n",
      "Epoch [19/300], Step [121/225], Training Accuracy: 49.4835%, Training Loss: 1.0131%\n",
      "Epoch [19/300], Step [122/225], Training Accuracy: 49.4621%, Training Loss: 1.0130%\n",
      "Epoch [19/300], Step [123/225], Training Accuracy: 49.3902%, Training Loss: 1.0130%\n",
      "Epoch [19/300], Step [124/225], Training Accuracy: 49.4078%, Training Loss: 1.0127%\n",
      "Epoch [19/300], Step [125/225], Training Accuracy: 49.3875%, Training Loss: 1.0137%\n",
      "Epoch [19/300], Step [126/225], Training Accuracy: 49.3304%, Training Loss: 1.0143%\n",
      "Epoch [19/300], Step [127/225], Training Accuracy: 49.3356%, Training Loss: 1.0141%\n",
      "Epoch [19/300], Step [128/225], Training Accuracy: 49.2920%, Training Loss: 1.0145%\n",
      "Epoch [19/300], Step [129/225], Training Accuracy: 49.2854%, Training Loss: 1.0147%\n",
      "Epoch [19/300], Step [130/225], Training Accuracy: 49.2788%, Training Loss: 1.0153%\n",
      "Epoch [19/300], Step [131/225], Training Accuracy: 49.2844%, Training Loss: 1.0150%\n",
      "Epoch [19/300], Step [132/225], Training Accuracy: 49.2779%, Training Loss: 1.0154%\n",
      "Epoch [19/300], Step [133/225], Training Accuracy: 49.3421%, Training Loss: 1.0147%\n",
      "Epoch [19/300], Step [134/225], Training Accuracy: 49.3703%, Training Loss: 1.0142%\n",
      "Epoch [19/300], Step [135/225], Training Accuracy: 49.3519%, Training Loss: 1.0140%\n",
      "Epoch [19/300], Step [136/225], Training Accuracy: 49.3222%, Training Loss: 1.0133%\n",
      "Epoch [19/300], Step [137/225], Training Accuracy: 49.3157%, Training Loss: 1.0132%\n",
      "Epoch [19/300], Step [138/225], Training Accuracy: 49.3320%, Training Loss: 1.0124%\n",
      "Epoch [19/300], Step [139/225], Training Accuracy: 49.3480%, Training Loss: 1.0127%\n",
      "Epoch [19/300], Step [140/225], Training Accuracy: 49.3973%, Training Loss: 1.0125%\n",
      "Epoch [19/300], Step [141/225], Training Accuracy: 49.4459%, Training Loss: 1.0125%\n",
      "Epoch [19/300], Step [142/225], Training Accuracy: 49.4388%, Training Loss: 1.0126%\n",
      "Epoch [19/300], Step [143/225], Training Accuracy: 49.4865%, Training Loss: 1.0121%\n",
      "Epoch [19/300], Step [144/225], Training Accuracy: 49.4575%, Training Loss: 1.0119%\n",
      "Epoch [19/300], Step [145/225], Training Accuracy: 49.5474%, Training Loss: 1.0110%\n",
      "Epoch [19/300], Step [146/225], Training Accuracy: 49.5505%, Training Loss: 1.0111%\n",
      "Epoch [19/300], Step [147/225], Training Accuracy: 49.5004%, Training Loss: 1.0120%\n",
      "Epoch [19/300], Step [148/225], Training Accuracy: 49.5883%, Training Loss: 1.0117%\n",
      "Epoch [19/300], Step [149/225], Training Accuracy: 49.5596%, Training Loss: 1.0121%\n",
      "Epoch [19/300], Step [150/225], Training Accuracy: 49.5417%, Training Loss: 1.0126%\n",
      "Epoch [19/300], Step [151/225], Training Accuracy: 49.5654%, Training Loss: 1.0119%\n",
      "Epoch [19/300], Step [152/225], Training Accuracy: 49.5683%, Training Loss: 1.0119%\n",
      "Epoch [19/300], Step [153/225], Training Accuracy: 49.5915%, Training Loss: 1.0113%\n",
      "Epoch [19/300], Step [154/225], Training Accuracy: 49.5333%, Training Loss: 1.0111%\n",
      "Epoch [19/300], Step [155/225], Training Accuracy: 49.4758%, Training Loss: 1.0118%\n",
      "Epoch [19/300], Step [156/225], Training Accuracy: 49.4591%, Training Loss: 1.0117%\n",
      "Epoch [19/300], Step [157/225], Training Accuracy: 49.4427%, Training Loss: 1.0123%\n",
      "Epoch [19/300], Step [158/225], Training Accuracy: 49.4759%, Training Loss: 1.0123%\n",
      "Epoch [19/300], Step [159/225], Training Accuracy: 49.4792%, Training Loss: 1.0120%\n",
      "Epoch [19/300], Step [160/225], Training Accuracy: 49.4238%, Training Loss: 1.0122%\n",
      "Epoch [19/300], Step [161/225], Training Accuracy: 49.5245%, Training Loss: 1.0111%\n",
      "Epoch [19/300], Step [162/225], Training Accuracy: 49.5274%, Training Loss: 1.0113%\n",
      "Epoch [19/300], Step [163/225], Training Accuracy: 49.4728%, Training Loss: 1.0109%\n",
      "Epoch [19/300], Step [164/225], Training Accuracy: 49.5522%, Training Loss: 1.0101%\n",
      "Epoch [19/300], Step [165/225], Training Accuracy: 49.5833%, Training Loss: 1.0100%\n",
      "Epoch [19/300], Step [166/225], Training Accuracy: 49.5953%, Training Loss: 1.0102%\n",
      "Epoch [19/300], Step [167/225], Training Accuracy: 49.6164%, Training Loss: 1.0095%\n",
      "Epoch [19/300], Step [168/225], Training Accuracy: 49.6094%, Training Loss: 1.0097%\n",
      "Epoch [19/300], Step [169/225], Training Accuracy: 49.6209%, Training Loss: 1.0097%\n",
      "Epoch [19/300], Step [170/225], Training Accuracy: 49.6048%, Training Loss: 1.0103%\n",
      "Epoch [19/300], Step [171/225], Training Accuracy: 49.6071%, Training Loss: 1.0102%\n",
      "Epoch [19/300], Step [172/225], Training Accuracy: 49.5458%, Training Loss: 1.0108%\n",
      "Epoch [19/300], Step [173/225], Training Accuracy: 49.5484%, Training Loss: 1.0104%\n",
      "Epoch [19/300], Step [174/225], Training Accuracy: 49.5690%, Training Loss: 1.0103%\n",
      "Epoch [19/300], Step [175/225], Training Accuracy: 49.5536%, Training Loss: 1.0104%\n",
      "Epoch [19/300], Step [176/225], Training Accuracy: 49.5650%, Training Loss: 1.0103%\n",
      "Epoch [19/300], Step [177/225], Training Accuracy: 49.5498%, Training Loss: 1.0100%\n",
      "Epoch [19/300], Step [178/225], Training Accuracy: 49.4996%, Training Loss: 1.0099%\n",
      "Epoch [19/300], Step [179/225], Training Accuracy: 49.5199%, Training Loss: 1.0100%\n",
      "Epoch [19/300], Step [180/225], Training Accuracy: 49.6267%, Training Loss: 1.0089%\n",
      "Epoch [19/300], Step [181/225], Training Accuracy: 49.5856%, Training Loss: 1.0096%\n",
      "Epoch [19/300], Step [182/225], Training Accuracy: 49.5622%, Training Loss: 1.0099%\n",
      "Epoch [19/300], Step [183/225], Training Accuracy: 49.5902%, Training Loss: 1.0095%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [184/225], Training Accuracy: 49.6094%, Training Loss: 1.0095%\n",
      "Epoch [19/300], Step [185/225], Training Accuracy: 49.6030%, Training Loss: 1.0095%\n",
      "Epoch [19/300], Step [186/225], Training Accuracy: 49.6388%, Training Loss: 1.0093%\n",
      "Epoch [19/300], Step [187/225], Training Accuracy: 49.6658%, Training Loss: 1.0086%\n",
      "Epoch [19/300], Step [188/225], Training Accuracy: 49.7257%, Training Loss: 1.0081%\n",
      "Epoch [19/300], Step [189/225], Training Accuracy: 49.7685%, Training Loss: 1.0076%\n",
      "Epoch [19/300], Step [190/225], Training Accuracy: 49.7615%, Training Loss: 1.0080%\n",
      "Epoch [19/300], Step [191/225], Training Accuracy: 49.7300%, Training Loss: 1.0081%\n",
      "Epoch [19/300], Step [192/225], Training Accuracy: 49.7721%, Training Loss: 1.0078%\n",
      "Epoch [19/300], Step [193/225], Training Accuracy: 49.7490%, Training Loss: 1.0078%\n",
      "Epoch [19/300], Step [194/225], Training Accuracy: 49.7906%, Training Loss: 1.0079%\n",
      "Epoch [19/300], Step [195/225], Training Accuracy: 49.8157%, Training Loss: 1.0074%\n",
      "Epoch [19/300], Step [196/225], Training Accuracy: 49.8406%, Training Loss: 1.0075%\n",
      "Epoch [19/300], Step [197/225], Training Accuracy: 49.8414%, Training Loss: 1.0069%\n",
      "Epoch [19/300], Step [198/225], Training Accuracy: 49.8501%, Training Loss: 1.0064%\n",
      "Epoch [19/300], Step [199/225], Training Accuracy: 49.8587%, Training Loss: 1.0060%\n",
      "Epoch [19/300], Step [200/225], Training Accuracy: 49.8828%, Training Loss: 1.0062%\n",
      "Epoch [19/300], Step [201/225], Training Accuracy: 49.8834%, Training Loss: 1.0065%\n",
      "Epoch [19/300], Step [202/225], Training Accuracy: 49.8994%, Training Loss: 1.0062%\n",
      "Epoch [19/300], Step [203/225], Training Accuracy: 49.8692%, Training Loss: 1.0065%\n",
      "Epoch [19/300], Step [204/225], Training Accuracy: 49.8928%, Training Loss: 1.0064%\n",
      "Epoch [19/300], Step [205/225], Training Accuracy: 49.8780%, Training Loss: 1.0064%\n",
      "Epoch [19/300], Step [206/225], Training Accuracy: 49.9166%, Training Loss: 1.0068%\n",
      "Epoch [19/300], Step [207/225], Training Accuracy: 49.8943%, Training Loss: 1.0072%\n",
      "Epoch [19/300], Step [208/225], Training Accuracy: 49.9474%, Training Loss: 1.0067%\n",
      "Epoch [19/300], Step [209/225], Training Accuracy: 49.9477%, Training Loss: 1.0069%\n",
      "Epoch [19/300], Step [210/225], Training Accuracy: 49.9405%, Training Loss: 1.0068%\n",
      "Epoch [19/300], Step [211/225], Training Accuracy: 49.9482%, Training Loss: 1.0065%\n",
      "Epoch [19/300], Step [212/225], Training Accuracy: 49.9263%, Training Loss: 1.0070%\n",
      "Epoch [19/300], Step [213/225], Training Accuracy: 49.8973%, Training Loss: 1.0076%\n",
      "Epoch [19/300], Step [214/225], Training Accuracy: 49.8832%, Training Loss: 1.0074%\n",
      "Epoch [19/300], Step [215/225], Training Accuracy: 49.8983%, Training Loss: 1.0074%\n",
      "Epoch [19/300], Step [216/225], Training Accuracy: 49.8987%, Training Loss: 1.0075%\n",
      "Epoch [19/300], Step [217/225], Training Accuracy: 49.8776%, Training Loss: 1.0076%\n",
      "Epoch [19/300], Step [218/225], Training Accuracy: 49.8280%, Training Loss: 1.0084%\n",
      "Epoch [19/300], Step [219/225], Training Accuracy: 49.7788%, Training Loss: 1.0088%\n",
      "Epoch [19/300], Step [220/225], Training Accuracy: 49.8366%, Training Loss: 1.0084%\n",
      "Epoch [19/300], Step [221/225], Training Accuracy: 49.8374%, Training Loss: 1.0086%\n",
      "Epoch [19/300], Step [222/225], Training Accuracy: 49.8452%, Training Loss: 1.0085%\n",
      "Epoch [19/300], Step [223/225], Training Accuracy: 49.7758%, Training Loss: 1.0092%\n",
      "Epoch [19/300], Step [224/225], Training Accuracy: 49.7628%, Training Loss: 1.0092%\n",
      "Epoch [19/300], Step [225/225], Training Accuracy: 49.7568%, Training Loss: 1.0094%\n",
      "Epoch [20/300], Step [1/225], Training Accuracy: 67.1875%, Training Loss: 0.8534%\n",
      "Epoch [20/300], Step [2/225], Training Accuracy: 54.6875%, Training Loss: 0.9960%\n",
      "Epoch [20/300], Step [3/225], Training Accuracy: 50.0000%, Training Loss: 1.0488%\n",
      "Epoch [20/300], Step [4/225], Training Accuracy: 48.8281%, Training Loss: 1.0386%\n",
      "Epoch [20/300], Step [5/225], Training Accuracy: 49.6875%, Training Loss: 1.0077%\n",
      "Epoch [20/300], Step [6/225], Training Accuracy: 49.4792%, Training Loss: 1.0298%\n",
      "Epoch [20/300], Step [7/225], Training Accuracy: 49.5536%, Training Loss: 1.0244%\n",
      "Epoch [20/300], Step [8/225], Training Accuracy: 50.3906%, Training Loss: 1.0290%\n",
      "Epoch [20/300], Step [9/225], Training Accuracy: 50.0000%, Training Loss: 1.0236%\n",
      "Epoch [20/300], Step [10/225], Training Accuracy: 50.3125%, Training Loss: 1.0178%\n",
      "Epoch [20/300], Step [11/225], Training Accuracy: 50.9943%, Training Loss: 1.0119%\n",
      "Epoch [20/300], Step [12/225], Training Accuracy: 51.0417%, Training Loss: 1.0177%\n",
      "Epoch [20/300], Step [13/225], Training Accuracy: 51.4423%, Training Loss: 1.0173%\n",
      "Epoch [20/300], Step [14/225], Training Accuracy: 51.1161%, Training Loss: 1.0228%\n",
      "Epoch [20/300], Step [15/225], Training Accuracy: 50.8333%, Training Loss: 1.0318%\n",
      "Epoch [20/300], Step [16/225], Training Accuracy: 50.9766%, Training Loss: 1.0334%\n",
      "Epoch [20/300], Step [17/225], Training Accuracy: 50.9191%, Training Loss: 1.0282%\n",
      "Epoch [20/300], Step [18/225], Training Accuracy: 50.7812%, Training Loss: 1.0270%\n",
      "Epoch [20/300], Step [19/225], Training Accuracy: 50.2467%, Training Loss: 1.0284%\n",
      "Epoch [20/300], Step [20/225], Training Accuracy: 50.3906%, Training Loss: 1.0255%\n",
      "Epoch [20/300], Step [21/225], Training Accuracy: 50.5952%, Training Loss: 1.0204%\n",
      "Epoch [20/300], Step [22/225], Training Accuracy: 50.3551%, Training Loss: 1.0215%\n",
      "Epoch [20/300], Step [23/225], Training Accuracy: 50.1359%, Training Loss: 1.0182%\n",
      "Epoch [20/300], Step [24/225], Training Accuracy: 49.9349%, Training Loss: 1.0221%\n",
      "Epoch [20/300], Step [25/225], Training Accuracy: 50.0000%, Training Loss: 1.0185%\n",
      "Epoch [20/300], Step [26/225], Training Accuracy: 49.5793%, Training Loss: 1.0207%\n",
      "Epoch [20/300], Step [27/225], Training Accuracy: 49.3634%, Training Loss: 1.0215%\n",
      "Epoch [20/300], Step [28/225], Training Accuracy: 49.5536%, Training Loss: 1.0185%\n",
      "Epoch [20/300], Step [29/225], Training Accuracy: 49.8384%, Training Loss: 1.0142%\n",
      "Epoch [20/300], Step [30/225], Training Accuracy: 49.7917%, Training Loss: 1.0125%\n",
      "Epoch [20/300], Step [31/225], Training Accuracy: 49.8992%, Training Loss: 1.0121%\n",
      "Epoch [20/300], Step [32/225], Training Accuracy: 50.0000%, Training Loss: 1.0098%\n",
      "Epoch [20/300], Step [33/225], Training Accuracy: 50.1420%, Training Loss: 1.0078%\n",
      "Epoch [20/300], Step [34/225], Training Accuracy: 50.1379%, Training Loss: 1.0108%\n",
      "Epoch [20/300], Step [35/225], Training Accuracy: 50.0000%, Training Loss: 1.0103%\n",
      "Epoch [20/300], Step [36/225], Training Accuracy: 50.0000%, Training Loss: 1.0102%\n",
      "Epoch [20/300], Step [37/225], Training Accuracy: 50.2111%, Training Loss: 1.0074%\n",
      "Epoch [20/300], Step [38/225], Training Accuracy: 50.3289%, Training Loss: 1.0054%\n",
      "Epoch [20/300], Step [39/225], Training Accuracy: 50.2003%, Training Loss: 1.0045%\n",
      "Epoch [20/300], Step [40/225], Training Accuracy: 50.1172%, Training Loss: 1.0047%\n",
      "Epoch [20/300], Step [41/225], Training Accuracy: 49.9619%, Training Loss: 1.0062%\n",
      "Epoch [20/300], Step [42/225], Training Accuracy: 50.0372%, Training Loss: 1.0044%\n",
      "Epoch [20/300], Step [43/225], Training Accuracy: 49.9637%, Training Loss: 1.0043%\n",
      "Epoch [20/300], Step [44/225], Training Accuracy: 50.0710%, Training Loss: 1.0025%\n",
      "Epoch [20/300], Step [45/225], Training Accuracy: 50.0347%, Training Loss: 1.0020%\n",
      "Epoch [20/300], Step [46/225], Training Accuracy: 50.2717%, Training Loss: 0.9985%\n",
      "Epoch [20/300], Step [47/225], Training Accuracy: 50.1662%, Training Loss: 0.9996%\n",
      "Epoch [20/300], Step [48/225], Training Accuracy: 50.3255%, Training Loss: 1.0000%\n",
      "Epoch [20/300], Step [49/225], Training Accuracy: 50.1594%, Training Loss: 1.0015%\n",
      "Epoch [20/300], Step [50/225], Training Accuracy: 50.2812%, Training Loss: 1.0000%\n",
      "Epoch [20/300], Step [51/225], Training Accuracy: 50.3064%, Training Loss: 0.9982%\n",
      "Epoch [20/300], Step [52/225], Training Accuracy: 50.4507%, Training Loss: 0.9964%\n",
      "Epoch [20/300], Step [53/225], Training Accuracy: 50.3243%, Training Loss: 0.9964%\n",
      "Epoch [20/300], Step [54/225], Training Accuracy: 50.1157%, Training Loss: 0.9980%\n",
      "Epoch [20/300], Step [55/225], Training Accuracy: 50.0568%, Training Loss: 0.9994%\n",
      "Epoch [20/300], Step [56/225], Training Accuracy: 50.0000%, Training Loss: 1.0006%\n",
      "Epoch [20/300], Step [57/225], Training Accuracy: 50.2467%, Training Loss: 0.9978%\n",
      "Epoch [20/300], Step [58/225], Training Accuracy: 50.1616%, Training Loss: 0.9982%\n",
      "Epoch [20/300], Step [59/225], Training Accuracy: 50.1854%, Training Loss: 0.9977%\n",
      "Epoch [20/300], Step [60/225], Training Accuracy: 50.0781%, Training Loss: 0.9983%\n",
      "Epoch [20/300], Step [61/225], Training Accuracy: 50.1281%, Training Loss: 0.9981%\n",
      "Epoch [20/300], Step [62/225], Training Accuracy: 50.0504%, Training Loss: 0.9983%\n",
      "Epoch [20/300], Step [63/225], Training Accuracy: 49.9504%, Training Loss: 0.9996%\n",
      "Epoch [20/300], Step [64/225], Training Accuracy: 49.9512%, Training Loss: 0.9996%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/300], Step [65/225], Training Accuracy: 49.8798%, Training Loss: 1.0000%\n",
      "Epoch [20/300], Step [66/225], Training Accuracy: 50.0710%, Training Loss: 0.9983%\n",
      "Epoch [20/300], Step [67/225], Training Accuracy: 49.9767%, Training Loss: 0.9985%\n",
      "Epoch [20/300], Step [68/225], Training Accuracy: 49.8851%, Training Loss: 0.9983%\n",
      "Epoch [20/300], Step [69/225], Training Accuracy: 49.8188%, Training Loss: 0.9985%\n",
      "Epoch [20/300], Step [70/225], Training Accuracy: 49.7321%, Training Loss: 0.9999%\n",
      "Epoch [20/300], Step [71/225], Training Accuracy: 49.8019%, Training Loss: 0.9988%\n",
      "Epoch [20/300], Step [72/225], Training Accuracy: 49.6962%, Training Loss: 1.0008%\n",
      "Epoch [20/300], Step [73/225], Training Accuracy: 49.6361%, Training Loss: 1.0034%\n",
      "Epoch [20/300], Step [74/225], Training Accuracy: 49.6622%, Training Loss: 1.0015%\n",
      "Epoch [20/300], Step [75/225], Training Accuracy: 49.6250%, Training Loss: 1.0009%\n",
      "Epoch [20/300], Step [76/225], Training Accuracy: 49.6916%, Training Loss: 1.0008%\n",
      "Epoch [20/300], Step [77/225], Training Accuracy: 49.7768%, Training Loss: 1.0003%\n",
      "Epoch [20/300], Step [78/225], Training Accuracy: 49.7596%, Training Loss: 1.0015%\n",
      "Epoch [20/300], Step [79/225], Training Accuracy: 49.7429%, Training Loss: 1.0026%\n",
      "Epoch [20/300], Step [80/225], Training Accuracy: 49.6680%, Training Loss: 1.0030%\n",
      "Epoch [20/300], Step [81/225], Training Accuracy: 49.6914%, Training Loss: 1.0036%\n",
      "Epoch [20/300], Step [82/225], Training Accuracy: 49.7713%, Training Loss: 1.0027%\n",
      "Epoch [20/300], Step [83/225], Training Accuracy: 49.7176%, Training Loss: 1.0024%\n",
      "Epoch [20/300], Step [84/225], Training Accuracy: 49.7582%, Training Loss: 1.0030%\n",
      "Epoch [20/300], Step [85/225], Training Accuracy: 49.8346%, Training Loss: 1.0021%\n",
      "Epoch [20/300], Step [86/225], Training Accuracy: 49.9273%, Training Loss: 1.0022%\n",
      "Epoch [20/300], Step [87/225], Training Accuracy: 49.9282%, Training Loss: 1.0020%\n",
      "Epoch [20/300], Step [88/225], Training Accuracy: 49.8580%, Training Loss: 1.0023%\n",
      "Epoch [20/300], Step [89/225], Training Accuracy: 49.8771%, Training Loss: 1.0037%\n",
      "Epoch [20/300], Step [90/225], Training Accuracy: 49.7569%, Training Loss: 1.0044%\n",
      "Epoch [20/300], Step [91/225], Training Accuracy: 49.7940%, Training Loss: 1.0026%\n",
      "Epoch [20/300], Step [92/225], Training Accuracy: 49.7792%, Training Loss: 1.0025%\n",
      "Epoch [20/300], Step [93/225], Training Accuracy: 49.7648%, Training Loss: 1.0027%\n",
      "Epoch [20/300], Step [94/225], Training Accuracy: 49.8504%, Training Loss: 1.0013%\n",
      "Epoch [20/300], Step [95/225], Training Accuracy: 49.8026%, Training Loss: 1.0021%\n",
      "Epoch [20/300], Step [96/225], Training Accuracy: 49.8372%, Training Loss: 1.0015%\n",
      "Epoch [20/300], Step [97/225], Training Accuracy: 49.8872%, Training Loss: 1.0007%\n",
      "Epoch [20/300], Step [98/225], Training Accuracy: 49.9362%, Training Loss: 1.0001%\n",
      "Epoch [20/300], Step [99/225], Training Accuracy: 49.9842%, Training Loss: 1.0001%\n",
      "Epoch [20/300], Step [100/225], Training Accuracy: 49.8594%, Training Loss: 1.0006%\n",
      "Epoch [20/300], Step [101/225], Training Accuracy: 49.8453%, Training Loss: 1.0011%\n",
      "Epoch [20/300], Step [102/225], Training Accuracy: 49.7855%, Training Loss: 1.0021%\n",
      "Epoch [20/300], Step [103/225], Training Accuracy: 49.6511%, Training Loss: 1.0034%\n",
      "Epoch [20/300], Step [104/225], Training Accuracy: 49.6695%, Training Loss: 1.0031%\n",
      "Epoch [20/300], Step [105/225], Training Accuracy: 49.6131%, Training Loss: 1.0023%\n",
      "Epoch [20/300], Step [106/225], Training Accuracy: 49.5578%, Training Loss: 1.0032%\n",
      "Epoch [20/300], Step [107/225], Training Accuracy: 49.6057%, Training Loss: 1.0030%\n",
      "Epoch [20/300], Step [108/225], Training Accuracy: 49.6094%, Training Loss: 1.0037%\n",
      "Epoch [20/300], Step [109/225], Training Accuracy: 49.6130%, Training Loss: 1.0036%\n",
      "Epoch [20/300], Step [110/225], Training Accuracy: 49.6449%, Training Loss: 1.0034%\n",
      "Epoch [20/300], Step [111/225], Training Accuracy: 49.7466%, Training Loss: 1.0038%\n",
      "Epoch [20/300], Step [112/225], Training Accuracy: 49.7907%, Training Loss: 1.0033%\n",
      "Epoch [20/300], Step [113/225], Training Accuracy: 49.7511%, Training Loss: 1.0046%\n",
      "Epoch [20/300], Step [114/225], Training Accuracy: 49.7259%, Training Loss: 1.0044%\n",
      "Epoch [20/300], Step [115/225], Training Accuracy: 49.7283%, Training Loss: 1.0036%\n",
      "Epoch [20/300], Step [116/225], Training Accuracy: 49.7710%, Training Loss: 1.0034%\n",
      "Epoch [20/300], Step [117/225], Training Accuracy: 49.6795%, Training Loss: 1.0048%\n",
      "Epoch [20/300], Step [118/225], Training Accuracy: 49.7087%, Training Loss: 1.0048%\n",
      "Epoch [20/300], Step [119/225], Training Accuracy: 49.6849%, Training Loss: 1.0051%\n",
      "Epoch [20/300], Step [120/225], Training Accuracy: 49.7526%, Training Loss: 1.0042%\n",
      "Epoch [20/300], Step [121/225], Training Accuracy: 49.7030%, Training Loss: 1.0045%\n",
      "Epoch [20/300], Step [122/225], Training Accuracy: 49.7182%, Training Loss: 1.0044%\n",
      "Epoch [20/300], Step [123/225], Training Accuracy: 49.6570%, Training Loss: 1.0043%\n",
      "Epoch [20/300], Step [124/225], Training Accuracy: 49.6976%, Training Loss: 1.0041%\n",
      "Epoch [20/300], Step [125/225], Training Accuracy: 49.6750%, Training Loss: 1.0051%\n",
      "Epoch [20/300], Step [126/225], Training Accuracy: 49.6156%, Training Loss: 1.0057%\n",
      "Epoch [20/300], Step [127/225], Training Accuracy: 49.5940%, Training Loss: 1.0055%\n",
      "Epoch [20/300], Step [128/225], Training Accuracy: 49.5483%, Training Loss: 1.0059%\n",
      "Epoch [20/300], Step [129/225], Training Accuracy: 49.5518%, Training Loss: 1.0062%\n",
      "Epoch [20/300], Step [130/225], Training Accuracy: 49.5312%, Training Loss: 1.0068%\n",
      "Epoch [20/300], Step [131/225], Training Accuracy: 49.5468%, Training Loss: 1.0065%\n",
      "Epoch [20/300], Step [132/225], Training Accuracy: 49.5502%, Training Loss: 1.0069%\n",
      "Epoch [20/300], Step [133/225], Training Accuracy: 49.6241%, Training Loss: 1.0062%\n",
      "Epoch [20/300], Step [134/225], Training Accuracy: 49.6385%, Training Loss: 1.0057%\n",
      "Epoch [20/300], Step [135/225], Training Accuracy: 49.6296%, Training Loss: 1.0055%\n",
      "Epoch [20/300], Step [136/225], Training Accuracy: 49.6094%, Training Loss: 1.0048%\n",
      "Epoch [20/300], Step [137/225], Training Accuracy: 49.5894%, Training Loss: 1.0046%\n",
      "Epoch [20/300], Step [138/225], Training Accuracy: 49.6264%, Training Loss: 1.0038%\n",
      "Epoch [20/300], Step [139/225], Training Accuracy: 49.6290%, Training Loss: 1.0041%\n",
      "Epoch [20/300], Step [140/225], Training Accuracy: 49.6875%, Training Loss: 1.0039%\n",
      "Epoch [20/300], Step [141/225], Training Accuracy: 49.7230%, Training Loss: 1.0038%\n",
      "Epoch [20/300], Step [142/225], Training Accuracy: 49.7139%, Training Loss: 1.0040%\n",
      "Epoch [20/300], Step [143/225], Training Accuracy: 49.7487%, Training Loss: 1.0035%\n",
      "Epoch [20/300], Step [144/225], Training Accuracy: 49.7287%, Training Loss: 1.0033%\n",
      "Epoch [20/300], Step [145/225], Training Accuracy: 49.8060%, Training Loss: 1.0025%\n",
      "Epoch [20/300], Step [146/225], Training Accuracy: 49.8181%, Training Loss: 1.0025%\n",
      "Epoch [20/300], Step [147/225], Training Accuracy: 49.7768%, Training Loss: 1.0033%\n",
      "Epoch [20/300], Step [148/225], Training Accuracy: 49.8628%, Training Loss: 1.0031%\n",
      "Epoch [20/300], Step [149/225], Training Accuracy: 49.8217%, Training Loss: 1.0034%\n",
      "Epoch [20/300], Step [150/225], Training Accuracy: 49.8021%, Training Loss: 1.0039%\n",
      "Epoch [20/300], Step [151/225], Training Accuracy: 49.8241%, Training Loss: 1.0032%\n",
      "Epoch [20/300], Step [152/225], Training Accuracy: 49.8047%, Training Loss: 1.0033%\n",
      "Epoch [20/300], Step [153/225], Training Accuracy: 49.8264%, Training Loss: 1.0027%\n",
      "Epoch [20/300], Step [154/225], Training Accuracy: 49.7666%, Training Loss: 1.0024%\n",
      "Epoch [20/300], Step [155/225], Training Accuracy: 49.7077%, Training Loss: 1.0032%\n",
      "Epoch [20/300], Step [156/225], Training Accuracy: 49.6895%, Training Loss: 1.0031%\n",
      "Epoch [20/300], Step [157/225], Training Accuracy: 49.6716%, Training Loss: 1.0036%\n",
      "Epoch [20/300], Step [158/225], Training Accuracy: 49.6934%, Training Loss: 1.0037%\n",
      "Epoch [20/300], Step [159/225], Training Accuracy: 49.6855%, Training Loss: 1.0034%\n",
      "Epoch [20/300], Step [160/225], Training Accuracy: 49.6484%, Training Loss: 1.0035%\n",
      "Epoch [20/300], Step [161/225], Training Accuracy: 49.7477%, Training Loss: 1.0025%\n",
      "Epoch [20/300], Step [162/225], Training Accuracy: 49.7492%, Training Loss: 1.0027%\n",
      "Epoch [20/300], Step [163/225], Training Accuracy: 49.6933%, Training Loss: 1.0023%\n",
      "Epoch [20/300], Step [164/225], Training Accuracy: 49.7618%, Training Loss: 1.0014%\n",
      "Epoch [20/300], Step [165/225], Training Accuracy: 49.7822%, Training Loss: 1.0013%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/300], Step [166/225], Training Accuracy: 49.7929%, Training Loss: 1.0015%\n",
      "Epoch [20/300], Step [167/225], Training Accuracy: 49.8129%, Training Loss: 1.0008%\n",
      "Epoch [20/300], Step [168/225], Training Accuracy: 49.8047%, Training Loss: 1.0010%\n",
      "Epoch [20/300], Step [169/225], Training Accuracy: 49.8151%, Training Loss: 1.0010%\n",
      "Epoch [20/300], Step [170/225], Training Accuracy: 49.7978%, Training Loss: 1.0016%\n",
      "Epoch [20/300], Step [171/225], Training Accuracy: 49.7990%, Training Loss: 1.0014%\n",
      "Epoch [20/300], Step [172/225], Training Accuracy: 49.7366%, Training Loss: 1.0021%\n",
      "Epoch [20/300], Step [173/225], Training Accuracy: 49.7471%, Training Loss: 1.0017%\n",
      "Epoch [20/300], Step [174/225], Training Accuracy: 49.7665%, Training Loss: 1.0017%\n",
      "Epoch [20/300], Step [175/225], Training Accuracy: 49.7500%, Training Loss: 1.0017%\n",
      "Epoch [20/300], Step [176/225], Training Accuracy: 49.7603%, Training Loss: 1.0016%\n",
      "Epoch [20/300], Step [177/225], Training Accuracy: 49.7440%, Training Loss: 1.0014%\n",
      "Epoch [20/300], Step [178/225], Training Accuracy: 49.7015%, Training Loss: 1.0013%\n",
      "Epoch [20/300], Step [179/225], Training Accuracy: 49.7119%, Training Loss: 1.0013%\n",
      "Epoch [20/300], Step [180/225], Training Accuracy: 49.8177%, Training Loss: 1.0002%\n",
      "Epoch [20/300], Step [181/225], Training Accuracy: 49.7842%, Training Loss: 1.0009%\n",
      "Epoch [20/300], Step [182/225], Training Accuracy: 49.7596%, Training Loss: 1.0013%\n",
      "Epoch [20/300], Step [183/225], Training Accuracy: 49.7865%, Training Loss: 1.0008%\n",
      "Epoch [20/300], Step [184/225], Training Accuracy: 49.8047%, Training Loss: 1.0008%\n",
      "Epoch [20/300], Step [185/225], Training Accuracy: 49.7973%, Training Loss: 1.0009%\n",
      "Epoch [20/300], Step [186/225], Training Accuracy: 49.8404%, Training Loss: 1.0006%\n",
      "Epoch [20/300], Step [187/225], Training Accuracy: 49.8747%, Training Loss: 0.9999%\n",
      "Epoch [20/300], Step [188/225], Training Accuracy: 49.9335%, Training Loss: 0.9994%\n",
      "Epoch [20/300], Step [189/225], Training Accuracy: 49.9917%, Training Loss: 0.9990%\n",
      "Epoch [20/300], Step [190/225], Training Accuracy: 50.0000%, Training Loss: 0.9993%\n",
      "Epoch [20/300], Step [191/225], Training Accuracy: 49.9673%, Training Loss: 0.9994%\n",
      "Epoch [20/300], Step [192/225], Training Accuracy: 50.0163%, Training Loss: 0.9991%\n",
      "Epoch [20/300], Step [193/225], Training Accuracy: 49.9838%, Training Loss: 0.9991%\n",
      "Epoch [20/300], Step [194/225], Training Accuracy: 50.0242%, Training Loss: 0.9992%\n",
      "Epoch [20/300], Step [195/225], Training Accuracy: 50.0481%, Training Loss: 0.9987%\n",
      "Epoch [20/300], Step [196/225], Training Accuracy: 50.0877%, Training Loss: 0.9988%\n",
      "Epoch [20/300], Step [197/225], Training Accuracy: 50.0872%, Training Loss: 0.9982%\n",
      "Epoch [20/300], Step [198/225], Training Accuracy: 50.0947%, Training Loss: 0.9977%\n",
      "Epoch [20/300], Step [199/225], Training Accuracy: 50.0942%, Training Loss: 0.9974%\n",
      "Epoch [20/300], Step [200/225], Training Accuracy: 50.1172%, Training Loss: 0.9975%\n",
      "Epoch [20/300], Step [201/225], Training Accuracy: 50.1166%, Training Loss: 0.9978%\n",
      "Epoch [20/300], Step [202/225], Training Accuracy: 50.1238%, Training Loss: 0.9975%\n",
      "Epoch [20/300], Step [203/225], Training Accuracy: 50.0924%, Training Loss: 0.9978%\n",
      "Epoch [20/300], Step [204/225], Training Accuracy: 50.1225%, Training Loss: 0.9977%\n",
      "Epoch [20/300], Step [205/225], Training Accuracy: 50.1067%, Training Loss: 0.9976%\n",
      "Epoch [20/300], Step [206/225], Training Accuracy: 50.1441%, Training Loss: 0.9980%\n",
      "Epoch [20/300], Step [207/225], Training Accuracy: 50.1359%, Training Loss: 0.9984%\n",
      "Epoch [20/300], Step [208/225], Training Accuracy: 50.1878%, Training Loss: 0.9979%\n",
      "Epoch [20/300], Step [209/225], Training Accuracy: 50.1869%, Training Loss: 0.9981%\n",
      "Epoch [20/300], Step [210/225], Training Accuracy: 50.1786%, Training Loss: 0.9980%\n",
      "Epoch [20/300], Step [211/225], Training Accuracy: 50.1999%, Training Loss: 0.9978%\n",
      "Epoch [20/300], Step [212/225], Training Accuracy: 50.1695%, Training Loss: 0.9982%\n",
      "Epoch [20/300], Step [213/225], Training Accuracy: 50.1467%, Training Loss: 0.9988%\n",
      "Epoch [20/300], Step [214/225], Training Accuracy: 50.1387%, Training Loss: 0.9987%\n",
      "Epoch [20/300], Step [215/225], Training Accuracy: 50.1599%, Training Loss: 0.9986%\n",
      "Epoch [20/300], Step [216/225], Training Accuracy: 50.1664%, Training Loss: 0.9987%\n",
      "Epoch [20/300], Step [217/225], Training Accuracy: 50.1368%, Training Loss: 0.9989%\n",
      "Epoch [20/300], Step [218/225], Training Accuracy: 50.0860%, Training Loss: 0.9997%\n",
      "Epoch [20/300], Step [219/225], Training Accuracy: 50.0357%, Training Loss: 1.0000%\n",
      "Epoch [20/300], Step [220/225], Training Accuracy: 50.0923%, Training Loss: 0.9996%\n",
      "Epoch [20/300], Step [221/225], Training Accuracy: 50.0919%, Training Loss: 0.9998%\n",
      "Epoch [20/300], Step [222/225], Training Accuracy: 50.0985%, Training Loss: 0.9998%\n",
      "Epoch [20/300], Step [223/225], Training Accuracy: 50.0280%, Training Loss: 1.0004%\n",
      "Epoch [20/300], Step [224/225], Training Accuracy: 50.0070%, Training Loss: 1.0004%\n",
      "Epoch [20/300], Step [225/225], Training Accuracy: 50.0069%, Training Loss: 1.0006%\n",
      "Epoch [21/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8398%\n",
      "Epoch [21/300], Step [2/225], Training Accuracy: 53.9062%, Training Loss: 0.9897%\n",
      "Epoch [21/300], Step [3/225], Training Accuracy: 48.9583%, Training Loss: 1.0433%\n",
      "Epoch [21/300], Step [4/225], Training Accuracy: 48.0469%, Training Loss: 1.0325%\n",
      "Epoch [21/300], Step [5/225], Training Accuracy: 49.3750%, Training Loss: 1.0007%\n",
      "Epoch [21/300], Step [6/225], Training Accuracy: 48.4375%, Training Loss: 1.0231%\n",
      "Epoch [21/300], Step [7/225], Training Accuracy: 48.4375%, Training Loss: 1.0176%\n",
      "Epoch [21/300], Step [8/225], Training Accuracy: 49.4141%, Training Loss: 1.0225%\n",
      "Epoch [21/300], Step [9/225], Training Accuracy: 49.3056%, Training Loss: 1.0167%\n",
      "Epoch [21/300], Step [10/225], Training Accuracy: 49.6875%, Training Loss: 1.0113%\n",
      "Epoch [21/300], Step [11/225], Training Accuracy: 50.4261%, Training Loss: 1.0054%\n",
      "Epoch [21/300], Step [12/225], Training Accuracy: 50.3906%, Training Loss: 1.0112%\n",
      "Epoch [21/300], Step [13/225], Training Accuracy: 51.0817%, Training Loss: 1.0106%\n",
      "Epoch [21/300], Step [14/225], Training Accuracy: 50.7812%, Training Loss: 1.0151%\n",
      "Epoch [21/300], Step [15/225], Training Accuracy: 50.4167%, Training Loss: 1.0249%\n",
      "Epoch [21/300], Step [16/225], Training Accuracy: 50.4883%, Training Loss: 1.0263%\n",
      "Epoch [21/300], Step [17/225], Training Accuracy: 50.4596%, Training Loss: 1.0212%\n",
      "Epoch [21/300], Step [18/225], Training Accuracy: 50.4340%, Training Loss: 1.0199%\n",
      "Epoch [21/300], Step [19/225], Training Accuracy: 49.8355%, Training Loss: 1.0215%\n",
      "Epoch [21/300], Step [20/225], Training Accuracy: 50.0000%, Training Loss: 1.0187%\n",
      "Epoch [21/300], Step [21/225], Training Accuracy: 50.2232%, Training Loss: 1.0137%\n",
      "Epoch [21/300], Step [22/225], Training Accuracy: 50.1420%, Training Loss: 1.0147%\n",
      "Epoch [21/300], Step [23/225], Training Accuracy: 49.9321%, Training Loss: 1.0112%\n",
      "Epoch [21/300], Step [24/225], Training Accuracy: 49.8047%, Training Loss: 1.0148%\n",
      "Epoch [21/300], Step [25/225], Training Accuracy: 49.8750%, Training Loss: 1.0110%\n",
      "Epoch [21/300], Step [26/225], Training Accuracy: 49.5192%, Training Loss: 1.0131%\n",
      "Epoch [21/300], Step [27/225], Training Accuracy: 49.3056%, Training Loss: 1.0141%\n",
      "Epoch [21/300], Step [28/225], Training Accuracy: 49.4420%, Training Loss: 1.0111%\n",
      "Epoch [21/300], Step [29/225], Training Accuracy: 49.7306%, Training Loss: 1.0068%\n",
      "Epoch [21/300], Step [30/225], Training Accuracy: 49.6875%, Training Loss: 1.0050%\n",
      "Epoch [21/300], Step [31/225], Training Accuracy: 49.7984%, Training Loss: 1.0043%\n",
      "Epoch [21/300], Step [32/225], Training Accuracy: 49.9512%, Training Loss: 1.0019%\n",
      "Epoch [21/300], Step [33/225], Training Accuracy: 50.0947%, Training Loss: 0.9999%\n",
      "Epoch [21/300], Step [34/225], Training Accuracy: 50.0919%, Training Loss: 1.0027%\n",
      "Epoch [21/300], Step [35/225], Training Accuracy: 49.9554%, Training Loss: 1.0024%\n",
      "Epoch [21/300], Step [36/225], Training Accuracy: 49.9132%, Training Loss: 1.0024%\n",
      "Epoch [21/300], Step [37/225], Training Accuracy: 50.0845%, Training Loss: 0.9995%\n",
      "Epoch [21/300], Step [38/225], Training Accuracy: 50.2056%, Training Loss: 0.9975%\n",
      "Epoch [21/300], Step [39/225], Training Accuracy: 50.0801%, Training Loss: 0.9968%\n",
      "Epoch [21/300], Step [40/225], Training Accuracy: 50.0000%, Training Loss: 0.9969%\n",
      "Epoch [21/300], Step [41/225], Training Accuracy: 49.8476%, Training Loss: 0.9984%\n",
      "Epoch [21/300], Step [42/225], Training Accuracy: 49.9256%, Training Loss: 0.9967%\n",
      "Epoch [21/300], Step [43/225], Training Accuracy: 49.8547%, Training Loss: 0.9964%\n",
      "Epoch [21/300], Step [44/225], Training Accuracy: 49.9645%, Training Loss: 0.9947%\n",
      "Epoch [21/300], Step [45/225], Training Accuracy: 49.8958%, Training Loss: 0.9943%\n",
      "Epoch [21/300], Step [46/225], Training Accuracy: 50.1359%, Training Loss: 0.9908%\n",
      "Epoch [21/300], Step [47/225], Training Accuracy: 50.0997%, Training Loss: 0.9920%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Step [48/225], Training Accuracy: 50.1953%, Training Loss: 0.9926%\n",
      "Epoch [21/300], Step [49/225], Training Accuracy: 50.0000%, Training Loss: 0.9941%\n",
      "Epoch [21/300], Step [50/225], Training Accuracy: 50.1562%, Training Loss: 0.9926%\n",
      "Epoch [21/300], Step [51/225], Training Accuracy: 50.2145%, Training Loss: 0.9905%\n",
      "Epoch [21/300], Step [52/225], Training Accuracy: 50.3606%, Training Loss: 0.9886%\n",
      "Epoch [21/300], Step [53/225], Training Accuracy: 50.1769%, Training Loss: 0.9886%\n",
      "Epoch [21/300], Step [54/225], Training Accuracy: 49.9711%, Training Loss: 0.9903%\n",
      "Epoch [21/300], Step [55/225], Training Accuracy: 49.8295%, Training Loss: 0.9920%\n",
      "Epoch [21/300], Step [56/225], Training Accuracy: 49.8047%, Training Loss: 0.9932%\n",
      "Epoch [21/300], Step [57/225], Training Accuracy: 50.0548%, Training Loss: 0.9901%\n",
      "Epoch [21/300], Step [58/225], Training Accuracy: 50.0000%, Training Loss: 0.9906%\n",
      "Epoch [21/300], Step [59/225], Training Accuracy: 50.1059%, Training Loss: 0.9900%\n",
      "Epoch [21/300], Step [60/225], Training Accuracy: 50.0781%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [61/225], Training Accuracy: 50.2049%, Training Loss: 0.9901%\n",
      "Epoch [21/300], Step [62/225], Training Accuracy: 50.1512%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [63/225], Training Accuracy: 50.0248%, Training Loss: 0.9917%\n",
      "Epoch [21/300], Step [64/225], Training Accuracy: 50.0488%, Training Loss: 0.9915%\n",
      "Epoch [21/300], Step [65/225], Training Accuracy: 50.0240%, Training Loss: 0.9918%\n",
      "Epoch [21/300], Step [66/225], Training Accuracy: 50.2367%, Training Loss: 0.9900%\n",
      "Epoch [21/300], Step [67/225], Training Accuracy: 50.1166%, Training Loss: 0.9903%\n",
      "Epoch [21/300], Step [68/225], Training Accuracy: 50.0460%, Training Loss: 0.9901%\n",
      "Epoch [21/300], Step [69/225], Training Accuracy: 49.9547%, Training Loss: 0.9902%\n",
      "Epoch [21/300], Step [70/225], Training Accuracy: 49.8661%, Training Loss: 0.9914%\n",
      "Epoch [21/300], Step [71/225], Training Accuracy: 49.9560%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [72/225], Training Accuracy: 49.8698%, Training Loss: 0.9924%\n",
      "Epoch [21/300], Step [73/225], Training Accuracy: 49.8074%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [74/225], Training Accuracy: 49.8733%, Training Loss: 0.9932%\n",
      "Epoch [21/300], Step [75/225], Training Accuracy: 49.8333%, Training Loss: 0.9927%\n",
      "Epoch [21/300], Step [76/225], Training Accuracy: 49.8766%, Training Loss: 0.9925%\n",
      "Epoch [21/300], Step [77/225], Training Accuracy: 50.0000%, Training Loss: 0.9920%\n",
      "Epoch [21/300], Step [78/225], Training Accuracy: 49.9800%, Training Loss: 0.9933%\n",
      "Epoch [21/300], Step [79/225], Training Accuracy: 49.9407%, Training Loss: 0.9944%\n",
      "Epoch [21/300], Step [80/225], Training Accuracy: 49.8633%, Training Loss: 0.9948%\n",
      "Epoch [21/300], Step [81/225], Training Accuracy: 49.9228%, Training Loss: 0.9954%\n",
      "Epoch [21/300], Step [82/225], Training Accuracy: 49.9809%, Training Loss: 0.9945%\n",
      "Epoch [21/300], Step [83/225], Training Accuracy: 49.9247%, Training Loss: 0.9941%\n",
      "Epoch [21/300], Step [84/225], Training Accuracy: 49.9256%, Training Loss: 0.9947%\n",
      "Epoch [21/300], Step [85/225], Training Accuracy: 50.0000%, Training Loss: 0.9938%\n",
      "Epoch [21/300], Step [86/225], Training Accuracy: 50.1272%, Training Loss: 0.9938%\n",
      "Epoch [21/300], Step [87/225], Training Accuracy: 50.1616%, Training Loss: 0.9937%\n",
      "Epoch [21/300], Step [88/225], Training Accuracy: 50.0888%, Training Loss: 0.9940%\n",
      "Epoch [21/300], Step [89/225], Training Accuracy: 50.0878%, Training Loss: 0.9955%\n",
      "Epoch [21/300], Step [90/225], Training Accuracy: 49.9653%, Training Loss: 0.9962%\n",
      "Epoch [21/300], Step [91/225], Training Accuracy: 50.0515%, Training Loss: 0.9944%\n",
      "Epoch [21/300], Step [92/225], Training Accuracy: 50.0170%, Training Loss: 0.9944%\n",
      "Epoch [21/300], Step [93/225], Training Accuracy: 50.0168%, Training Loss: 0.9945%\n",
      "Epoch [21/300], Step [94/225], Training Accuracy: 50.1164%, Training Loss: 0.9931%\n",
      "Epoch [21/300], Step [95/225], Training Accuracy: 50.0658%, Training Loss: 0.9938%\n",
      "Epoch [21/300], Step [96/225], Training Accuracy: 50.0814%, Training Loss: 0.9932%\n",
      "Epoch [21/300], Step [97/225], Training Accuracy: 50.1289%, Training Loss: 0.9924%\n",
      "Epoch [21/300], Step [98/225], Training Accuracy: 50.1594%, Training Loss: 0.9918%\n",
      "Epoch [21/300], Step [99/225], Training Accuracy: 50.2210%, Training Loss: 0.9917%\n",
      "Epoch [21/300], Step [100/225], Training Accuracy: 50.0781%, Training Loss: 0.9923%\n",
      "Epoch [21/300], Step [101/225], Training Accuracy: 50.0774%, Training Loss: 0.9928%\n",
      "Epoch [21/300], Step [102/225], Training Accuracy: 50.0306%, Training Loss: 0.9939%\n",
      "Epoch [21/300], Step [103/225], Training Accuracy: 49.9242%, Training Loss: 0.9953%\n",
      "Epoch [21/300], Step [104/225], Training Accuracy: 49.9399%, Training Loss: 0.9950%\n",
      "Epoch [21/300], Step [105/225], Training Accuracy: 49.8958%, Training Loss: 0.9942%\n",
      "Epoch [21/300], Step [106/225], Training Accuracy: 49.8231%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [107/225], Training Accuracy: 49.8978%, Training Loss: 0.9949%\n",
      "Epoch [21/300], Step [108/225], Training Accuracy: 49.9132%, Training Loss: 0.9956%\n",
      "Epoch [21/300], Step [109/225], Training Accuracy: 49.9140%, Training Loss: 0.9955%\n",
      "Epoch [21/300], Step [110/225], Training Accuracy: 49.9574%, Training Loss: 0.9952%\n",
      "Epoch [21/300], Step [111/225], Training Accuracy: 50.0704%, Training Loss: 0.9957%\n",
      "Epoch [21/300], Step [112/225], Training Accuracy: 50.1256%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [113/225], Training Accuracy: 50.0968%, Training Loss: 0.9962%\n",
      "Epoch [21/300], Step [114/225], Training Accuracy: 50.0548%, Training Loss: 0.9960%\n",
      "Epoch [21/300], Step [115/225], Training Accuracy: 50.1087%, Training Loss: 0.9953%\n",
      "Epoch [21/300], Step [116/225], Training Accuracy: 50.1482%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [117/225], Training Accuracy: 50.0668%, Training Loss: 0.9965%\n",
      "Epoch [21/300], Step [118/225], Training Accuracy: 50.0927%, Training Loss: 0.9965%\n",
      "Epoch [21/300], Step [119/225], Training Accuracy: 50.0657%, Training Loss: 0.9969%\n",
      "Epoch [21/300], Step [120/225], Training Accuracy: 50.1302%, Training Loss: 0.9961%\n",
      "Epoch [21/300], Step [121/225], Training Accuracy: 50.0775%, Training Loss: 0.9963%\n",
      "Epoch [21/300], Step [122/225], Training Accuracy: 50.0897%, Training Loss: 0.9962%\n",
      "Epoch [21/300], Step [123/225], Training Accuracy: 50.0381%, Training Loss: 0.9961%\n",
      "Epoch [21/300], Step [124/225], Training Accuracy: 50.0630%, Training Loss: 0.9959%\n",
      "Epoch [21/300], Step [125/225], Training Accuracy: 50.0250%, Training Loss: 0.9969%\n",
      "Epoch [21/300], Step [126/225], Training Accuracy: 49.9752%, Training Loss: 0.9976%\n",
      "Epoch [21/300], Step [127/225], Training Accuracy: 49.9508%, Training Loss: 0.9975%\n",
      "Epoch [21/300], Step [128/225], Training Accuracy: 49.9146%, Training Loss: 0.9978%\n",
      "Epoch [21/300], Step [129/225], Training Accuracy: 49.9031%, Training Loss: 0.9981%\n",
      "Epoch [21/300], Step [130/225], Training Accuracy: 49.8798%, Training Loss: 0.9987%\n",
      "Epoch [21/300], Step [131/225], Training Accuracy: 49.8927%, Training Loss: 0.9984%\n",
      "Epoch [21/300], Step [132/225], Training Accuracy: 49.8935%, Training Loss: 0.9988%\n",
      "Epoch [21/300], Step [133/225], Training Accuracy: 49.9765%, Training Loss: 0.9981%\n",
      "Epoch [21/300], Step [134/225], Training Accuracy: 49.9883%, Training Loss: 0.9977%\n",
      "Epoch [21/300], Step [135/225], Training Accuracy: 50.0000%, Training Loss: 0.9974%\n",
      "Epoch [21/300], Step [136/225], Training Accuracy: 49.9770%, Training Loss: 0.9968%\n",
      "Epoch [21/300], Step [137/225], Training Accuracy: 49.9658%, Training Loss: 0.9965%\n",
      "Epoch [21/300], Step [138/225], Training Accuracy: 50.0226%, Training Loss: 0.9956%\n",
      "Epoch [21/300], Step [139/225], Training Accuracy: 50.0337%, Training Loss: 0.9960%\n",
      "Epoch [21/300], Step [140/225], Training Accuracy: 50.1116%, Training Loss: 0.9957%\n",
      "Epoch [21/300], Step [141/225], Training Accuracy: 50.1551%, Training Loss: 0.9957%\n",
      "Epoch [21/300], Step [142/225], Training Accuracy: 50.1540%, Training Loss: 0.9958%\n",
      "Epoch [21/300], Step [143/225], Training Accuracy: 50.1967%, Training Loss: 0.9953%\n",
      "Epoch [21/300], Step [144/225], Training Accuracy: 50.1736%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [145/225], Training Accuracy: 50.2371%, Training Loss: 0.9943%\n",
      "Epoch [21/300], Step [146/225], Training Accuracy: 50.2568%, Training Loss: 0.9943%\n",
      "Epoch [21/300], Step [147/225], Training Accuracy: 50.2020%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [148/225], Training Accuracy: 50.2851%, Training Loss: 0.9948%\n",
      "Epoch [21/300], Step [149/225], Training Accuracy: 50.2517%, Training Loss: 0.9951%\n",
      "Epoch [21/300], Step [150/225], Training Accuracy: 50.2188%, Training Loss: 0.9956%\n",
      "Epoch [21/300], Step [151/225], Training Accuracy: 50.2380%, Training Loss: 0.9949%\n",
      "Epoch [21/300], Step [152/225], Training Accuracy: 50.1953%, Training Loss: 0.9950%\n",
      "Epoch [21/300], Step [153/225], Training Accuracy: 50.2145%, Training Loss: 0.9944%\n",
      "Epoch [21/300], Step [154/225], Training Accuracy: 50.1826%, Training Loss: 0.9941%\n",
      "Epoch [21/300], Step [155/225], Training Accuracy: 50.1109%, Training Loss: 0.9949%\n",
      "Epoch [21/300], Step [156/225], Training Accuracy: 50.0901%, Training Loss: 0.9949%\n",
      "Epoch [21/300], Step [157/225], Training Accuracy: 50.0796%, Training Loss: 0.9954%\n",
      "Epoch [21/300], Step [158/225], Training Accuracy: 50.0989%, Training Loss: 0.9955%\n",
      "Epoch [21/300], Step [159/225], Training Accuracy: 50.0786%, Training Loss: 0.9952%\n",
      "Epoch [21/300], Step [160/225], Training Accuracy: 50.0488%, Training Loss: 0.9952%\n",
      "Epoch [21/300], Step [161/225], Training Accuracy: 50.1359%, Training Loss: 0.9942%\n",
      "Epoch [21/300], Step [162/225], Training Accuracy: 50.1350%, Training Loss: 0.9944%\n",
      "Epoch [21/300], Step [163/225], Training Accuracy: 50.0959%, Training Loss: 0.9941%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Step [164/225], Training Accuracy: 50.1620%, Training Loss: 0.9931%\n",
      "Epoch [21/300], Step [165/225], Training Accuracy: 50.1799%, Training Loss: 0.9930%\n",
      "Epoch [21/300], Step [166/225], Training Accuracy: 50.1788%, Training Loss: 0.9931%\n",
      "Epoch [21/300], Step [167/225], Training Accuracy: 50.1965%, Training Loss: 0.9924%\n",
      "Epoch [21/300], Step [168/225], Training Accuracy: 50.1860%, Training Loss: 0.9927%\n",
      "Epoch [21/300], Step [169/225], Training Accuracy: 50.1849%, Training Loss: 0.9926%\n",
      "Epoch [21/300], Step [170/225], Training Accuracy: 50.1746%, Training Loss: 0.9932%\n",
      "Epoch [21/300], Step [171/225], Training Accuracy: 50.1645%, Training Loss: 0.9931%\n",
      "Epoch [21/300], Step [172/225], Training Accuracy: 50.0999%, Training Loss: 0.9937%\n",
      "Epoch [21/300], Step [173/225], Training Accuracy: 50.0993%, Training Loss: 0.9934%\n",
      "Epoch [21/300], Step [174/225], Training Accuracy: 50.1167%, Training Loss: 0.9933%\n",
      "Epoch [21/300], Step [175/225], Training Accuracy: 50.0982%, Training Loss: 0.9934%\n",
      "Epoch [21/300], Step [176/225], Training Accuracy: 50.1243%, Training Loss: 0.9932%\n",
      "Epoch [21/300], Step [177/225], Training Accuracy: 50.1059%, Training Loss: 0.9930%\n",
      "Epoch [21/300], Step [178/225], Training Accuracy: 50.0614%, Training Loss: 0.9929%\n",
      "Epoch [21/300], Step [179/225], Training Accuracy: 50.0698%, Training Loss: 0.9930%\n",
      "Epoch [21/300], Step [180/225], Training Accuracy: 50.1823%, Training Loss: 0.9919%\n",
      "Epoch [21/300], Step [181/225], Training Accuracy: 50.1640%, Training Loss: 0.9926%\n",
      "Epoch [21/300], Step [182/225], Training Accuracy: 50.1459%, Training Loss: 0.9930%\n",
      "Epoch [21/300], Step [183/225], Training Accuracy: 50.1708%, Training Loss: 0.9925%\n",
      "Epoch [21/300], Step [184/225], Training Accuracy: 50.1783%, Training Loss: 0.9925%\n",
      "Epoch [21/300], Step [185/225], Training Accuracy: 50.1605%, Training Loss: 0.9926%\n",
      "Epoch [21/300], Step [186/225], Training Accuracy: 50.2016%, Training Loss: 0.9924%\n",
      "Epoch [21/300], Step [187/225], Training Accuracy: 50.2423%, Training Loss: 0.9916%\n",
      "Epoch [21/300], Step [188/225], Training Accuracy: 50.2909%, Training Loss: 0.9911%\n",
      "Epoch [21/300], Step [189/225], Training Accuracy: 50.3472%, Training Loss: 0.9906%\n",
      "Epoch [21/300], Step [190/225], Training Accuracy: 50.3454%, Training Loss: 0.9910%\n",
      "Epoch [21/300], Step [191/225], Training Accuracy: 50.3190%, Training Loss: 0.9911%\n",
      "Epoch [21/300], Step [192/225], Training Accuracy: 50.3581%, Training Loss: 0.9908%\n",
      "Epoch [21/300], Step [193/225], Training Accuracy: 50.3238%, Training Loss: 0.9908%\n",
      "Epoch [21/300], Step [194/225], Training Accuracy: 50.3624%, Training Loss: 0.9909%\n",
      "Epoch [21/300], Step [195/225], Training Accuracy: 50.3846%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [196/225], Training Accuracy: 50.4305%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [197/225], Training Accuracy: 50.4283%, Training Loss: 0.9899%\n",
      "Epoch [21/300], Step [198/225], Training Accuracy: 50.4261%, Training Loss: 0.9893%\n",
      "Epoch [21/300], Step [199/225], Training Accuracy: 50.4161%, Training Loss: 0.9890%\n",
      "Epoch [21/300], Step [200/225], Training Accuracy: 50.4297%, Training Loss: 0.9891%\n",
      "Epoch [21/300], Step [201/225], Training Accuracy: 50.4353%, Training Loss: 0.9894%\n",
      "Epoch [21/300], Step [202/225], Training Accuracy: 50.4332%, Training Loss: 0.9891%\n",
      "Epoch [21/300], Step [203/225], Training Accuracy: 50.4079%, Training Loss: 0.9894%\n",
      "Epoch [21/300], Step [204/225], Training Accuracy: 50.4366%, Training Loss: 0.9893%\n",
      "Epoch [21/300], Step [205/225], Training Accuracy: 50.4192%, Training Loss: 0.9892%\n",
      "Epoch [21/300], Step [206/225], Training Accuracy: 50.4551%, Training Loss: 0.9896%\n",
      "Epoch [21/300], Step [207/225], Training Accuracy: 50.4454%, Training Loss: 0.9900%\n",
      "Epoch [21/300], Step [208/225], Training Accuracy: 50.4883%, Training Loss: 0.9895%\n",
      "Epoch [21/300], Step [209/225], Training Accuracy: 50.4785%, Training Loss: 0.9897%\n",
      "Epoch [21/300], Step [210/225], Training Accuracy: 50.4539%, Training Loss: 0.9896%\n",
      "Epoch [21/300], Step [211/225], Training Accuracy: 50.4961%, Training Loss: 0.9894%\n",
      "Epoch [21/300], Step [212/225], Training Accuracy: 50.4717%, Training Loss: 0.9898%\n",
      "Epoch [21/300], Step [213/225], Training Accuracy: 50.4475%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [214/225], Training Accuracy: 50.4308%, Training Loss: 0.9903%\n",
      "Epoch [21/300], Step [215/225], Training Accuracy: 50.4506%, Training Loss: 0.9902%\n",
      "Epoch [21/300], Step [216/225], Training Accuracy: 50.4557%, Training Loss: 0.9904%\n",
      "Epoch [21/300], Step [217/225], Training Accuracy: 50.4320%, Training Loss: 0.9905%\n",
      "Epoch [21/300], Step [218/225], Training Accuracy: 50.3942%, Training Loss: 0.9913%\n",
      "Epoch [21/300], Step [219/225], Training Accuracy: 50.3496%, Training Loss: 0.9916%\n",
      "Epoch [21/300], Step [220/225], Training Accuracy: 50.4048%, Training Loss: 0.9912%\n",
      "Epoch [21/300], Step [221/225], Training Accuracy: 50.4101%, Training Loss: 0.9914%\n",
      "Epoch [21/300], Step [222/225], Training Accuracy: 50.4153%, Training Loss: 0.9914%\n",
      "Epoch [21/300], Step [223/225], Training Accuracy: 50.3363%, Training Loss: 0.9920%\n",
      "Epoch [21/300], Step [224/225], Training Accuracy: 50.3139%, Training Loss: 0.9920%\n",
      "Epoch [21/300], Step [225/225], Training Accuracy: 50.3127%, Training Loss: 0.9921%\n",
      "Epoch [22/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8281%\n",
      "Epoch [22/300], Step [2/225], Training Accuracy: 54.6875%, Training Loss: 0.9858%\n",
      "Epoch [22/300], Step [3/225], Training Accuracy: 51.0417%, Training Loss: 1.0391%\n",
      "Epoch [22/300], Step [4/225], Training Accuracy: 50.0000%, Training Loss: 1.0275%\n",
      "Epoch [22/300], Step [5/225], Training Accuracy: 50.9375%, Training Loss: 0.9946%\n",
      "Epoch [22/300], Step [6/225], Training Accuracy: 50.0000%, Training Loss: 1.0167%\n",
      "Epoch [22/300], Step [7/225], Training Accuracy: 50.2232%, Training Loss: 1.0109%\n",
      "Epoch [22/300], Step [8/225], Training Accuracy: 50.5859%, Training Loss: 1.0163%\n",
      "Epoch [22/300], Step [9/225], Training Accuracy: 50.5208%, Training Loss: 1.0103%\n",
      "Epoch [22/300], Step [10/225], Training Accuracy: 50.6250%, Training Loss: 1.0054%\n",
      "Epoch [22/300], Step [11/225], Training Accuracy: 51.2784%, Training Loss: 0.9997%\n",
      "Epoch [22/300], Step [12/225], Training Accuracy: 51.1719%, Training Loss: 1.0056%\n",
      "Epoch [22/300], Step [13/225], Training Accuracy: 51.8029%, Training Loss: 1.0045%\n",
      "Epoch [22/300], Step [14/225], Training Accuracy: 51.5625%, Training Loss: 1.0082%\n",
      "Epoch [22/300], Step [15/225], Training Accuracy: 51.0417%, Training Loss: 1.0184%\n",
      "Epoch [22/300], Step [16/225], Training Accuracy: 51.0742%, Training Loss: 1.0198%\n",
      "Epoch [22/300], Step [17/225], Training Accuracy: 50.9191%, Training Loss: 1.0148%\n",
      "Epoch [22/300], Step [18/225], Training Accuracy: 50.8681%, Training Loss: 1.0137%\n",
      "Epoch [22/300], Step [19/225], Training Accuracy: 50.1645%, Training Loss: 1.0153%\n",
      "Epoch [22/300], Step [20/225], Training Accuracy: 50.3906%, Training Loss: 1.0126%\n",
      "Epoch [22/300], Step [21/225], Training Accuracy: 50.6696%, Training Loss: 1.0077%\n",
      "Epoch [22/300], Step [22/225], Training Accuracy: 50.5682%, Training Loss: 1.0085%\n",
      "Epoch [22/300], Step [23/225], Training Accuracy: 50.3397%, Training Loss: 1.0050%\n",
      "Epoch [22/300], Step [24/225], Training Accuracy: 50.1953%, Training Loss: 1.0082%\n",
      "Epoch [22/300], Step [25/225], Training Accuracy: 50.2500%, Training Loss: 1.0043%\n",
      "Epoch [22/300], Step [26/225], Training Accuracy: 49.8798%, Training Loss: 1.0063%\n",
      "Epoch [22/300], Step [27/225], Training Accuracy: 49.7106%, Training Loss: 1.0075%\n",
      "Epoch [22/300], Step [28/225], Training Accuracy: 49.8326%, Training Loss: 1.0044%\n",
      "Epoch [22/300], Step [29/225], Training Accuracy: 50.1078%, Training Loss: 1.0000%\n",
      "Epoch [22/300], Step [30/225], Training Accuracy: 50.0521%, Training Loss: 0.9981%\n",
      "Epoch [22/300], Step [31/225], Training Accuracy: 50.1512%, Training Loss: 0.9974%\n",
      "Epoch [22/300], Step [32/225], Training Accuracy: 50.2930%, Training Loss: 0.9948%\n",
      "Epoch [22/300], Step [33/225], Training Accuracy: 50.4261%, Training Loss: 0.9928%\n",
      "Epoch [22/300], Step [34/225], Training Accuracy: 50.4596%, Training Loss: 0.9955%\n",
      "Epoch [22/300], Step [35/225], Training Accuracy: 50.3125%, Training Loss: 0.9952%\n",
      "Epoch [22/300], Step [36/225], Training Accuracy: 50.3038%, Training Loss: 0.9952%\n",
      "Epoch [22/300], Step [37/225], Training Accuracy: 50.5068%, Training Loss: 0.9923%\n",
      "Epoch [22/300], Step [38/225], Training Accuracy: 50.6579%, Training Loss: 0.9903%\n",
      "Epoch [22/300], Step [39/225], Training Accuracy: 50.5609%, Training Loss: 0.9896%\n",
      "Epoch [22/300], Step [40/225], Training Accuracy: 50.4688%, Training Loss: 0.9897%\n",
      "Epoch [22/300], Step [41/225], Training Accuracy: 50.3430%, Training Loss: 0.9913%\n",
      "Epoch [22/300], Step [42/225], Training Accuracy: 50.4464%, Training Loss: 0.9896%\n",
      "Epoch [22/300], Step [43/225], Training Accuracy: 50.3997%, Training Loss: 0.9892%\n",
      "Epoch [22/300], Step [44/225], Training Accuracy: 50.4972%, Training Loss: 0.9875%\n",
      "Epoch [22/300], Step [45/225], Training Accuracy: 50.4167%, Training Loss: 0.9872%\n",
      "Epoch [22/300], Step [46/225], Training Accuracy: 50.6454%, Training Loss: 0.9837%\n",
      "Epoch [22/300], Step [47/225], Training Accuracy: 50.5984%, Training Loss: 0.9850%\n",
      "Epoch [22/300], Step [48/225], Training Accuracy: 50.6836%, Training Loss: 0.9857%\n",
      "Epoch [22/300], Step [49/225], Training Accuracy: 50.5102%, Training Loss: 0.9873%\n",
      "Epoch [22/300], Step [50/225], Training Accuracy: 50.6562%, Training Loss: 0.9858%\n",
      "Epoch [22/300], Step [51/225], Training Accuracy: 50.7353%, Training Loss: 0.9834%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Step [52/225], Training Accuracy: 50.8413%, Training Loss: 0.9814%\n",
      "Epoch [22/300], Step [53/225], Training Accuracy: 50.6781%, Training Loss: 0.9814%\n",
      "Epoch [22/300], Step [54/225], Training Accuracy: 50.4340%, Training Loss: 0.9832%\n",
      "Epoch [22/300], Step [55/225], Training Accuracy: 50.2841%, Training Loss: 0.9850%\n",
      "Epoch [22/300], Step [56/225], Training Accuracy: 50.2511%, Training Loss: 0.9861%\n",
      "Epoch [22/300], Step [57/225], Training Accuracy: 50.4934%, Training Loss: 0.9830%\n",
      "Epoch [22/300], Step [58/225], Training Accuracy: 50.4310%, Training Loss: 0.9834%\n",
      "Epoch [22/300], Step [59/225], Training Accuracy: 50.5561%, Training Loss: 0.9828%\n",
      "Epoch [22/300], Step [60/225], Training Accuracy: 50.5208%, Training Loss: 0.9830%\n",
      "Epoch [22/300], Step [61/225], Training Accuracy: 50.6404%, Training Loss: 0.9826%\n",
      "Epoch [22/300], Step [62/225], Training Accuracy: 50.5796%, Training Loss: 0.9831%\n",
      "Epoch [22/300], Step [63/225], Training Accuracy: 50.4464%, Training Loss: 0.9843%\n",
      "Epoch [22/300], Step [64/225], Training Accuracy: 50.4883%, Training Loss: 0.9840%\n",
      "Epoch [22/300], Step [65/225], Training Accuracy: 50.4327%, Training Loss: 0.9843%\n",
      "Epoch [22/300], Step [66/225], Training Accuracy: 50.6392%, Training Loss: 0.9824%\n",
      "Epoch [22/300], Step [67/225], Training Accuracy: 50.5131%, Training Loss: 0.9828%\n",
      "Epoch [22/300], Step [68/225], Training Accuracy: 50.5055%, Training Loss: 0.9825%\n",
      "Epoch [22/300], Step [69/225], Training Accuracy: 50.4303%, Training Loss: 0.9826%\n",
      "Epoch [22/300], Step [70/225], Training Accuracy: 50.3348%, Training Loss: 0.9838%\n",
      "Epoch [22/300], Step [71/225], Training Accuracy: 50.3961%, Training Loss: 0.9828%\n",
      "Epoch [22/300], Step [72/225], Training Accuracy: 50.3038%, Training Loss: 0.9847%\n",
      "Epoch [22/300], Step [73/225], Training Accuracy: 50.2568%, Training Loss: 0.9874%\n",
      "Epoch [22/300], Step [74/225], Training Accuracy: 50.3167%, Training Loss: 0.9855%\n",
      "Epoch [22/300], Step [75/225], Training Accuracy: 50.2708%, Training Loss: 0.9851%\n",
      "Epoch [22/300], Step [76/225], Training Accuracy: 50.3289%, Training Loss: 0.9848%\n",
      "Epoch [22/300], Step [77/225], Training Accuracy: 50.4870%, Training Loss: 0.9843%\n",
      "Epoch [22/300], Step [78/225], Training Accuracy: 50.4407%, Training Loss: 0.9856%\n",
      "Epoch [22/300], Step [79/225], Training Accuracy: 50.3956%, Training Loss: 0.9867%\n",
      "Epoch [22/300], Step [80/225], Training Accuracy: 50.3125%, Training Loss: 0.9871%\n",
      "Epoch [22/300], Step [81/225], Training Accuracy: 50.4244%, Training Loss: 0.9877%\n",
      "Epoch [22/300], Step [82/225], Training Accuracy: 50.4764%, Training Loss: 0.9868%\n",
      "Epoch [22/300], Step [83/225], Training Accuracy: 50.4142%, Training Loss: 0.9863%\n",
      "Epoch [22/300], Step [84/225], Training Accuracy: 50.3906%, Training Loss: 0.9869%\n",
      "Epoch [22/300], Step [85/225], Training Accuracy: 50.4228%, Training Loss: 0.9859%\n",
      "Epoch [22/300], Step [86/225], Training Accuracy: 50.5269%, Training Loss: 0.9858%\n",
      "Epoch [22/300], Step [87/225], Training Accuracy: 50.5568%, Training Loss: 0.9857%\n",
      "Epoch [22/300], Step [88/225], Training Accuracy: 50.4439%, Training Loss: 0.9861%\n",
      "Epoch [22/300], Step [89/225], Training Accuracy: 50.4389%, Training Loss: 0.9875%\n",
      "Epoch [22/300], Step [90/225], Training Accuracy: 50.3125%, Training Loss: 0.9883%\n",
      "Epoch [22/300], Step [91/225], Training Accuracy: 50.3777%, Training Loss: 0.9865%\n",
      "Epoch [22/300], Step [92/225], Training Accuracy: 50.3567%, Training Loss: 0.9865%\n",
      "Epoch [22/300], Step [93/225], Training Accuracy: 50.3528%, Training Loss: 0.9866%\n",
      "Epoch [22/300], Step [94/225], Training Accuracy: 50.4654%, Training Loss: 0.9852%\n",
      "Epoch [22/300], Step [95/225], Training Accuracy: 50.4441%, Training Loss: 0.9858%\n",
      "Epoch [22/300], Step [96/225], Training Accuracy: 50.4883%, Training Loss: 0.9852%\n",
      "Epoch [22/300], Step [97/225], Training Accuracy: 50.5477%, Training Loss: 0.9844%\n",
      "Epoch [22/300], Step [98/225], Training Accuracy: 50.5740%, Training Loss: 0.9838%\n",
      "Epoch [22/300], Step [99/225], Training Accuracy: 50.6155%, Training Loss: 0.9837%\n",
      "Epoch [22/300], Step [100/225], Training Accuracy: 50.4688%, Training Loss: 0.9843%\n",
      "Epoch [22/300], Step [101/225], Training Accuracy: 50.4486%, Training Loss: 0.9848%\n",
      "Epoch [22/300], Step [102/225], Training Accuracy: 50.3983%, Training Loss: 0.9859%\n",
      "Epoch [22/300], Step [103/225], Training Accuracy: 50.2882%, Training Loss: 0.9875%\n",
      "Epoch [22/300], Step [104/225], Training Accuracy: 50.3155%, Training Loss: 0.9871%\n",
      "Epoch [22/300], Step [105/225], Training Accuracy: 50.2530%, Training Loss: 0.9864%\n",
      "Epoch [22/300], Step [106/225], Training Accuracy: 50.1916%, Training Loss: 0.9873%\n",
      "Epoch [22/300], Step [107/225], Training Accuracy: 50.2629%, Training Loss: 0.9871%\n",
      "Epoch [22/300], Step [108/225], Training Accuracy: 50.2749%, Training Loss: 0.9878%\n",
      "Epoch [22/300], Step [109/225], Training Accuracy: 50.2724%, Training Loss: 0.9877%\n",
      "Epoch [22/300], Step [110/225], Training Accuracy: 50.3125%, Training Loss: 0.9874%\n",
      "Epoch [22/300], Step [111/225], Training Accuracy: 50.4364%, Training Loss: 0.9879%\n",
      "Epoch [22/300], Step [112/225], Training Accuracy: 50.5022%, Training Loss: 0.9872%\n",
      "Epoch [22/300], Step [113/225], Training Accuracy: 50.4701%, Training Loss: 0.9883%\n",
      "Epoch [22/300], Step [114/225], Training Accuracy: 50.4660%, Training Loss: 0.9881%\n",
      "Epoch [22/300], Step [115/225], Training Accuracy: 50.5435%, Training Loss: 0.9873%\n",
      "Epoch [22/300], Step [116/225], Training Accuracy: 50.5792%, Training Loss: 0.9872%\n",
      "Epoch [22/300], Step [117/225], Training Accuracy: 50.4808%, Training Loss: 0.9886%\n",
      "Epoch [22/300], Step [118/225], Training Accuracy: 50.5032%, Training Loss: 0.9887%\n",
      "Epoch [22/300], Step [119/225], Training Accuracy: 50.4727%, Training Loss: 0.9891%\n",
      "Epoch [22/300], Step [120/225], Training Accuracy: 50.5339%, Training Loss: 0.9883%\n",
      "Epoch [22/300], Step [121/225], Training Accuracy: 50.4907%, Training Loss: 0.9886%\n",
      "Epoch [22/300], Step [122/225], Training Accuracy: 50.5123%, Training Loss: 0.9885%\n",
      "Epoch [22/300], Step [123/225], Training Accuracy: 50.4700%, Training Loss: 0.9884%\n",
      "Epoch [22/300], Step [124/225], Training Accuracy: 50.5040%, Training Loss: 0.9882%\n",
      "Epoch [22/300], Step [125/225], Training Accuracy: 50.4500%, Training Loss: 0.9892%\n",
      "Epoch [22/300], Step [126/225], Training Accuracy: 50.3968%, Training Loss: 0.9899%\n",
      "Epoch [22/300], Step [127/225], Training Accuracy: 50.3691%, Training Loss: 0.9898%\n",
      "Epoch [22/300], Step [128/225], Training Accuracy: 50.3418%, Training Loss: 0.9901%\n",
      "Epoch [22/300], Step [129/225], Training Accuracy: 50.3391%, Training Loss: 0.9904%\n",
      "Epoch [22/300], Step [130/225], Training Accuracy: 50.3245%, Training Loss: 0.9910%\n",
      "Epoch [22/300], Step [131/225], Training Accuracy: 50.3220%, Training Loss: 0.9907%\n",
      "Epoch [22/300], Step [132/225], Training Accuracy: 50.3196%, Training Loss: 0.9911%\n",
      "Epoch [22/300], Step [133/225], Training Accuracy: 50.3994%, Training Loss: 0.9904%\n",
      "Epoch [22/300], Step [134/225], Training Accuracy: 50.4314%, Training Loss: 0.9900%\n",
      "Epoch [22/300], Step [135/225], Training Accuracy: 50.4398%, Training Loss: 0.9897%\n",
      "Epoch [22/300], Step [136/225], Training Accuracy: 50.4136%, Training Loss: 0.9891%\n",
      "Epoch [22/300], Step [137/225], Training Accuracy: 50.4220%, Training Loss: 0.9887%\n",
      "Epoch [22/300], Step [138/225], Training Accuracy: 50.4755%, Training Loss: 0.9878%\n",
      "Epoch [22/300], Step [139/225], Training Accuracy: 50.4834%, Training Loss: 0.9881%\n",
      "Epoch [22/300], Step [140/225], Training Accuracy: 50.5580%, Training Loss: 0.9878%\n",
      "Epoch [22/300], Step [141/225], Training Accuracy: 50.5984%, Training Loss: 0.9878%\n",
      "Epoch [22/300], Step [142/225], Training Accuracy: 50.5942%, Training Loss: 0.9879%\n",
      "Epoch [22/300], Step [143/225], Training Accuracy: 50.6337%, Training Loss: 0.9875%\n",
      "Epoch [22/300], Step [144/225], Training Accuracy: 50.6076%, Training Loss: 0.9873%\n",
      "Epoch [22/300], Step [145/225], Training Accuracy: 50.6897%, Training Loss: 0.9864%\n",
      "Epoch [22/300], Step [146/225], Training Accuracy: 50.7063%, Training Loss: 0.9864%\n",
      "Epoch [22/300], Step [147/225], Training Accuracy: 50.6484%, Training Loss: 0.9872%\n",
      "Epoch [22/300], Step [148/225], Training Accuracy: 50.7390%, Training Loss: 0.9869%\n",
      "Epoch [22/300], Step [149/225], Training Accuracy: 50.7236%, Training Loss: 0.9872%\n",
      "Epoch [22/300], Step [150/225], Training Accuracy: 50.6979%, Training Loss: 0.9877%\n",
      "Epoch [22/300], Step [151/225], Training Accuracy: 50.7140%, Training Loss: 0.9870%\n",
      "Epoch [22/300], Step [152/225], Training Accuracy: 50.6682%, Training Loss: 0.9871%\n",
      "Epoch [22/300], Step [153/225], Training Accuracy: 50.7047%, Training Loss: 0.9865%\n",
      "Epoch [22/300], Step [154/225], Training Accuracy: 50.6899%, Training Loss: 0.9861%\n",
      "Epoch [22/300], Step [155/225], Training Accuracy: 50.6149%, Training Loss: 0.9870%\n",
      "Epoch [22/300], Step [156/225], Training Accuracy: 50.5909%, Training Loss: 0.9870%\n",
      "Epoch [22/300], Step [157/225], Training Accuracy: 50.5872%, Training Loss: 0.9874%\n",
      "Epoch [22/300], Step [158/225], Training Accuracy: 50.6032%, Training Loss: 0.9875%\n",
      "Epoch [22/300], Step [159/225], Training Accuracy: 50.5994%, Training Loss: 0.9872%\n",
      "Epoch [22/300], Step [160/225], Training Accuracy: 50.5664%, Training Loss: 0.9873%\n",
      "Epoch [22/300], Step [161/225], Training Accuracy: 50.6502%, Training Loss: 0.9862%\n",
      "Epoch [22/300], Step [162/225], Training Accuracy: 50.6559%, Training Loss: 0.9864%\n",
      "Epoch [22/300], Step [163/225], Training Accuracy: 50.5943%, Training Loss: 0.9861%\n",
      "Epoch [22/300], Step [164/225], Training Accuracy: 50.6479%, Training Loss: 0.9852%\n",
      "Epoch [22/300], Step [165/225], Training Accuracy: 50.6629%, Training Loss: 0.9850%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Step [166/225], Training Accuracy: 50.6683%, Training Loss: 0.9851%\n",
      "Epoch [22/300], Step [167/225], Training Accuracy: 50.6830%, Training Loss: 0.9844%\n",
      "Epoch [22/300], Step [168/225], Training Accuracy: 50.6603%, Training Loss: 0.9847%\n",
      "Epoch [22/300], Step [169/225], Training Accuracy: 50.6564%, Training Loss: 0.9847%\n",
      "Epoch [22/300], Step [170/225], Training Accuracy: 50.6342%, Training Loss: 0.9852%\n",
      "Epoch [22/300], Step [171/225], Training Accuracy: 50.6213%, Training Loss: 0.9851%\n",
      "Epoch [22/300], Step [172/225], Training Accuracy: 50.5723%, Training Loss: 0.9858%\n",
      "Epoch [22/300], Step [173/225], Training Accuracy: 50.5690%, Training Loss: 0.9854%\n",
      "Epoch [22/300], Step [174/225], Training Accuracy: 50.5837%, Training Loss: 0.9854%\n",
      "Epoch [22/300], Step [175/225], Training Accuracy: 50.5625%, Training Loss: 0.9855%\n",
      "Epoch [22/300], Step [176/225], Training Accuracy: 50.5859%, Training Loss: 0.9853%\n",
      "Epoch [22/300], Step [177/225], Training Accuracy: 50.5473%, Training Loss: 0.9851%\n",
      "Epoch [22/300], Step [178/225], Training Accuracy: 50.5004%, Training Loss: 0.9850%\n",
      "Epoch [22/300], Step [179/225], Training Accuracy: 50.5150%, Training Loss: 0.9850%\n",
      "Epoch [22/300], Step [180/225], Training Accuracy: 50.6163%, Training Loss: 0.9839%\n",
      "Epoch [22/300], Step [181/225], Training Accuracy: 50.5956%, Training Loss: 0.9847%\n",
      "Epoch [22/300], Step [182/225], Training Accuracy: 50.5838%, Training Loss: 0.9850%\n",
      "Epoch [22/300], Step [183/225], Training Accuracy: 50.5977%, Training Loss: 0.9846%\n",
      "Epoch [22/300], Step [184/225], Training Accuracy: 50.6029%, Training Loss: 0.9846%\n",
      "Epoch [22/300], Step [185/225], Training Accuracy: 50.5743%, Training Loss: 0.9846%\n",
      "Epoch [22/300], Step [186/225], Training Accuracy: 50.5964%, Training Loss: 0.9844%\n",
      "Epoch [22/300], Step [187/225], Training Accuracy: 50.6350%, Training Loss: 0.9837%\n",
      "Epoch [22/300], Step [188/225], Training Accuracy: 50.6815%, Training Loss: 0.9832%\n",
      "Epoch [22/300], Step [189/225], Training Accuracy: 50.7358%, Training Loss: 0.9827%\n",
      "Epoch [22/300], Step [190/225], Training Accuracy: 50.7319%, Training Loss: 0.9830%\n",
      "Epoch [22/300], Step [191/225], Training Accuracy: 50.7117%, Training Loss: 0.9831%\n",
      "Epoch [22/300], Step [192/225], Training Accuracy: 50.7568%, Training Loss: 0.9828%\n",
      "Epoch [22/300], Step [193/225], Training Accuracy: 50.7205%, Training Loss: 0.9828%\n",
      "Epoch [22/300], Step [194/225], Training Accuracy: 50.7490%, Training Loss: 0.9829%\n",
      "Epoch [22/300], Step [195/225], Training Accuracy: 50.7692%, Training Loss: 0.9824%\n",
      "Epoch [22/300], Step [196/225], Training Accuracy: 50.8131%, Training Loss: 0.9825%\n",
      "Epoch [22/300], Step [197/225], Training Accuracy: 50.8328%, Training Loss: 0.9819%\n",
      "Epoch [22/300], Step [198/225], Training Accuracy: 50.8286%, Training Loss: 0.9814%\n",
      "Epoch [22/300], Step [199/225], Training Accuracy: 50.8244%, Training Loss: 0.9811%\n",
      "Epoch [22/300], Step [200/225], Training Accuracy: 50.8516%, Training Loss: 0.9812%\n",
      "Epoch [22/300], Step [201/225], Training Accuracy: 50.8473%, Training Loss: 0.9814%\n",
      "Epoch [22/300], Step [202/225], Training Accuracy: 50.8509%, Training Loss: 0.9811%\n",
      "Epoch [22/300], Step [203/225], Training Accuracy: 50.8313%, Training Loss: 0.9815%\n",
      "Epoch [22/300], Step [204/225], Training Accuracy: 50.8655%, Training Loss: 0.9813%\n",
      "Epoch [22/300], Step [205/225], Training Accuracy: 50.8537%, Training Loss: 0.9811%\n",
      "Epoch [22/300], Step [206/225], Training Accuracy: 50.8799%, Training Loss: 0.9815%\n",
      "Epoch [22/300], Step [207/225], Training Accuracy: 50.8832%, Training Loss: 0.9820%\n",
      "Epoch [22/300], Step [208/225], Training Accuracy: 50.9315%, Training Loss: 0.9814%\n",
      "Epoch [22/300], Step [209/225], Training Accuracy: 50.9345%, Training Loss: 0.9817%\n",
      "Epoch [22/300], Step [210/225], Training Accuracy: 50.9077%, Training Loss: 0.9816%\n",
      "Epoch [22/300], Step [211/225], Training Accuracy: 50.9479%, Training Loss: 0.9814%\n",
      "Epoch [22/300], Step [212/225], Training Accuracy: 50.9139%, Training Loss: 0.9818%\n",
      "Epoch [22/300], Step [213/225], Training Accuracy: 50.8950%, Training Loss: 0.9824%\n",
      "Epoch [22/300], Step [214/225], Training Accuracy: 50.8762%, Training Loss: 0.9823%\n",
      "Epoch [22/300], Step [215/225], Training Accuracy: 50.9012%, Training Loss: 0.9822%\n",
      "Epoch [22/300], Step [216/225], Training Accuracy: 50.8970%, Training Loss: 0.9824%\n",
      "Epoch [22/300], Step [217/225], Training Accuracy: 50.8785%, Training Loss: 0.9825%\n",
      "Epoch [22/300], Step [218/225], Training Accuracy: 50.8386%, Training Loss: 0.9833%\n",
      "Epoch [22/300], Step [219/225], Training Accuracy: 50.7920%, Training Loss: 0.9836%\n",
      "Epoch [22/300], Step [220/225], Training Accuracy: 50.8381%, Training Loss: 0.9832%\n",
      "Epoch [22/300], Step [221/225], Training Accuracy: 50.8484%, Training Loss: 0.9834%\n",
      "Epoch [22/300], Step [222/225], Training Accuracy: 50.8516%, Training Loss: 0.9834%\n",
      "Epoch [22/300], Step [223/225], Training Accuracy: 50.7777%, Training Loss: 0.9840%\n",
      "Epoch [22/300], Step [224/225], Training Accuracy: 50.7603%, Training Loss: 0.9840%\n",
      "Epoch [22/300], Step [225/225], Training Accuracy: 50.7574%, Training Loss: 0.9841%\n",
      "Epoch [23/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.8156%\n",
      "Epoch [23/300], Step [2/225], Training Accuracy: 54.6875%, Training Loss: 0.9788%\n",
      "Epoch [23/300], Step [3/225], Training Accuracy: 51.0417%, Training Loss: 1.0325%\n",
      "Epoch [23/300], Step [4/225], Training Accuracy: 50.3906%, Training Loss: 1.0202%\n",
      "Epoch [23/300], Step [5/225], Training Accuracy: 51.2500%, Training Loss: 0.9872%\n",
      "Epoch [23/300], Step [6/225], Training Accuracy: 50.0000%, Training Loss: 1.0097%\n",
      "Epoch [23/300], Step [7/225], Training Accuracy: 50.2232%, Training Loss: 1.0041%\n",
      "Epoch [23/300], Step [8/225], Training Accuracy: 50.7812%, Training Loss: 1.0098%\n",
      "Epoch [23/300], Step [9/225], Training Accuracy: 50.6944%, Training Loss: 1.0034%\n",
      "Epoch [23/300], Step [10/225], Training Accuracy: 50.9375%, Training Loss: 0.9984%\n",
      "Epoch [23/300], Step [11/225], Training Accuracy: 51.5625%, Training Loss: 0.9929%\n",
      "Epoch [23/300], Step [12/225], Training Accuracy: 51.4323%, Training Loss: 0.9989%\n",
      "Epoch [23/300], Step [13/225], Training Accuracy: 51.8029%, Training Loss: 0.9974%\n",
      "Epoch [23/300], Step [14/225], Training Accuracy: 51.6741%, Training Loss: 1.0005%\n",
      "Epoch [23/300], Step [15/225], Training Accuracy: 51.1458%, Training Loss: 1.0113%\n",
      "Epoch [23/300], Step [16/225], Training Accuracy: 51.1719%, Training Loss: 1.0125%\n",
      "Epoch [23/300], Step [17/225], Training Accuracy: 50.9191%, Training Loss: 1.0074%\n",
      "Epoch [23/300], Step [18/225], Training Accuracy: 50.8681%, Training Loss: 1.0061%\n",
      "Epoch [23/300], Step [19/225], Training Accuracy: 50.1645%, Training Loss: 1.0077%\n",
      "Epoch [23/300], Step [20/225], Training Accuracy: 50.3906%, Training Loss: 1.0051%\n",
      "Epoch [23/300], Step [21/225], Training Accuracy: 50.6696%, Training Loss: 1.0004%\n",
      "Epoch [23/300], Step [22/225], Training Accuracy: 50.4261%, Training Loss: 1.0011%\n",
      "Epoch [23/300], Step [23/225], Training Accuracy: 50.2038%, Training Loss: 0.9975%\n",
      "Epoch [23/300], Step [24/225], Training Accuracy: 50.0651%, Training Loss: 1.0005%\n",
      "Epoch [23/300], Step [25/225], Training Accuracy: 50.1250%, Training Loss: 0.9965%\n",
      "Epoch [23/300], Step [26/225], Training Accuracy: 49.7596%, Training Loss: 0.9982%\n",
      "Epoch [23/300], Step [27/225], Training Accuracy: 49.5949%, Training Loss: 0.9996%\n",
      "Epoch [23/300], Step [28/225], Training Accuracy: 49.6652%, Training Loss: 0.9964%\n",
      "Epoch [23/300], Step [29/225], Training Accuracy: 49.9461%, Training Loss: 0.9921%\n",
      "Epoch [23/300], Step [30/225], Training Accuracy: 50.0000%, Training Loss: 0.9901%\n",
      "Epoch [23/300], Step [31/225], Training Accuracy: 50.1008%, Training Loss: 0.9893%\n",
      "Epoch [23/300], Step [32/225], Training Accuracy: 50.2441%, Training Loss: 0.9867%\n",
      "Epoch [23/300], Step [33/225], Training Accuracy: 50.3788%, Training Loss: 0.9847%\n",
      "Epoch [23/300], Step [34/225], Training Accuracy: 50.3676%, Training Loss: 0.9874%\n",
      "Epoch [23/300], Step [35/225], Training Accuracy: 50.2232%, Training Loss: 0.9872%\n",
      "Epoch [23/300], Step [36/225], Training Accuracy: 50.2604%, Training Loss: 0.9873%\n",
      "Epoch [23/300], Step [37/225], Training Accuracy: 50.5068%, Training Loss: 0.9844%\n",
      "Epoch [23/300], Step [38/225], Training Accuracy: 50.7401%, Training Loss: 0.9825%\n",
      "Epoch [23/300], Step [39/225], Training Accuracy: 50.6410%, Training Loss: 0.9818%\n",
      "Epoch [23/300], Step [40/225], Training Accuracy: 50.5859%, Training Loss: 0.9819%\n",
      "Epoch [23/300], Step [41/225], Training Accuracy: 50.4573%, Training Loss: 0.9836%\n",
      "Epoch [23/300], Step [42/225], Training Accuracy: 50.5580%, Training Loss: 0.9820%\n",
      "Epoch [23/300], Step [43/225], Training Accuracy: 50.5451%, Training Loss: 0.9815%\n",
      "Epoch [23/300], Step [44/225], Training Accuracy: 50.6392%, Training Loss: 0.9800%\n",
      "Epoch [23/300], Step [45/225], Training Accuracy: 50.5903%, Training Loss: 0.9797%\n",
      "Epoch [23/300], Step [46/225], Training Accuracy: 50.8492%, Training Loss: 0.9762%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Step [47/225], Training Accuracy: 50.7979%, Training Loss: 0.9777%\n",
      "Epoch [23/300], Step [48/225], Training Accuracy: 50.8464%, Training Loss: 0.9785%\n",
      "Epoch [23/300], Step [49/225], Training Accuracy: 50.7334%, Training Loss: 0.9801%\n",
      "Epoch [23/300], Step [50/225], Training Accuracy: 50.8750%, Training Loss: 0.9786%\n",
      "Epoch [23/300], Step [51/225], Training Accuracy: 50.9498%, Training Loss: 0.9760%\n",
      "Epoch [23/300], Step [52/225], Training Accuracy: 51.0817%, Training Loss: 0.9739%\n",
      "Epoch [23/300], Step [53/225], Training Accuracy: 50.9434%, Training Loss: 0.9739%\n",
      "Epoch [23/300], Step [54/225], Training Accuracy: 50.6944%, Training Loss: 0.9758%\n",
      "Epoch [23/300], Step [55/225], Training Accuracy: 50.5398%, Training Loss: 0.9776%\n",
      "Epoch [23/300], Step [56/225], Training Accuracy: 50.4743%, Training Loss: 0.9788%\n",
      "Epoch [23/300], Step [57/225], Training Accuracy: 50.7127%, Training Loss: 0.9756%\n",
      "Epoch [23/300], Step [58/225], Training Accuracy: 50.6466%, Training Loss: 0.9760%\n",
      "Epoch [23/300], Step [59/225], Training Accuracy: 50.7680%, Training Loss: 0.9753%\n",
      "Epoch [23/300], Step [60/225], Training Accuracy: 50.7292%, Training Loss: 0.9754%\n",
      "Epoch [23/300], Step [61/225], Training Accuracy: 50.8453%, Training Loss: 0.9751%\n",
      "Epoch [23/300], Step [62/225], Training Accuracy: 50.8065%, Training Loss: 0.9756%\n",
      "Epoch [23/300], Step [63/225], Training Accuracy: 50.6696%, Training Loss: 0.9767%\n",
      "Epoch [23/300], Step [64/225], Training Accuracy: 50.7568%, Training Loss: 0.9763%\n",
      "Epoch [23/300], Step [65/225], Training Accuracy: 50.6971%, Training Loss: 0.9766%\n",
      "Epoch [23/300], Step [66/225], Training Accuracy: 50.8996%, Training Loss: 0.9746%\n",
      "Epoch [23/300], Step [67/225], Training Accuracy: 50.7463%, Training Loss: 0.9751%\n",
      "Epoch [23/300], Step [68/225], Training Accuracy: 50.7583%, Training Loss: 0.9748%\n",
      "Epoch [23/300], Step [69/225], Training Accuracy: 50.6793%, Training Loss: 0.9750%\n",
      "Epoch [23/300], Step [70/225], Training Accuracy: 50.6027%, Training Loss: 0.9760%\n",
      "Epoch [23/300], Step [71/225], Training Accuracy: 50.6602%, Training Loss: 0.9751%\n",
      "Epoch [23/300], Step [72/225], Training Accuracy: 50.5642%, Training Loss: 0.9770%\n",
      "Epoch [23/300], Step [73/225], Training Accuracy: 50.5565%, Training Loss: 0.9797%\n",
      "Epoch [23/300], Step [74/225], Training Accuracy: 50.6123%, Training Loss: 0.9778%\n",
      "Epoch [23/300], Step [75/225], Training Accuracy: 50.6042%, Training Loss: 0.9774%\n",
      "Epoch [23/300], Step [76/225], Training Accuracy: 50.6579%, Training Loss: 0.9771%\n",
      "Epoch [23/300], Step [77/225], Training Accuracy: 50.8117%, Training Loss: 0.9766%\n",
      "Epoch [23/300], Step [78/225], Training Accuracy: 50.7612%, Training Loss: 0.9778%\n",
      "Epoch [23/300], Step [79/225], Training Accuracy: 50.7318%, Training Loss: 0.9790%\n",
      "Epoch [23/300], Step [80/225], Training Accuracy: 50.6250%, Training Loss: 0.9794%\n",
      "Epoch [23/300], Step [81/225], Training Accuracy: 50.7330%, Training Loss: 0.9799%\n",
      "Epoch [23/300], Step [82/225], Training Accuracy: 50.7812%, Training Loss: 0.9790%\n",
      "Epoch [23/300], Step [83/225], Training Accuracy: 50.7154%, Training Loss: 0.9784%\n",
      "Epoch [23/300], Step [84/225], Training Accuracy: 50.7068%, Training Loss: 0.9789%\n",
      "Epoch [23/300], Step [85/225], Training Accuracy: 50.7353%, Training Loss: 0.9779%\n",
      "Epoch [23/300], Step [86/225], Training Accuracy: 50.8358%, Training Loss: 0.9777%\n",
      "Epoch [23/300], Step [87/225], Training Accuracy: 50.8800%, Training Loss: 0.9776%\n",
      "Epoch [23/300], Step [88/225], Training Accuracy: 50.7812%, Training Loss: 0.9781%\n",
      "Epoch [23/300], Step [89/225], Training Accuracy: 50.7725%, Training Loss: 0.9795%\n",
      "Epoch [23/300], Step [90/225], Training Accuracy: 50.6597%, Training Loss: 0.9804%\n",
      "Epoch [23/300], Step [91/225], Training Accuracy: 50.7212%, Training Loss: 0.9786%\n",
      "Epoch [23/300], Step [92/225], Training Accuracy: 50.6963%, Training Loss: 0.9786%\n",
      "Epoch [23/300], Step [93/225], Training Accuracy: 50.6888%, Training Loss: 0.9787%\n",
      "Epoch [23/300], Step [94/225], Training Accuracy: 50.7812%, Training Loss: 0.9773%\n",
      "Epoch [23/300], Step [95/225], Training Accuracy: 50.7566%, Training Loss: 0.9779%\n",
      "Epoch [23/300], Step [96/225], Training Accuracy: 50.7975%, Training Loss: 0.9772%\n",
      "Epoch [23/300], Step [97/225], Training Accuracy: 50.8698%, Training Loss: 0.9764%\n",
      "Epoch [23/300], Step [98/225], Training Accuracy: 50.8769%, Training Loss: 0.9758%\n",
      "Epoch [23/300], Step [99/225], Training Accuracy: 50.9470%, Training Loss: 0.9756%\n",
      "Epoch [23/300], Step [100/225], Training Accuracy: 50.7969%, Training Loss: 0.9763%\n",
      "Epoch [23/300], Step [101/225], Training Accuracy: 50.7735%, Training Loss: 0.9769%\n",
      "Epoch [23/300], Step [102/225], Training Accuracy: 50.7047%, Training Loss: 0.9780%\n",
      "Epoch [23/300], Step [103/225], Training Accuracy: 50.5916%, Training Loss: 0.9797%\n",
      "Epoch [23/300], Step [104/225], Training Accuracy: 50.6310%, Training Loss: 0.9793%\n",
      "Epoch [23/300], Step [105/225], Training Accuracy: 50.5804%, Training Loss: 0.9785%\n",
      "Epoch [23/300], Step [106/225], Training Accuracy: 50.5159%, Training Loss: 0.9795%\n",
      "Epoch [23/300], Step [107/225], Training Accuracy: 50.5987%, Training Loss: 0.9793%\n",
      "Epoch [23/300], Step [108/225], Training Accuracy: 50.5932%, Training Loss: 0.9800%\n",
      "Epoch [23/300], Step [109/225], Training Accuracy: 50.5877%, Training Loss: 0.9799%\n",
      "Epoch [23/300], Step [110/225], Training Accuracy: 50.6250%, Training Loss: 0.9796%\n",
      "Epoch [23/300], Step [111/225], Training Accuracy: 50.7461%, Training Loss: 0.9801%\n",
      "Epoch [23/300], Step [112/225], Training Accuracy: 50.7952%, Training Loss: 0.9794%\n",
      "Epoch [23/300], Step [113/225], Training Accuracy: 50.7882%, Training Loss: 0.9804%\n",
      "Epoch [23/300], Step [114/225], Training Accuracy: 50.7812%, Training Loss: 0.9802%\n",
      "Epoch [23/300], Step [115/225], Training Accuracy: 50.8832%, Training Loss: 0.9794%\n",
      "Epoch [23/300], Step [116/225], Training Accuracy: 50.9025%, Training Loss: 0.9793%\n",
      "Epoch [23/300], Step [117/225], Training Accuracy: 50.8013%, Training Loss: 0.9807%\n",
      "Epoch [23/300], Step [118/225], Training Accuracy: 50.8342%, Training Loss: 0.9808%\n",
      "Epoch [23/300], Step [119/225], Training Accuracy: 50.8009%, Training Loss: 0.9812%\n",
      "Epoch [23/300], Step [120/225], Training Accuracy: 50.8594%, Training Loss: 0.9805%\n",
      "Epoch [23/300], Step [121/225], Training Accuracy: 50.8006%, Training Loss: 0.9808%\n",
      "Epoch [23/300], Step [122/225], Training Accuracy: 50.8197%, Training Loss: 0.9808%\n",
      "Epoch [23/300], Step [123/225], Training Accuracy: 50.7622%, Training Loss: 0.9806%\n",
      "Epoch [23/300], Step [124/225], Training Accuracy: 50.7939%, Training Loss: 0.9804%\n",
      "Epoch [23/300], Step [125/225], Training Accuracy: 50.7625%, Training Loss: 0.9815%\n",
      "Epoch [23/300], Step [126/225], Training Accuracy: 50.7068%, Training Loss: 0.9822%\n",
      "Epoch [23/300], Step [127/225], Training Accuracy: 50.6767%, Training Loss: 0.9821%\n",
      "Epoch [23/300], Step [128/225], Training Accuracy: 50.6592%, Training Loss: 0.9824%\n",
      "Epoch [23/300], Step [129/225], Training Accuracy: 50.6662%, Training Loss: 0.9828%\n",
      "Epoch [23/300], Step [130/225], Training Accuracy: 50.6490%, Training Loss: 0.9834%\n",
      "Epoch [23/300], Step [131/225], Training Accuracy: 50.6322%, Training Loss: 0.9831%\n",
      "Epoch [23/300], Step [132/225], Training Accuracy: 50.6155%, Training Loss: 0.9835%\n",
      "Epoch [23/300], Step [133/225], Training Accuracy: 50.7049%, Training Loss: 0.9827%\n",
      "Epoch [23/300], Step [134/225], Training Accuracy: 50.7463%, Training Loss: 0.9824%\n",
      "Epoch [23/300], Step [135/225], Training Accuracy: 50.7523%, Training Loss: 0.9822%\n",
      "Epoch [23/300], Step [136/225], Training Accuracy: 50.7353%, Training Loss: 0.9815%\n",
      "Epoch [23/300], Step [137/225], Training Accuracy: 50.7641%, Training Loss: 0.9811%\n",
      "Epoch [23/300], Step [138/225], Training Accuracy: 50.8265%, Training Loss: 0.9801%\n",
      "Epoch [23/300], Step [139/225], Training Accuracy: 50.8318%, Training Loss: 0.9804%\n",
      "Epoch [23/300], Step [140/225], Training Accuracy: 50.9040%, Training Loss: 0.9801%\n",
      "Epoch [23/300], Step [141/225], Training Accuracy: 50.9419%, Training Loss: 0.9801%\n",
      "Epoch [23/300], Step [142/225], Training Accuracy: 50.9463%, Training Loss: 0.9802%\n",
      "Epoch [23/300], Step [143/225], Training Accuracy: 50.9834%, Training Loss: 0.9797%\n",
      "Epoch [23/300], Step [144/225], Training Accuracy: 50.9549%, Training Loss: 0.9795%\n",
      "Epoch [23/300], Step [145/225], Training Accuracy: 51.0345%, Training Loss: 0.9787%\n",
      "Epoch [23/300], Step [146/225], Training Accuracy: 51.0488%, Training Loss: 0.9786%\n",
      "Epoch [23/300], Step [147/225], Training Accuracy: 50.9991%, Training Loss: 0.9794%\n",
      "Epoch [23/300], Step [148/225], Training Accuracy: 51.0874%, Training Loss: 0.9790%\n",
      "Epoch [23/300], Step [149/225], Training Accuracy: 51.0696%, Training Loss: 0.9793%\n",
      "Epoch [23/300], Step [150/225], Training Accuracy: 51.0521%, Training Loss: 0.9798%\n",
      "Epoch [23/300], Step [151/225], Training Accuracy: 51.0762%, Training Loss: 0.9792%\n",
      "Epoch [23/300], Step [152/225], Training Accuracy: 51.0280%, Training Loss: 0.9792%\n",
      "Epoch [23/300], Step [153/225], Training Accuracy: 51.0621%, Training Loss: 0.9787%\n",
      "Epoch [23/300], Step [154/225], Training Accuracy: 51.0450%, Training Loss: 0.9782%\n",
      "Epoch [23/300], Step [155/225], Training Accuracy: 50.9677%, Training Loss: 0.9792%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Step [156/225], Training Accuracy: 50.9415%, Training Loss: 0.9792%\n",
      "Epoch [23/300], Step [157/225], Training Accuracy: 50.9455%, Training Loss: 0.9797%\n",
      "Epoch [23/300], Step [158/225], Training Accuracy: 50.9691%, Training Loss: 0.9798%\n",
      "Epoch [23/300], Step [159/225], Training Accuracy: 50.9631%, Training Loss: 0.9795%\n",
      "Epoch [23/300], Step [160/225], Training Accuracy: 50.9180%, Training Loss: 0.9795%\n",
      "Epoch [23/300], Step [161/225], Training Accuracy: 51.0093%, Training Loss: 0.9785%\n",
      "Epoch [23/300], Step [162/225], Training Accuracy: 51.0417%, Training Loss: 0.9787%\n",
      "Epoch [23/300], Step [163/225], Training Accuracy: 50.9778%, Training Loss: 0.9784%\n",
      "Epoch [23/300], Step [164/225], Training Accuracy: 51.0290%, Training Loss: 0.9774%\n",
      "Epoch [23/300], Step [165/225], Training Accuracy: 51.0322%, Training Loss: 0.9772%\n",
      "Epoch [23/300], Step [166/225], Training Accuracy: 51.0354%, Training Loss: 0.9774%\n",
      "Epoch [23/300], Step [167/225], Training Accuracy: 51.0479%, Training Loss: 0.9767%\n",
      "Epoch [23/300], Step [168/225], Training Accuracy: 51.0231%, Training Loss: 0.9769%\n",
      "Epoch [23/300], Step [169/225], Training Accuracy: 51.0170%, Training Loss: 0.9769%\n",
      "Epoch [23/300], Step [170/225], Training Accuracy: 50.9926%, Training Loss: 0.9775%\n",
      "Epoch [23/300], Step [171/225], Training Accuracy: 50.9868%, Training Loss: 0.9774%\n",
      "Epoch [23/300], Step [172/225], Training Accuracy: 50.9539%, Training Loss: 0.9780%\n",
      "Epoch [23/300], Step [173/225], Training Accuracy: 50.9664%, Training Loss: 0.9776%\n",
      "Epoch [23/300], Step [174/225], Training Accuracy: 50.9878%, Training Loss: 0.9777%\n",
      "Epoch [23/300], Step [175/225], Training Accuracy: 50.9732%, Training Loss: 0.9777%\n",
      "Epoch [23/300], Step [176/225], Training Accuracy: 51.0121%, Training Loss: 0.9775%\n",
      "Epoch [23/300], Step [177/225], Training Accuracy: 50.9710%, Training Loss: 0.9774%\n",
      "Epoch [23/300], Step [178/225], Training Accuracy: 50.9305%, Training Loss: 0.9773%\n",
      "Epoch [23/300], Step [179/225], Training Accuracy: 50.9427%, Training Loss: 0.9773%\n",
      "Epoch [23/300], Step [180/225], Training Accuracy: 51.0417%, Training Loss: 0.9762%\n",
      "Epoch [23/300], Step [181/225], Training Accuracy: 51.0186%, Training Loss: 0.9770%\n",
      "Epoch [23/300], Step [182/225], Training Accuracy: 51.0045%, Training Loss: 0.9772%\n",
      "Epoch [23/300], Step [183/225], Training Accuracy: 51.0331%, Training Loss: 0.9769%\n",
      "Epoch [23/300], Step [184/225], Training Accuracy: 51.0275%, Training Loss: 0.9769%\n",
      "Epoch [23/300], Step [185/225], Training Accuracy: 50.9882%, Training Loss: 0.9769%\n",
      "Epoch [23/300], Step [186/225], Training Accuracy: 50.9997%, Training Loss: 0.9767%\n",
      "Epoch [23/300], Step [187/225], Training Accuracy: 51.0361%, Training Loss: 0.9760%\n",
      "Epoch [23/300], Step [188/225], Training Accuracy: 51.0805%, Training Loss: 0.9754%\n",
      "Epoch [23/300], Step [189/225], Training Accuracy: 51.1326%, Training Loss: 0.9749%\n",
      "Epoch [23/300], Step [190/225], Training Accuracy: 51.1349%, Training Loss: 0.9753%\n",
      "Epoch [23/300], Step [191/225], Training Accuracy: 51.1044%, Training Loss: 0.9754%\n",
      "Epoch [23/300], Step [192/225], Training Accuracy: 51.1556%, Training Loss: 0.9751%\n",
      "Epoch [23/300], Step [193/225], Training Accuracy: 51.1172%, Training Loss: 0.9751%\n",
      "Epoch [23/300], Step [194/225], Training Accuracy: 51.1437%, Training Loss: 0.9752%\n",
      "Epoch [23/300], Step [195/225], Training Accuracy: 51.1619%, Training Loss: 0.9747%\n",
      "Epoch [23/300], Step [196/225], Training Accuracy: 51.1958%, Training Loss: 0.9747%\n",
      "Epoch [23/300], Step [197/225], Training Accuracy: 51.2294%, Training Loss: 0.9742%\n",
      "Epoch [23/300], Step [198/225], Training Accuracy: 51.2311%, Training Loss: 0.9737%\n",
      "Epoch [23/300], Step [199/225], Training Accuracy: 51.2406%, Training Loss: 0.9734%\n",
      "Epoch [23/300], Step [200/225], Training Accuracy: 51.2734%, Training Loss: 0.9735%\n",
      "Epoch [23/300], Step [201/225], Training Accuracy: 51.2593%, Training Loss: 0.9737%\n",
      "Epoch [23/300], Step [202/225], Training Accuracy: 51.2608%, Training Loss: 0.9734%\n",
      "Epoch [23/300], Step [203/225], Training Accuracy: 51.2469%, Training Loss: 0.9737%\n",
      "Epoch [23/300], Step [204/225], Training Accuracy: 51.2714%, Training Loss: 0.9736%\n",
      "Epoch [23/300], Step [205/225], Training Accuracy: 51.2576%, Training Loss: 0.9734%\n",
      "Epoch [23/300], Step [206/225], Training Accuracy: 51.2819%, Training Loss: 0.9738%\n",
      "Epoch [23/300], Step [207/225], Training Accuracy: 51.2681%, Training Loss: 0.9742%\n",
      "Epoch [23/300], Step [208/225], Training Accuracy: 51.3221%, Training Loss: 0.9737%\n",
      "Epoch [23/300], Step [209/225], Training Accuracy: 51.3233%, Training Loss: 0.9740%\n",
      "Epoch [23/300], Step [210/225], Training Accuracy: 51.3095%, Training Loss: 0.9739%\n",
      "Epoch [23/300], Step [211/225], Training Accuracy: 51.3477%, Training Loss: 0.9737%\n",
      "Epoch [23/300], Step [212/225], Training Accuracy: 51.2972%, Training Loss: 0.9741%\n",
      "Epoch [23/300], Step [213/225], Training Accuracy: 51.2691%, Training Loss: 0.9747%\n",
      "Epoch [23/300], Step [214/225], Training Accuracy: 51.2412%, Training Loss: 0.9746%\n",
      "Epoch [23/300], Step [215/225], Training Accuracy: 51.2645%, Training Loss: 0.9745%\n",
      "Epoch [23/300], Step [216/225], Training Accuracy: 51.2587%, Training Loss: 0.9747%\n",
      "Epoch [23/300], Step [217/225], Training Accuracy: 51.2529%, Training Loss: 0.9748%\n",
      "Epoch [23/300], Step [218/225], Training Accuracy: 51.2185%, Training Loss: 0.9756%\n",
      "Epoch [23/300], Step [219/225], Training Accuracy: 51.1772%, Training Loss: 0.9759%\n",
      "Epoch [23/300], Step [220/225], Training Accuracy: 51.2216%, Training Loss: 0.9756%\n",
      "Epoch [23/300], Step [221/225], Training Accuracy: 51.2373%, Training Loss: 0.9757%\n",
      "Epoch [23/300], Step [222/225], Training Accuracy: 51.2387%, Training Loss: 0.9757%\n",
      "Epoch [23/300], Step [223/225], Training Accuracy: 51.1771%, Training Loss: 0.9763%\n",
      "Epoch [23/300], Step [224/225], Training Accuracy: 51.1579%, Training Loss: 0.9764%\n",
      "Epoch [23/300], Step [225/225], Training Accuracy: 51.1604%, Training Loss: 0.9764%\n",
      "Epoch [24/300], Step [1/225], Training Accuracy: 65.6250%, Training Loss: 0.8060%\n",
      "Epoch [24/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 0.9738%\n",
      "Epoch [24/300], Step [3/225], Training Accuracy: 51.5625%, Training Loss: 1.0281%\n",
      "Epoch [24/300], Step [4/225], Training Accuracy: 50.7812%, Training Loss: 1.0143%\n",
      "Epoch [24/300], Step [5/225], Training Accuracy: 51.8750%, Training Loss: 0.9808%\n",
      "Epoch [24/300], Step [6/225], Training Accuracy: 50.7812%, Training Loss: 1.0032%\n",
      "Epoch [24/300], Step [7/225], Training Accuracy: 50.6696%, Training Loss: 0.9977%\n",
      "Epoch [24/300], Step [8/225], Training Accuracy: 51.1719%, Training Loss: 1.0035%\n",
      "Epoch [24/300], Step [9/225], Training Accuracy: 51.3889%, Training Loss: 0.9965%\n",
      "Epoch [24/300], Step [10/225], Training Accuracy: 51.7188%, Training Loss: 0.9915%\n",
      "Epoch [24/300], Step [11/225], Training Accuracy: 52.2727%, Training Loss: 0.9863%\n",
      "Epoch [24/300], Step [12/225], Training Accuracy: 52.0833%, Training Loss: 0.9924%\n",
      "Epoch [24/300], Step [13/225], Training Accuracy: 52.7644%, Training Loss: 0.9904%\n",
      "Epoch [24/300], Step [14/225], Training Accuracy: 52.5670%, Training Loss: 0.9932%\n",
      "Epoch [24/300], Step [15/225], Training Accuracy: 51.8750%, Training Loss: 1.0045%\n",
      "Epoch [24/300], Step [16/225], Training Accuracy: 51.7578%, Training Loss: 1.0054%\n",
      "Epoch [24/300], Step [17/225], Training Accuracy: 51.4706%, Training Loss: 1.0002%\n",
      "Epoch [24/300], Step [18/225], Training Accuracy: 51.3889%, Training Loss: 0.9986%\n",
      "Epoch [24/300], Step [19/225], Training Accuracy: 50.8224%, Training Loss: 1.0002%\n",
      "Epoch [24/300], Step [20/225], Training Accuracy: 51.0156%, Training Loss: 0.9974%\n",
      "Epoch [24/300], Step [21/225], Training Accuracy: 51.1905%, Training Loss: 0.9929%\n",
      "Epoch [24/300], Step [22/225], Training Accuracy: 50.9233%, Training Loss: 0.9936%\n",
      "Epoch [24/300], Step [23/225], Training Accuracy: 50.6793%, Training Loss: 0.9899%\n",
      "Epoch [24/300], Step [24/225], Training Accuracy: 50.5859%, Training Loss: 0.9928%\n",
      "Epoch [24/300], Step [25/225], Training Accuracy: 50.6250%, Training Loss: 0.9886%\n",
      "Epoch [24/300], Step [26/225], Training Accuracy: 50.2404%, Training Loss: 0.9900%\n",
      "Epoch [24/300], Step [27/225], Training Accuracy: 50.0000%, Training Loss: 0.9915%\n",
      "Epoch [24/300], Step [28/225], Training Accuracy: 50.0558%, Training Loss: 0.9884%\n",
      "Epoch [24/300], Step [29/225], Training Accuracy: 50.3233%, Training Loss: 0.9841%\n",
      "Epoch [24/300], Step [30/225], Training Accuracy: 50.3646%, Training Loss: 0.9821%\n",
      "Epoch [24/300], Step [31/225], Training Accuracy: 50.5040%, Training Loss: 0.9813%\n",
      "Epoch [24/300], Step [32/225], Training Accuracy: 50.6836%, Training Loss: 0.9786%\n",
      "Epoch [24/300], Step [33/225], Training Accuracy: 50.8049%, Training Loss: 0.9766%\n",
      "Epoch [24/300], Step [34/225], Training Accuracy: 50.8732%, Training Loss: 0.9792%\n",
      "Epoch [24/300], Step [35/225], Training Accuracy: 50.7143%, Training Loss: 0.9792%\n",
      "Epoch [24/300], Step [36/225], Training Accuracy: 50.7812%, Training Loss: 0.9792%\n",
      "Epoch [24/300], Step [37/225], Training Accuracy: 51.0135%, Training Loss: 0.9764%\n",
      "Epoch [24/300], Step [38/225], Training Accuracy: 51.2336%, Training Loss: 0.9746%\n",
      "Epoch [24/300], Step [39/225], Training Accuracy: 51.2019%, Training Loss: 0.9740%\n",
      "Epoch [24/300], Step [40/225], Training Accuracy: 51.1328%, Training Loss: 0.9741%\n",
      "Epoch [24/300], Step [41/225], Training Accuracy: 50.9527%, Training Loss: 0.9759%\n",
      "Epoch [24/300], Step [42/225], Training Accuracy: 51.0417%, Training Loss: 0.9743%\n",
      "Epoch [24/300], Step [43/225], Training Accuracy: 51.0174%, Training Loss: 0.9737%\n",
      "Epoch [24/300], Step [44/225], Training Accuracy: 51.1009%, Training Loss: 0.9723%\n",
      "Epoch [24/300], Step [45/225], Training Accuracy: 51.0069%, Training Loss: 0.9721%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Step [46/225], Training Accuracy: 51.2908%, Training Loss: 0.9687%\n",
      "Epoch [24/300], Step [47/225], Training Accuracy: 51.2965%, Training Loss: 0.9703%\n",
      "Epoch [24/300], Step [48/225], Training Accuracy: 51.3021%, Training Loss: 0.9712%\n",
      "Epoch [24/300], Step [49/225], Training Accuracy: 51.1798%, Training Loss: 0.9727%\n",
      "Epoch [24/300], Step [50/225], Training Accuracy: 51.2812%, Training Loss: 0.9713%\n",
      "Epoch [24/300], Step [51/225], Training Accuracy: 51.3480%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [52/225], Training Accuracy: 51.4724%, Training Loss: 0.9664%\n",
      "Epoch [24/300], Step [53/225], Training Accuracy: 51.3267%, Training Loss: 0.9664%\n",
      "Epoch [24/300], Step [54/225], Training Accuracy: 51.0127%, Training Loss: 0.9684%\n",
      "Epoch [24/300], Step [55/225], Training Accuracy: 50.8807%, Training Loss: 0.9703%\n",
      "Epoch [24/300], Step [56/225], Training Accuracy: 50.8371%, Training Loss: 0.9714%\n",
      "Epoch [24/300], Step [57/225], Training Accuracy: 51.0417%, Training Loss: 0.9683%\n",
      "Epoch [24/300], Step [58/225], Training Accuracy: 50.9698%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [59/225], Training Accuracy: 51.1123%, Training Loss: 0.9679%\n",
      "Epoch [24/300], Step [60/225], Training Accuracy: 51.0677%, Training Loss: 0.9678%\n",
      "Epoch [24/300], Step [61/225], Training Accuracy: 51.1783%, Training Loss: 0.9675%\n",
      "Epoch [24/300], Step [62/225], Training Accuracy: 51.1341%, Training Loss: 0.9680%\n",
      "Epoch [24/300], Step [63/225], Training Accuracy: 51.0169%, Training Loss: 0.9691%\n",
      "Epoch [24/300], Step [64/225], Training Accuracy: 51.0986%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [65/225], Training Accuracy: 51.0337%, Training Loss: 0.9690%\n",
      "Epoch [24/300], Step [66/225], Training Accuracy: 51.2311%, Training Loss: 0.9669%\n",
      "Epoch [24/300], Step [67/225], Training Accuracy: 51.0961%, Training Loss: 0.9675%\n",
      "Epoch [24/300], Step [68/225], Training Accuracy: 51.0800%, Training Loss: 0.9671%\n",
      "Epoch [24/300], Step [69/225], Training Accuracy: 50.9737%, Training Loss: 0.9673%\n",
      "Epoch [24/300], Step [70/225], Training Accuracy: 50.9152%, Training Loss: 0.9683%\n",
      "Epoch [24/300], Step [71/225], Training Accuracy: 50.9243%, Training Loss: 0.9675%\n",
      "Epoch [24/300], Step [72/225], Training Accuracy: 50.8247%, Training Loss: 0.9694%\n",
      "Epoch [24/300], Step [73/225], Training Accuracy: 50.8134%, Training Loss: 0.9720%\n",
      "Epoch [24/300], Step [74/225], Training Accuracy: 50.8657%, Training Loss: 0.9701%\n",
      "Epoch [24/300], Step [75/225], Training Accuracy: 50.8542%, Training Loss: 0.9698%\n",
      "Epoch [24/300], Step [76/225], Training Accuracy: 50.9252%, Training Loss: 0.9695%\n",
      "Epoch [24/300], Step [77/225], Training Accuracy: 51.0755%, Training Loss: 0.9689%\n",
      "Epoch [24/300], Step [78/225], Training Accuracy: 51.0216%, Training Loss: 0.9701%\n",
      "Epoch [24/300], Step [79/225], Training Accuracy: 50.9889%, Training Loss: 0.9714%\n",
      "Epoch [24/300], Step [80/225], Training Accuracy: 50.9180%, Training Loss: 0.9719%\n",
      "Epoch [24/300], Step [81/225], Training Accuracy: 51.0224%, Training Loss: 0.9723%\n",
      "Epoch [24/300], Step [82/225], Training Accuracy: 51.0861%, Training Loss: 0.9714%\n",
      "Epoch [24/300], Step [83/225], Training Accuracy: 51.0730%, Training Loss: 0.9707%\n",
      "Epoch [24/300], Step [84/225], Training Accuracy: 51.0789%, Training Loss: 0.9712%\n",
      "Epoch [24/300], Step [85/225], Training Accuracy: 51.1029%, Training Loss: 0.9702%\n",
      "Epoch [24/300], Step [86/225], Training Accuracy: 51.1810%, Training Loss: 0.9698%\n",
      "Epoch [24/300], Step [87/225], Training Accuracy: 51.2392%, Training Loss: 0.9697%\n",
      "Epoch [24/300], Step [88/225], Training Accuracy: 51.1186%, Training Loss: 0.9704%\n",
      "Epoch [24/300], Step [89/225], Training Accuracy: 51.0885%, Training Loss: 0.9717%\n",
      "Epoch [24/300], Step [90/225], Training Accuracy: 50.9549%, Training Loss: 0.9727%\n",
      "Epoch [24/300], Step [91/225], Training Accuracy: 50.9959%, Training Loss: 0.9709%\n",
      "Epoch [24/300], Step [92/225], Training Accuracy: 50.9511%, Training Loss: 0.9709%\n",
      "Epoch [24/300], Step [93/225], Training Accuracy: 50.9409%, Training Loss: 0.9710%\n",
      "Epoch [24/300], Step [94/225], Training Accuracy: 51.0306%, Training Loss: 0.9696%\n",
      "Epoch [24/300], Step [95/225], Training Accuracy: 51.0197%, Training Loss: 0.9701%\n",
      "Epoch [24/300], Step [96/225], Training Accuracy: 51.0579%, Training Loss: 0.9695%\n",
      "Epoch [24/300], Step [97/225], Training Accuracy: 51.1437%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [98/225], Training Accuracy: 51.1320%, Training Loss: 0.9681%\n",
      "Epoch [24/300], Step [99/225], Training Accuracy: 51.2153%, Training Loss: 0.9679%\n",
      "Epoch [24/300], Step [100/225], Training Accuracy: 51.0625%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [101/225], Training Accuracy: 51.0365%, Training Loss: 0.9692%\n",
      "Epoch [24/300], Step [102/225], Training Accuracy: 50.9651%, Training Loss: 0.9703%\n",
      "Epoch [24/300], Step [103/225], Training Accuracy: 50.8799%, Training Loss: 0.9720%\n",
      "Epoch [24/300], Step [104/225], Training Accuracy: 50.9165%, Training Loss: 0.9717%\n",
      "Epoch [24/300], Step [105/225], Training Accuracy: 50.8780%, Training Loss: 0.9710%\n",
      "Epoch [24/300], Step [106/225], Training Accuracy: 50.8107%, Training Loss: 0.9720%\n",
      "Epoch [24/300], Step [107/225], Training Accuracy: 50.8908%, Training Loss: 0.9718%\n",
      "Epoch [24/300], Step [108/225], Training Accuracy: 50.8970%, Training Loss: 0.9725%\n",
      "Epoch [24/300], Step [109/225], Training Accuracy: 50.9031%, Training Loss: 0.9724%\n",
      "Epoch [24/300], Step [110/225], Training Accuracy: 50.9375%, Training Loss: 0.9720%\n",
      "Epoch [24/300], Step [111/225], Training Accuracy: 51.0417%, Training Loss: 0.9726%\n",
      "Epoch [24/300], Step [112/225], Training Accuracy: 51.1021%, Training Loss: 0.9719%\n",
      "Epoch [24/300], Step [113/225], Training Accuracy: 51.0924%, Training Loss: 0.9727%\n",
      "Epoch [24/300], Step [114/225], Training Accuracy: 51.0828%, Training Loss: 0.9725%\n",
      "Epoch [24/300], Step [115/225], Training Accuracy: 51.1821%, Training Loss: 0.9718%\n",
      "Epoch [24/300], Step [116/225], Training Accuracy: 51.1853%, Training Loss: 0.9717%\n",
      "Epoch [24/300], Step [117/225], Training Accuracy: 51.0684%, Training Loss: 0.9732%\n",
      "Epoch [24/300], Step [118/225], Training Accuracy: 51.0990%, Training Loss: 0.9733%\n",
      "Epoch [24/300], Step [119/225], Training Accuracy: 51.0636%, Training Loss: 0.9738%\n",
      "Epoch [24/300], Step [120/225], Training Accuracy: 51.1198%, Training Loss: 0.9730%\n",
      "Epoch [24/300], Step [121/225], Training Accuracy: 51.0460%, Training Loss: 0.9734%\n",
      "Epoch [24/300], Step [122/225], Training Accuracy: 51.0630%, Training Loss: 0.9734%\n",
      "Epoch [24/300], Step [123/225], Training Accuracy: 51.0163%, Training Loss: 0.9732%\n",
      "Epoch [24/300], Step [124/225], Training Accuracy: 51.0459%, Training Loss: 0.9730%\n",
      "Epoch [24/300], Step [125/225], Training Accuracy: 51.0125%, Training Loss: 0.9741%\n",
      "Epoch [24/300], Step [126/225], Training Accuracy: 50.9673%, Training Loss: 0.9749%\n",
      "Epoch [24/300], Step [127/225], Training Accuracy: 50.9350%, Training Loss: 0.9748%\n",
      "Epoch [24/300], Step [128/225], Training Accuracy: 50.9277%, Training Loss: 0.9751%\n",
      "Epoch [24/300], Step [129/225], Training Accuracy: 50.9327%, Training Loss: 0.9755%\n",
      "Epoch [24/300], Step [130/225], Training Accuracy: 50.9135%, Training Loss: 0.9761%\n",
      "Epoch [24/300], Step [131/225], Training Accuracy: 50.8826%, Training Loss: 0.9759%\n",
      "Epoch [24/300], Step [132/225], Training Accuracy: 50.8641%, Training Loss: 0.9762%\n",
      "Epoch [24/300], Step [133/225], Training Accuracy: 50.9398%, Training Loss: 0.9755%\n",
      "Epoch [24/300], Step [134/225], Training Accuracy: 50.9795%, Training Loss: 0.9752%\n",
      "Epoch [24/300], Step [135/225], Training Accuracy: 50.9838%, Training Loss: 0.9749%\n",
      "Epoch [24/300], Step [136/225], Training Accuracy: 50.9651%, Training Loss: 0.9742%\n",
      "Epoch [24/300], Step [137/225], Training Accuracy: 50.9922%, Training Loss: 0.9738%\n",
      "Epoch [24/300], Step [138/225], Training Accuracy: 51.0530%, Training Loss: 0.9727%\n",
      "Epoch [24/300], Step [139/225], Training Accuracy: 51.0454%, Training Loss: 0.9731%\n",
      "Epoch [24/300], Step [140/225], Training Accuracy: 51.1049%, Training Loss: 0.9728%\n",
      "Epoch [24/300], Step [141/225], Training Accuracy: 51.1414%, Training Loss: 0.9727%\n",
      "Epoch [24/300], Step [142/225], Training Accuracy: 51.1554%, Training Loss: 0.9728%\n",
      "Epoch [24/300], Step [143/225], Training Accuracy: 51.1910%, Training Loss: 0.9723%\n",
      "Epoch [24/300], Step [144/225], Training Accuracy: 51.1719%, Training Loss: 0.9721%\n",
      "Epoch [24/300], Step [145/225], Training Accuracy: 51.2392%, Training Loss: 0.9713%\n",
      "Epoch [24/300], Step [146/225], Training Accuracy: 51.2628%, Training Loss: 0.9712%\n",
      "Epoch [24/300], Step [147/225], Training Accuracy: 51.2117%, Training Loss: 0.9720%\n",
      "Epoch [24/300], Step [148/225], Training Accuracy: 51.2986%, Training Loss: 0.9716%\n",
      "Epoch [24/300], Step [149/225], Training Accuracy: 51.2794%, Training Loss: 0.9718%\n",
      "Epoch [24/300], Step [150/225], Training Accuracy: 51.2500%, Training Loss: 0.9724%\n",
      "Epoch [24/300], Step [151/225], Training Accuracy: 51.2728%, Training Loss: 0.9717%\n",
      "Epoch [24/300], Step [152/225], Training Accuracy: 51.2336%, Training Loss: 0.9718%\n",
      "Epoch [24/300], Step [153/225], Training Accuracy: 51.2663%, Training Loss: 0.9712%\n",
      "Epoch [24/300], Step [154/225], Training Accuracy: 51.2480%, Training Loss: 0.9707%\n",
      "Epoch [24/300], Step [155/225], Training Accuracy: 51.1895%, Training Loss: 0.9717%\n",
      "Epoch [24/300], Step [156/225], Training Accuracy: 51.1518%, Training Loss: 0.9718%\n",
      "Epoch [24/300], Step [157/225], Training Accuracy: 51.1545%, Training Loss: 0.9723%\n",
      "Epoch [24/300], Step [158/225], Training Accuracy: 51.1867%, Training Loss: 0.9724%\n",
      "Epoch [24/300], Step [159/225], Training Accuracy: 51.1989%, Training Loss: 0.9721%\n",
      "Epoch [24/300], Step [160/225], Training Accuracy: 51.1426%, Training Loss: 0.9720%\n",
      "Epoch [24/300], Step [161/225], Training Accuracy: 51.2325%, Training Loss: 0.9710%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Step [162/225], Training Accuracy: 51.2635%, Training Loss: 0.9713%\n",
      "Epoch [24/300], Step [163/225], Training Accuracy: 51.1982%, Training Loss: 0.9710%\n",
      "Epoch [24/300], Step [164/225], Training Accuracy: 51.2481%, Training Loss: 0.9700%\n",
      "Epoch [24/300], Step [165/225], Training Accuracy: 51.2500%, Training Loss: 0.9698%\n",
      "Epoch [24/300], Step [166/225], Training Accuracy: 51.2519%, Training Loss: 0.9699%\n",
      "Epoch [24/300], Step [167/225], Training Accuracy: 51.2725%, Training Loss: 0.9692%\n",
      "Epoch [24/300], Step [168/225], Training Accuracy: 51.2463%, Training Loss: 0.9694%\n",
      "Epoch [24/300], Step [169/225], Training Accuracy: 51.2389%, Training Loss: 0.9694%\n",
      "Epoch [24/300], Step [170/225], Training Accuracy: 51.2040%, Training Loss: 0.9700%\n",
      "Epoch [24/300], Step [171/225], Training Accuracy: 51.2061%, Training Loss: 0.9699%\n",
      "Epoch [24/300], Step [172/225], Training Accuracy: 51.1719%, Training Loss: 0.9705%\n",
      "Epoch [24/300], Step [173/225], Training Accuracy: 51.1832%, Training Loss: 0.9702%\n",
      "Epoch [24/300], Step [174/225], Training Accuracy: 51.1943%, Training Loss: 0.9702%\n",
      "Epoch [24/300], Step [175/225], Training Accuracy: 51.1786%, Training Loss: 0.9703%\n",
      "Epoch [24/300], Step [176/225], Training Accuracy: 51.2163%, Training Loss: 0.9701%\n",
      "Epoch [24/300], Step [177/225], Training Accuracy: 51.1741%, Training Loss: 0.9700%\n",
      "Epoch [24/300], Step [178/225], Training Accuracy: 51.1324%, Training Loss: 0.9699%\n",
      "Epoch [24/300], Step [179/225], Training Accuracy: 51.1610%, Training Loss: 0.9699%\n",
      "Epoch [24/300], Step [180/225], Training Accuracy: 51.2587%, Training Loss: 0.9688%\n",
      "Epoch [24/300], Step [181/225], Training Accuracy: 51.2172%, Training Loss: 0.9695%\n",
      "Epoch [24/300], Step [182/225], Training Accuracy: 51.2105%, Training Loss: 0.9698%\n",
      "Epoch [24/300], Step [183/225], Training Accuracy: 51.2380%, Training Loss: 0.9695%\n",
      "Epoch [24/300], Step [184/225], Training Accuracy: 51.2313%, Training Loss: 0.9695%\n",
      "Epoch [24/300], Step [185/225], Training Accuracy: 51.1909%, Training Loss: 0.9695%\n",
      "Epoch [24/300], Step [186/225], Training Accuracy: 51.2013%, Training Loss: 0.9693%\n",
      "Epoch [24/300], Step [187/225], Training Accuracy: 51.2366%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [188/225], Training Accuracy: 51.2882%, Training Loss: 0.9681%\n",
      "Epoch [24/300], Step [189/225], Training Accuracy: 51.3393%, Training Loss: 0.9676%\n",
      "Epoch [24/300], Step [190/225], Training Accuracy: 51.3569%, Training Loss: 0.9679%\n",
      "Epoch [24/300], Step [191/225], Training Accuracy: 51.3171%, Training Loss: 0.9681%\n",
      "Epoch [24/300], Step [192/225], Training Accuracy: 51.3590%, Training Loss: 0.9677%\n",
      "Epoch [24/300], Step [193/225], Training Accuracy: 51.3277%, Training Loss: 0.9678%\n",
      "Epoch [24/300], Step [194/225], Training Accuracy: 51.3370%, Training Loss: 0.9679%\n",
      "Epoch [24/300], Step [195/225], Training Accuracy: 51.3622%, Training Loss: 0.9674%\n",
      "Epoch [24/300], Step [196/225], Training Accuracy: 51.3951%, Training Loss: 0.9674%\n",
      "Epoch [24/300], Step [197/225], Training Accuracy: 51.4277%, Training Loss: 0.9669%\n",
      "Epoch [24/300], Step [198/225], Training Accuracy: 51.4283%, Training Loss: 0.9664%\n",
      "Epoch [24/300], Step [199/225], Training Accuracy: 51.4447%, Training Loss: 0.9661%\n",
      "Epoch [24/300], Step [200/225], Training Accuracy: 51.4688%, Training Loss: 0.9662%\n",
      "Epoch [24/300], Step [201/225], Training Accuracy: 51.4459%, Training Loss: 0.9664%\n",
      "Epoch [24/300], Step [202/225], Training Accuracy: 51.4542%, Training Loss: 0.9661%\n",
      "Epoch [24/300], Step [203/225], Training Accuracy: 51.4470%, Training Loss: 0.9664%\n",
      "Epoch [24/300], Step [204/225], Training Accuracy: 51.4706%, Training Loss: 0.9662%\n",
      "Epoch [24/300], Step [205/225], Training Accuracy: 51.4634%, Training Loss: 0.9660%\n",
      "Epoch [24/300], Step [206/225], Training Accuracy: 51.4639%, Training Loss: 0.9663%\n",
      "Epoch [24/300], Step [207/225], Training Accuracy: 51.4493%, Training Loss: 0.9668%\n",
      "Epoch [24/300], Step [208/225], Training Accuracy: 51.5099%, Training Loss: 0.9663%\n",
      "Epoch [24/300], Step [209/225], Training Accuracy: 51.4952%, Training Loss: 0.9666%\n",
      "Epoch [24/300], Step [210/225], Training Accuracy: 51.4732%, Training Loss: 0.9666%\n",
      "Epoch [24/300], Step [211/225], Training Accuracy: 51.5033%, Training Loss: 0.9663%\n",
      "Epoch [24/300], Step [212/225], Training Accuracy: 51.4519%, Training Loss: 0.9667%\n",
      "Epoch [24/300], Step [213/225], Training Accuracy: 51.4305%, Training Loss: 0.9673%\n",
      "Epoch [24/300], Step [214/225], Training Accuracy: 51.4165%, Training Loss: 0.9672%\n",
      "Epoch [24/300], Step [215/225], Training Accuracy: 51.4390%, Training Loss: 0.9671%\n",
      "Epoch [24/300], Step [216/225], Training Accuracy: 51.4395%, Training Loss: 0.9673%\n",
      "Epoch [24/300], Step [217/225], Training Accuracy: 51.4329%, Training Loss: 0.9675%\n",
      "Epoch [24/300], Step [218/225], Training Accuracy: 51.4120%, Training Loss: 0.9682%\n",
      "Epoch [24/300], Step [219/225], Training Accuracy: 51.3627%, Training Loss: 0.9686%\n",
      "Epoch [24/300], Step [220/225], Training Accuracy: 51.4134%, Training Loss: 0.9682%\n",
      "Epoch [24/300], Step [221/225], Training Accuracy: 51.4282%, Training Loss: 0.9684%\n",
      "Epoch [24/300], Step [222/225], Training Accuracy: 51.4288%, Training Loss: 0.9683%\n",
      "Epoch [24/300], Step [223/225], Training Accuracy: 51.3733%, Training Loss: 0.9689%\n",
      "Epoch [24/300], Step [224/225], Training Accuracy: 51.3532%, Training Loss: 0.9690%\n",
      "Epoch [24/300], Step [225/225], Training Accuracy: 51.3619%, Training Loss: 0.9690%\n",
      "Epoch [25/300], Step [1/225], Training Accuracy: 65.6250%, Training Loss: 0.7974%\n",
      "Epoch [25/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 0.9706%\n",
      "Epoch [25/300], Step [3/225], Training Accuracy: 51.5625%, Training Loss: 1.0238%\n",
      "Epoch [25/300], Step [4/225], Training Accuracy: 50.7812%, Training Loss: 1.0085%\n",
      "Epoch [25/300], Step [5/225], Training Accuracy: 52.1875%, Training Loss: 0.9744%\n",
      "Epoch [25/300], Step [6/225], Training Accuracy: 51.0417%, Training Loss: 0.9964%\n",
      "Epoch [25/300], Step [7/225], Training Accuracy: 50.8929%, Training Loss: 0.9911%\n",
      "Epoch [25/300], Step [8/225], Training Accuracy: 51.3672%, Training Loss: 0.9972%\n",
      "Epoch [25/300], Step [9/225], Training Accuracy: 51.3889%, Training Loss: 0.9900%\n",
      "Epoch [25/300], Step [10/225], Training Accuracy: 51.7188%, Training Loss: 0.9850%\n",
      "Epoch [25/300], Step [11/225], Training Accuracy: 52.4148%, Training Loss: 0.9799%\n",
      "Epoch [25/300], Step [12/225], Training Accuracy: 52.2135%, Training Loss: 0.9861%\n",
      "Epoch [25/300], Step [13/225], Training Accuracy: 53.0048%, Training Loss: 0.9837%\n",
      "Epoch [25/300], Step [14/225], Training Accuracy: 52.7902%, Training Loss: 0.9861%\n",
      "Epoch [25/300], Step [15/225], Training Accuracy: 52.1875%, Training Loss: 0.9979%\n",
      "Epoch [25/300], Step [16/225], Training Accuracy: 52.0508%, Training Loss: 0.9983%\n",
      "Epoch [25/300], Step [17/225], Training Accuracy: 51.7463%, Training Loss: 0.9931%\n",
      "Epoch [25/300], Step [18/225], Training Accuracy: 51.6493%, Training Loss: 0.9913%\n",
      "Epoch [25/300], Step [19/225], Training Accuracy: 50.9868%, Training Loss: 0.9930%\n",
      "Epoch [25/300], Step [20/225], Training Accuracy: 51.2500%, Training Loss: 0.9901%\n",
      "Epoch [25/300], Step [21/225], Training Accuracy: 51.3393%, Training Loss: 0.9857%\n",
      "Epoch [25/300], Step [22/225], Training Accuracy: 51.0653%, Training Loss: 0.9865%\n",
      "Epoch [25/300], Step [23/225], Training Accuracy: 50.8832%, Training Loss: 0.9828%\n",
      "Epoch [25/300], Step [24/225], Training Accuracy: 50.8464%, Training Loss: 0.9855%\n",
      "Epoch [25/300], Step [25/225], Training Accuracy: 50.8750%, Training Loss: 0.9811%\n",
      "Epoch [25/300], Step [26/225], Training Accuracy: 50.5409%, Training Loss: 0.9822%\n",
      "Epoch [25/300], Step [27/225], Training Accuracy: 50.2894%, Training Loss: 0.9839%\n",
      "Epoch [25/300], Step [28/225], Training Accuracy: 50.4464%, Training Loss: 0.9807%\n",
      "Epoch [25/300], Step [29/225], Training Accuracy: 50.7004%, Training Loss: 0.9765%\n",
      "Epoch [25/300], Step [30/225], Training Accuracy: 50.7292%, Training Loss: 0.9743%\n",
      "Epoch [25/300], Step [31/225], Training Accuracy: 50.8569%, Training Loss: 0.9734%\n",
      "Epoch [25/300], Step [32/225], Training Accuracy: 51.0254%, Training Loss: 0.9708%\n",
      "Epoch [25/300], Step [33/225], Training Accuracy: 51.1837%, Training Loss: 0.9688%\n",
      "Epoch [25/300], Step [34/225], Training Accuracy: 51.1949%, Training Loss: 0.9713%\n",
      "Epoch [25/300], Step [35/225], Training Accuracy: 51.0268%, Training Loss: 0.9715%\n",
      "Epoch [25/300], Step [36/225], Training Accuracy: 51.0417%, Training Loss: 0.9715%\n",
      "Epoch [25/300], Step [37/225], Training Accuracy: 51.1824%, Training Loss: 0.9687%\n",
      "Epoch [25/300], Step [38/225], Training Accuracy: 51.3980%, Training Loss: 0.9670%\n",
      "Epoch [25/300], Step [39/225], Training Accuracy: 51.4423%, Training Loss: 0.9665%\n",
      "Epoch [25/300], Step [40/225], Training Accuracy: 51.4062%, Training Loss: 0.9665%\n",
      "Epoch [25/300], Step [41/225], Training Accuracy: 51.1814%, Training Loss: 0.9685%\n",
      "Epoch [25/300], Step [42/225], Training Accuracy: 51.3021%, Training Loss: 0.9671%\n",
      "Epoch [25/300], Step [43/225], Training Accuracy: 51.2718%, Training Loss: 0.9664%\n",
      "Epoch [25/300], Step [44/225], Training Accuracy: 51.3849%, Training Loss: 0.9651%\n",
      "Epoch [25/300], Step [45/225], Training Accuracy: 51.3194%, Training Loss: 0.9649%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Step [46/225], Training Accuracy: 51.6304%, Training Loss: 0.9616%\n",
      "Epoch [25/300], Step [47/225], Training Accuracy: 51.6290%, Training Loss: 0.9633%\n",
      "Epoch [25/300], Step [48/225], Training Accuracy: 51.6276%, Training Loss: 0.9643%\n",
      "Epoch [25/300], Step [49/225], Training Accuracy: 51.4987%, Training Loss: 0.9658%\n",
      "Epoch [25/300], Step [50/225], Training Accuracy: 51.5625%, Training Loss: 0.9643%\n",
      "Epoch [25/300], Step [51/225], Training Accuracy: 51.7157%, Training Loss: 0.9614%\n",
      "Epoch [25/300], Step [52/225], Training Accuracy: 51.8329%, Training Loss: 0.9593%\n",
      "Epoch [25/300], Step [53/225], Training Accuracy: 51.7394%, Training Loss: 0.9592%\n",
      "Epoch [25/300], Step [54/225], Training Accuracy: 51.4468%, Training Loss: 0.9613%\n",
      "Epoch [25/300], Step [55/225], Training Accuracy: 51.3636%, Training Loss: 0.9634%\n",
      "Epoch [25/300], Step [56/225], Training Accuracy: 51.3672%, Training Loss: 0.9644%\n",
      "Epoch [25/300], Step [57/225], Training Accuracy: 51.5625%, Training Loss: 0.9613%\n",
      "Epoch [25/300], Step [58/225], Training Accuracy: 51.5894%, Training Loss: 0.9616%\n",
      "Epoch [25/300], Step [59/225], Training Accuracy: 51.7214%, Training Loss: 0.9608%\n",
      "Epoch [25/300], Step [60/225], Training Accuracy: 51.6927%, Training Loss: 0.9606%\n",
      "Epoch [25/300], Step [61/225], Training Accuracy: 51.7674%, Training Loss: 0.9603%\n",
      "Epoch [25/300], Step [62/225], Training Accuracy: 51.7137%, Training Loss: 0.9610%\n",
      "Epoch [25/300], Step [63/225], Training Accuracy: 51.5873%, Training Loss: 0.9619%\n",
      "Epoch [25/300], Step [64/225], Training Accuracy: 51.6357%, Training Loss: 0.9613%\n",
      "Epoch [25/300], Step [65/225], Training Accuracy: 51.5385%, Training Loss: 0.9618%\n",
      "Epoch [25/300], Step [66/225], Training Accuracy: 51.7282%, Training Loss: 0.9596%\n",
      "Epoch [25/300], Step [67/225], Training Accuracy: 51.5858%, Training Loss: 0.9603%\n",
      "Epoch [25/300], Step [68/225], Training Accuracy: 51.5625%, Training Loss: 0.9599%\n",
      "Epoch [25/300], Step [69/225], Training Accuracy: 51.4719%, Training Loss: 0.9602%\n",
      "Epoch [25/300], Step [70/225], Training Accuracy: 51.3839%, Training Loss: 0.9611%\n",
      "Epoch [25/300], Step [71/225], Training Accuracy: 51.3864%, Training Loss: 0.9603%\n",
      "Epoch [25/300], Step [72/225], Training Accuracy: 51.3021%, Training Loss: 0.9622%\n",
      "Epoch [25/300], Step [73/225], Training Accuracy: 51.2842%, Training Loss: 0.9648%\n",
      "Epoch [25/300], Step [74/225], Training Accuracy: 51.3302%, Training Loss: 0.9629%\n",
      "Epoch [25/300], Step [75/225], Training Accuracy: 51.3125%, Training Loss: 0.9627%\n",
      "Epoch [25/300], Step [76/225], Training Accuracy: 51.3569%, Training Loss: 0.9623%\n",
      "Epoch [25/300], Step [77/225], Training Accuracy: 51.5016%, Training Loss: 0.9617%\n",
      "Epoch [25/300], Step [78/225], Training Accuracy: 51.4423%, Training Loss: 0.9629%\n",
      "Epoch [25/300], Step [79/225], Training Accuracy: 51.4043%, Training Loss: 0.9643%\n",
      "Epoch [25/300], Step [80/225], Training Accuracy: 51.3086%, Training Loss: 0.9648%\n",
      "Epoch [25/300], Step [81/225], Training Accuracy: 51.4082%, Training Loss: 0.9650%\n",
      "Epoch [25/300], Step [82/225], Training Accuracy: 51.4863%, Training Loss: 0.9642%\n",
      "Epoch [25/300], Step [83/225], Training Accuracy: 51.4684%, Training Loss: 0.9635%\n",
      "Epoch [25/300], Step [84/225], Training Accuracy: 51.4695%, Training Loss: 0.9638%\n",
      "Epoch [25/300], Step [85/225], Training Accuracy: 51.5074%, Training Loss: 0.9628%\n",
      "Epoch [25/300], Step [86/225], Training Accuracy: 51.5625%, Training Loss: 0.9624%\n",
      "Epoch [25/300], Step [87/225], Training Accuracy: 51.5805%, Training Loss: 0.9623%\n",
      "Epoch [25/300], Step [88/225], Training Accuracy: 51.4560%, Training Loss: 0.9630%\n",
      "Epoch [25/300], Step [89/225], Training Accuracy: 51.4045%, Training Loss: 0.9644%\n",
      "Epoch [25/300], Step [90/225], Training Accuracy: 51.2847%, Training Loss: 0.9653%\n",
      "Epoch [25/300], Step [91/225], Training Accuracy: 51.3393%, Training Loss: 0.9636%\n",
      "Epoch [25/300], Step [92/225], Training Accuracy: 51.2908%, Training Loss: 0.9637%\n",
      "Epoch [25/300], Step [93/225], Training Accuracy: 51.3105%, Training Loss: 0.9638%\n",
      "Epoch [25/300], Step [94/225], Training Accuracy: 51.3963%, Training Loss: 0.9624%\n",
      "Epoch [25/300], Step [95/225], Training Accuracy: 51.3980%, Training Loss: 0.9628%\n",
      "Epoch [25/300], Step [96/225], Training Accuracy: 51.4160%, Training Loss: 0.9622%\n",
      "Epoch [25/300], Step [97/225], Training Accuracy: 51.4981%, Training Loss: 0.9613%\n",
      "Epoch [25/300], Step [98/225], Training Accuracy: 51.4987%, Training Loss: 0.9608%\n",
      "Epoch [25/300], Step [99/225], Training Accuracy: 51.5625%, Training Loss: 0.9605%\n",
      "Epoch [25/300], Step [100/225], Training Accuracy: 51.3906%, Training Loss: 0.9614%\n",
      "Epoch [25/300], Step [101/225], Training Accuracy: 51.3614%, Training Loss: 0.9620%\n",
      "Epoch [25/300], Step [102/225], Training Accuracy: 51.2868%, Training Loss: 0.9631%\n",
      "Epoch [25/300], Step [103/225], Training Accuracy: 51.1833%, Training Loss: 0.9649%\n",
      "Epoch [25/300], Step [104/225], Training Accuracy: 51.2019%, Training Loss: 0.9646%\n",
      "Epoch [25/300], Step [105/225], Training Accuracy: 51.1458%, Training Loss: 0.9639%\n",
      "Epoch [25/300], Step [106/225], Training Accuracy: 51.0908%, Training Loss: 0.9649%\n",
      "Epoch [25/300], Step [107/225], Training Accuracy: 51.1682%, Training Loss: 0.9647%\n",
      "Epoch [25/300], Step [108/225], Training Accuracy: 51.1574%, Training Loss: 0.9655%\n",
      "Epoch [25/300], Step [109/225], Training Accuracy: 51.1611%, Training Loss: 0.9654%\n",
      "Epoch [25/300], Step [110/225], Training Accuracy: 51.1932%, Training Loss: 0.9650%\n",
      "Epoch [25/300], Step [111/225], Training Accuracy: 51.3091%, Training Loss: 0.9655%\n",
      "Epoch [25/300], Step [112/225], Training Accuracy: 51.3672%, Training Loss: 0.9648%\n",
      "Epoch [25/300], Step [113/225], Training Accuracy: 51.3551%, Training Loss: 0.9656%\n",
      "Epoch [25/300], Step [114/225], Training Accuracy: 51.3432%, Training Loss: 0.9654%\n",
      "Epoch [25/300], Step [115/225], Training Accuracy: 51.4402%, Training Loss: 0.9646%\n",
      "Epoch [25/300], Step [116/225], Training Accuracy: 51.4682%, Training Loss: 0.9646%\n",
      "Epoch [25/300], Step [117/225], Training Accuracy: 51.3488%, Training Loss: 0.9661%\n",
      "Epoch [25/300], Step [118/225], Training Accuracy: 51.3771%, Training Loss: 0.9663%\n",
      "Epoch [25/300], Step [119/225], Training Accuracy: 51.3262%, Training Loss: 0.9667%\n",
      "Epoch [25/300], Step [120/225], Training Accuracy: 51.3802%, Training Loss: 0.9660%\n",
      "Epoch [25/300], Step [121/225], Training Accuracy: 51.3042%, Training Loss: 0.9665%\n",
      "Epoch [25/300], Step [122/225], Training Accuracy: 51.3192%, Training Loss: 0.9664%\n",
      "Epoch [25/300], Step [123/225], Training Accuracy: 51.2957%, Training Loss: 0.9662%\n",
      "Epoch [25/300], Step [124/225], Training Accuracy: 51.3357%, Training Loss: 0.9661%\n",
      "Epoch [25/300], Step [125/225], Training Accuracy: 51.3000%, Training Loss: 0.9673%\n",
      "Epoch [25/300], Step [126/225], Training Accuracy: 51.2773%, Training Loss: 0.9680%\n",
      "Epoch [25/300], Step [127/225], Training Accuracy: 51.2426%, Training Loss: 0.9680%\n",
      "Epoch [25/300], Step [128/225], Training Accuracy: 51.2329%, Training Loss: 0.9683%\n",
      "Epoch [25/300], Step [129/225], Training Accuracy: 51.2112%, Training Loss: 0.9688%\n",
      "Epoch [25/300], Step [130/225], Training Accuracy: 51.1899%, Training Loss: 0.9693%\n",
      "Epoch [25/300], Step [131/225], Training Accuracy: 51.1570%, Training Loss: 0.9691%\n",
      "Epoch [25/300], Step [132/225], Training Accuracy: 51.1364%, Training Loss: 0.9695%\n",
      "Epoch [25/300], Step [133/225], Training Accuracy: 51.2336%, Training Loss: 0.9687%\n",
      "Epoch [25/300], Step [134/225], Training Accuracy: 51.2710%, Training Loss: 0.9684%\n",
      "Epoch [25/300], Step [135/225], Training Accuracy: 51.2731%, Training Loss: 0.9682%\n",
      "Epoch [25/300], Step [136/225], Training Accuracy: 51.2408%, Training Loss: 0.9675%\n",
      "Epoch [25/300], Step [137/225], Training Accuracy: 51.2660%, Training Loss: 0.9669%\n",
      "Epoch [25/300], Step [138/225], Training Accuracy: 51.3134%, Training Loss: 0.9658%\n",
      "Epoch [25/300], Step [139/225], Training Accuracy: 51.3040%, Training Loss: 0.9661%\n",
      "Epoch [25/300], Step [140/225], Training Accuracy: 51.3616%, Training Loss: 0.9659%\n",
      "Epoch [25/300], Step [141/225], Training Accuracy: 51.3963%, Training Loss: 0.9657%\n",
      "Epoch [25/300], Step [142/225], Training Accuracy: 51.3974%, Training Loss: 0.9658%\n",
      "Epoch [25/300], Step [143/225], Training Accuracy: 51.4314%, Training Loss: 0.9653%\n",
      "Epoch [25/300], Step [144/225], Training Accuracy: 51.4214%, Training Loss: 0.9651%\n",
      "Epoch [25/300], Step [145/225], Training Accuracy: 51.4763%, Training Loss: 0.9643%\n",
      "Epoch [25/300], Step [146/225], Training Accuracy: 51.5090%, Training Loss: 0.9642%\n",
      "Epoch [25/300], Step [147/225], Training Accuracy: 51.4456%, Training Loss: 0.9649%\n",
      "Epoch [25/300], Step [148/225], Training Accuracy: 51.5308%, Training Loss: 0.9645%\n",
      "Epoch [25/300], Step [149/225], Training Accuracy: 51.5101%, Training Loss: 0.9647%\n",
      "Epoch [25/300], Step [150/225], Training Accuracy: 51.4792%, Training Loss: 0.9652%\n",
      "Epoch [25/300], Step [151/225], Training Accuracy: 51.5108%, Training Loss: 0.9646%\n",
      "Epoch [25/300], Step [152/225], Training Accuracy: 51.4803%, Training Loss: 0.9647%\n",
      "Epoch [25/300], Step [153/225], Training Accuracy: 51.5114%, Training Loss: 0.9641%\n",
      "Epoch [25/300], Step [154/225], Training Accuracy: 51.5016%, Training Loss: 0.9636%\n",
      "Epoch [25/300], Step [155/225], Training Accuracy: 51.4617%, Training Loss: 0.9647%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/300], Step [156/225], Training Accuracy: 51.4223%, Training Loss: 0.9648%\n",
      "Epoch [25/300], Step [157/225], Training Accuracy: 51.4232%, Training Loss: 0.9652%\n",
      "Epoch [25/300], Step [158/225], Training Accuracy: 51.4537%, Training Loss: 0.9654%\n",
      "Epoch [25/300], Step [159/225], Training Accuracy: 51.4642%, Training Loss: 0.9650%\n",
      "Epoch [25/300], Step [160/225], Training Accuracy: 51.4160%, Training Loss: 0.9650%\n",
      "Epoch [25/300], Step [161/225], Training Accuracy: 51.5043%, Training Loss: 0.9640%\n",
      "Epoch [25/300], Step [162/225], Training Accuracy: 51.5336%, Training Loss: 0.9642%\n",
      "Epoch [25/300], Step [163/225], Training Accuracy: 51.4762%, Training Loss: 0.9639%\n",
      "Epoch [25/300], Step [164/225], Training Accuracy: 51.5339%, Training Loss: 0.9629%\n",
      "Epoch [25/300], Step [165/225], Training Accuracy: 51.5436%, Training Loss: 0.9627%\n",
      "Epoch [25/300], Step [166/225], Training Accuracy: 51.5437%, Training Loss: 0.9628%\n",
      "Epoch [25/300], Step [167/225], Training Accuracy: 51.5719%, Training Loss: 0.9621%\n",
      "Epoch [25/300], Step [168/225], Training Accuracy: 51.5439%, Training Loss: 0.9623%\n",
      "Epoch [25/300], Step [169/225], Training Accuracy: 51.5440%, Training Loss: 0.9623%\n",
      "Epoch [25/300], Step [170/225], Training Accuracy: 51.5074%, Training Loss: 0.9629%\n",
      "Epoch [25/300], Step [171/225], Training Accuracy: 51.5077%, Training Loss: 0.9628%\n",
      "Epoch [25/300], Step [172/225], Training Accuracy: 51.4807%, Training Loss: 0.9634%\n",
      "Epoch [25/300], Step [173/225], Training Accuracy: 51.4993%, Training Loss: 0.9631%\n",
      "Epoch [25/300], Step [174/225], Training Accuracy: 51.4907%, Training Loss: 0.9632%\n",
      "Epoch [25/300], Step [175/225], Training Accuracy: 51.4643%, Training Loss: 0.9632%\n",
      "Epoch [25/300], Step [176/225], Training Accuracy: 51.5004%, Training Loss: 0.9630%\n",
      "Epoch [25/300], Step [177/225], Training Accuracy: 51.4654%, Training Loss: 0.9629%\n",
      "Epoch [25/300], Step [178/225], Training Accuracy: 51.4308%, Training Loss: 0.9628%\n",
      "Epoch [25/300], Step [179/225], Training Accuracy: 51.4578%, Training Loss: 0.9627%\n",
      "Epoch [25/300], Step [180/225], Training Accuracy: 51.5538%, Training Loss: 0.9617%\n",
      "Epoch [25/300], Step [181/225], Training Accuracy: 51.5107%, Training Loss: 0.9624%\n",
      "Epoch [25/300], Step [182/225], Training Accuracy: 51.5196%, Training Loss: 0.9627%\n",
      "Epoch [25/300], Step [183/225], Training Accuracy: 51.5454%, Training Loss: 0.9623%\n",
      "Epoch [25/300], Step [184/225], Training Accuracy: 51.5370%, Training Loss: 0.9624%\n",
      "Epoch [25/300], Step [185/225], Training Accuracy: 51.5034%, Training Loss: 0.9624%\n",
      "Epoch [25/300], Step [186/225], Training Accuracy: 51.5205%, Training Loss: 0.9622%\n",
      "Epoch [25/300], Step [187/225], Training Accuracy: 51.5541%, Training Loss: 0.9615%\n",
      "Epoch [25/300], Step [188/225], Training Accuracy: 51.6041%, Training Loss: 0.9610%\n",
      "Epoch [25/300], Step [189/225], Training Accuracy: 51.6534%, Training Loss: 0.9605%\n",
      "Epoch [25/300], Step [190/225], Training Accuracy: 51.6776%, Training Loss: 0.9608%\n",
      "Epoch [25/300], Step [191/225], Training Accuracy: 51.6361%, Training Loss: 0.9610%\n",
      "Epoch [25/300], Step [192/225], Training Accuracy: 51.6764%, Training Loss: 0.9606%\n",
      "Epoch [25/300], Step [193/225], Training Accuracy: 51.6354%, Training Loss: 0.9607%\n",
      "Epoch [25/300], Step [194/225], Training Accuracy: 51.6511%, Training Loss: 0.9608%\n",
      "Epoch [25/300], Step [195/225], Training Accuracy: 51.6827%, Training Loss: 0.9603%\n",
      "Epoch [25/300], Step [196/225], Training Accuracy: 51.7060%, Training Loss: 0.9603%\n",
      "Epoch [25/300], Step [197/225], Training Accuracy: 51.7449%, Training Loss: 0.9598%\n",
      "Epoch [25/300], Step [198/225], Training Accuracy: 51.7440%, Training Loss: 0.9593%\n",
      "Epoch [25/300], Step [199/225], Training Accuracy: 51.7588%, Training Loss: 0.9591%\n",
      "Epoch [25/300], Step [200/225], Training Accuracy: 51.7734%, Training Loss: 0.9591%\n",
      "Epoch [25/300], Step [201/225], Training Accuracy: 51.7568%, Training Loss: 0.9594%\n",
      "Epoch [25/300], Step [202/225], Training Accuracy: 51.7636%, Training Loss: 0.9590%\n",
      "Epoch [25/300], Step [203/225], Training Accuracy: 51.7549%, Training Loss: 0.9593%\n",
      "Epoch [25/300], Step [204/225], Training Accuracy: 51.7846%, Training Loss: 0.9591%\n",
      "Epoch [25/300], Step [205/225], Training Accuracy: 51.7759%, Training Loss: 0.9588%\n",
      "Epoch [25/300], Step [206/225], Training Accuracy: 51.7673%, Training Loss: 0.9592%\n",
      "Epoch [25/300], Step [207/225], Training Accuracy: 51.7512%, Training Loss: 0.9597%\n",
      "Epoch [25/300], Step [208/225], Training Accuracy: 51.8179%, Training Loss: 0.9592%\n",
      "Epoch [25/300], Step [209/225], Training Accuracy: 51.8092%, Training Loss: 0.9595%\n",
      "Epoch [25/300], Step [210/225], Training Accuracy: 51.7857%, Training Loss: 0.9595%\n",
      "Epoch [25/300], Step [211/225], Training Accuracy: 51.8069%, Training Loss: 0.9592%\n",
      "Epoch [25/300], Step [212/225], Training Accuracy: 51.7541%, Training Loss: 0.9596%\n",
      "Epoch [25/300], Step [213/225], Training Accuracy: 51.7386%, Training Loss: 0.9602%\n",
      "Epoch [25/300], Step [214/225], Training Accuracy: 51.7231%, Training Loss: 0.9601%\n",
      "Epoch [25/300], Step [215/225], Training Accuracy: 51.7442%, Training Loss: 0.9600%\n",
      "Epoch [25/300], Step [216/225], Training Accuracy: 51.7433%, Training Loss: 0.9603%\n",
      "Epoch [25/300], Step [217/225], Training Accuracy: 51.7353%, Training Loss: 0.9604%\n",
      "Epoch [25/300], Step [218/225], Training Accuracy: 51.7274%, Training Loss: 0.9611%\n",
      "Epoch [25/300], Step [219/225], Training Accuracy: 51.6767%, Training Loss: 0.9615%\n",
      "Epoch [25/300], Step [220/225], Training Accuracy: 51.7188%, Training Loss: 0.9611%\n",
      "Epoch [25/300], Step [221/225], Training Accuracy: 51.7322%, Training Loss: 0.9613%\n",
      "Epoch [25/300], Step [222/225], Training Accuracy: 51.7314%, Training Loss: 0.9613%\n",
      "Epoch [25/300], Step [223/225], Training Accuracy: 51.6816%, Training Loss: 0.9619%\n",
      "Epoch [25/300], Step [224/225], Training Accuracy: 51.6602%, Training Loss: 0.9619%\n",
      "Epoch [25/300], Step [225/225], Training Accuracy: 51.6537%, Training Loss: 0.9619%\n",
      "Epoch [26/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.7901%\n",
      "Epoch [26/300], Step [2/225], Training Accuracy: 54.6875%, Training Loss: 0.9661%\n",
      "Epoch [26/300], Step [3/225], Training Accuracy: 50.5208%, Training Loss: 1.0185%\n",
      "Epoch [26/300], Step [4/225], Training Accuracy: 50.3906%, Training Loss: 1.0022%\n",
      "Epoch [26/300], Step [5/225], Training Accuracy: 51.8750%, Training Loss: 0.9679%\n",
      "Epoch [26/300], Step [6/225], Training Accuracy: 50.7812%, Training Loss: 0.9900%\n",
      "Epoch [26/300], Step [7/225], Training Accuracy: 50.6696%, Training Loss: 0.9848%\n",
      "Epoch [26/300], Step [8/225], Training Accuracy: 50.9766%, Training Loss: 0.9912%\n",
      "Epoch [26/300], Step [9/225], Training Accuracy: 51.2153%, Training Loss: 0.9837%\n",
      "Epoch [26/300], Step [10/225], Training Accuracy: 51.5625%, Training Loss: 0.9789%\n",
      "Epoch [26/300], Step [11/225], Training Accuracy: 52.4148%, Training Loss: 0.9740%\n",
      "Epoch [26/300], Step [12/225], Training Accuracy: 52.2135%, Training Loss: 0.9803%\n",
      "Epoch [26/300], Step [13/225], Training Accuracy: 53.0048%, Training Loss: 0.9776%\n",
      "Epoch [26/300], Step [14/225], Training Accuracy: 52.7902%, Training Loss: 0.9799%\n",
      "Epoch [26/300], Step [15/225], Training Accuracy: 52.2917%, Training Loss: 0.9922%\n",
      "Epoch [26/300], Step [16/225], Training Accuracy: 52.1484%, Training Loss: 0.9920%\n",
      "Epoch [26/300], Step [17/225], Training Accuracy: 51.9301%, Training Loss: 0.9869%\n",
      "Epoch [26/300], Step [18/225], Training Accuracy: 51.8229%, Training Loss: 0.9851%\n",
      "Epoch [26/300], Step [19/225], Training Accuracy: 51.0691%, Training Loss: 0.9868%\n",
      "Epoch [26/300], Step [20/225], Training Accuracy: 51.3281%, Training Loss: 0.9837%\n",
      "Epoch [26/300], Step [21/225], Training Accuracy: 51.4137%, Training Loss: 0.9794%\n",
      "Epoch [26/300], Step [22/225], Training Accuracy: 51.2074%, Training Loss: 0.9801%\n",
      "Epoch [26/300], Step [23/225], Training Accuracy: 51.2228%, Training Loss: 0.9764%\n",
      "Epoch [26/300], Step [24/225], Training Accuracy: 51.2370%, Training Loss: 0.9790%\n",
      "Epoch [26/300], Step [25/225], Training Accuracy: 51.3125%, Training Loss: 0.9747%\n",
      "Epoch [26/300], Step [26/225], Training Accuracy: 51.0216%, Training Loss: 0.9755%\n",
      "Epoch [26/300], Step [27/225], Training Accuracy: 50.8681%, Training Loss: 0.9774%\n",
      "Epoch [26/300], Step [28/225], Training Accuracy: 50.9487%, Training Loss: 0.9742%\n",
      "Epoch [26/300], Step [29/225], Training Accuracy: 51.1853%, Training Loss: 0.9700%\n",
      "Epoch [26/300], Step [30/225], Training Accuracy: 51.1979%, Training Loss: 0.9680%\n",
      "Epoch [26/300], Step [31/225], Training Accuracy: 51.3105%, Training Loss: 0.9670%\n",
      "Epoch [26/300], Step [32/225], Training Accuracy: 51.5137%, Training Loss: 0.9643%\n",
      "Epoch [26/300], Step [33/225], Training Accuracy: 51.6572%, Training Loss: 0.9625%\n",
      "Epoch [26/300], Step [34/225], Training Accuracy: 51.7463%, Training Loss: 0.9647%\n",
      "Epoch [26/300], Step [35/225], Training Accuracy: 51.5625%, Training Loss: 0.9649%\n",
      "Epoch [26/300], Step [36/225], Training Accuracy: 51.6059%, Training Loss: 0.9650%\n",
      "Epoch [26/300], Step [37/225], Training Accuracy: 51.8581%, Training Loss: 0.9622%\n",
      "Epoch [26/300], Step [38/225], Training Accuracy: 52.0559%, Training Loss: 0.9605%\n",
      "Epoch [26/300], Step [39/225], Training Accuracy: 52.1234%, Training Loss: 0.9599%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Step [40/225], Training Accuracy: 52.0703%, Training Loss: 0.9600%\n",
      "Epoch [26/300], Step [41/225], Training Accuracy: 51.7912%, Training Loss: 0.9620%\n",
      "Epoch [26/300], Step [42/225], Training Accuracy: 51.9345%, Training Loss: 0.9606%\n",
      "Epoch [26/300], Step [43/225], Training Accuracy: 51.8532%, Training Loss: 0.9599%\n",
      "Epoch [26/300], Step [44/225], Training Accuracy: 51.9176%, Training Loss: 0.9587%\n",
      "Epoch [26/300], Step [45/225], Training Accuracy: 51.8403%, Training Loss: 0.9587%\n",
      "Epoch [26/300], Step [46/225], Training Accuracy: 52.1399%, Training Loss: 0.9553%\n",
      "Epoch [26/300], Step [47/225], Training Accuracy: 52.1609%, Training Loss: 0.9570%\n",
      "Epoch [26/300], Step [48/225], Training Accuracy: 52.1810%, Training Loss: 0.9582%\n",
      "Epoch [26/300], Step [49/225], Training Accuracy: 51.9770%, Training Loss: 0.9597%\n",
      "Epoch [26/300], Step [50/225], Training Accuracy: 52.0000%, Training Loss: 0.9583%\n",
      "Epoch [26/300], Step [51/225], Training Accuracy: 52.1446%, Training Loss: 0.9553%\n",
      "Epoch [26/300], Step [52/225], Training Accuracy: 52.3438%, Training Loss: 0.9531%\n",
      "Epoch [26/300], Step [53/225], Training Accuracy: 52.2406%, Training Loss: 0.9530%\n",
      "Epoch [26/300], Step [54/225], Training Accuracy: 51.9387%, Training Loss: 0.9553%\n",
      "Epoch [26/300], Step [55/225], Training Accuracy: 51.8182%, Training Loss: 0.9573%\n",
      "Epoch [26/300], Step [56/225], Training Accuracy: 51.7857%, Training Loss: 0.9585%\n",
      "Epoch [26/300], Step [57/225], Training Accuracy: 51.9737%, Training Loss: 0.9553%\n",
      "Epoch [26/300], Step [58/225], Training Accuracy: 51.9666%, Training Loss: 0.9556%\n",
      "Epoch [26/300], Step [59/225], Training Accuracy: 52.0657%, Training Loss: 0.9547%\n",
      "Epoch [26/300], Step [60/225], Training Accuracy: 52.0573%, Training Loss: 0.9544%\n",
      "Epoch [26/300], Step [61/225], Training Accuracy: 52.1773%, Training Loss: 0.9541%\n",
      "Epoch [26/300], Step [62/225], Training Accuracy: 52.1169%, Training Loss: 0.9547%\n",
      "Epoch [26/300], Step [63/225], Training Accuracy: 51.9841%, Training Loss: 0.9556%\n",
      "Epoch [26/300], Step [64/225], Training Accuracy: 52.0264%, Training Loss: 0.9549%\n",
      "Epoch [26/300], Step [65/225], Training Accuracy: 51.9471%, Training Loss: 0.9554%\n",
      "Epoch [26/300], Step [66/225], Training Accuracy: 52.1780%, Training Loss: 0.9532%\n",
      "Epoch [26/300], Step [67/225], Training Accuracy: 52.0756%, Training Loss: 0.9539%\n",
      "Epoch [26/300], Step [68/225], Training Accuracy: 52.1140%, Training Loss: 0.9536%\n",
      "Epoch [26/300], Step [69/225], Training Accuracy: 52.0154%, Training Loss: 0.9539%\n",
      "Epoch [26/300], Step [70/225], Training Accuracy: 51.9420%, Training Loss: 0.9547%\n",
      "Epoch [26/300], Step [71/225], Training Accuracy: 51.9366%, Training Loss: 0.9540%\n",
      "Epoch [26/300], Step [72/225], Training Accuracy: 51.8663%, Training Loss: 0.9558%\n",
      "Epoch [26/300], Step [73/225], Training Accuracy: 51.8193%, Training Loss: 0.9584%\n",
      "Epoch [26/300], Step [74/225], Training Accuracy: 51.8581%, Training Loss: 0.9565%\n",
      "Epoch [26/300], Step [75/225], Training Accuracy: 51.8542%, Training Loss: 0.9563%\n",
      "Epoch [26/300], Step [76/225], Training Accuracy: 51.8709%, Training Loss: 0.9559%\n",
      "Epoch [26/300], Step [77/225], Training Accuracy: 52.0292%, Training Loss: 0.9554%\n",
      "Epoch [26/300], Step [78/225], Training Accuracy: 51.9631%, Training Loss: 0.9566%\n",
      "Epoch [26/300], Step [79/225], Training Accuracy: 51.8987%, Training Loss: 0.9580%\n",
      "Epoch [26/300], Step [80/225], Training Accuracy: 51.7969%, Training Loss: 0.9585%\n",
      "Epoch [26/300], Step [81/225], Training Accuracy: 51.9676%, Training Loss: 0.9587%\n",
      "Epoch [26/300], Step [82/225], Training Accuracy: 52.0198%, Training Loss: 0.9578%\n",
      "Epoch [26/300], Step [83/225], Training Accuracy: 52.0331%, Training Loss: 0.9571%\n",
      "Epoch [26/300], Step [84/225], Training Accuracy: 52.0275%, Training Loss: 0.9574%\n",
      "Epoch [26/300], Step [85/225], Training Accuracy: 52.0588%, Training Loss: 0.9564%\n",
      "Epoch [26/300], Step [86/225], Training Accuracy: 52.1076%, Training Loss: 0.9559%\n",
      "Epoch [26/300], Step [87/225], Training Accuracy: 52.1372%, Training Loss: 0.9558%\n",
      "Epoch [26/300], Step [88/225], Training Accuracy: 52.0419%, Training Loss: 0.9566%\n",
      "Epoch [26/300], Step [89/225], Training Accuracy: 51.9838%, Training Loss: 0.9580%\n",
      "Epoch [26/300], Step [90/225], Training Accuracy: 51.8924%, Training Loss: 0.9589%\n",
      "Epoch [26/300], Step [91/225], Training Accuracy: 51.9402%, Training Loss: 0.9572%\n",
      "Epoch [26/300], Step [92/225], Training Accuracy: 51.8852%, Training Loss: 0.9573%\n",
      "Epoch [26/300], Step [93/225], Training Accuracy: 51.9153%, Training Loss: 0.9574%\n",
      "Epoch [26/300], Step [94/225], Training Accuracy: 51.9781%, Training Loss: 0.9560%\n",
      "Epoch [26/300], Step [95/225], Training Accuracy: 51.9572%, Training Loss: 0.9564%\n",
      "Epoch [26/300], Step [96/225], Training Accuracy: 51.9857%, Training Loss: 0.9558%\n",
      "Epoch [26/300], Step [97/225], Training Accuracy: 52.0619%, Training Loss: 0.9548%\n",
      "Epoch [26/300], Step [98/225], Training Accuracy: 52.0408%, Training Loss: 0.9543%\n",
      "Epoch [26/300], Step [99/225], Training Accuracy: 52.1149%, Training Loss: 0.9540%\n",
      "Epoch [26/300], Step [100/225], Training Accuracy: 51.9375%, Training Loss: 0.9549%\n",
      "Epoch [26/300], Step [101/225], Training Accuracy: 51.9028%, Training Loss: 0.9554%\n",
      "Epoch [26/300], Step [102/225], Training Accuracy: 51.8382%, Training Loss: 0.9566%\n",
      "Epoch [26/300], Step [103/225], Training Accuracy: 51.7294%, Training Loss: 0.9584%\n",
      "Epoch [26/300], Step [104/225], Training Accuracy: 51.7428%, Training Loss: 0.9581%\n",
      "Epoch [26/300], Step [105/225], Training Accuracy: 51.6815%, Training Loss: 0.9574%\n",
      "Epoch [26/300], Step [106/225], Training Accuracy: 51.6067%, Training Loss: 0.9584%\n",
      "Epoch [26/300], Step [107/225], Training Accuracy: 51.6793%, Training Loss: 0.9583%\n",
      "Epoch [26/300], Step [108/225], Training Accuracy: 51.6638%, Training Loss: 0.9590%\n",
      "Epoch [26/300], Step [109/225], Training Accuracy: 51.6485%, Training Loss: 0.9590%\n",
      "Epoch [26/300], Step [110/225], Training Accuracy: 51.6761%, Training Loss: 0.9585%\n",
      "Epoch [26/300], Step [111/225], Training Accuracy: 51.8018%, Training Loss: 0.9591%\n",
      "Epoch [26/300], Step [112/225], Training Accuracy: 51.8555%, Training Loss: 0.9583%\n",
      "Epoch [26/300], Step [113/225], Training Accuracy: 51.8390%, Training Loss: 0.9590%\n",
      "Epoch [26/300], Step [114/225], Training Accuracy: 51.8229%, Training Loss: 0.9588%\n",
      "Epoch [26/300], Step [115/225], Training Accuracy: 51.9022%, Training Loss: 0.9581%\n",
      "Epoch [26/300], Step [116/225], Training Accuracy: 51.9262%, Training Loss: 0.9581%\n",
      "Epoch [26/300], Step [117/225], Training Accuracy: 51.8296%, Training Loss: 0.9595%\n",
      "Epoch [26/300], Step [118/225], Training Accuracy: 51.8538%, Training Loss: 0.9597%\n",
      "Epoch [26/300], Step [119/225], Training Accuracy: 51.8120%, Training Loss: 0.9602%\n",
      "Epoch [26/300], Step [120/225], Training Accuracy: 51.8620%, Training Loss: 0.9595%\n",
      "Epoch [26/300], Step [121/225], Training Accuracy: 51.7820%, Training Loss: 0.9600%\n",
      "Epoch [26/300], Step [122/225], Training Accuracy: 51.7546%, Training Loss: 0.9600%\n",
      "Epoch [26/300], Step [123/225], Training Accuracy: 51.7403%, Training Loss: 0.9597%\n",
      "Epoch [26/300], Step [124/225], Training Accuracy: 51.7767%, Training Loss: 0.9596%\n",
      "Epoch [26/300], Step [125/225], Training Accuracy: 51.7500%, Training Loss: 0.9608%\n",
      "Epoch [26/300], Step [126/225], Training Accuracy: 51.7361%, Training Loss: 0.9616%\n",
      "Epoch [26/300], Step [127/225], Training Accuracy: 51.7224%, Training Loss: 0.9616%\n",
      "Epoch [26/300], Step [128/225], Training Accuracy: 51.7212%, Training Loss: 0.9619%\n",
      "Epoch [26/300], Step [129/225], Training Accuracy: 51.7078%, Training Loss: 0.9624%\n",
      "Epoch [26/300], Step [130/225], Training Accuracy: 51.6827%, Training Loss: 0.9630%\n",
      "Epoch [26/300], Step [131/225], Training Accuracy: 51.6460%, Training Loss: 0.9628%\n",
      "Epoch [26/300], Step [132/225], Training Accuracy: 51.5980%, Training Loss: 0.9631%\n",
      "Epoch [26/300], Step [133/225], Training Accuracy: 51.6917%, Training Loss: 0.9623%\n",
      "Epoch [26/300], Step [134/225], Training Accuracy: 51.7141%, Training Loss: 0.9621%\n",
      "Epoch [26/300], Step [135/225], Training Accuracy: 51.7130%, Training Loss: 0.9619%\n",
      "Epoch [26/300], Step [136/225], Training Accuracy: 51.6774%, Training Loss: 0.9611%\n",
      "Epoch [26/300], Step [137/225], Training Accuracy: 51.7108%, Training Loss: 0.9605%\n",
      "Epoch [26/300], Step [138/225], Training Accuracy: 51.7550%, Training Loss: 0.9594%\n",
      "Epoch [26/300], Step [139/225], Training Accuracy: 51.7311%, Training Loss: 0.9597%\n",
      "Epoch [26/300], Step [140/225], Training Accuracy: 51.7857%, Training Loss: 0.9594%\n",
      "Epoch [26/300], Step [141/225], Training Accuracy: 51.8174%, Training Loss: 0.9592%\n",
      "Epoch [26/300], Step [142/225], Training Accuracy: 51.8266%, Training Loss: 0.9593%\n",
      "Epoch [26/300], Step [143/225], Training Accuracy: 51.8684%, Training Loss: 0.9588%\n",
      "Epoch [26/300], Step [144/225], Training Accuracy: 51.8555%, Training Loss: 0.9586%\n",
      "Epoch [26/300], Step [145/225], Training Accuracy: 51.9073%, Training Loss: 0.9578%\n",
      "Epoch [26/300], Step [146/225], Training Accuracy: 51.9371%, Training Loss: 0.9577%\n",
      "Epoch [26/300], Step [147/225], Training Accuracy: 51.8814%, Training Loss: 0.9584%\n",
      "Epoch [26/300], Step [148/225], Training Accuracy: 51.9637%, Training Loss: 0.9579%\n",
      "Epoch [26/300], Step [149/225], Training Accuracy: 51.9505%, Training Loss: 0.9582%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Step [150/225], Training Accuracy: 51.9167%, Training Loss: 0.9586%\n",
      "Epoch [26/300], Step [151/225], Training Accuracy: 51.9350%, Training Loss: 0.9580%\n",
      "Epoch [26/300], Step [152/225], Training Accuracy: 51.9120%, Training Loss: 0.9580%\n",
      "Epoch [26/300], Step [153/225], Training Accuracy: 51.9404%, Training Loss: 0.9575%\n",
      "Epoch [26/300], Step [154/225], Training Accuracy: 51.9278%, Training Loss: 0.9570%\n",
      "Epoch [26/300], Step [155/225], Training Accuracy: 51.8851%, Training Loss: 0.9581%\n",
      "Epoch [26/300], Step [156/225], Training Accuracy: 51.8429%, Training Loss: 0.9583%\n",
      "Epoch [26/300], Step [157/225], Training Accuracy: 51.8412%, Training Loss: 0.9586%\n",
      "Epoch [26/300], Step [158/225], Training Accuracy: 51.8592%, Training Loss: 0.9588%\n",
      "Epoch [26/300], Step [159/225], Training Accuracy: 51.8770%, Training Loss: 0.9585%\n",
      "Epoch [26/300], Step [160/225], Training Accuracy: 51.8359%, Training Loss: 0.9584%\n",
      "Epoch [26/300], Step [161/225], Training Accuracy: 51.9313%, Training Loss: 0.9574%\n",
      "Epoch [26/300], Step [162/225], Training Accuracy: 51.9676%, Training Loss: 0.9576%\n",
      "Epoch [26/300], Step [163/225], Training Accuracy: 51.9172%, Training Loss: 0.9573%\n",
      "Epoch [26/300], Step [164/225], Training Accuracy: 51.9627%, Training Loss: 0.9563%\n",
      "Epoch [26/300], Step [165/225], Training Accuracy: 51.9602%, Training Loss: 0.9561%\n",
      "Epoch [26/300], Step [166/225], Training Accuracy: 51.9578%, Training Loss: 0.9562%\n",
      "Epoch [26/300], Step [167/225], Training Accuracy: 51.9929%, Training Loss: 0.9555%\n",
      "Epoch [26/300], Step [168/225], Training Accuracy: 51.9717%, Training Loss: 0.9557%\n",
      "Epoch [26/300], Step [169/225], Training Accuracy: 51.9693%, Training Loss: 0.9557%\n",
      "Epoch [26/300], Step [170/225], Training Accuracy: 51.9301%, Training Loss: 0.9563%\n",
      "Epoch [26/300], Step [171/225], Training Accuracy: 51.9280%, Training Loss: 0.9562%\n",
      "Epoch [26/300], Step [172/225], Training Accuracy: 51.8986%, Training Loss: 0.9568%\n",
      "Epoch [26/300], Step [173/225], Training Accuracy: 51.9147%, Training Loss: 0.9564%\n",
      "Epoch [26/300], Step [174/225], Training Accuracy: 51.8948%, Training Loss: 0.9566%\n",
      "Epoch [26/300], Step [175/225], Training Accuracy: 51.8661%, Training Loss: 0.9567%\n",
      "Epoch [26/300], Step [176/225], Training Accuracy: 51.8910%, Training Loss: 0.9564%\n",
      "Epoch [26/300], Step [177/225], Training Accuracy: 51.8450%, Training Loss: 0.9563%\n",
      "Epoch [26/300], Step [178/225], Training Accuracy: 51.8083%, Training Loss: 0.9562%\n",
      "Epoch [26/300], Step [179/225], Training Accuracy: 51.8331%, Training Loss: 0.9562%\n",
      "Epoch [26/300], Step [180/225], Training Accuracy: 51.9271%, Training Loss: 0.9551%\n",
      "Epoch [26/300], Step [181/225], Training Accuracy: 51.8733%, Training Loss: 0.9559%\n",
      "Epoch [26/300], Step [182/225], Training Accuracy: 51.8716%, Training Loss: 0.9562%\n",
      "Epoch [26/300], Step [183/225], Training Accuracy: 51.8955%, Training Loss: 0.9558%\n",
      "Epoch [26/300], Step [184/225], Training Accuracy: 51.8852%, Training Loss: 0.9558%\n",
      "Epoch [26/300], Step [185/225], Training Accuracy: 51.8581%, Training Loss: 0.9558%\n",
      "Epoch [26/300], Step [186/225], Training Accuracy: 51.8817%, Training Loss: 0.9557%\n",
      "Epoch [26/300], Step [187/225], Training Accuracy: 51.9218%, Training Loss: 0.9549%\n",
      "Epoch [26/300], Step [188/225], Training Accuracy: 51.9781%, Training Loss: 0.9545%\n",
      "Epoch [26/300], Step [189/225], Training Accuracy: 52.0172%, Training Loss: 0.9540%\n",
      "Epoch [26/300], Step [190/225], Training Accuracy: 52.0395%, Training Loss: 0.9543%\n",
      "Epoch [26/300], Step [191/225], Training Accuracy: 51.9961%, Training Loss: 0.9545%\n",
      "Epoch [26/300], Step [192/225], Training Accuracy: 52.0426%, Training Loss: 0.9541%\n",
      "Epoch [26/300], Step [193/225], Training Accuracy: 51.9997%, Training Loss: 0.9542%\n",
      "Epoch [26/300], Step [194/225], Training Accuracy: 52.0216%, Training Loss: 0.9544%\n",
      "Epoch [26/300], Step [195/225], Training Accuracy: 52.0513%, Training Loss: 0.9539%\n",
      "Epoch [26/300], Step [196/225], Training Accuracy: 52.0727%, Training Loss: 0.9539%\n",
      "Epoch [26/300], Step [197/225], Training Accuracy: 52.1177%, Training Loss: 0.9534%\n",
      "Epoch [26/300], Step [198/225], Training Accuracy: 52.1149%, Training Loss: 0.9529%\n",
      "Epoch [26/300], Step [199/225], Training Accuracy: 52.1278%, Training Loss: 0.9527%\n",
      "Epoch [26/300], Step [200/225], Training Accuracy: 52.1484%, Training Loss: 0.9527%\n",
      "Epoch [26/300], Step [201/225], Training Accuracy: 52.1377%, Training Loss: 0.9529%\n",
      "Epoch [26/300], Step [202/225], Training Accuracy: 52.1426%, Training Loss: 0.9526%\n",
      "Epoch [26/300], Step [203/225], Training Accuracy: 52.1321%, Training Loss: 0.9529%\n",
      "Epoch [26/300], Step [204/225], Training Accuracy: 52.1676%, Training Loss: 0.9526%\n",
      "Epoch [26/300], Step [205/225], Training Accuracy: 52.1570%, Training Loss: 0.9523%\n",
      "Epoch [26/300], Step [206/225], Training Accuracy: 52.1465%, Training Loss: 0.9527%\n",
      "Epoch [26/300], Step [207/225], Training Accuracy: 52.1286%, Training Loss: 0.9532%\n",
      "Epoch [26/300], Step [208/225], Training Accuracy: 52.1935%, Training Loss: 0.9527%\n",
      "Epoch [26/300], Step [209/225], Training Accuracy: 52.1755%, Training Loss: 0.9530%\n",
      "Epoch [26/300], Step [210/225], Training Accuracy: 52.1503%, Training Loss: 0.9530%\n",
      "Epoch [26/300], Step [211/225], Training Accuracy: 52.1771%, Training Loss: 0.9527%\n",
      "Epoch [26/300], Step [212/225], Training Accuracy: 52.1300%, Training Loss: 0.9531%\n",
      "Epoch [26/300], Step [213/225], Training Accuracy: 52.1053%, Training Loss: 0.9537%\n",
      "Epoch [26/300], Step [214/225], Training Accuracy: 52.0882%, Training Loss: 0.9536%\n",
      "Epoch [26/300], Step [215/225], Training Accuracy: 52.1148%, Training Loss: 0.9535%\n",
      "Epoch [26/300], Step [216/225], Training Accuracy: 52.1123%, Training Loss: 0.9538%\n",
      "Epoch [26/300], Step [217/225], Training Accuracy: 52.1025%, Training Loss: 0.9539%\n",
      "Epoch [26/300], Step [218/225], Training Accuracy: 52.0929%, Training Loss: 0.9547%\n",
      "Epoch [26/300], Step [219/225], Training Accuracy: 52.0477%, Training Loss: 0.9550%\n",
      "Epoch [26/300], Step [220/225], Training Accuracy: 52.0952%, Training Loss: 0.9546%\n",
      "Epoch [26/300], Step [221/225], Training Accuracy: 52.1069%, Training Loss: 0.9548%\n",
      "Epoch [26/300], Step [222/225], Training Accuracy: 52.1115%, Training Loss: 0.9548%\n",
      "Epoch [26/300], Step [223/225], Training Accuracy: 52.0600%, Training Loss: 0.9554%\n",
      "Epoch [26/300], Step [224/225], Training Accuracy: 52.0368%, Training Loss: 0.9554%\n",
      "Epoch [26/300], Step [225/225], Training Accuracy: 52.0289%, Training Loss: 0.9554%\n",
      "Epoch [27/300], Step [1/225], Training Accuracy: 65.6250%, Training Loss: 0.7833%\n",
      "Epoch [27/300], Step [2/225], Training Accuracy: 55.4688%, Training Loss: 0.9655%\n",
      "Epoch [27/300], Step [3/225], Training Accuracy: 51.0417%, Training Loss: 1.0175%\n",
      "Epoch [27/300], Step [4/225], Training Accuracy: 51.1719%, Training Loss: 0.9982%\n",
      "Epoch [27/300], Step [5/225], Training Accuracy: 52.8125%, Training Loss: 0.9633%\n",
      "Epoch [27/300], Step [6/225], Training Accuracy: 51.8229%, Training Loss: 0.9844%\n",
      "Epoch [27/300], Step [7/225], Training Accuracy: 51.5625%, Training Loss: 0.9786%\n",
      "Epoch [27/300], Step [8/225], Training Accuracy: 51.7578%, Training Loss: 0.9850%\n",
      "Epoch [27/300], Step [9/225], Training Accuracy: 51.9097%, Training Loss: 0.9772%\n",
      "Epoch [27/300], Step [10/225], Training Accuracy: 52.1875%, Training Loss: 0.9722%\n",
      "Epoch [27/300], Step [11/225], Training Accuracy: 52.8409%, Training Loss: 0.9674%\n",
      "Epoch [27/300], Step [12/225], Training Accuracy: 52.4740%, Training Loss: 0.9737%\n",
      "Epoch [27/300], Step [13/225], Training Accuracy: 53.7260%, Training Loss: 0.9706%\n",
      "Epoch [27/300], Step [14/225], Training Accuracy: 53.3482%, Training Loss: 0.9730%\n",
      "Epoch [27/300], Step [15/225], Training Accuracy: 52.8125%, Training Loss: 0.9857%\n",
      "Epoch [27/300], Step [16/225], Training Accuracy: 52.8320%, Training Loss: 0.9851%\n",
      "Epoch [27/300], Step [17/225], Training Accuracy: 52.3897%, Training Loss: 0.9799%\n",
      "Epoch [27/300], Step [18/225], Training Accuracy: 52.2569%, Training Loss: 0.9780%\n",
      "Epoch [27/300], Step [19/225], Training Accuracy: 51.5625%, Training Loss: 0.9797%\n",
      "Epoch [27/300], Step [20/225], Training Accuracy: 51.8750%, Training Loss: 0.9763%\n",
      "Epoch [27/300], Step [21/225], Training Accuracy: 51.9345%, Training Loss: 0.9722%\n",
      "Epoch [27/300], Step [22/225], Training Accuracy: 51.8466%, Training Loss: 0.9729%\n",
      "Epoch [27/300], Step [23/225], Training Accuracy: 51.8342%, Training Loss: 0.9692%\n",
      "Epoch [27/300], Step [24/225], Training Accuracy: 51.9531%, Training Loss: 0.9717%\n",
      "Epoch [27/300], Step [25/225], Training Accuracy: 52.0000%, Training Loss: 0.9675%\n",
      "Epoch [27/300], Step [26/225], Training Accuracy: 51.6226%, Training Loss: 0.9680%\n",
      "Epoch [27/300], Step [27/225], Training Accuracy: 51.5046%, Training Loss: 0.9701%\n",
      "Epoch [27/300], Step [28/225], Training Accuracy: 51.5625%, Training Loss: 0.9669%\n",
      "Epoch [27/300], Step [29/225], Training Accuracy: 51.8319%, Training Loss: 0.9628%\n",
      "Epoch [27/300], Step [30/225], Training Accuracy: 51.8229%, Training Loss: 0.9609%\n",
      "Epoch [27/300], Step [31/225], Training Accuracy: 51.9153%, Training Loss: 0.9600%\n",
      "Epoch [27/300], Step [32/225], Training Accuracy: 52.0996%, Training Loss: 0.9573%\n",
      "Epoch [27/300], Step [33/225], Training Accuracy: 52.2254%, Training Loss: 0.9555%\n",
      "Epoch [27/300], Step [34/225], Training Accuracy: 52.2978%, Training Loss: 0.9576%\n",
      "Epoch [27/300], Step [35/225], Training Accuracy: 52.1429%, Training Loss: 0.9579%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Step [36/225], Training Accuracy: 52.0833%, Training Loss: 0.9580%\n",
      "Epoch [27/300], Step [37/225], Training Accuracy: 52.3649%, Training Loss: 0.9552%\n",
      "Epoch [27/300], Step [38/225], Training Accuracy: 52.5905%, Training Loss: 0.9536%\n",
      "Epoch [27/300], Step [39/225], Training Accuracy: 52.6442%, Training Loss: 0.9530%\n",
      "Epoch [27/300], Step [40/225], Training Accuracy: 52.5781%, Training Loss: 0.9530%\n",
      "Epoch [27/300], Step [41/225], Training Accuracy: 52.1723%, Training Loss: 0.9551%\n",
      "Epoch [27/300], Step [42/225], Training Accuracy: 52.3065%, Training Loss: 0.9539%\n",
      "Epoch [27/300], Step [43/225], Training Accuracy: 52.1802%, Training Loss: 0.9531%\n",
      "Epoch [27/300], Step [44/225], Training Accuracy: 52.2372%, Training Loss: 0.9521%\n",
      "Epoch [27/300], Step [45/225], Training Accuracy: 52.1875%, Training Loss: 0.9521%\n",
      "Epoch [27/300], Step [46/225], Training Accuracy: 52.5136%, Training Loss: 0.9488%\n",
      "Epoch [27/300], Step [47/225], Training Accuracy: 52.5266%, Training Loss: 0.9506%\n",
      "Epoch [27/300], Step [48/225], Training Accuracy: 52.5391%, Training Loss: 0.9519%\n",
      "Epoch [27/300], Step [49/225], Training Accuracy: 52.3597%, Training Loss: 0.9534%\n",
      "Epoch [27/300], Step [50/225], Training Accuracy: 52.4062%, Training Loss: 0.9520%\n",
      "Epoch [27/300], Step [51/225], Training Accuracy: 52.5735%, Training Loss: 0.9490%\n",
      "Epoch [27/300], Step [52/225], Training Accuracy: 52.7644%, Training Loss: 0.9467%\n",
      "Epoch [27/300], Step [53/225], Training Accuracy: 52.6533%, Training Loss: 0.9465%\n",
      "Epoch [27/300], Step [54/225], Training Accuracy: 52.3727%, Training Loss: 0.9489%\n",
      "Epoch [27/300], Step [55/225], Training Accuracy: 52.2443%, Training Loss: 0.9511%\n",
      "Epoch [27/300], Step [56/225], Training Accuracy: 52.2321%, Training Loss: 0.9522%\n",
      "Epoch [27/300], Step [57/225], Training Accuracy: 52.4123%, Training Loss: 0.9491%\n",
      "Epoch [27/300], Step [58/225], Training Accuracy: 52.3976%, Training Loss: 0.9493%\n",
      "Epoch [27/300], Step [59/225], Training Accuracy: 52.5424%, Training Loss: 0.9485%\n",
      "Epoch [27/300], Step [60/225], Training Accuracy: 52.5781%, Training Loss: 0.9481%\n",
      "Epoch [27/300], Step [61/225], Training Accuracy: 52.6895%, Training Loss: 0.9477%\n",
      "Epoch [27/300], Step [62/225], Training Accuracy: 52.6210%, Training Loss: 0.9484%\n",
      "Epoch [27/300], Step [63/225], Training Accuracy: 52.5050%, Training Loss: 0.9492%\n",
      "Epoch [27/300], Step [64/225], Training Accuracy: 52.5635%, Training Loss: 0.9484%\n",
      "Epoch [27/300], Step [65/225], Training Accuracy: 52.4760%, Training Loss: 0.9489%\n",
      "Epoch [27/300], Step [66/225], Training Accuracy: 52.6989%, Training Loss: 0.9467%\n",
      "Epoch [27/300], Step [67/225], Training Accuracy: 52.5886%, Training Loss: 0.9475%\n",
      "Epoch [27/300], Step [68/225], Training Accuracy: 52.6195%, Training Loss: 0.9471%\n",
      "Epoch [27/300], Step [69/225], Training Accuracy: 52.5362%, Training Loss: 0.9475%\n",
      "Epoch [27/300], Step [70/225], Training Accuracy: 52.4330%, Training Loss: 0.9483%\n",
      "Epoch [27/300], Step [71/225], Training Accuracy: 52.4208%, Training Loss: 0.9476%\n",
      "Epoch [27/300], Step [72/225], Training Accuracy: 52.3438%, Training Loss: 0.9494%\n",
      "Epoch [27/300], Step [73/225], Training Accuracy: 52.3116%, Training Loss: 0.9518%\n",
      "Epoch [27/300], Step [74/225], Training Accuracy: 52.3438%, Training Loss: 0.9499%\n",
      "Epoch [27/300], Step [75/225], Training Accuracy: 52.3542%, Training Loss: 0.9498%\n",
      "Epoch [27/300], Step [76/225], Training Accuracy: 52.3849%, Training Loss: 0.9494%\n",
      "Epoch [27/300], Step [77/225], Training Accuracy: 52.5162%, Training Loss: 0.9489%\n",
      "Epoch [27/300], Step [78/225], Training Accuracy: 52.4639%, Training Loss: 0.9501%\n",
      "Epoch [27/300], Step [79/225], Training Accuracy: 52.4130%, Training Loss: 0.9515%\n",
      "Epoch [27/300], Step [80/225], Training Accuracy: 52.3047%, Training Loss: 0.9521%\n",
      "Epoch [27/300], Step [81/225], Training Accuracy: 52.4691%, Training Loss: 0.9522%\n",
      "Epoch [27/300], Step [82/225], Training Accuracy: 52.5343%, Training Loss: 0.9514%\n",
      "Epoch [27/300], Step [83/225], Training Accuracy: 52.5226%, Training Loss: 0.9506%\n",
      "Epoch [27/300], Step [84/225], Training Accuracy: 52.4926%, Training Loss: 0.9508%\n",
      "Epoch [27/300], Step [85/225], Training Accuracy: 52.5184%, Training Loss: 0.9498%\n",
      "Epoch [27/300], Step [86/225], Training Accuracy: 52.5799%, Training Loss: 0.9492%\n",
      "Epoch [27/300], Step [87/225], Training Accuracy: 52.6042%, Training Loss: 0.9492%\n",
      "Epoch [27/300], Step [88/225], Training Accuracy: 52.5036%, Training Loss: 0.9500%\n",
      "Epoch [27/300], Step [89/225], Training Accuracy: 52.4403%, Training Loss: 0.9513%\n",
      "Epoch [27/300], Step [90/225], Training Accuracy: 52.3438%, Training Loss: 0.9523%\n",
      "Epoch [27/300], Step [91/225], Training Accuracy: 52.4210%, Training Loss: 0.9506%\n",
      "Epoch [27/300], Step [92/225], Training Accuracy: 52.3607%, Training Loss: 0.9508%\n",
      "Epoch [27/300], Step [93/225], Training Accuracy: 52.4026%, Training Loss: 0.9508%\n",
      "Epoch [27/300], Step [94/225], Training Accuracy: 52.4601%, Training Loss: 0.9494%\n",
      "Epoch [27/300], Step [95/225], Training Accuracy: 52.4507%, Training Loss: 0.9499%\n",
      "Epoch [27/300], Step [96/225], Training Accuracy: 52.4740%, Training Loss: 0.9492%\n",
      "Epoch [27/300], Step [97/225], Training Accuracy: 52.5451%, Training Loss: 0.9482%\n",
      "Epoch [27/300], Step [98/225], Training Accuracy: 52.5191%, Training Loss: 0.9477%\n",
      "Epoch [27/300], Step [99/225], Training Accuracy: 52.5726%, Training Loss: 0.9474%\n",
      "Epoch [27/300], Step [100/225], Training Accuracy: 52.3750%, Training Loss: 0.9483%\n",
      "Epoch [27/300], Step [101/225], Training Accuracy: 52.3360%, Training Loss: 0.9488%\n",
      "Epoch [27/300], Step [102/225], Training Accuracy: 52.2978%, Training Loss: 0.9500%\n",
      "Epoch [27/300], Step [103/225], Training Accuracy: 52.1693%, Training Loss: 0.9519%\n",
      "Epoch [27/300], Step [104/225], Training Accuracy: 52.1635%, Training Loss: 0.9516%\n",
      "Epoch [27/300], Step [105/225], Training Accuracy: 52.0982%, Training Loss: 0.9509%\n",
      "Epoch [27/300], Step [106/225], Training Accuracy: 52.0047%, Training Loss: 0.9520%\n",
      "Epoch [27/300], Step [107/225], Training Accuracy: 52.0590%, Training Loss: 0.9518%\n",
      "Epoch [27/300], Step [108/225], Training Accuracy: 52.0399%, Training Loss: 0.9526%\n",
      "Epoch [27/300], Step [109/225], Training Accuracy: 52.0212%, Training Loss: 0.9525%\n",
      "Epoch [27/300], Step [110/225], Training Accuracy: 52.0312%, Training Loss: 0.9521%\n",
      "Epoch [27/300], Step [111/225], Training Accuracy: 52.1537%, Training Loss: 0.9526%\n",
      "Epoch [27/300], Step [112/225], Training Accuracy: 52.2042%, Training Loss: 0.9518%\n",
      "Epoch [27/300], Step [113/225], Training Accuracy: 52.1986%, Training Loss: 0.9525%\n",
      "Epoch [27/300], Step [114/225], Training Accuracy: 52.1793%, Training Loss: 0.9523%\n",
      "Epoch [27/300], Step [115/225], Training Accuracy: 52.2554%, Training Loss: 0.9515%\n",
      "Epoch [27/300], Step [116/225], Training Accuracy: 52.2629%, Training Loss: 0.9516%\n",
      "Epoch [27/300], Step [117/225], Training Accuracy: 52.1768%, Training Loss: 0.9529%\n",
      "Epoch [27/300], Step [118/225], Training Accuracy: 52.1849%, Training Loss: 0.9532%\n",
      "Epoch [27/300], Step [119/225], Training Accuracy: 52.1402%, Training Loss: 0.9538%\n",
      "Epoch [27/300], Step [120/225], Training Accuracy: 52.1745%, Training Loss: 0.9531%\n",
      "Epoch [27/300], Step [121/225], Training Accuracy: 52.1049%, Training Loss: 0.9537%\n",
      "Epoch [27/300], Step [122/225], Training Accuracy: 52.0620%, Training Loss: 0.9536%\n",
      "Epoch [27/300], Step [123/225], Training Accuracy: 52.0452%, Training Loss: 0.9533%\n",
      "Epoch [27/300], Step [124/225], Training Accuracy: 52.0917%, Training Loss: 0.9533%\n",
      "Epoch [27/300], Step [125/225], Training Accuracy: 52.0500%, Training Loss: 0.9546%\n",
      "Epoch [27/300], Step [126/225], Training Accuracy: 52.0461%, Training Loss: 0.9554%\n",
      "Epoch [27/300], Step [127/225], Training Accuracy: 52.0177%, Training Loss: 0.9554%\n",
      "Epoch [27/300], Step [128/225], Training Accuracy: 52.0264%, Training Loss: 0.9557%\n",
      "Epoch [27/300], Step [129/225], Training Accuracy: 52.0107%, Training Loss: 0.9562%\n",
      "Epoch [27/300], Step [130/225], Training Accuracy: 51.9712%, Training Loss: 0.9568%\n",
      "Epoch [27/300], Step [131/225], Training Accuracy: 51.9442%, Training Loss: 0.9566%\n",
      "Epoch [27/300], Step [132/225], Training Accuracy: 51.9058%, Training Loss: 0.9569%\n",
      "Epoch [27/300], Step [133/225], Training Accuracy: 52.0207%, Training Loss: 0.9562%\n",
      "Epoch [27/300], Step [134/225], Training Accuracy: 52.0289%, Training Loss: 0.9560%\n",
      "Epoch [27/300], Step [135/225], Training Accuracy: 52.0255%, Training Loss: 0.9558%\n",
      "Epoch [27/300], Step [136/225], Training Accuracy: 52.0106%, Training Loss: 0.9550%\n",
      "Epoch [27/300], Step [137/225], Training Accuracy: 52.0529%, Training Loss: 0.9543%\n",
      "Epoch [27/300], Step [138/225], Training Accuracy: 52.1060%, Training Loss: 0.9532%\n",
      "Epoch [27/300], Step [139/225], Training Accuracy: 52.0796%, Training Loss: 0.9535%\n",
      "Epoch [27/300], Step [140/225], Training Accuracy: 52.1317%, Training Loss: 0.9532%\n",
      "Epoch [27/300], Step [141/225], Training Accuracy: 52.1720%, Training Loss: 0.9530%\n",
      "Epoch [27/300], Step [142/225], Training Accuracy: 52.2007%, Training Loss: 0.9530%\n",
      "Epoch [27/300], Step [143/225], Training Accuracy: 52.2509%, Training Loss: 0.9525%\n",
      "Epoch [27/300], Step [144/225], Training Accuracy: 52.2352%, Training Loss: 0.9523%\n",
      "Epoch [27/300], Step [145/225], Training Accuracy: 52.2737%, Training Loss: 0.9515%\n",
      "Epoch [27/300], Step [146/225], Training Accuracy: 52.2902%, Training Loss: 0.9514%\n",
      "Epoch [27/300], Step [147/225], Training Accuracy: 52.2321%, Training Loss: 0.9521%\n",
      "Epoch [27/300], Step [148/225], Training Accuracy: 52.3226%, Training Loss: 0.9516%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Step [149/225], Training Accuracy: 52.2966%, Training Loss: 0.9518%\n",
      "Epoch [27/300], Step [150/225], Training Accuracy: 52.2604%, Training Loss: 0.9523%\n",
      "Epoch [27/300], Step [151/225], Training Accuracy: 52.2868%, Training Loss: 0.9517%\n",
      "Epoch [27/300], Step [152/225], Training Accuracy: 52.2615%, Training Loss: 0.9517%\n",
      "Epoch [27/300], Step [153/225], Training Accuracy: 52.2774%, Training Loss: 0.9512%\n",
      "Epoch [27/300], Step [154/225], Training Accuracy: 52.2727%, Training Loss: 0.9506%\n",
      "Epoch [27/300], Step [155/225], Training Accuracy: 52.2278%, Training Loss: 0.9517%\n",
      "Epoch [27/300], Step [156/225], Training Accuracy: 52.1835%, Training Loss: 0.9520%\n",
      "Epoch [27/300], Step [157/225], Training Accuracy: 52.1895%, Training Loss: 0.9523%\n",
      "Epoch [27/300], Step [158/225], Training Accuracy: 52.1954%, Training Loss: 0.9525%\n",
      "Epoch [27/300], Step [159/225], Training Accuracy: 52.2209%, Training Loss: 0.9522%\n",
      "Epoch [27/300], Step [160/225], Training Accuracy: 52.2070%, Training Loss: 0.9521%\n",
      "Epoch [27/300], Step [161/225], Training Accuracy: 52.2904%, Training Loss: 0.9511%\n",
      "Epoch [27/300], Step [162/225], Training Accuracy: 52.3245%, Training Loss: 0.9513%\n",
      "Epoch [27/300], Step [163/225], Training Accuracy: 52.2623%, Training Loss: 0.9510%\n",
      "Epoch [27/300], Step [164/225], Training Accuracy: 52.3056%, Training Loss: 0.9500%\n",
      "Epoch [27/300], Step [165/225], Training Accuracy: 52.3011%, Training Loss: 0.9498%\n",
      "Epoch [27/300], Step [166/225], Training Accuracy: 52.3061%, Training Loss: 0.9499%\n",
      "Epoch [27/300], Step [167/225], Training Accuracy: 52.3391%, Training Loss: 0.9491%\n",
      "Epoch [27/300], Step [168/225], Training Accuracy: 52.3251%, Training Loss: 0.9494%\n",
      "Epoch [27/300], Step [169/225], Training Accuracy: 52.3206%, Training Loss: 0.9493%\n",
      "Epoch [27/300], Step [170/225], Training Accuracy: 52.2886%, Training Loss: 0.9499%\n",
      "Epoch [27/300], Step [171/225], Training Accuracy: 52.3026%, Training Loss: 0.9498%\n",
      "Epoch [27/300], Step [172/225], Training Accuracy: 52.2620%, Training Loss: 0.9503%\n",
      "Epoch [27/300], Step [173/225], Training Accuracy: 52.2760%, Training Loss: 0.9500%\n",
      "Epoch [27/300], Step [174/225], Training Accuracy: 52.2540%, Training Loss: 0.9502%\n",
      "Epoch [27/300], Step [175/225], Training Accuracy: 52.2411%, Training Loss: 0.9503%\n",
      "Epoch [27/300], Step [176/225], Training Accuracy: 52.2727%, Training Loss: 0.9500%\n",
      "Epoch [27/300], Step [177/225], Training Accuracy: 52.2422%, Training Loss: 0.9499%\n",
      "Epoch [27/300], Step [178/225], Training Accuracy: 52.2209%, Training Loss: 0.9498%\n",
      "Epoch [27/300], Step [179/225], Training Accuracy: 52.2346%, Training Loss: 0.9497%\n",
      "Epoch [27/300], Step [180/225], Training Accuracy: 52.3177%, Training Loss: 0.9487%\n",
      "Epoch [27/300], Step [181/225], Training Accuracy: 52.2617%, Training Loss: 0.9495%\n",
      "Epoch [27/300], Step [182/225], Training Accuracy: 52.2579%, Training Loss: 0.9498%\n",
      "Epoch [27/300], Step [183/225], Training Accuracy: 52.2797%, Training Loss: 0.9494%\n",
      "Epoch [27/300], Step [184/225], Training Accuracy: 52.2503%, Training Loss: 0.9495%\n",
      "Epoch [27/300], Step [185/225], Training Accuracy: 52.2213%, Training Loss: 0.9494%\n",
      "Epoch [27/300], Step [186/225], Training Accuracy: 52.2429%, Training Loss: 0.9492%\n",
      "Epoch [27/300], Step [187/225], Training Accuracy: 52.2811%, Training Loss: 0.9485%\n",
      "Epoch [27/300], Step [188/225], Training Accuracy: 52.3354%, Training Loss: 0.9481%\n",
      "Epoch [27/300], Step [189/225], Training Accuracy: 52.3727%, Training Loss: 0.9475%\n",
      "Epoch [27/300], Step [190/225], Training Accuracy: 52.3931%, Training Loss: 0.9478%\n",
      "Epoch [27/300], Step [191/225], Training Accuracy: 52.3478%, Training Loss: 0.9481%\n",
      "Epoch [27/300], Step [192/225], Training Accuracy: 52.3926%, Training Loss: 0.9477%\n",
      "Epoch [27/300], Step [193/225], Training Accuracy: 52.3478%, Training Loss: 0.9479%\n",
      "Epoch [27/300], Step [194/225], Training Accuracy: 52.3679%, Training Loss: 0.9480%\n",
      "Epoch [27/300], Step [195/225], Training Accuracy: 52.3958%, Training Loss: 0.9475%\n",
      "Epoch [27/300], Step [196/225], Training Accuracy: 52.4235%, Training Loss: 0.9475%\n",
      "Epoch [27/300], Step [197/225], Training Accuracy: 52.4667%, Training Loss: 0.9470%\n",
      "Epoch [27/300], Step [198/225], Training Accuracy: 52.4700%, Training Loss: 0.9465%\n",
      "Epoch [27/300], Step [199/225], Training Accuracy: 52.4969%, Training Loss: 0.9463%\n",
      "Epoch [27/300], Step [200/225], Training Accuracy: 52.5156%, Training Loss: 0.9464%\n",
      "Epoch [27/300], Step [201/225], Training Accuracy: 52.5109%, Training Loss: 0.9466%\n",
      "Epoch [27/300], Step [202/225], Training Accuracy: 52.5217%, Training Loss: 0.9463%\n",
      "Epoch [27/300], Step [203/225], Training Accuracy: 52.5092%, Training Loss: 0.9465%\n",
      "Epoch [27/300], Step [204/225], Training Accuracy: 52.5506%, Training Loss: 0.9462%\n",
      "Epoch [27/300], Step [205/225], Training Accuracy: 52.5381%, Training Loss: 0.9459%\n",
      "Epoch [27/300], Step [206/225], Training Accuracy: 52.5258%, Training Loss: 0.9463%\n",
      "Epoch [27/300], Step [207/225], Training Accuracy: 52.5060%, Training Loss: 0.9468%\n",
      "Epoch [27/300], Step [208/225], Training Accuracy: 52.5691%, Training Loss: 0.9463%\n",
      "Epoch [27/300], Step [209/225], Training Accuracy: 52.5568%, Training Loss: 0.9466%\n",
      "Epoch [27/300], Step [210/225], Training Accuracy: 52.5298%, Training Loss: 0.9466%\n",
      "Epoch [27/300], Step [211/225], Training Accuracy: 52.5548%, Training Loss: 0.9463%\n",
      "Epoch [27/300], Step [212/225], Training Accuracy: 52.5059%, Training Loss: 0.9467%\n",
      "Epoch [27/300], Step [213/225], Training Accuracy: 52.4721%, Training Loss: 0.9473%\n",
      "Epoch [27/300], Step [214/225], Training Accuracy: 52.4533%, Training Loss: 0.9471%\n",
      "Epoch [27/300], Step [215/225], Training Accuracy: 52.4782%, Training Loss: 0.9471%\n",
      "Epoch [27/300], Step [216/225], Training Accuracy: 52.4812%, Training Loss: 0.9474%\n",
      "Epoch [27/300], Step [217/225], Training Accuracy: 52.4770%, Training Loss: 0.9475%\n",
      "Epoch [27/300], Step [218/225], Training Accuracy: 52.4513%, Training Loss: 0.9482%\n",
      "Epoch [27/300], Step [219/225], Training Accuracy: 52.4044%, Training Loss: 0.9485%\n",
      "Epoch [27/300], Step [220/225], Training Accuracy: 52.4503%, Training Loss: 0.9481%\n",
      "Epoch [27/300], Step [221/225], Training Accuracy: 52.4604%, Training Loss: 0.9483%\n",
      "Epoch [27/300], Step [222/225], Training Accuracy: 52.4634%, Training Loss: 0.9483%\n",
      "Epoch [27/300], Step [223/225], Training Accuracy: 52.4103%, Training Loss: 0.9489%\n",
      "Epoch [27/300], Step [224/225], Training Accuracy: 52.3856%, Training Loss: 0.9489%\n",
      "Epoch [27/300], Step [225/225], Training Accuracy: 52.3902%, Training Loss: 0.9489%\n",
      "Epoch [28/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.7790%\n",
      "Epoch [28/300], Step [2/225], Training Accuracy: 53.9062%, Training Loss: 0.9620%\n",
      "Epoch [28/300], Step [3/225], Training Accuracy: 50.0000%, Training Loss: 1.0133%\n",
      "Epoch [28/300], Step [4/225], Training Accuracy: 50.7812%, Training Loss: 0.9927%\n",
      "Epoch [28/300], Step [5/225], Training Accuracy: 52.1875%, Training Loss: 0.9575%\n",
      "Epoch [28/300], Step [6/225], Training Accuracy: 51.3021%, Training Loss: 0.9783%\n",
      "Epoch [28/300], Step [7/225], Training Accuracy: 51.3393%, Training Loss: 0.9723%\n",
      "Epoch [28/300], Step [8/225], Training Accuracy: 51.7578%, Training Loss: 0.9790%\n",
      "Epoch [28/300], Step [9/225], Training Accuracy: 51.9097%, Training Loss: 0.9709%\n",
      "Epoch [28/300], Step [10/225], Training Accuracy: 52.3438%, Training Loss: 0.9658%\n",
      "Epoch [28/300], Step [11/225], Training Accuracy: 52.9830%, Training Loss: 0.9613%\n",
      "Epoch [28/300], Step [12/225], Training Accuracy: 52.6042%, Training Loss: 0.9676%\n",
      "Epoch [28/300], Step [13/225], Training Accuracy: 53.7260%, Training Loss: 0.9641%\n",
      "Epoch [28/300], Step [14/225], Training Accuracy: 53.2366%, Training Loss: 0.9662%\n",
      "Epoch [28/300], Step [15/225], Training Accuracy: 52.6042%, Training Loss: 0.9792%\n",
      "Epoch [28/300], Step [16/225], Training Accuracy: 52.7344%, Training Loss: 0.9783%\n",
      "Epoch [28/300], Step [17/225], Training Accuracy: 52.4816%, Training Loss: 0.9732%\n",
      "Epoch [28/300], Step [18/225], Training Accuracy: 52.3438%, Training Loss: 0.9713%\n",
      "Epoch [28/300], Step [19/225], Training Accuracy: 51.5625%, Training Loss: 0.9730%\n",
      "Epoch [28/300], Step [20/225], Training Accuracy: 51.7969%, Training Loss: 0.9694%\n",
      "Epoch [28/300], Step [21/225], Training Accuracy: 51.8601%, Training Loss: 0.9655%\n",
      "Epoch [28/300], Step [22/225], Training Accuracy: 51.7045%, Training Loss: 0.9662%\n",
      "Epoch [28/300], Step [23/225], Training Accuracy: 51.6304%, Training Loss: 0.9625%\n",
      "Epoch [28/300], Step [24/225], Training Accuracy: 51.8880%, Training Loss: 0.9649%\n",
      "Epoch [28/300], Step [25/225], Training Accuracy: 51.9375%, Training Loss: 0.9609%\n",
      "Epoch [28/300], Step [26/225], Training Accuracy: 51.6226%, Training Loss: 0.9611%\n",
      "Epoch [28/300], Step [27/225], Training Accuracy: 51.5046%, Training Loss: 0.9634%\n",
      "Epoch [28/300], Step [28/225], Training Accuracy: 51.5625%, Training Loss: 0.9603%\n",
      "Epoch [28/300], Step [29/225], Training Accuracy: 51.8319%, Training Loss: 0.9562%\n",
      "Epoch [28/300], Step [30/225], Training Accuracy: 51.8750%, Training Loss: 0.9543%\n",
      "Epoch [28/300], Step [31/225], Training Accuracy: 52.0161%, Training Loss: 0.9535%\n",
      "Epoch [28/300], Step [32/225], Training Accuracy: 52.1973%, Training Loss: 0.9507%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Step [33/225], Training Accuracy: 52.2727%, Training Loss: 0.9490%\n",
      "Epoch [28/300], Step [34/225], Training Accuracy: 52.2978%, Training Loss: 0.9510%\n",
      "Epoch [28/300], Step [35/225], Training Accuracy: 52.1875%, Training Loss: 0.9513%\n",
      "Epoch [28/300], Step [36/225], Training Accuracy: 52.1701%, Training Loss: 0.9513%\n",
      "Epoch [28/300], Step [37/225], Training Accuracy: 52.4493%, Training Loss: 0.9486%\n",
      "Epoch [28/300], Step [38/225], Training Accuracy: 52.6727%, Training Loss: 0.9470%\n",
      "Epoch [28/300], Step [39/225], Training Accuracy: 52.7644%, Training Loss: 0.9464%\n",
      "Epoch [28/300], Step [40/225], Training Accuracy: 52.6953%, Training Loss: 0.9465%\n",
      "Epoch [28/300], Step [41/225], Training Accuracy: 52.2866%, Training Loss: 0.9487%\n",
      "Epoch [28/300], Step [42/225], Training Accuracy: 52.4182%, Training Loss: 0.9477%\n",
      "Epoch [28/300], Step [43/225], Training Accuracy: 52.3256%, Training Loss: 0.9468%\n",
      "Epoch [28/300], Step [44/225], Training Accuracy: 52.3793%, Training Loss: 0.9459%\n",
      "Epoch [28/300], Step [45/225], Training Accuracy: 52.3264%, Training Loss: 0.9460%\n",
      "Epoch [28/300], Step [46/225], Training Accuracy: 52.6495%, Training Loss: 0.9428%\n",
      "Epoch [28/300], Step [47/225], Training Accuracy: 52.6596%, Training Loss: 0.9447%\n",
      "Epoch [28/300], Step [48/225], Training Accuracy: 52.7018%, Training Loss: 0.9461%\n",
      "Epoch [28/300], Step [49/225], Training Accuracy: 52.4872%, Training Loss: 0.9478%\n",
      "Epoch [28/300], Step [50/225], Training Accuracy: 52.5000%, Training Loss: 0.9464%\n",
      "Epoch [28/300], Step [51/225], Training Accuracy: 52.7267%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [52/225], Training Accuracy: 52.9147%, Training Loss: 0.9410%\n",
      "Epoch [28/300], Step [53/225], Training Accuracy: 52.8302%, Training Loss: 0.9409%\n",
      "Epoch [28/300], Step [54/225], Training Accuracy: 52.5174%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [55/225], Training Accuracy: 52.4716%, Training Loss: 0.9456%\n",
      "Epoch [28/300], Step [56/225], Training Accuracy: 52.4833%, Training Loss: 0.9468%\n",
      "Epoch [28/300], Step [57/225], Training Accuracy: 52.6590%, Training Loss: 0.9437%\n",
      "Epoch [28/300], Step [58/225], Training Accuracy: 52.6670%, Training Loss: 0.9439%\n",
      "Epoch [28/300], Step [59/225], Training Accuracy: 52.8072%, Training Loss: 0.9430%\n",
      "Epoch [28/300], Step [60/225], Training Accuracy: 52.8385%, Training Loss: 0.9426%\n",
      "Epoch [28/300], Step [61/225], Training Accuracy: 52.9457%, Training Loss: 0.9422%\n",
      "Epoch [28/300], Step [62/225], Training Accuracy: 52.8982%, Training Loss: 0.9428%\n",
      "Epoch [28/300], Step [63/225], Training Accuracy: 52.7778%, Training Loss: 0.9435%\n",
      "Epoch [28/300], Step [64/225], Training Accuracy: 52.8564%, Training Loss: 0.9427%\n",
      "Epoch [28/300], Step [65/225], Training Accuracy: 52.7404%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [66/225], Training Accuracy: 52.9356%, Training Loss: 0.9410%\n",
      "Epoch [28/300], Step [67/225], Training Accuracy: 52.8218%, Training Loss: 0.9419%\n",
      "Epoch [28/300], Step [68/225], Training Accuracy: 52.8493%, Training Loss: 0.9415%\n",
      "Epoch [28/300], Step [69/225], Training Accuracy: 52.7627%, Training Loss: 0.9419%\n",
      "Epoch [28/300], Step [70/225], Training Accuracy: 52.6562%, Training Loss: 0.9426%\n",
      "Epoch [28/300], Step [71/225], Training Accuracy: 52.6408%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [72/225], Training Accuracy: 52.5608%, Training Loss: 0.9437%\n",
      "Epoch [28/300], Step [73/225], Training Accuracy: 52.5257%, Training Loss: 0.9458%\n",
      "Epoch [28/300], Step [74/225], Training Accuracy: 52.5338%, Training Loss: 0.9439%\n",
      "Epoch [28/300], Step [75/225], Training Accuracy: 52.5417%, Training Loss: 0.9440%\n",
      "Epoch [28/300], Step [76/225], Training Accuracy: 52.6110%, Training Loss: 0.9435%\n",
      "Epoch [28/300], Step [77/225], Training Accuracy: 52.6989%, Training Loss: 0.9429%\n",
      "Epoch [28/300], Step [78/225], Training Accuracy: 52.6242%, Training Loss: 0.9442%\n",
      "Epoch [28/300], Step [79/225], Training Accuracy: 52.5712%, Training Loss: 0.9456%\n",
      "Epoch [28/300], Step [80/225], Training Accuracy: 52.4609%, Training Loss: 0.9462%\n",
      "Epoch [28/300], Step [81/225], Training Accuracy: 52.6427%, Training Loss: 0.9463%\n",
      "Epoch [28/300], Step [82/225], Training Accuracy: 52.7058%, Training Loss: 0.9454%\n",
      "Epoch [28/300], Step [83/225], Training Accuracy: 52.6920%, Training Loss: 0.9446%\n",
      "Epoch [28/300], Step [84/225], Training Accuracy: 52.6600%, Training Loss: 0.9447%\n",
      "Epoch [28/300], Step [85/225], Training Accuracy: 52.6654%, Training Loss: 0.9437%\n",
      "Epoch [28/300], Step [86/225], Training Accuracy: 52.7071%, Training Loss: 0.9431%\n",
      "Epoch [28/300], Step [87/225], Training Accuracy: 52.7299%, Training Loss: 0.9430%\n",
      "Epoch [28/300], Step [88/225], Training Accuracy: 52.6278%, Training Loss: 0.9439%\n",
      "Epoch [28/300], Step [89/225], Training Accuracy: 52.5632%, Training Loss: 0.9451%\n",
      "Epoch [28/300], Step [90/225], Training Accuracy: 52.4653%, Training Loss: 0.9461%\n",
      "Epoch [28/300], Step [91/225], Training Accuracy: 52.5240%, Training Loss: 0.9445%\n",
      "Epoch [28/300], Step [92/225], Training Accuracy: 52.4796%, Training Loss: 0.9446%\n",
      "Epoch [28/300], Step [93/225], Training Accuracy: 52.5202%, Training Loss: 0.9447%\n",
      "Epoch [28/300], Step [94/225], Training Accuracy: 52.5598%, Training Loss: 0.9432%\n",
      "Epoch [28/300], Step [95/225], Training Accuracy: 52.5329%, Training Loss: 0.9438%\n",
      "Epoch [28/300], Step [96/225], Training Accuracy: 52.5716%, Training Loss: 0.9431%\n",
      "Epoch [28/300], Step [97/225], Training Accuracy: 52.6579%, Training Loss: 0.9421%\n",
      "Epoch [28/300], Step [98/225], Training Accuracy: 52.6307%, Training Loss: 0.9416%\n",
      "Epoch [28/300], Step [99/225], Training Accuracy: 52.6831%, Training Loss: 0.9412%\n",
      "Epoch [28/300], Step [100/225], Training Accuracy: 52.4844%, Training Loss: 0.9422%\n",
      "Epoch [28/300], Step [101/225], Training Accuracy: 52.4598%, Training Loss: 0.9427%\n",
      "Epoch [28/300], Step [102/225], Training Accuracy: 52.4050%, Training Loss: 0.9439%\n",
      "Epoch [28/300], Step [103/225], Training Accuracy: 52.2755%, Training Loss: 0.9458%\n",
      "Epoch [28/300], Step [104/225], Training Accuracy: 52.2686%, Training Loss: 0.9455%\n",
      "Epoch [28/300], Step [105/225], Training Accuracy: 52.2173%, Training Loss: 0.9449%\n",
      "Epoch [28/300], Step [106/225], Training Accuracy: 52.1226%, Training Loss: 0.9460%\n",
      "Epoch [28/300], Step [107/225], Training Accuracy: 52.1758%, Training Loss: 0.9459%\n",
      "Epoch [28/300], Step [108/225], Training Accuracy: 52.1557%, Training Loss: 0.9466%\n",
      "Epoch [28/300], Step [109/225], Training Accuracy: 52.1359%, Training Loss: 0.9466%\n",
      "Epoch [28/300], Step [110/225], Training Accuracy: 52.1591%, Training Loss: 0.9461%\n",
      "Epoch [28/300], Step [111/225], Training Accuracy: 52.2804%, Training Loss: 0.9467%\n",
      "Epoch [28/300], Step [112/225], Training Accuracy: 52.3438%, Training Loss: 0.9459%\n",
      "Epoch [28/300], Step [113/225], Training Accuracy: 52.3368%, Training Loss: 0.9465%\n",
      "Epoch [28/300], Step [114/225], Training Accuracy: 52.3026%, Training Loss: 0.9463%\n",
      "Epoch [28/300], Step [115/225], Training Accuracy: 52.3777%, Training Loss: 0.9455%\n",
      "Epoch [28/300], Step [116/225], Training Accuracy: 52.3707%, Training Loss: 0.9456%\n",
      "Epoch [28/300], Step [117/225], Training Accuracy: 52.2837%, Training Loss: 0.9469%\n",
      "Epoch [28/300], Step [118/225], Training Accuracy: 52.2908%, Training Loss: 0.9472%\n",
      "Epoch [28/300], Step [119/225], Training Accuracy: 52.2321%, Training Loss: 0.9478%\n",
      "Epoch [28/300], Step [120/225], Training Accuracy: 52.2656%, Training Loss: 0.9471%\n",
      "Epoch [28/300], Step [121/225], Training Accuracy: 52.2082%, Training Loss: 0.9477%\n",
      "Epoch [28/300], Step [122/225], Training Accuracy: 52.1644%, Training Loss: 0.9476%\n",
      "Epoch [28/300], Step [123/225], Training Accuracy: 52.1596%, Training Loss: 0.9474%\n",
      "Epoch [28/300], Step [124/225], Training Accuracy: 52.1925%, Training Loss: 0.9474%\n",
      "Epoch [28/300], Step [125/225], Training Accuracy: 52.1625%, Training Loss: 0.9486%\n",
      "Epoch [28/300], Step [126/225], Training Accuracy: 52.1577%, Training Loss: 0.9495%\n",
      "Epoch [28/300], Step [127/225], Training Accuracy: 52.1284%, Training Loss: 0.9495%\n",
      "Epoch [28/300], Step [128/225], Training Accuracy: 52.1484%, Training Loss: 0.9498%\n",
      "Epoch [28/300], Step [129/225], Training Accuracy: 52.1318%, Training Loss: 0.9504%\n",
      "Epoch [28/300], Step [130/225], Training Accuracy: 52.1034%, Training Loss: 0.9510%\n",
      "Epoch [28/300], Step [131/225], Training Accuracy: 52.0992%, Training Loss: 0.9508%\n",
      "Epoch [28/300], Step [132/225], Training Accuracy: 52.0597%, Training Loss: 0.9511%\n",
      "Epoch [28/300], Step [133/225], Training Accuracy: 52.1734%, Training Loss: 0.9504%\n",
      "Epoch [28/300], Step [134/225], Training Accuracy: 52.1805%, Training Loss: 0.9502%\n",
      "Epoch [28/300], Step [135/225], Training Accuracy: 52.1644%, Training Loss: 0.9500%\n",
      "Epoch [28/300], Step [136/225], Training Accuracy: 52.1484%, Training Loss: 0.9492%\n",
      "Epoch [28/300], Step [137/225], Training Accuracy: 52.2012%, Training Loss: 0.9485%\n",
      "Epoch [28/300], Step [138/225], Training Accuracy: 52.2645%, Training Loss: 0.9474%\n",
      "Epoch [28/300], Step [139/225], Training Accuracy: 52.2257%, Training Loss: 0.9476%\n",
      "Epoch [28/300], Step [140/225], Training Accuracy: 52.2656%, Training Loss: 0.9473%\n",
      "Epoch [28/300], Step [141/225], Training Accuracy: 52.2828%, Training Loss: 0.9471%\n",
      "Epoch [28/300], Step [142/225], Training Accuracy: 52.3107%, Training Loss: 0.9471%\n",
      "Epoch [28/300], Step [143/225], Training Accuracy: 52.3711%, Training Loss: 0.9466%\n",
      "Epoch [28/300], Step [144/225], Training Accuracy: 52.3655%, Training Loss: 0.9464%\n",
      "Epoch [28/300], Step [145/225], Training Accuracy: 52.4030%, Training Loss: 0.9456%\n",
      "Epoch [28/300], Step [146/225], Training Accuracy: 52.4187%, Training Loss: 0.9455%\n",
      "Epoch [28/300], Step [147/225], Training Accuracy: 52.3491%, Training Loss: 0.9462%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Step [148/225], Training Accuracy: 52.4388%, Training Loss: 0.9457%\n",
      "Epoch [28/300], Step [149/225], Training Accuracy: 52.4119%, Training Loss: 0.9458%\n",
      "Epoch [28/300], Step [150/225], Training Accuracy: 52.3750%, Training Loss: 0.9463%\n",
      "Epoch [28/300], Step [151/225], Training Accuracy: 52.4007%, Training Loss: 0.9457%\n",
      "Epoch [28/300], Step [152/225], Training Accuracy: 52.3746%, Training Loss: 0.9457%\n",
      "Epoch [28/300], Step [153/225], Training Accuracy: 52.3795%, Training Loss: 0.9452%\n",
      "Epoch [28/300], Step [154/225], Training Accuracy: 52.3742%, Training Loss: 0.9446%\n",
      "Epoch [28/300], Step [155/225], Training Accuracy: 52.3185%, Training Loss: 0.9458%\n",
      "Epoch [28/300], Step [156/225], Training Accuracy: 52.2736%, Training Loss: 0.9460%\n",
      "Epoch [28/300], Step [157/225], Training Accuracy: 52.2791%, Training Loss: 0.9464%\n",
      "Epoch [28/300], Step [158/225], Training Accuracy: 52.2844%, Training Loss: 0.9466%\n",
      "Epoch [28/300], Step [159/225], Training Accuracy: 52.3192%, Training Loss: 0.9462%\n",
      "Epoch [28/300], Step [160/225], Training Accuracy: 52.3047%, Training Loss: 0.9461%\n",
      "Epoch [28/300], Step [161/225], Training Accuracy: 52.3874%, Training Loss: 0.9452%\n",
      "Epoch [28/300], Step [162/225], Training Accuracy: 52.4498%, Training Loss: 0.9453%\n",
      "Epoch [28/300], Step [163/225], Training Accuracy: 52.3869%, Training Loss: 0.9450%\n",
      "Epoch [28/300], Step [164/225], Training Accuracy: 52.4200%, Training Loss: 0.9440%\n",
      "Epoch [28/300], Step [165/225], Training Accuracy: 52.4148%, Training Loss: 0.9438%\n",
      "Epoch [28/300], Step [166/225], Training Accuracy: 52.4191%, Training Loss: 0.9438%\n",
      "Epoch [28/300], Step [167/225], Training Accuracy: 52.4513%, Training Loss: 0.9431%\n",
      "Epoch [28/300], Step [168/225], Training Accuracy: 52.4368%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [169/225], Training Accuracy: 52.4408%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [170/225], Training Accuracy: 52.4081%, Training Loss: 0.9438%\n",
      "Epoch [28/300], Step [171/225], Training Accuracy: 52.4306%, Training Loss: 0.9437%\n",
      "Epoch [28/300], Step [172/225], Training Accuracy: 52.4164%, Training Loss: 0.9443%\n",
      "Epoch [28/300], Step [173/225], Training Accuracy: 52.4386%, Training Loss: 0.9440%\n",
      "Epoch [28/300], Step [174/225], Training Accuracy: 52.4246%, Training Loss: 0.9442%\n",
      "Epoch [28/300], Step [175/225], Training Accuracy: 52.4286%, Training Loss: 0.9443%\n",
      "Epoch [28/300], Step [176/225], Training Accuracy: 52.4503%, Training Loss: 0.9439%\n",
      "Epoch [28/300], Step [177/225], Training Accuracy: 52.4276%, Training Loss: 0.9438%\n",
      "Epoch [28/300], Step [178/225], Training Accuracy: 52.3964%, Training Loss: 0.9437%\n",
      "Epoch [28/300], Step [179/225], Training Accuracy: 52.4179%, Training Loss: 0.9436%\n",
      "Epoch [28/300], Step [180/225], Training Accuracy: 52.5000%, Training Loss: 0.9426%\n",
      "Epoch [28/300], Step [181/225], Training Accuracy: 52.4430%, Training Loss: 0.9434%\n",
      "Epoch [28/300], Step [182/225], Training Accuracy: 52.4296%, Training Loss: 0.9437%\n",
      "Epoch [28/300], Step [183/225], Training Accuracy: 52.4419%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [184/225], Training Accuracy: 52.4117%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [185/225], Training Accuracy: 52.3733%, Training Loss: 0.9433%\n",
      "Epoch [28/300], Step [186/225], Training Accuracy: 52.4110%, Training Loss: 0.9431%\n",
      "Epoch [28/300], Step [187/225], Training Accuracy: 52.4566%, Training Loss: 0.9424%\n",
      "Epoch [28/300], Step [188/225], Training Accuracy: 52.5266%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [189/225], Training Accuracy: 52.5628%, Training Loss: 0.9415%\n",
      "Epoch [28/300], Step [190/225], Training Accuracy: 52.5822%, Training Loss: 0.9417%\n",
      "Epoch [28/300], Step [191/225], Training Accuracy: 52.5360%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [192/225], Training Accuracy: 52.5798%, Training Loss: 0.9417%\n",
      "Epoch [28/300], Step [193/225], Training Accuracy: 52.5421%, Training Loss: 0.9418%\n",
      "Epoch [28/300], Step [194/225], Training Accuracy: 52.5612%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [195/225], Training Accuracy: 52.5881%, Training Loss: 0.9415%\n",
      "Epoch [28/300], Step [196/225], Training Accuracy: 52.6068%, Training Loss: 0.9415%\n",
      "Epoch [28/300], Step [197/225], Training Accuracy: 52.6491%, Training Loss: 0.9410%\n",
      "Epoch [28/300], Step [198/225], Training Accuracy: 52.6515%, Training Loss: 0.9405%\n",
      "Epoch [28/300], Step [199/225], Training Accuracy: 52.6932%, Training Loss: 0.9403%\n",
      "Epoch [28/300], Step [200/225], Training Accuracy: 52.7109%, Training Loss: 0.9404%\n",
      "Epoch [28/300], Step [201/225], Training Accuracy: 52.7130%, Training Loss: 0.9406%\n",
      "Epoch [28/300], Step [202/225], Training Accuracy: 52.7150%, Training Loss: 0.9403%\n",
      "Epoch [28/300], Step [203/225], Training Accuracy: 52.7094%, Training Loss: 0.9405%\n",
      "Epoch [28/300], Step [204/225], Training Accuracy: 52.7497%, Training Loss: 0.9400%\n",
      "Epoch [28/300], Step [205/225], Training Accuracy: 52.7363%, Training Loss: 0.9397%\n",
      "Epoch [28/300], Step [206/225], Training Accuracy: 52.7306%, Training Loss: 0.9402%\n",
      "Epoch [28/300], Step [207/225], Training Accuracy: 52.7098%, Training Loss: 0.9406%\n",
      "Epoch [28/300], Step [208/225], Training Accuracy: 52.7719%, Training Loss: 0.9401%\n",
      "Epoch [28/300], Step [209/225], Training Accuracy: 52.7437%, Training Loss: 0.9404%\n",
      "Epoch [28/300], Step [210/225], Training Accuracy: 52.7158%, Training Loss: 0.9405%\n",
      "Epoch [28/300], Step [211/225], Training Accuracy: 52.7399%, Training Loss: 0.9401%\n",
      "Epoch [28/300], Step [212/225], Training Accuracy: 52.6975%, Training Loss: 0.9405%\n",
      "Epoch [28/300], Step [213/225], Training Accuracy: 52.6629%, Training Loss: 0.9411%\n",
      "Epoch [28/300], Step [214/225], Training Accuracy: 52.6504%, Training Loss: 0.9410%\n",
      "Epoch [28/300], Step [215/225], Training Accuracy: 52.6744%, Training Loss: 0.9409%\n",
      "Epoch [28/300], Step [216/225], Training Accuracy: 52.6765%, Training Loss: 0.9412%\n",
      "Epoch [28/300], Step [217/225], Training Accuracy: 52.6714%, Training Loss: 0.9413%\n",
      "Epoch [28/300], Step [218/225], Training Accuracy: 52.6663%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [219/225], Training Accuracy: 52.6256%, Training Loss: 0.9422%\n",
      "Epoch [28/300], Step [220/225], Training Accuracy: 52.6705%, Training Loss: 0.9418%\n",
      "Epoch [28/300], Step [221/225], Training Accuracy: 52.6867%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [222/225], Training Accuracy: 52.6957%, Training Loss: 0.9420%\n",
      "Epoch [28/300], Step [223/225], Training Accuracy: 52.6415%, Training Loss: 0.9426%\n",
      "Epoch [28/300], Step [224/225], Training Accuracy: 52.6158%, Training Loss: 0.9426%\n",
      "Epoch [28/300], Step [225/225], Training Accuracy: 52.6334%, Training Loss: 0.9427%\n",
      "Epoch [29/300], Step [1/225], Training Accuracy: 64.0625%, Training Loss: 0.7763%\n",
      "Epoch [29/300], Step [2/225], Training Accuracy: 53.9062%, Training Loss: 0.9590%\n",
      "Epoch [29/300], Step [3/225], Training Accuracy: 50.0000%, Training Loss: 1.0096%\n",
      "Epoch [29/300], Step [4/225], Training Accuracy: 51.1719%, Training Loss: 0.9869%\n",
      "Epoch [29/300], Step [5/225], Training Accuracy: 53.1250%, Training Loss: 0.9514%\n",
      "Epoch [29/300], Step [6/225], Training Accuracy: 52.0833%, Training Loss: 0.9709%\n",
      "Epoch [29/300], Step [7/225], Training Accuracy: 52.2321%, Training Loss: 0.9645%\n",
      "Epoch [29/300], Step [8/225], Training Accuracy: 52.9297%, Training Loss: 0.9714%\n",
      "Epoch [29/300], Step [9/225], Training Accuracy: 53.1250%, Training Loss: 0.9635%\n",
      "Epoch [29/300], Step [10/225], Training Accuracy: 53.4375%, Training Loss: 0.9577%\n",
      "Epoch [29/300], Step [11/225], Training Accuracy: 53.9773%, Training Loss: 0.9533%\n",
      "Epoch [29/300], Step [12/225], Training Accuracy: 53.3854%, Training Loss: 0.9597%\n",
      "Epoch [29/300], Step [13/225], Training Accuracy: 54.4471%, Training Loss: 0.9554%\n",
      "Epoch [29/300], Step [14/225], Training Accuracy: 54.1295%, Training Loss: 0.9584%\n",
      "Epoch [29/300], Step [15/225], Training Accuracy: 53.4375%, Training Loss: 0.9721%\n",
      "Epoch [29/300], Step [16/225], Training Accuracy: 53.4180%, Training Loss: 0.9710%\n",
      "Epoch [29/300], Step [17/225], Training Accuracy: 53.2169%, Training Loss: 0.9657%\n",
      "Epoch [29/300], Step [18/225], Training Accuracy: 52.9514%, Training Loss: 0.9636%\n",
      "Epoch [29/300], Step [19/225], Training Accuracy: 52.3849%, Training Loss: 0.9654%\n",
      "Epoch [29/300], Step [20/225], Training Accuracy: 52.5781%, Training Loss: 0.9618%\n",
      "Epoch [29/300], Step [21/225], Training Accuracy: 52.6042%, Training Loss: 0.9581%\n",
      "Epoch [29/300], Step [22/225], Training Accuracy: 52.3438%, Training Loss: 0.9589%\n",
      "Epoch [29/300], Step [23/225], Training Accuracy: 52.3098%, Training Loss: 0.9553%\n",
      "Epoch [29/300], Step [24/225], Training Accuracy: 52.6042%, Training Loss: 0.9577%\n",
      "Epoch [29/300], Step [25/225], Training Accuracy: 52.6250%, Training Loss: 0.9538%\n",
      "Epoch [29/300], Step [26/225], Training Accuracy: 52.2837%, Training Loss: 0.9538%\n",
      "Epoch [29/300], Step [27/225], Training Accuracy: 52.0255%, Training Loss: 0.9563%\n",
      "Epoch [29/300], Step [28/225], Training Accuracy: 52.0647%, Training Loss: 0.9532%\n",
      "Epoch [29/300], Step [29/225], Training Accuracy: 52.3168%, Training Loss: 0.9491%\n",
      "Epoch [29/300], Step [30/225], Training Accuracy: 52.2917%, Training Loss: 0.9473%\n",
      "Epoch [29/300], Step [31/225], Training Accuracy: 52.4194%, Training Loss: 0.9463%\n",
      "Epoch [29/300], Step [32/225], Training Accuracy: 52.5879%, Training Loss: 0.9436%\n",
      "Epoch [29/300], Step [33/225], Training Accuracy: 52.6042%, Training Loss: 0.9422%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [34/225], Training Accuracy: 52.6195%, Training Loss: 0.9442%\n",
      "Epoch [29/300], Step [35/225], Training Accuracy: 52.4554%, Training Loss: 0.9449%\n",
      "Epoch [29/300], Step [36/225], Training Accuracy: 52.3872%, Training Loss: 0.9450%\n",
      "Epoch [29/300], Step [37/225], Training Accuracy: 52.7027%, Training Loss: 0.9423%\n",
      "Epoch [29/300], Step [38/225], Training Accuracy: 52.8783%, Training Loss: 0.9408%\n",
      "Epoch [29/300], Step [39/225], Training Accuracy: 52.9247%, Training Loss: 0.9402%\n",
      "Epoch [29/300], Step [40/225], Training Accuracy: 52.8906%, Training Loss: 0.9402%\n",
      "Epoch [29/300], Step [41/225], Training Accuracy: 52.4771%, Training Loss: 0.9424%\n",
      "Epoch [29/300], Step [42/225], Training Accuracy: 52.5670%, Training Loss: 0.9416%\n",
      "Epoch [29/300], Step [43/225], Training Accuracy: 52.4709%, Training Loss: 0.9407%\n",
      "Epoch [29/300], Step [44/225], Training Accuracy: 52.5213%, Training Loss: 0.9399%\n",
      "Epoch [29/300], Step [45/225], Training Accuracy: 52.5000%, Training Loss: 0.9401%\n",
      "Epoch [29/300], Step [46/225], Training Accuracy: 52.7853%, Training Loss: 0.9370%\n",
      "Epoch [29/300], Step [47/225], Training Accuracy: 52.8590%, Training Loss: 0.9387%\n",
      "Epoch [29/300], Step [48/225], Training Accuracy: 52.8971%, Training Loss: 0.9402%\n",
      "Epoch [29/300], Step [49/225], Training Accuracy: 52.6786%, Training Loss: 0.9417%\n",
      "Epoch [29/300], Step [50/225], Training Accuracy: 52.7188%, Training Loss: 0.9404%\n",
      "Epoch [29/300], Step [51/225], Training Accuracy: 52.9412%, Training Loss: 0.9372%\n",
      "Epoch [29/300], Step [52/225], Training Accuracy: 53.1550%, Training Loss: 0.9349%\n",
      "Epoch [29/300], Step [53/225], Training Accuracy: 53.0660%, Training Loss: 0.9347%\n",
      "Epoch [29/300], Step [54/225], Training Accuracy: 52.8067%, Training Loss: 0.9372%\n",
      "Epoch [29/300], Step [55/225], Training Accuracy: 52.7557%, Training Loss: 0.9395%\n",
      "Epoch [29/300], Step [56/225], Training Accuracy: 52.7344%, Training Loss: 0.9407%\n",
      "Epoch [29/300], Step [57/225], Training Accuracy: 52.9605%, Training Loss: 0.9375%\n",
      "Epoch [29/300], Step [58/225], Training Accuracy: 52.9364%, Training Loss: 0.9378%\n",
      "Epoch [29/300], Step [59/225], Training Accuracy: 53.0720%, Training Loss: 0.9368%\n",
      "Epoch [29/300], Step [60/225], Training Accuracy: 53.0990%, Training Loss: 0.9364%\n",
      "Epoch [29/300], Step [61/225], Training Accuracy: 53.2018%, Training Loss: 0.9359%\n",
      "Epoch [29/300], Step [62/225], Training Accuracy: 53.1502%, Training Loss: 0.9365%\n",
      "Epoch [29/300], Step [63/225], Training Accuracy: 53.0506%, Training Loss: 0.9372%\n",
      "Epoch [29/300], Step [64/225], Training Accuracy: 53.1250%, Training Loss: 0.9363%\n",
      "Epoch [29/300], Step [65/225], Training Accuracy: 53.0048%, Training Loss: 0.9368%\n",
      "Epoch [29/300], Step [66/225], Training Accuracy: 53.2197%, Training Loss: 0.9345%\n",
      "Epoch [29/300], Step [67/225], Training Accuracy: 53.1017%, Training Loss: 0.9354%\n",
      "Epoch [29/300], Step [68/225], Training Accuracy: 53.1250%, Training Loss: 0.9351%\n",
      "Epoch [29/300], Step [69/225], Training Accuracy: 53.0344%, Training Loss: 0.9355%\n",
      "Epoch [29/300], Step [70/225], Training Accuracy: 52.9464%, Training Loss: 0.9362%\n",
      "Epoch [29/300], Step [71/225], Training Accuracy: 52.9489%, Training Loss: 0.9357%\n",
      "Epoch [29/300], Step [72/225], Training Accuracy: 52.8863%, Training Loss: 0.9373%\n",
      "Epoch [29/300], Step [73/225], Training Accuracy: 52.8253%, Training Loss: 0.9393%\n",
      "Epoch [29/300], Step [74/225], Training Accuracy: 52.8294%, Training Loss: 0.9374%\n",
      "Epoch [29/300], Step [75/225], Training Accuracy: 52.8333%, Training Loss: 0.9375%\n",
      "Epoch [29/300], Step [76/225], Training Accuracy: 52.8988%, Training Loss: 0.9370%\n",
      "Epoch [29/300], Step [77/225], Training Accuracy: 52.9627%, Training Loss: 0.9364%\n",
      "Epoch [29/300], Step [78/225], Training Accuracy: 52.9247%, Training Loss: 0.9377%\n",
      "Epoch [29/300], Step [79/225], Training Accuracy: 52.8679%, Training Loss: 0.9391%\n",
      "Epoch [29/300], Step [80/225], Training Accuracy: 52.7539%, Training Loss: 0.9398%\n",
      "Epoch [29/300], Step [81/225], Training Accuracy: 52.9321%, Training Loss: 0.9398%\n",
      "Epoch [29/300], Step [82/225], Training Accuracy: 52.9916%, Training Loss: 0.9389%\n",
      "Epoch [29/300], Step [83/225], Training Accuracy: 52.9932%, Training Loss: 0.9380%\n",
      "Epoch [29/300], Step [84/225], Training Accuracy: 52.9762%, Training Loss: 0.9382%\n",
      "Epoch [29/300], Step [85/225], Training Accuracy: 52.9596%, Training Loss: 0.9371%\n",
      "Epoch [29/300], Step [86/225], Training Accuracy: 53.0160%, Training Loss: 0.9364%\n",
      "Epoch [29/300], Step [87/225], Training Accuracy: 53.0532%, Training Loss: 0.9364%\n",
      "Epoch [29/300], Step [88/225], Training Accuracy: 52.9830%, Training Loss: 0.9373%\n",
      "Epoch [29/300], Step [89/225], Training Accuracy: 52.9319%, Training Loss: 0.9385%\n",
      "Epoch [29/300], Step [90/225], Training Accuracy: 52.8299%, Training Loss: 0.9394%\n",
      "Epoch [29/300], Step [91/225], Training Accuracy: 52.9018%, Training Loss: 0.9378%\n",
      "Epoch [29/300], Step [92/225], Training Accuracy: 52.8363%, Training Loss: 0.9381%\n",
      "Epoch [29/300], Step [93/225], Training Accuracy: 52.8730%, Training Loss: 0.9381%\n",
      "Epoch [29/300], Step [94/225], Training Accuracy: 52.9255%, Training Loss: 0.9366%\n",
      "Epoch [29/300], Step [95/225], Training Accuracy: 52.8783%, Training Loss: 0.9372%\n",
      "Epoch [29/300], Step [96/225], Training Accuracy: 52.9134%, Training Loss: 0.9364%\n",
      "Epoch [29/300], Step [97/225], Training Accuracy: 52.9961%, Training Loss: 0.9354%\n",
      "Epoch [29/300], Step [98/225], Training Accuracy: 52.9656%, Training Loss: 0.9349%\n",
      "Epoch [29/300], Step [99/225], Training Accuracy: 52.9987%, Training Loss: 0.9345%\n",
      "Epoch [29/300], Step [100/225], Training Accuracy: 52.8594%, Training Loss: 0.9356%\n",
      "Epoch [29/300], Step [101/225], Training Accuracy: 52.8311%, Training Loss: 0.9361%\n",
      "Epoch [29/300], Step [102/225], Training Accuracy: 52.8033%, Training Loss: 0.9372%\n",
      "Epoch [29/300], Step [103/225], Training Accuracy: 52.6699%, Training Loss: 0.9391%\n",
      "Epoch [29/300], Step [104/225], Training Accuracy: 52.6593%, Training Loss: 0.9389%\n",
      "Epoch [29/300], Step [105/225], Training Accuracy: 52.6190%, Training Loss: 0.9382%\n",
      "Epoch [29/300], Step [106/225], Training Accuracy: 52.5354%, Training Loss: 0.9394%\n",
      "Epoch [29/300], Step [107/225], Training Accuracy: 52.5847%, Training Loss: 0.9392%\n",
      "Epoch [29/300], Step [108/225], Training Accuracy: 52.5463%, Training Loss: 0.9399%\n",
      "Epoch [29/300], Step [109/225], Training Accuracy: 52.5229%, Training Loss: 0.9399%\n",
      "Epoch [29/300], Step [110/225], Training Accuracy: 52.5426%, Training Loss: 0.9394%\n",
      "Epoch [29/300], Step [111/225], Training Accuracy: 52.6605%, Training Loss: 0.9399%\n",
      "Epoch [29/300], Step [112/225], Training Accuracy: 52.7204%, Training Loss: 0.9391%\n",
      "Epoch [29/300], Step [113/225], Training Accuracy: 52.7378%, Training Loss: 0.9397%\n",
      "Epoch [29/300], Step [114/225], Training Accuracy: 52.7549%, Training Loss: 0.9395%\n",
      "Epoch [29/300], Step [115/225], Training Accuracy: 52.8261%, Training Loss: 0.9387%\n",
      "Epoch [29/300], Step [116/225], Training Accuracy: 52.8017%, Training Loss: 0.9389%\n",
      "Epoch [29/300], Step [117/225], Training Accuracy: 52.7110%, Training Loss: 0.9400%\n",
      "Epoch [29/300], Step [118/225], Training Accuracy: 52.7013%, Training Loss: 0.9404%\n",
      "Epoch [29/300], Step [119/225], Training Accuracy: 52.6392%, Training Loss: 0.9410%\n",
      "Epoch [29/300], Step [120/225], Training Accuracy: 52.6693%, Training Loss: 0.9404%\n",
      "Epoch [29/300], Step [121/225], Training Accuracy: 52.6214%, Training Loss: 0.9410%\n",
      "Epoch [29/300], Step [122/225], Training Accuracy: 52.5999%, Training Loss: 0.9410%\n",
      "Epoch [29/300], Step [123/225], Training Accuracy: 52.6042%, Training Loss: 0.9407%\n",
      "Epoch [29/300], Step [124/225], Training Accuracy: 52.6336%, Training Loss: 0.9407%\n",
      "Epoch [29/300], Step [125/225], Training Accuracy: 52.5875%, Training Loss: 0.9420%\n",
      "Epoch [29/300], Step [126/225], Training Accuracy: 52.5794%, Training Loss: 0.9429%\n",
      "Epoch [29/300], Step [127/225], Training Accuracy: 52.5468%, Training Loss: 0.9430%\n",
      "Epoch [29/300], Step [128/225], Training Accuracy: 52.5757%, Training Loss: 0.9433%\n",
      "Epoch [29/300], Step [129/225], Training Accuracy: 52.5678%, Training Loss: 0.9438%\n",
      "Epoch [29/300], Step [130/225], Training Accuracy: 52.5601%, Training Loss: 0.9444%\n",
      "Epoch [29/300], Step [131/225], Training Accuracy: 52.5406%, Training Loss: 0.9443%\n",
      "Epoch [29/300], Step [132/225], Training Accuracy: 52.5095%, Training Loss: 0.9446%\n",
      "Epoch [29/300], Step [133/225], Training Accuracy: 52.6198%, Training Loss: 0.9438%\n",
      "Epoch [29/300], Step [134/225], Training Accuracy: 52.6119%, Training Loss: 0.9437%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [135/225], Training Accuracy: 52.6042%, Training Loss: 0.9434%\n",
      "Epoch [29/300], Step [136/225], Training Accuracy: 52.5965%, Training Loss: 0.9427%\n",
      "Epoch [29/300], Step [137/225], Training Accuracy: 52.6574%, Training Loss: 0.9419%\n",
      "Epoch [29/300], Step [138/225], Training Accuracy: 52.7174%, Training Loss: 0.9407%\n",
      "Epoch [29/300], Step [139/225], Training Accuracy: 52.6866%, Training Loss: 0.9409%\n",
      "Epoch [29/300], Step [140/225], Training Accuracy: 52.7455%, Training Loss: 0.9407%\n",
      "Epoch [29/300], Step [141/225], Training Accuracy: 52.7704%, Training Loss: 0.9404%\n",
      "Epoch [29/300], Step [142/225], Training Accuracy: 52.8169%, Training Loss: 0.9404%\n",
      "Epoch [29/300], Step [143/225], Training Accuracy: 52.8846%, Training Loss: 0.9399%\n",
      "Epoch [29/300], Step [144/225], Training Accuracy: 52.8754%, Training Loss: 0.9397%\n",
      "Epoch [29/300], Step [145/225], Training Accuracy: 52.9203%, Training Loss: 0.9389%\n",
      "Epoch [29/300], Step [146/225], Training Accuracy: 52.9217%, Training Loss: 0.9387%\n",
      "Epoch [29/300], Step [147/225], Training Accuracy: 52.8380%, Training Loss: 0.9395%\n",
      "Epoch [29/300], Step [148/225], Training Accuracy: 52.9244%, Training Loss: 0.9390%\n",
      "Epoch [29/300], Step [149/225], Training Accuracy: 52.9048%, Training Loss: 0.9391%\n",
      "Epoch [29/300], Step [150/225], Training Accuracy: 52.8750%, Training Loss: 0.9395%\n",
      "Epoch [29/300], Step [151/225], Training Accuracy: 52.8870%, Training Loss: 0.9390%\n",
      "Epoch [29/300], Step [152/225], Training Accuracy: 52.8680%, Training Loss: 0.9389%\n",
      "Epoch [29/300], Step [153/225], Training Accuracy: 52.8901%, Training Loss: 0.9385%\n",
      "Epoch [29/300], Step [154/225], Training Accuracy: 52.8815%, Training Loss: 0.9379%\n",
      "Epoch [29/300], Step [155/225], Training Accuracy: 52.8226%, Training Loss: 0.9391%\n",
      "Epoch [29/300], Step [156/225], Training Accuracy: 52.7744%, Training Loss: 0.9394%\n",
      "Epoch [29/300], Step [157/225], Training Accuracy: 52.7767%, Training Loss: 0.9397%\n",
      "Epoch [29/300], Step [158/225], Training Accuracy: 52.7888%, Training Loss: 0.9399%\n",
      "Epoch [29/300], Step [159/225], Training Accuracy: 52.8007%, Training Loss: 0.9396%\n",
      "Epoch [29/300], Step [160/225], Training Accuracy: 52.7832%, Training Loss: 0.9394%\n",
      "Epoch [29/300], Step [161/225], Training Accuracy: 52.8630%, Training Loss: 0.9385%\n",
      "Epoch [29/300], Step [162/225], Training Accuracy: 52.9225%, Training Loss: 0.9386%\n",
      "Epoch [29/300], Step [163/225], Training Accuracy: 52.8662%, Training Loss: 0.9383%\n",
      "Epoch [29/300], Step [164/225], Training Accuracy: 52.8963%, Training Loss: 0.9373%\n",
      "Epoch [29/300], Step [165/225], Training Accuracy: 52.8883%, Training Loss: 0.9371%\n",
      "Epoch [29/300], Step [166/225], Training Accuracy: 52.8897%, Training Loss: 0.9371%\n",
      "Epoch [29/300], Step [167/225], Training Accuracy: 52.9192%, Training Loss: 0.9363%\n",
      "Epoch [29/300], Step [168/225], Training Accuracy: 52.9204%, Training Loss: 0.9366%\n",
      "Epoch [29/300], Step [169/225], Training Accuracy: 52.9216%, Training Loss: 0.9365%\n",
      "Epoch [29/300], Step [170/225], Training Accuracy: 52.8860%, Training Loss: 0.9370%\n",
      "Epoch [29/300], Step [171/225], Training Accuracy: 52.9240%, Training Loss: 0.9369%\n",
      "Epoch [29/300], Step [172/225], Training Accuracy: 52.8979%, Training Loss: 0.9375%\n",
      "Epoch [29/300], Step [173/225], Training Accuracy: 52.9173%, Training Loss: 0.9372%\n",
      "Epoch [29/300], Step [174/225], Training Accuracy: 52.9005%, Training Loss: 0.9374%\n",
      "Epoch [29/300], Step [175/225], Training Accuracy: 52.8929%, Training Loss: 0.9375%\n",
      "Epoch [29/300], Step [176/225], Training Accuracy: 52.9119%, Training Loss: 0.9371%\n",
      "Epoch [29/300], Step [177/225], Training Accuracy: 52.8867%, Training Loss: 0.9370%\n",
      "Epoch [29/300], Step [178/225], Training Accuracy: 52.8353%, Training Loss: 0.9370%\n",
      "Epoch [29/300], Step [179/225], Training Accuracy: 52.8544%, Training Loss: 0.9369%\n",
      "Epoch [29/300], Step [180/225], Training Accuracy: 52.9340%, Training Loss: 0.9359%\n",
      "Epoch [29/300], Step [181/225], Training Accuracy: 52.8574%, Training Loss: 0.9367%\n",
      "Epoch [29/300], Step [182/225], Training Accuracy: 52.8417%, Training Loss: 0.9369%\n",
      "Epoch [29/300], Step [183/225], Training Accuracy: 52.8518%, Training Loss: 0.9366%\n",
      "Epoch [29/300], Step [184/225], Training Accuracy: 52.8193%, Training Loss: 0.9366%\n",
      "Epoch [29/300], Step [185/225], Training Accuracy: 52.7787%, Training Loss: 0.9366%\n",
      "Epoch [29/300], Step [186/225], Training Accuracy: 52.8142%, Training Loss: 0.9364%\n",
      "Epoch [29/300], Step [187/225], Training Accuracy: 52.8576%, Training Loss: 0.9358%\n",
      "Epoch [29/300], Step [188/225], Training Accuracy: 52.9006%, Training Loss: 0.9353%\n",
      "Epoch [29/300], Step [189/225], Training Accuracy: 52.9266%, Training Loss: 0.9348%\n",
      "Epoch [29/300], Step [190/225], Training Accuracy: 52.9441%, Training Loss: 0.9351%\n",
      "Epoch [29/300], Step [191/225], Training Accuracy: 52.8878%, Training Loss: 0.9354%\n",
      "Epoch [29/300], Step [192/225], Training Accuracy: 52.7018%, Training Loss: nan%\n",
      "Epoch [29/300], Step [193/225], Training Accuracy: 52.5421%, Training Loss: nan%\n",
      "Epoch [29/300], Step [194/225], Training Accuracy: 52.3760%, Training Loss: nan%\n",
      "Epoch [29/300], Step [195/225], Training Accuracy: 52.1955%, Training Loss: nan%\n",
      "Epoch [29/300], Step [196/225], Training Accuracy: 52.0408%, Training Loss: nan%\n",
      "Epoch [29/300], Step [197/225], Training Accuracy: 51.8956%, Training Loss: nan%\n",
      "Epoch [29/300], Step [198/225], Training Accuracy: 51.7361%, Training Loss: nan%\n",
      "Epoch [29/300], Step [199/225], Training Accuracy: 51.5861%, Training Loss: nan%\n",
      "Epoch [29/300], Step [200/225], Training Accuracy: 51.4141%, Training Loss: nan%\n",
      "Epoch [29/300], Step [201/225], Training Accuracy: 51.3060%, Training Loss: nan%\n",
      "Epoch [29/300], Step [202/225], Training Accuracy: 51.1371%, Training Loss: nan%\n",
      "Epoch [29/300], Step [203/225], Training Accuracy: 51.0314%, Training Loss: nan%\n",
      "Epoch [29/300], Step [204/225], Training Accuracy: 50.9498%, Training Loss: nan%\n",
      "Epoch [29/300], Step [205/225], Training Accuracy: 50.8003%, Training Loss: nan%\n",
      "Epoch [29/300], Step [206/225], Training Accuracy: 50.6902%, Training Loss: nan%\n",
      "Epoch [29/300], Step [207/225], Training Accuracy: 50.5737%, Training Loss: nan%\n",
      "Epoch [29/300], Step [208/225], Training Accuracy: 50.4507%, Training Loss: nan%\n",
      "Epoch [29/300], Step [209/225], Training Accuracy: 50.3663%, Training Loss: nan%\n",
      "Epoch [29/300], Step [210/225], Training Accuracy: 50.2679%, Training Loss: nan%\n",
      "Epoch [29/300], Step [211/225], Training Accuracy: 50.1185%, Training Loss: nan%\n",
      "Epoch [29/300], Step [212/225], Training Accuracy: 50.0295%, Training Loss: nan%\n",
      "Epoch [29/300], Step [213/225], Training Accuracy: 49.9266%, Training Loss: nan%\n",
      "Epoch [29/300], Step [214/225], Training Accuracy: 49.8029%, Training Loss: nan%\n",
      "Epoch [29/300], Step [215/225], Training Accuracy: 49.6294%, Training Loss: nan%\n",
      "Epoch [29/300], Step [216/225], Training Accuracy: 49.4647%, Training Loss: nan%\n",
      "Epoch [29/300], Step [217/225], Training Accuracy: 49.3520%, Training Loss: nan%\n",
      "Epoch [29/300], Step [218/225], Training Accuracy: 49.2474%, Training Loss: nan%\n",
      "Epoch [29/300], Step [219/225], Training Accuracy: 49.1581%, Training Loss: nan%\n",
      "Epoch [29/300], Step [220/225], Training Accuracy: 49.0341%, Training Loss: nan%\n",
      "Epoch [29/300], Step [221/225], Training Accuracy: 48.9465%, Training Loss: nan%\n",
      "Epoch [29/300], Step [222/225], Training Accuracy: 48.7965%, Training Loss: nan%\n",
      "Epoch [29/300], Step [223/225], Training Accuracy: 48.7178%, Training Loss: nan%\n",
      "Epoch [29/300], Step [224/225], Training Accuracy: 48.5979%, Training Loss: nan%\n",
      "Epoch [29/300], Step [225/225], Training Accuracy: 48.5270%, Training Loss: nan%\n",
      "Epoch [30/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [30/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [30/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [30/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [30/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [30/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [30/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [30/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [30/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [30/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [30/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [30/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [30/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [30/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [30/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [30/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [30/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [30/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [30/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [30/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [30/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [30/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [30/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [30/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [30/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [30/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [30/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [30/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [30/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [30/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [30/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [30/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [30/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [30/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [30/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [30/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [30/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [30/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [30/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [30/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [30/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [30/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [30/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [30/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [30/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [30/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [30/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [30/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [30/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [30/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [30/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [30/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [30/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [30/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [30/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [30/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [30/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [30/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [30/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [30/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [30/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [30/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [30/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [30/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [30/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [30/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [30/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [30/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [30/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [30/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [30/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [30/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [30/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [30/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [30/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [30/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [30/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [30/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [30/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [30/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [30/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [30/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [30/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [30/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [30/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [30/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [30/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [30/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [30/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [30/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [30/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [30/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [30/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [30/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [30/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [30/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [30/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [30/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [30/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [30/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [30/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [30/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [30/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [30/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [30/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [30/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [30/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [30/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [30/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [30/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [30/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [30/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [30/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [30/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [30/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [30/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [30/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [30/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [30/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [30/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [30/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [30/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [30/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [30/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [30/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [30/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [30/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [30/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [30/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [30/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [30/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [30/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [30/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [30/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [30/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [30/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [30/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [30/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [30/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [30/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [30/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [30/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [30/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [30/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [30/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [30/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [30/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [30/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [30/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [30/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [30/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [30/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [30/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [30/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [30/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [30/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [30/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [30/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [30/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [30/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [30/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [30/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [30/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [30/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [30/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [30/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [30/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [30/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [30/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [30/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [30/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [30/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [30/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [30/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [30/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [30/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [30/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [30/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [30/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [30/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [30/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [30/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [30/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [30/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [30/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [30/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [30/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [30/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [30/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [30/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [30/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [30/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [30/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [30/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [30/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [30/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [30/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [30/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [30/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [30/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [30/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [30/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [30/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [30/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [30/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [30/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [30/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [30/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [30/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [30/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [30/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [30/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [30/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [30/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [30/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [30/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [30/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [30/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [30/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [30/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [30/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [30/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [30/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [31/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [31/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [31/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [31/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [31/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [31/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [31/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [31/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [31/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [31/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [31/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [31/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [31/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [31/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [31/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [31/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [31/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [31/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [31/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [31/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [31/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [31/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [31/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [31/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [31/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [31/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [31/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [31/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [31/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [31/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [31/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [31/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [31/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [31/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [31/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [31/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [31/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [31/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [31/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [31/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [31/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [31/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [31/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [31/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [31/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [31/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [31/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [31/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [31/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [31/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [31/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [31/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [31/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [31/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [31/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [31/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [31/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [31/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [31/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [31/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [31/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [31/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [31/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [31/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [31/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [31/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [31/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [31/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [31/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [31/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [31/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [31/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [31/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [31/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [31/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [31/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [31/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [31/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [31/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [31/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [31/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [31/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [31/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [31/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [31/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [31/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [31/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [31/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [31/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [31/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [31/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [31/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [31/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [31/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [31/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [31/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [31/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [31/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [31/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [31/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [31/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [31/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [31/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [31/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [31/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [31/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [31/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [31/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [31/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [31/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [31/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [31/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [31/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [31/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [31/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [31/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [31/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [31/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [31/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [31/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [31/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [31/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [31/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [31/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [31/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [31/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [31/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [31/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [31/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [31/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [31/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [31/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [31/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [31/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [31/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [31/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [31/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [31/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [31/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [31/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [31/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [31/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [31/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [31/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [31/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [31/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [31/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [31/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [31/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [31/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [31/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [31/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [31/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [31/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [31/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [31/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [31/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [31/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [31/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [31/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [31/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [31/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [31/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [31/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [31/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [31/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [31/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [31/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [31/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [31/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [31/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [31/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [31/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [31/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [31/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [31/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [31/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [31/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [31/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [31/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [31/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [31/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [31/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [31/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [31/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [31/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [31/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [31/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [31/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [31/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [31/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [31/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [31/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [31/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [31/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [31/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [31/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [31/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [31/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [31/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [31/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [31/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [31/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [31/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [31/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [31/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [31/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [31/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [31/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [31/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [31/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [31/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [31/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [31/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [31/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [31/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [31/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [31/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [31/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [31/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [31/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [31/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [31/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [32/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [32/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [32/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [32/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [32/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [32/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [32/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [32/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [32/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [32/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [32/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [32/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [32/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [32/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [32/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [32/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [32/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [32/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [32/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [32/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [32/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [32/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [32/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [32/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [32/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [32/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [32/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [32/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [32/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [32/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [32/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [32/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [32/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [32/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [32/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [32/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [32/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [32/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [32/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [32/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [32/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [32/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [32/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [32/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [32/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [32/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [32/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [32/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [32/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [32/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [32/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [32/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [32/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [32/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [32/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [32/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [32/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [32/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [32/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [32/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [32/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [32/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [32/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [32/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [32/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [32/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [32/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [32/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [32/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [32/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [32/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [32/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [32/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [32/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [32/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [32/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [32/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [32/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [32/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [32/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [32/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [32/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [32/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [32/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [32/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [32/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [32/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [32/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [32/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [32/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [32/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [32/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [32/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [32/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [32/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [32/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [32/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [32/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [32/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [32/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [32/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [32/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [32/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [32/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [32/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [32/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [32/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [32/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [32/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [32/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [32/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [32/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [32/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [32/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [32/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [32/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [32/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [32/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [32/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [32/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [32/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [32/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [32/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [32/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [32/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [32/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [32/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [32/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [32/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [32/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [32/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [32/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [32/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [32/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [32/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [32/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [32/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [32/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [32/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [32/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [32/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [32/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [32/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [32/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [32/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [32/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [32/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [32/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [32/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [32/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [32/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [32/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [32/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [32/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [32/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [32/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [32/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [32/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [32/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [32/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [32/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [32/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [32/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [32/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [32/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [32/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [32/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [32/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [32/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [32/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [32/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [32/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [32/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [32/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [32/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [32/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [32/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [32/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [32/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [32/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [32/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [32/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [32/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [32/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [32/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [32/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [32/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [32/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [32/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [32/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [32/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [32/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [32/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [32/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [32/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [32/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [32/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [32/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [32/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [32/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [32/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [32/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [32/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [32/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [32/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [32/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [32/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [32/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [32/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [32/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [32/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [32/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [32/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [32/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [32/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [32/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [32/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [32/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [32/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [32/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [32/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [32/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [32/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [33/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [33/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [33/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [33/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [33/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [33/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [33/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [33/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [33/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [33/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [33/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [33/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [33/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [33/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [33/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [33/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [33/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [33/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [33/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [33/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [33/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [33/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [33/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [33/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [33/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [33/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [33/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [33/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [33/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [33/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [33/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [33/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [33/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [33/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [33/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [33/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [33/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [33/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [33/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [33/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [33/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [33/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [33/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [33/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [33/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [33/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [33/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [33/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [33/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [33/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [33/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [33/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [33/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [33/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [33/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [33/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [33/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [33/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [33/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [33/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [33/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [33/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [33/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [33/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [33/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [33/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [33/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [33/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [33/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [33/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [33/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [33/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [33/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [33/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [33/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [33/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [33/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [33/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [33/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [33/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [33/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [33/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [33/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [33/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [33/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [33/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [33/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [33/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [33/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [33/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [33/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [33/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [33/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [33/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [33/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [33/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [33/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [33/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [33/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [33/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [33/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [33/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [33/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [33/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [33/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [33/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [33/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [33/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [33/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [33/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [33/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [33/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [33/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [33/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [33/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [33/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [33/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [33/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [33/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [33/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [33/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [33/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [33/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [33/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [33/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [33/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [33/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [33/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [33/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [33/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [33/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [33/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [33/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [33/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [33/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [33/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [33/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [33/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [33/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [33/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [33/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [33/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [33/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [33/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [33/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [33/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [33/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [33/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [33/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [33/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [33/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [33/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [33/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [33/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [33/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [33/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [33/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [33/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [33/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [33/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [33/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [33/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [33/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [33/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [33/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [33/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [33/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [33/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [33/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [33/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [33/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [33/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [33/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [33/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [33/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [33/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [33/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [33/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [33/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [33/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [33/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [33/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [33/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [33/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [33/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [33/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [33/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [33/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [33/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [33/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [33/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [33/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [33/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [33/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [33/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [33/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [33/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [33/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [33/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [33/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [33/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [33/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [33/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [33/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [33/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [33/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [33/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [33/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [33/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [33/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [33/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [33/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [33/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [33/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [33/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [33/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [33/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [33/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [33/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [33/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [33/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [33/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [33/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [34/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [34/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [34/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [34/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [34/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [34/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [34/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [34/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [34/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [34/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [34/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [34/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [34/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [34/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [34/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [34/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [34/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [34/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [34/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [34/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [34/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [34/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [34/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [34/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [34/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [34/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [34/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [34/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [34/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [34/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [34/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [34/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [34/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [34/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [34/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [34/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [34/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [34/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [34/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [34/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [34/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [34/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [34/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [34/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [34/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [34/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [34/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [34/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [34/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [34/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [34/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [34/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [34/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [34/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [34/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [34/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [34/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [34/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [34/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [34/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [34/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [34/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [34/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [34/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [34/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [34/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [34/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [34/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [34/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [34/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [34/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [34/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [34/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [34/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [34/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [34/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [34/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [34/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [34/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [34/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [34/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [34/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [34/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [34/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [34/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [34/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [34/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [34/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [34/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [34/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [34/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [34/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [34/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [34/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [34/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [34/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [34/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [34/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [34/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [34/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [34/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [34/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [34/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [34/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [34/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [34/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [34/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [34/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [34/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [34/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [34/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [34/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [34/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [34/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [34/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [34/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [34/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [34/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [34/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [34/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [34/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [34/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [34/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [34/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [34/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [34/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [34/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [34/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [34/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [34/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [34/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [34/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [34/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [34/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [34/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [34/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [34/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [34/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [34/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [34/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [34/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [34/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [34/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [34/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [34/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [34/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [34/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [34/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [34/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [34/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [34/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [34/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [34/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [34/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [34/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [34/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [34/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [34/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [34/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [34/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [34/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [34/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [34/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [34/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [34/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [34/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [34/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [34/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [34/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [34/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [34/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [34/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [34/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [34/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [34/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [34/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [34/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [34/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [34/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [34/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [34/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [34/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [34/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [34/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [34/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [34/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [34/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [34/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [34/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [34/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [34/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [34/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [34/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [34/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [34/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [34/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [34/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [34/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [34/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [34/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [34/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [34/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [34/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [34/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [34/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [34/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [34/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [34/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [34/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [34/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [34/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [34/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [34/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [34/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [34/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [34/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [34/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [34/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [34/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [34/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [34/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [34/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [34/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [35/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [35/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [35/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [35/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [35/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [35/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [35/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [35/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [35/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [35/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [35/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [35/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [35/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [35/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [35/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [35/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [35/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [35/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [35/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [35/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [35/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [35/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [35/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [35/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [35/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [35/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [35/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [35/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [35/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [35/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [35/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [35/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [35/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [35/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [35/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [35/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [35/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [35/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [35/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [35/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [35/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [35/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [35/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [35/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [35/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [35/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [35/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [35/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [35/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [35/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [35/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [35/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [35/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [35/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [35/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [35/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [35/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [35/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [35/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [35/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [35/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [35/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [35/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [35/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [35/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [35/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [35/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [35/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [35/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [35/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [35/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [35/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [35/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [35/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [35/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [35/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [35/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [35/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [35/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [35/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [35/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [35/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [35/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [35/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [35/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [35/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [35/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [35/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [35/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [35/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [35/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [35/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [35/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [35/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [35/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [35/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [35/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [35/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [35/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [35/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [35/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [35/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [35/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [35/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [35/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [35/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [35/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [35/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [35/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [35/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [35/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [35/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [35/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [35/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [35/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [35/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [35/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [35/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [35/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [35/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [35/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [35/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [35/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [35/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [35/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [35/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [35/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [35/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [35/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [35/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [35/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [35/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [35/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [35/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [35/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [35/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [35/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [35/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [35/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [35/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [35/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [35/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [35/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [35/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [35/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [35/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [35/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [35/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [35/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [35/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [35/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [35/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [35/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [35/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [35/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [35/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [35/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [35/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [35/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [35/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [35/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [35/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [35/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [35/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [35/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [35/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [35/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [35/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [35/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [35/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [35/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [35/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [35/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [35/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [35/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [35/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [35/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [35/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [35/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [35/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [35/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [35/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [35/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [35/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [35/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [35/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [35/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [35/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [35/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [35/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [35/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [35/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [35/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [35/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [35/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [35/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [35/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [35/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [35/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [35/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [35/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [35/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [35/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [35/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [35/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [35/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [35/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [35/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [35/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [35/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [35/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [35/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [35/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [35/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [35/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [35/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [35/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [35/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [35/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [35/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [35/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [35/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [35/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [36/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [36/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [36/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [36/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [36/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [36/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [36/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [36/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [36/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [36/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [36/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [36/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [36/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [36/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [36/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [36/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [36/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [36/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [36/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [36/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [36/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [36/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [36/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [36/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [36/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [36/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [36/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [36/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [36/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [36/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [36/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [36/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [36/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [36/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [36/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [36/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [36/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [36/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [36/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [36/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [36/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [36/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [36/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [36/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [36/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [36/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [36/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [36/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [36/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [36/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [36/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [36/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [36/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [36/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [36/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [36/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [36/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [36/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [36/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [36/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [36/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [36/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [36/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [36/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [36/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [36/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [36/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [36/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [36/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [36/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [36/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [36/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [36/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [36/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [36/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [36/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [36/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [36/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [36/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [36/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [36/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [36/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [36/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [36/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [36/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [36/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [36/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [36/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [36/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [36/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [36/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [36/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [36/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [36/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [36/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [36/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [36/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [36/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [36/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [36/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [36/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [36/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [36/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [36/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [36/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [36/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [36/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [36/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [36/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [36/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [36/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [36/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [36/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [36/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [36/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [36/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [36/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [36/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [36/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [36/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [36/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [36/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [36/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [36/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [36/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [36/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [36/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [36/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [36/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [36/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [36/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [36/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [36/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [36/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [36/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [36/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [36/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [36/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [36/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [36/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [36/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [36/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [36/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [36/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [36/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [36/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [36/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [36/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [36/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [36/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [36/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [36/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [36/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [36/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [36/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [36/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [36/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [36/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [36/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [36/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [36/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [36/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [36/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [36/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [36/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [36/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [36/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [36/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [36/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [36/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [36/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [36/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [36/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [36/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [36/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [36/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [36/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [36/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [36/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [36/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [36/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [36/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [36/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [36/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [36/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [36/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [36/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [36/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [36/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [36/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [36/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [36/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [36/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [36/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [36/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [36/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [36/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [36/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [36/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [36/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [36/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [36/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [36/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [36/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [36/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [36/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [36/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [36/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [36/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [36/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [36/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [36/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [36/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [36/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [36/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [36/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [36/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [36/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [36/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [36/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [36/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [36/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [37/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [37/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [37/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [37/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [37/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [37/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [37/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [37/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [37/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [37/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [37/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [37/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [37/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [37/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [37/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [37/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [37/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [37/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [37/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [37/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [37/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [37/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [37/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [37/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [37/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [37/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [37/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [37/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [37/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [37/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [37/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [37/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [37/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [37/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [37/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [37/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [37/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [37/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [37/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [37/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [37/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [37/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [37/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [37/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [37/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [37/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [37/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [37/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [37/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [37/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [37/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [37/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [37/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [37/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [37/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [37/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [37/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [37/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [37/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [37/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [37/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [37/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [37/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [37/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [37/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [37/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [37/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [37/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [37/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [37/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [37/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [37/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [37/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [37/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [37/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [37/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [37/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [37/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [37/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [37/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [37/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [37/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [37/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [37/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [37/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [37/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [37/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [37/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [37/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [37/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [37/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [37/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [37/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [37/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [37/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [37/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [37/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [37/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [37/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [37/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [37/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [37/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [37/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [37/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [37/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [37/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [37/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [37/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [37/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [37/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [37/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [37/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [37/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [37/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [37/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [37/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [37/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [37/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [37/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [37/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [37/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [37/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [37/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [37/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [37/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [37/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [37/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [37/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [37/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [37/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [37/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [37/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [37/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [37/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [37/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [37/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [37/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [37/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [37/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [37/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [37/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [37/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [37/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [37/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [37/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [37/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [37/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [37/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [37/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [37/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [37/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [37/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [37/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [37/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [37/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [37/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [37/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [37/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [37/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [37/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [37/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [37/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [37/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [37/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [37/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [37/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [37/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [37/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [37/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [37/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [37/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [37/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [37/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [37/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [37/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [37/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [37/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [37/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [37/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [37/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [37/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [37/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [37/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [37/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [37/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [37/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [37/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [37/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [37/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [37/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [37/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [37/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [37/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [37/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [37/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [37/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [37/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [37/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [37/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [37/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [37/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [37/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [37/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [37/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [37/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [37/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [37/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [37/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [37/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [37/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [37/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [37/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [37/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [37/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [37/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [37/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [37/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [37/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [37/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [37/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [37/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [37/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [37/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [38/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [38/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [38/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [38/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [38/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [38/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [38/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [38/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [38/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [38/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [38/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [38/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [38/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [38/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [38/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [38/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [38/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [38/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [38/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [38/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [38/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [38/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [38/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [38/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [38/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [38/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [38/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [38/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [38/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [38/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [38/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [38/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [38/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [38/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [38/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [38/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [38/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [38/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [38/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [38/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [38/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [38/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [38/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [38/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [38/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [38/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [38/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [38/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [38/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [38/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [38/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [38/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [38/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [38/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [38/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [38/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [38/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [38/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [38/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [38/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [38/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [38/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [38/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [38/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [38/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [38/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [38/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [38/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [38/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [38/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [38/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [38/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [38/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [38/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [38/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [38/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [38/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [38/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [38/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [38/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [38/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [38/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [38/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [38/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [38/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [38/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [38/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [38/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [38/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [38/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [38/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [38/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [38/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [38/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [38/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [38/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [38/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [38/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [38/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [38/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [38/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [38/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [38/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [38/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [38/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [38/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [38/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [38/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [38/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [38/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [38/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [38/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [38/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [38/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [38/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [38/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [38/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [38/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [38/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [38/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [38/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [38/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [38/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [38/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [38/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [38/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [38/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [38/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [38/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [38/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [38/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [38/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [38/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [38/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [38/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [38/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [38/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [38/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [38/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [38/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [38/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [38/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [38/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [38/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [38/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [38/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [38/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [38/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [38/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [38/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [38/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [38/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [38/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [38/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [38/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [38/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [38/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [38/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [38/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [38/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [38/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [38/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [38/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [38/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [38/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [38/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [38/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [38/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [38/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [38/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [38/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [38/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [38/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [38/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [38/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [38/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [38/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [38/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [38/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [38/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [38/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [38/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [38/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [38/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [38/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [38/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [38/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [38/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [38/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [38/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [38/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [38/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [38/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [38/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [38/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [38/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [38/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [38/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [38/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [38/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [38/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [38/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [38/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [38/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [38/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [38/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [38/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [38/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [38/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [38/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [38/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [38/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [38/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [38/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [38/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [38/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [38/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [38/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [38/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [38/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [38/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [38/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [38/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [39/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [39/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [39/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [39/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [39/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [39/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [39/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [39/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [39/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [39/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [39/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [39/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [39/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [39/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [39/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [39/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [39/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [39/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [39/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [39/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [39/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [39/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [39/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [39/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [39/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [39/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [39/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [39/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [39/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [39/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [39/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [39/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [39/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [39/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [39/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [39/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [39/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [39/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [39/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [39/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [39/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [39/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [39/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [39/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [39/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [39/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [39/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [39/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [39/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [39/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [39/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [39/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [39/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [39/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [39/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [39/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [39/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [39/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [39/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [39/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [39/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [39/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [39/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [39/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [39/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [39/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [39/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [39/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [39/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [39/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [39/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [39/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [39/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [39/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [39/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [39/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [39/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [39/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [39/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [39/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [39/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [39/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [39/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [39/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [39/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [39/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [39/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [39/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [39/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [39/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [39/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [39/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [39/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [39/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [39/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [39/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [39/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [39/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [39/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [39/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [39/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [39/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [39/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [39/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [39/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [39/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [39/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [39/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [39/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [39/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [39/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [39/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [39/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [39/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [39/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [39/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [39/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [39/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [39/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [39/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [39/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [39/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [39/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [39/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [39/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [39/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [39/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [39/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [39/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [39/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [39/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [39/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [39/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [39/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [39/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [39/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [39/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [39/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [39/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [39/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [39/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [39/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [39/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [39/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [39/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [39/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [39/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [39/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [39/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [39/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [39/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [39/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [39/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [39/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [39/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [39/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [39/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [39/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [39/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [39/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [39/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [39/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [39/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [39/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [39/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [39/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [39/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [39/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [39/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [39/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [39/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [39/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [39/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [39/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [39/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [39/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [39/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [39/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [39/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [39/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [39/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [39/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [39/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [39/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [39/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [39/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [39/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [39/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [39/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [39/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [39/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [39/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [39/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [39/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [39/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [39/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [39/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [39/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [39/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [39/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [39/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [39/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [39/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [39/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [39/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [39/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [39/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [39/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [39/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [39/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [39/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [39/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [39/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [39/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [39/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [39/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [39/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [39/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [39/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [39/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [39/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [39/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [39/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [40/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [40/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [40/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [40/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [40/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [40/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [40/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [40/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [40/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [40/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [40/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [40/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [40/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [40/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [40/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [40/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [40/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [40/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [40/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [40/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [40/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [40/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [40/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [40/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [40/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [40/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [40/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [40/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [40/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [40/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [40/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [40/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [40/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [40/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [40/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [40/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [40/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [40/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [40/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [40/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [40/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [40/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [40/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [40/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [40/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [40/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [40/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [40/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [40/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [40/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [40/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [40/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [40/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [40/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [40/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [40/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [40/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [40/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [40/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [40/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [40/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [40/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [40/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [40/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [40/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [40/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [40/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [40/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [40/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [40/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [40/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [40/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [40/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [40/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [40/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [40/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [40/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [40/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [40/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [40/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [40/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [40/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [40/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [40/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [40/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [40/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [40/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [40/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [40/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [40/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [40/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [40/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [40/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [40/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [40/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [40/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [40/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [40/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [40/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [40/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [40/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [40/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [40/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [40/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [40/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [40/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [40/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [40/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [40/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [40/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [40/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [40/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [40/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [40/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [40/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [40/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [40/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [40/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [40/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [40/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [40/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [40/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [40/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [40/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [40/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [40/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [40/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [40/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [40/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [40/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [40/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [40/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [40/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [40/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [40/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [40/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [40/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [40/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [40/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [40/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [40/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [40/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [40/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [40/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [40/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [40/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [40/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [40/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [40/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [40/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [40/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [40/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [40/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [40/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [40/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [40/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [40/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [40/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [40/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [40/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [40/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [40/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [40/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [40/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [40/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [40/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [40/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [40/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [40/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [40/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [40/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [40/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [40/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [40/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [40/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [40/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [40/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [40/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [40/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [40/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [40/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [40/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [40/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [40/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [40/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [40/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [40/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [40/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [40/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [40/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [40/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [40/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [40/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [40/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [40/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [40/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [40/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [40/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [40/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [40/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [40/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [40/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [40/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [40/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [40/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [40/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [40/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [40/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [40/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [40/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [40/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [40/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [40/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [40/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [40/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [40/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [40/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [40/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [40/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [40/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [40/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [40/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [40/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [41/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [41/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [41/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [41/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [41/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [41/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [41/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [41/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [41/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [41/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [41/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [41/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [41/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [41/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [41/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [41/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [41/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [41/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [41/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [41/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [41/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [41/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [41/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [41/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [41/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [41/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [41/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [41/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [41/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [41/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [41/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [41/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [41/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [41/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [41/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [41/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [41/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [41/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [41/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [41/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [41/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [41/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [41/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [41/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [41/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [41/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [41/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [41/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [41/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [41/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [41/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [41/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [41/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [41/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [41/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [41/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [41/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [41/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [41/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [41/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [41/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [41/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [41/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [41/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [41/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [41/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [41/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [41/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [41/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [41/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [41/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [41/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [41/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [41/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [41/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [41/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [41/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [41/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [41/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [41/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [41/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [41/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [41/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [41/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [41/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [41/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [41/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [41/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [41/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [41/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [41/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [41/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [41/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [41/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [41/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [41/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [41/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [41/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [41/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [41/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [41/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [41/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [41/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [41/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [41/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [41/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [41/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [41/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [41/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [41/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [41/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [41/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [41/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [41/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [41/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [41/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [41/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [41/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [41/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [41/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [41/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [41/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [41/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [41/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [41/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [41/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [41/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [41/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [41/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [41/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [41/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [41/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [41/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [41/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [41/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [41/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [41/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [41/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [41/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [41/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [41/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [41/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [41/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [41/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [41/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [41/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [41/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [41/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [41/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [41/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [41/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [41/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [41/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [41/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [41/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [41/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [41/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [41/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [41/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [41/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [41/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [41/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [41/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [41/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [41/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [41/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [41/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [41/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [41/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [41/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [41/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [41/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [41/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [41/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [41/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [41/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [41/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [41/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [41/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [41/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [41/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [41/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [41/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [41/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [41/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [41/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [41/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [41/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [41/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [41/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [41/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [41/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [41/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [41/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [41/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [41/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [41/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [41/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [41/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [41/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [41/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [41/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [41/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [41/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [41/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [41/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [41/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [41/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [41/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [41/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [41/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [41/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [41/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [41/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [41/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [41/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [41/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [41/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [41/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [41/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [41/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [41/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [41/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [42/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [42/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [42/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [42/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [42/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [42/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [42/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [42/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [42/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [42/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [42/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [42/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [42/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [42/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [42/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [42/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [42/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [42/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [42/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [42/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [42/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [42/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [42/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [42/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [42/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [42/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [42/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [42/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [42/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [42/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [42/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [42/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [42/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [42/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [42/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [42/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [42/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [42/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [42/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [42/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [42/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [42/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [42/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [42/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [42/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [42/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [42/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [42/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [42/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [42/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [42/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [42/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [42/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [42/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [42/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [42/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [42/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [42/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [42/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [42/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [42/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [42/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [42/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [42/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [42/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [42/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [42/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [42/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [42/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [42/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [42/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [42/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [42/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [42/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [42/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [42/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [42/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [42/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [42/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [42/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [42/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [42/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [42/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [42/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [42/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [42/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [42/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [42/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [42/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [42/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [42/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [42/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [42/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [42/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [42/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [42/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [42/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [42/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [42/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [42/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [42/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [42/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [42/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [42/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [42/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [42/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [42/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [42/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [42/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [42/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [42/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [42/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [42/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [42/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [42/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [42/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [42/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [42/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [42/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [42/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [42/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [42/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [42/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [42/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [42/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [42/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [42/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [42/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [42/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [42/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [42/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [42/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [42/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [42/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [42/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [42/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [42/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [42/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [42/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [42/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [42/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [42/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [42/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [42/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [42/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [42/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [42/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [42/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [42/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [42/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [42/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [42/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [42/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [42/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [42/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [42/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [42/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [42/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [42/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [42/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [42/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [42/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [42/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [42/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [42/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [42/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [42/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [42/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [42/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [42/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [42/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [42/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [42/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [42/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [42/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [42/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [42/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [42/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [42/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [42/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [42/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [42/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [42/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [42/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [42/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [42/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [42/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [42/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [42/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [42/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [42/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [42/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [42/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [42/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [42/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [42/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [42/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [42/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [42/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [42/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [42/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [42/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [42/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [42/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [42/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [42/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [42/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [42/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [42/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [42/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [42/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [42/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [42/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [42/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [42/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [42/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [42/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [42/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [42/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [42/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [42/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [42/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [42/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [43/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [43/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [43/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [43/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [43/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [43/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [43/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [43/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [43/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [43/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [43/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [43/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [43/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [43/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [43/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [43/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [43/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [43/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [43/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [43/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [43/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [43/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [43/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [43/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [43/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [43/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [43/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [43/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [43/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [43/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [43/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [43/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [43/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [43/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [43/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [43/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [43/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [43/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [43/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [43/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [43/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [43/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [43/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [43/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [43/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [43/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [43/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [43/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [43/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [43/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [43/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [43/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [43/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [43/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [43/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [43/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [43/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [43/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [43/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [43/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [43/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [43/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [43/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [43/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [43/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [43/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [43/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [43/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [43/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [43/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [43/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [43/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [43/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [43/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [43/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [43/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [43/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [43/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [43/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [43/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [43/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [43/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [43/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [43/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [43/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [43/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [43/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [43/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [43/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [43/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [43/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [43/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [43/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [43/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [43/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [43/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [43/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [43/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [43/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [43/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [43/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [43/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [43/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [43/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [43/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [43/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [43/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [43/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [43/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [43/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [43/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [43/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [43/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [43/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [43/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [43/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [43/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [43/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [43/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [43/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [43/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [43/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [43/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [43/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [43/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [43/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [43/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [43/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [43/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [43/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [43/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [43/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [43/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [43/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [43/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [43/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [43/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [43/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [43/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [43/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [43/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [43/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [43/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [43/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [43/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [43/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [43/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [43/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [43/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [43/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [43/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [43/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [43/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [43/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [43/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [43/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [43/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [43/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [43/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [43/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [43/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [43/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [43/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [43/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [43/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [43/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [43/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [43/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [43/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [43/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [43/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [43/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [43/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [43/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [43/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [43/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [43/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [43/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [43/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [43/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [43/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [43/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [43/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [43/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [43/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [43/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [43/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [43/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [43/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [43/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [43/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [43/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [43/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [43/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [43/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [43/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [43/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [43/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [43/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [43/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [43/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [43/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [43/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [43/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [43/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [43/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [43/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [43/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [43/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [43/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [43/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [43/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [43/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [43/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [43/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [43/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [43/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [43/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [43/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [43/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [43/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [43/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [43/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [44/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [44/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [44/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [44/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [44/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [44/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [44/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [44/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [44/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [44/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [44/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [44/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [44/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [44/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [44/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [44/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [44/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [44/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [44/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [44/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [44/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [44/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [44/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [44/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [44/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [44/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [44/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [44/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [44/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [44/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [44/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [44/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [44/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [44/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [44/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [44/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [44/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [44/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [44/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [44/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [44/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [44/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [44/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [44/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [44/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [44/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [44/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [44/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [44/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [44/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [44/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [44/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [44/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [44/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [44/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [44/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [44/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [44/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [44/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [44/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [44/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [44/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [44/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [44/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [44/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [44/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [44/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [44/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [44/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [44/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [44/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [44/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [44/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [44/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [44/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [44/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [44/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [44/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [44/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [44/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [44/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [44/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [44/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [44/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [44/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [44/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [44/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [44/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [44/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [44/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [44/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [44/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [44/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [44/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [44/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [44/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [44/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [44/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [44/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [44/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [44/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [44/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [44/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [44/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [44/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [44/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [44/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [44/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [44/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [44/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [44/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [44/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [44/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [44/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [44/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [44/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [44/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [44/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [44/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [44/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [44/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [44/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [44/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [44/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [44/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [44/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [44/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [44/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [44/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [44/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [44/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [44/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [44/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [44/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [44/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [44/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [44/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [44/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [44/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [44/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [44/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [44/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [44/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [44/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [44/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [44/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [44/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [44/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [44/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [44/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [44/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [44/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [44/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [44/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [44/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [44/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [44/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [44/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [44/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [44/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [44/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [44/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [44/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [44/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [44/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [44/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [44/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [44/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [44/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [44/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [44/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [44/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [44/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [44/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [44/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [44/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [44/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [44/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [44/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [44/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [44/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [44/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [44/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [44/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [44/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [44/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [44/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [44/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [44/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [44/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [44/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [44/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [44/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [44/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [44/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [44/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [44/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [44/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [44/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [44/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [44/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [44/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [44/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [44/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [44/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [44/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [44/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [44/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [44/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [44/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [44/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [44/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [44/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [44/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [44/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [44/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [44/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [44/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [44/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [44/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [44/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [44/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [44/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [45/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [45/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [45/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [45/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [45/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [45/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [45/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [45/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [45/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [45/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [45/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [45/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [45/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [45/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [45/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [45/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [45/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [45/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [45/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [45/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [45/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [45/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [45/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [45/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [45/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [45/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [45/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [45/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [45/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [45/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [45/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [45/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [45/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [45/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [45/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [45/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [45/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [45/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [45/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [45/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [45/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [45/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [45/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [45/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [45/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [45/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [45/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [45/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [45/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [45/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [45/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [45/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [45/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [45/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [45/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [45/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [45/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [45/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [45/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [45/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [45/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [45/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [45/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [45/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [45/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [45/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [45/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [45/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [45/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [45/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [45/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [45/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [45/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [45/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [45/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [45/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [45/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [45/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [45/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [45/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [45/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [45/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [45/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [45/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [45/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [45/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [45/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [45/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [45/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [45/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [45/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [45/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [45/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [45/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [45/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [45/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [45/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [45/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [45/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [45/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [45/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [45/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [45/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [45/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [45/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [45/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [45/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [45/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [45/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [45/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [45/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [45/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [45/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [45/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [45/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [45/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [45/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [45/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [45/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [45/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [45/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [45/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [45/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [45/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [45/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [45/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [45/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [45/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [45/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [45/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [45/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [45/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [45/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [45/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [45/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [45/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [45/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [45/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [45/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [45/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [45/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [45/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [45/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [45/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [45/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [45/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [45/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [45/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [45/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [45/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [45/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [45/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [45/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [45/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [45/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [45/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [45/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [45/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [45/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [45/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [45/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [45/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [45/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [45/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [45/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [45/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [45/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [45/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [45/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [45/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [45/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [45/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [45/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [45/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [45/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [45/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [45/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [45/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [45/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [45/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [45/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [45/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [45/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [45/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [45/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [45/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [45/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [45/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [45/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [45/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [45/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [45/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [45/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [45/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [45/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [45/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [45/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [45/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [45/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [45/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [45/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [45/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [45/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [45/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [45/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [45/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [45/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [45/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [45/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [45/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [45/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [45/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [45/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [45/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [45/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [45/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [45/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [45/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [45/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [45/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [45/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [45/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [45/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [46/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [46/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [46/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [46/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [46/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [46/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [46/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [46/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [46/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [46/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [46/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [46/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [46/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [46/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [46/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [46/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [46/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [46/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [46/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [46/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [46/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [46/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [46/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [46/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [46/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [46/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [46/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [46/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [46/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [46/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [46/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [46/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [46/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [46/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [46/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [46/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [46/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [46/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [46/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [46/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [46/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [46/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [46/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [46/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [46/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [46/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [46/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [46/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [46/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [46/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [46/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [46/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [46/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [46/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [46/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [46/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [46/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [46/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [46/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [46/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [46/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [46/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [46/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [46/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [46/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [46/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [46/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [46/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [46/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [46/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [46/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [46/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [46/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [46/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [46/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [46/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [46/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [46/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [46/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [46/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [46/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [46/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [46/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [46/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [46/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [46/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [46/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [46/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [46/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [46/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [46/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [46/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [46/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [46/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [46/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [46/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [46/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [46/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [46/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [46/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [46/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [46/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [46/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [46/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [46/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [46/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [46/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [46/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [46/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [46/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [46/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [46/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [46/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [46/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [46/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [46/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [46/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [46/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [46/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [46/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [46/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [46/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [46/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [46/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [46/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [46/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [46/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [46/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [46/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [46/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [46/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [46/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [46/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [46/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [46/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [46/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [46/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [46/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [46/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [46/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [46/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [46/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [46/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [46/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [46/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [46/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [46/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [46/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [46/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [46/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [46/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [46/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [46/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [46/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [46/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [46/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [46/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [46/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [46/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [46/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [46/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [46/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [46/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [46/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [46/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [46/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [46/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [46/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [46/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [46/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [46/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [46/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [46/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [46/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [46/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [46/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [46/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [46/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [46/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [46/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [46/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [46/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [46/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [46/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [46/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [46/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [46/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [46/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [46/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [46/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [46/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [46/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [46/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [46/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [46/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [46/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [46/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [46/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [46/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [46/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [46/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [46/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [46/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [46/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [46/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [46/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [46/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [46/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [46/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [46/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [46/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [46/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [46/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [46/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [46/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [46/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [46/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [46/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [46/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [46/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [46/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [46/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [46/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [47/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [47/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [47/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [47/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [47/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [47/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [47/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [47/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [47/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [47/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [47/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [47/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [47/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [47/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [47/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [47/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [47/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [47/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [47/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [47/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [47/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [47/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [47/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [47/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [47/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [47/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [47/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [47/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [47/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [47/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [47/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [47/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [47/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [47/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [47/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [47/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [47/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [47/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [47/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [47/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [47/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [47/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [47/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [47/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [47/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [47/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [47/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [47/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [47/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [47/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [47/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [47/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [47/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [47/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [47/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [47/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [47/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [47/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [47/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [47/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [47/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [47/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [47/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [47/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [47/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [47/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [47/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [47/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [47/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [47/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [47/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [47/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [47/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [47/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [47/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [47/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [47/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [47/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [47/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [47/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [47/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [47/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [47/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [47/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [47/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [47/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [47/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [47/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [47/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [47/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [47/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [47/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [47/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [47/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [47/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [47/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [47/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [47/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [47/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [47/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [47/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [47/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [47/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [47/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [47/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [47/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [47/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [47/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [47/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [47/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [47/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [47/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [47/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [47/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [47/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [47/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [47/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [47/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [47/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [47/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [47/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [47/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [47/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [47/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [47/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [47/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [47/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [47/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [47/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [47/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [47/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [47/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [47/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [47/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [47/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [47/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [47/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [47/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [47/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [47/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [47/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [47/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [47/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [47/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [47/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [47/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [47/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [47/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [47/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [47/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [47/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [47/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [47/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [47/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [47/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [47/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [47/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [47/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [47/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [47/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [47/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [47/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [47/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [47/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [47/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [47/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [47/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [47/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [47/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [47/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [47/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [47/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [47/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [47/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [47/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [47/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [47/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [47/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [47/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [47/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [47/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [47/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [47/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [47/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [47/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [47/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [47/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [47/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [47/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [47/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [47/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [47/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [47/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [47/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [47/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [47/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [47/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [47/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [47/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [47/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [47/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [47/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [47/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [47/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [47/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [47/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [47/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [47/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [47/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [47/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [47/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [47/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [47/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [47/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [47/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [47/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [47/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [47/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [47/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [47/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [47/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [47/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [47/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [48/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [48/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [48/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [48/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [48/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [48/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [48/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [48/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [48/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [48/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [48/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [48/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [48/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [48/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [48/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [48/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [48/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [48/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [48/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [48/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [48/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [48/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [48/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [48/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [48/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [48/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [48/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [48/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [48/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [48/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [48/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [48/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [48/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [48/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [48/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [48/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [48/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [48/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [48/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [48/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [48/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [48/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [48/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [48/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [48/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [48/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [48/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [48/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [48/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [48/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [48/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [48/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [48/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [48/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [48/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [48/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [48/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [48/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [48/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [48/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [48/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [48/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [48/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [48/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [48/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [48/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [48/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [48/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [48/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [48/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [48/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [48/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [48/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [48/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [48/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [48/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [48/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [48/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [48/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [48/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [48/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [48/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [48/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [48/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [48/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [48/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [48/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [48/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [48/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [48/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [48/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [48/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [48/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [48/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [48/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [48/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [48/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [48/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [48/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [48/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [48/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [48/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [48/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [48/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [48/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [48/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [48/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [48/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [48/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [48/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [48/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [48/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [48/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [48/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [48/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [48/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [48/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [48/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [48/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [48/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [48/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [48/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [48/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [48/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [48/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [48/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [48/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [48/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [48/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [48/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [48/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [48/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [48/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [48/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [48/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [48/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [48/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [48/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [48/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [48/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [48/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [48/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [48/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [48/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [48/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [48/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [48/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [48/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [48/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [48/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [48/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [48/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [48/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [48/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [48/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [48/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [48/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [48/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [48/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [48/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [48/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [48/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [48/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [48/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [48/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [48/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [48/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [48/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [48/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [48/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [48/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [48/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [48/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [48/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [48/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [48/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [48/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [48/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [48/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [48/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [48/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [48/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [48/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [48/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [48/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [48/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [48/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [48/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [48/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [48/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [48/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [48/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [48/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [48/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [48/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [48/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [48/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [48/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [48/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [48/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [48/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [48/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [48/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [48/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [48/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [48/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [48/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [48/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [48/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [48/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [48/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [48/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [48/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [48/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [48/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [48/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [48/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [48/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [48/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [48/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [48/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [48/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [48/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [49/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [49/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [49/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [49/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [49/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [49/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [49/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [49/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [49/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [49/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [49/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [49/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [49/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [49/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [49/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [49/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [49/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [49/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [49/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [49/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [49/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [49/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [49/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [49/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [49/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [49/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [49/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [49/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [49/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [49/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [49/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [49/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [49/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [49/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [49/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [49/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [49/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [49/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [49/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [49/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [49/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [49/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [49/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [49/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [49/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [49/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [49/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [49/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [49/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [49/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [49/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [49/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [49/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [49/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [49/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [49/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [49/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [49/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [49/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [49/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [49/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [49/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [49/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [49/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [49/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [49/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [49/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [49/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [49/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [49/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [49/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [49/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [49/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [49/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [49/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [49/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [49/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [49/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [49/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [49/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [49/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [49/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [49/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [49/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [49/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [49/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [49/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [49/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [49/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [49/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [49/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [49/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [49/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [49/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [49/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [49/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [49/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [49/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [49/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [49/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [49/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [49/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [49/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [49/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [49/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [49/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [49/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [49/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [49/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [49/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [49/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [49/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [49/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [49/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [49/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [49/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [49/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [49/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [49/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [49/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [49/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [49/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [49/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [49/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [49/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [49/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [49/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [49/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [49/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [49/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [49/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [49/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [49/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [49/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [49/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [49/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [49/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [49/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [49/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [49/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [49/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [49/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [49/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [49/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [49/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [49/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [49/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [49/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [49/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [49/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [49/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [49/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [49/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [49/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [49/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [49/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [49/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [49/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [49/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [49/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [49/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [49/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [49/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [49/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [49/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [49/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [49/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [49/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [49/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [49/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [49/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [49/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [49/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [49/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [49/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [49/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [49/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [49/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [49/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [49/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [49/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [49/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [49/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [49/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [49/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [49/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [49/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [49/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [49/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [49/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [49/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [49/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [49/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [49/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [49/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [49/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [49/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [49/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [49/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [49/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [49/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [49/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [49/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [49/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [49/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [49/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [49/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [49/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [49/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [49/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [49/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [49/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [49/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [49/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [49/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [49/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [49/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [49/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [49/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [49/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [49/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [49/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [49/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [50/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [50/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [50/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [50/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [50/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [50/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [50/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [50/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [50/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [50/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [50/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [50/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [50/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [50/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [50/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [50/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [50/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [50/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [50/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [50/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [50/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [50/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [50/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [50/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [50/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [50/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [50/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [50/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [50/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [50/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [50/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [50/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [50/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [50/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [50/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [50/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [50/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [50/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [50/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [50/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [50/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [50/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [50/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [50/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [50/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [50/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [50/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [50/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [50/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [50/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [50/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [50/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [50/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [50/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [50/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [50/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [50/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [50/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [50/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [50/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [50/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [50/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [50/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [50/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [50/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [50/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [50/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [50/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [50/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [50/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [50/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [50/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [50/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [50/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [50/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [50/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [50/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [50/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [50/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [50/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [50/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [50/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [50/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [50/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [50/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [50/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [50/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [50/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [50/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [50/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [50/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [50/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [50/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [50/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [50/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [50/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [50/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [50/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [50/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [50/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [50/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [50/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [50/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [50/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [50/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [50/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [50/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [50/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [50/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [50/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [50/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [50/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [50/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [50/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [50/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [50/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [50/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [50/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [50/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [50/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [50/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [50/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [50/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [50/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [50/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [50/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [50/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [50/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [50/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [50/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [50/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [50/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [50/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [50/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [50/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [50/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [50/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [50/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [50/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [50/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [50/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [50/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [50/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [50/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [50/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [50/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [50/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [50/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [50/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [50/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [50/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [50/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [50/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [50/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [50/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [50/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [50/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [50/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [50/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [50/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [50/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [50/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [50/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [50/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [50/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [50/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [50/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [50/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [50/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [50/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [50/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [50/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [50/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [50/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [50/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [50/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [50/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [50/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [50/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [50/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [50/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [50/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [50/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [50/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [50/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [50/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [50/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [50/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [50/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [50/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [50/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [50/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [50/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [50/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [50/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [50/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [50/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [50/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [50/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [50/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [50/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [50/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [50/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [50/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [50/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [50/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [50/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [50/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [50/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [50/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [50/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [50/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [50/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [50/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [50/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [50/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [50/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [50/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [50/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [50/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [50/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [50/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [50/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [51/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [51/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [51/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [51/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [51/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [51/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [51/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [51/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [51/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [51/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [51/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [51/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [51/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [51/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [51/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [51/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [51/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [51/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [51/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [51/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [51/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [51/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [51/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [51/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [51/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [51/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [51/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [51/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [51/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [51/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [51/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [51/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [51/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [51/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [51/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [51/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [51/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [51/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [51/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [51/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [51/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [51/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [51/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [51/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [51/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [51/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [51/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [51/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [51/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [51/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [51/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [51/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [51/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [51/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [51/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [51/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [51/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [51/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [51/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [51/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [51/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [51/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [51/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [51/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [51/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [51/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [51/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [51/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [51/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [51/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [51/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [51/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [51/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [51/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [51/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [51/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [51/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [51/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [51/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [51/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [51/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [51/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [51/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [51/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [51/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [51/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [51/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [51/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [51/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [51/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [51/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [51/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [51/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [51/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [51/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [51/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [51/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [51/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [51/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [51/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [51/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [51/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [51/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [51/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [51/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [51/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [51/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [51/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [51/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [51/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [51/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [51/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [51/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [51/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [51/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [51/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [51/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [51/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [51/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [51/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [51/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [51/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [51/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [51/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [51/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [51/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [51/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [51/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [51/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [51/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [51/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [51/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [51/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [51/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [51/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [51/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [51/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [51/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [51/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [51/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [51/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [51/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [51/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [51/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [51/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [51/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [51/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [51/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [51/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [51/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [51/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [51/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [51/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [51/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [51/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [51/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [51/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [51/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [51/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [51/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [51/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [51/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [51/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [51/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [51/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [51/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [51/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [51/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [51/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [51/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [51/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [51/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [51/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [51/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [51/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [51/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [51/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [51/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [51/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [51/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [51/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [51/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [51/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [51/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [51/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [51/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [51/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [51/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [51/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [51/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [51/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [51/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [51/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [51/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [51/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [51/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [51/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [51/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [51/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [51/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [51/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [51/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [51/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [51/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [51/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [51/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [51/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [51/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [51/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [51/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [51/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [51/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [51/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [51/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [51/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [51/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [51/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [51/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [51/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [51/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [51/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [51/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [51/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [52/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [52/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [52/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [52/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [52/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [52/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [52/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [52/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [52/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [52/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [52/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [52/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [52/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [52/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [52/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [52/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [52/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [52/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [52/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [52/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [52/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [52/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [52/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [52/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [52/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [52/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [52/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [52/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [52/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [52/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [52/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [52/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [52/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [52/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [52/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [52/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [52/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [52/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [52/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [52/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [52/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [52/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [52/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [52/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [52/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [52/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [52/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [52/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [52/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [52/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [52/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [52/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [52/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [52/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [52/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [52/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [52/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [52/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [52/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [52/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [52/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [52/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [52/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [52/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [52/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [52/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [52/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [52/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [52/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [52/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [52/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [52/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [52/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [52/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [52/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [52/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [52/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [52/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [52/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [52/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [52/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [52/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [52/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [52/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [52/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [52/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [52/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [52/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [52/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [52/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [52/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [52/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [52/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [52/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [52/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [52/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [52/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [52/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [52/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [52/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [52/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [52/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [52/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [52/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [52/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [52/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [52/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [52/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [52/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [52/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [52/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [52/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [52/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [52/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [52/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [52/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [52/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [52/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [52/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [52/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [52/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [52/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [52/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [52/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [52/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [52/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [52/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [52/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [52/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [52/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [52/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [52/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [52/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [52/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [52/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [52/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [52/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [52/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [52/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [52/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [52/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [52/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [52/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [52/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [52/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [52/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [52/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [52/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [52/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [52/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [52/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [52/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [52/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [52/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [52/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [52/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [52/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [52/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [52/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [52/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [52/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [52/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [52/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [52/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [52/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [52/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [52/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [52/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [52/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [52/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [52/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [52/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [52/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [52/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [52/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [52/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [52/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [52/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [52/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [52/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [52/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [52/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [52/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [52/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [52/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [52/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [52/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [52/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [52/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [52/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [52/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [52/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [52/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [52/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [52/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [52/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [52/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [52/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [52/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [52/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [52/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [52/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [52/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [52/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [52/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [52/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [52/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [52/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [52/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [52/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [52/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [52/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [52/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [52/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [52/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [52/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [52/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [52/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [52/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [52/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [52/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [52/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [52/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [53/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [53/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [53/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [53/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [53/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [53/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [53/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [53/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [53/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [53/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [53/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [53/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [53/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [53/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [53/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [53/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [53/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [53/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [53/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [53/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [53/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [53/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [53/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [53/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [53/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [53/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [53/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [53/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [53/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [53/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [53/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [53/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [53/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [53/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [53/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [53/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [53/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [53/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [53/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [53/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [53/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [53/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [53/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [53/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [53/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [53/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [53/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [53/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [53/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [53/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [53/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [53/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [53/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [53/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [53/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [53/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [53/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [53/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [53/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [53/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [53/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [53/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [53/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [53/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [53/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [53/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [53/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [53/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [53/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [53/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [53/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [53/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [53/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [53/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [53/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [53/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [53/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [53/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [53/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [53/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [53/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [53/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [53/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [53/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [53/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [53/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [53/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [53/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [53/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [53/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [53/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [53/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [53/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [53/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [53/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [53/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [53/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [53/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [53/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [53/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [53/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [53/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [53/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [53/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [53/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [53/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [53/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [53/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [53/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [53/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [53/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [53/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [53/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [53/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [53/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [53/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [53/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [53/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [53/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [53/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [53/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [53/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [53/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [53/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [53/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [53/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [53/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [53/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [53/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [53/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [53/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [53/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [53/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [53/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [53/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [53/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [53/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [53/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [53/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [53/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [53/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [53/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [53/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [53/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [53/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [53/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [53/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [53/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [53/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [53/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [53/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [53/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [53/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [53/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [53/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [53/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [53/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [53/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [53/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [53/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [53/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [53/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [53/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [53/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [53/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [53/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [53/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [53/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [53/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [53/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [53/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [53/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [53/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [53/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [53/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [53/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [53/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [53/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [53/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [53/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [53/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [53/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [53/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [53/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [53/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [53/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [53/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [53/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [53/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [53/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [53/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [53/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [53/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [53/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [53/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [53/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [53/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [53/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [53/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [53/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [53/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [53/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [53/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [53/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [53/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [53/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [53/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [53/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [53/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [53/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [53/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [53/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [53/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [53/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [53/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [53/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [53/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [53/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [53/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [53/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [53/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [53/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [53/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [54/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [54/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [54/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [54/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [54/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [54/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [54/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [54/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [54/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [54/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [54/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [54/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [54/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [54/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [54/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [54/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [54/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [54/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [54/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [54/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [54/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [54/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [54/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [54/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [54/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [54/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [54/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [54/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [54/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [54/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [54/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [54/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [54/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [54/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [54/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [54/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [54/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [54/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [54/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [54/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [54/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [54/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [54/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [54/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [54/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [54/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [54/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [54/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [54/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [54/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [54/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [54/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [54/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [54/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [54/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [54/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [54/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [54/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [54/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [54/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [54/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [54/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [54/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [54/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [54/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [54/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [54/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [54/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [54/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [54/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [54/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [54/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [54/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [54/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [54/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [54/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [54/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [54/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [54/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [54/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [54/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [54/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [54/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [54/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [54/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [54/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [54/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [54/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [54/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [54/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [54/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [54/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [54/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [54/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [54/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [54/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [54/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [54/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [54/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [54/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [54/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [54/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [54/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [54/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [54/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [54/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [54/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [54/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [54/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [54/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [54/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [54/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [54/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [54/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [54/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [54/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [54/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [54/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [54/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [54/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [54/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [54/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [54/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [54/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [54/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [54/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [54/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [54/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [54/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [54/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [54/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [54/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [54/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [54/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [54/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [54/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [54/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [54/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [54/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [54/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [54/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [54/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [54/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [54/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [54/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [54/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [54/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [54/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [54/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [54/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [54/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [54/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [54/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [54/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [54/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [54/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [54/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [54/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [54/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [54/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [54/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [54/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [54/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [54/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [54/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [54/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [54/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [54/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [54/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [54/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [54/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [54/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [54/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [54/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [54/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [54/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [54/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [54/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [54/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [54/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [54/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [54/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [54/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [54/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [54/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [54/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [54/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [54/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [54/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [54/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [54/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [54/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [54/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [54/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [54/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [54/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [54/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [54/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [54/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [54/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [54/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [54/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [54/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [54/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [54/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [54/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [54/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [54/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [54/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [54/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [54/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [54/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [54/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [54/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [54/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [54/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [54/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [54/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [54/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [54/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [54/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [54/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [54/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [55/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [55/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [55/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [55/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [55/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [55/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [55/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [55/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [55/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [55/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [55/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [55/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [55/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [55/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [55/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [55/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [55/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [55/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [55/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [55/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [55/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [55/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [55/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [55/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [55/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [55/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [55/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [55/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [55/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [55/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [55/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [55/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [55/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [55/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [55/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [55/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [55/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [55/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [55/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [55/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [55/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [55/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [55/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [55/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [55/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [55/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [55/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [55/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [55/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [55/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [55/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [55/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [55/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [55/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [55/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [55/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [55/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [55/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [55/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [55/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [55/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [55/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [55/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [55/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [55/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [55/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [55/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [55/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [55/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [55/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [55/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [55/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [55/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [55/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [55/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [55/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [55/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [55/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [55/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [55/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [55/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [55/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [55/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [55/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [55/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [55/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [55/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [55/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [55/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [55/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [55/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [55/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [55/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [55/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [55/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [55/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [55/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [55/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [55/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [55/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [55/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [55/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [55/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [55/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [55/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [55/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [55/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [55/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [55/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [55/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [55/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [55/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [55/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [55/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [55/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [55/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [55/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [55/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [55/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [55/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [55/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [55/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [55/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [55/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [55/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [55/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [55/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [55/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [55/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [55/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [55/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [55/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [55/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [55/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [55/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [55/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [55/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [55/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [55/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [55/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [55/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [55/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [55/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [55/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [55/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [55/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [55/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [55/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [55/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [55/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [55/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [55/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [55/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [55/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [55/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [55/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [55/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [55/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [55/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [55/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [55/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [55/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [55/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [55/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [55/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [55/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [55/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [55/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [55/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [55/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [55/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [55/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [55/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [55/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [55/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [55/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [55/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [55/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [55/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [55/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [55/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [55/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [55/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [55/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [55/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [55/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [55/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [55/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [55/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [55/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [55/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [55/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [55/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [55/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [55/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [55/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [55/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [55/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [55/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [55/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [55/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [55/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [55/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [55/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [55/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [55/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [55/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [55/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [55/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [55/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [55/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [55/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [55/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [55/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [55/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [55/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [55/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [55/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [55/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [55/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [55/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [55/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [55/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [56/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [56/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [56/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [56/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [56/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [56/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [56/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [56/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [56/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [56/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [56/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [56/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [56/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [56/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [56/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [56/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [56/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [56/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [56/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [56/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [56/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [56/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [56/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [56/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [56/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [56/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [56/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [56/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [56/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [56/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [56/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [56/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [56/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [56/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [56/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [56/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [56/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [56/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [56/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [56/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [56/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [56/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [56/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [56/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [56/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [56/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [56/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [56/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [56/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [56/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [56/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [56/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [56/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [56/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [56/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [56/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [56/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [56/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [56/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [56/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [56/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [56/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [56/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [56/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [56/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [56/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [56/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [56/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [56/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [56/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [56/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [56/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [56/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [56/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [56/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [56/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [56/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [56/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [56/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [56/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [56/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [56/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [56/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [56/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [56/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [56/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [56/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [56/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [56/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [56/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [56/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [56/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [56/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [56/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [56/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [56/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [56/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [56/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [56/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [56/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [56/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [56/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [56/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [56/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [56/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [56/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [56/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [56/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [56/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [56/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [56/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [56/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [56/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [56/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [56/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [56/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [56/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [56/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [56/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [56/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [56/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [56/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [56/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [56/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [56/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [56/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [56/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [56/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [56/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [56/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [56/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [56/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [56/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [56/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [56/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [56/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [56/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [56/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [56/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [56/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [56/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [56/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [56/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [56/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [56/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [56/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [56/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [56/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [56/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [56/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [56/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [56/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [56/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [56/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [56/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [56/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [56/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [56/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [56/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [56/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [56/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [56/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [56/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [56/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [56/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [56/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [56/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [56/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [56/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [56/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [56/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [56/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [56/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [56/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [56/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [56/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [56/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [56/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [56/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [56/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [56/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [56/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [56/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [56/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [56/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [56/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [56/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [56/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [56/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [56/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [56/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [56/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [56/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [56/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [56/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [56/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [56/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [56/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [56/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [56/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [56/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [56/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [56/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [56/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [56/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [56/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [56/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [56/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [56/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [56/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [56/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [56/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [56/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [56/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [56/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [56/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [56/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [56/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [56/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [56/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [56/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [56/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [56/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [57/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [57/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [57/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [57/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [57/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [57/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [57/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [57/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [57/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [57/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [57/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [57/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [57/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [57/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [57/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [57/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [57/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [57/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [57/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [57/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [57/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [57/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [57/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [57/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [57/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [57/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [57/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [57/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [57/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [57/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [57/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [57/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [57/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [57/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [57/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [57/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [57/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [57/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [57/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [57/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [57/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [57/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [57/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [57/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [57/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [57/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [57/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [57/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [57/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [57/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [57/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [57/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [57/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [57/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [57/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [57/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [57/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [57/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [57/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [57/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [57/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [57/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [57/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [57/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [57/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [57/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [57/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [57/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [57/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [57/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [57/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [57/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [57/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [57/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [57/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [57/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [57/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [57/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [57/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [57/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [57/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [57/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [57/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [57/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [57/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [57/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [57/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [57/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [57/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [57/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [57/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [57/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [57/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [57/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [57/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [57/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [57/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [57/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [57/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [57/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [57/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [57/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [57/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [57/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [57/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [57/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [57/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [57/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [57/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [57/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [57/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [57/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [57/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [57/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [57/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [57/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [57/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [57/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [57/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [57/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [57/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [57/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [57/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [57/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [57/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [57/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [57/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [57/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [57/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [57/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [57/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [57/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [57/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [57/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [57/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [57/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [57/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [57/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [57/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [57/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [57/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [57/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [57/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [57/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [57/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [57/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [57/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [57/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [57/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [57/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [57/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [57/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [57/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [57/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [57/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [57/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [57/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [57/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [57/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [57/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [57/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [57/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [57/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [57/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [57/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [57/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [57/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [57/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [57/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [57/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [57/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [57/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [57/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [57/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [57/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [57/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [57/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [57/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [57/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [57/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [57/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [57/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [57/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [57/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [57/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [57/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [57/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [57/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [57/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [57/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [57/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [57/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [57/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [57/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [57/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [57/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [57/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [57/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [57/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [57/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [57/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [57/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [57/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [57/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [57/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [57/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [57/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [57/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [57/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [57/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [57/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [57/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [57/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [57/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [57/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [57/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [57/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [57/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [57/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [57/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [57/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [57/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [57/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [58/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [58/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [58/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [58/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [58/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [58/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [58/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [58/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [58/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [58/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [58/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [58/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [58/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [58/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [58/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [58/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [58/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [58/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [58/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [58/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [58/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [58/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [58/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [58/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [58/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [58/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [58/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [58/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [58/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [58/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [58/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [58/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [58/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [58/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [58/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [58/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [58/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [58/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [58/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [58/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [58/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [58/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [58/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [58/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [58/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [58/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [58/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [58/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [58/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [58/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [58/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [58/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [58/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [58/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [58/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [58/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [58/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [58/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [58/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [58/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [58/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [58/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [58/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [58/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [58/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [58/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [58/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [58/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [58/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [58/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [58/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [58/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [58/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [58/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [58/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [58/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [58/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [58/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [58/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [58/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [58/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [58/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [58/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [58/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [58/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [58/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [58/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [58/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [58/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [58/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [58/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [58/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [58/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [58/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [58/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [58/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [58/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [58/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [58/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [58/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [58/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [58/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [58/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [58/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [58/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [58/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [58/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [58/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [58/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [58/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [58/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [58/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [58/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [58/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [58/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [58/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [58/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [58/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [58/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [58/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [58/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [58/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [58/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [58/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [58/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [58/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [58/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [58/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [58/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [58/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [58/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [58/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [58/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [58/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [58/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [58/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [58/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [58/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [58/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [58/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [58/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [58/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [58/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [58/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [58/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [58/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [58/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [58/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [58/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [58/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [58/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [58/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [58/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [58/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [58/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [58/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [58/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [58/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [58/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [58/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [58/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [58/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [58/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [58/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [58/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [58/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [58/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [58/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [58/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [58/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [58/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [58/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [58/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [58/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [58/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [58/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [58/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [58/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [58/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [58/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [58/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [58/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [58/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [58/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [58/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [58/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [58/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [58/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [58/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [58/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [58/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [58/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [58/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [58/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [58/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [58/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [58/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [58/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [58/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [58/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [58/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [58/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [58/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [58/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [58/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [58/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [58/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [58/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [58/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [58/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [58/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [58/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [58/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [58/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [58/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [58/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [58/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [58/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [58/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [58/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [58/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [58/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [58/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [59/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [59/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [59/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [59/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [59/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [59/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [59/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [59/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [59/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [59/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [59/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [59/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [59/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [59/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [59/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [59/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [59/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [59/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [59/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [59/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [59/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [59/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [59/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [59/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [59/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [59/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [59/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [59/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [59/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [59/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [59/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [59/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [59/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [59/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [59/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [59/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [59/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [59/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [59/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [59/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [59/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [59/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [59/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [59/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [59/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [59/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [59/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [59/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [59/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [59/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [59/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [59/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [59/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [59/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [59/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [59/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [59/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [59/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [59/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [59/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [59/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [59/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [59/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [59/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [59/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [59/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [59/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [59/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [59/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [59/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [59/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [59/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [59/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [59/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [59/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [59/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [59/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [59/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [59/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [59/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [59/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [59/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [59/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [59/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [59/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [59/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [59/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [59/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [59/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [59/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [59/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [59/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [59/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [59/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [59/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [59/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [59/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [59/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [59/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [59/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [59/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [59/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [59/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [59/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [59/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [59/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [59/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [59/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [59/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [59/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [59/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [59/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [59/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [59/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [59/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [59/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [59/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [59/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [59/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [59/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [59/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [59/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [59/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [59/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [59/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [59/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [59/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [59/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [59/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [59/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [59/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [59/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [59/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [59/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [59/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [59/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [59/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [59/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [59/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [59/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [59/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [59/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [59/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [59/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [59/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [59/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [59/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [59/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [59/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [59/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [59/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [59/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [59/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [59/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [59/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [59/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [59/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [59/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [59/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [59/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [59/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [59/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [59/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [59/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [59/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [59/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [59/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [59/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [59/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [59/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [59/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [59/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [59/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [59/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [59/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [59/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [59/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [59/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [59/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [59/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [59/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [59/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [59/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [59/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [59/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [59/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [59/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [59/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [59/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [59/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [59/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [59/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [59/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [59/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [59/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [59/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [59/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [59/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [59/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [59/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [59/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [59/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [59/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [59/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [59/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [59/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [59/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [59/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [59/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [59/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [59/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [59/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [59/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [59/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [59/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [59/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [59/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [59/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [59/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [59/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [59/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [59/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [59/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [60/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [60/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [60/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [60/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [60/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [60/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [60/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [60/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [60/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [60/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [60/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [60/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [60/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [60/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [60/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [60/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [60/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [60/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [60/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [60/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [60/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [60/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [60/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [60/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [60/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [60/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [60/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [60/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [60/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [60/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [60/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [60/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [60/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [60/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [60/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [60/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [60/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [60/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [60/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [60/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [60/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [60/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [60/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [60/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [60/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [60/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [60/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [60/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [60/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [60/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [60/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [60/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [60/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [60/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [60/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [60/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [60/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [60/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [60/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [60/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [60/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [60/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [60/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [60/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [60/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [60/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [60/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [60/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [60/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [60/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [60/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [60/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [60/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [60/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [60/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [60/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [60/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [60/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [60/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [60/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [60/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [60/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [60/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [60/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [60/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [60/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [60/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [60/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [60/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [60/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [60/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [60/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [60/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [60/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [60/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [60/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [60/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [60/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [60/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [60/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [60/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [60/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [60/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [60/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [60/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [60/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [60/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [60/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [60/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [60/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [60/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [60/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [60/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [60/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [60/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [60/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [60/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [60/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [60/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [60/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [60/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [60/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [60/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [60/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [60/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [60/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [60/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [60/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [60/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [60/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [60/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [60/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [60/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [60/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [60/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [60/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [60/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [60/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [60/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [60/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [60/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [60/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [60/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [60/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [60/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [60/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [60/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [60/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [60/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [60/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [60/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [60/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [60/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [60/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [60/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [60/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [60/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [60/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [60/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [60/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [60/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [60/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [60/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [60/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [60/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [60/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [60/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [60/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [60/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [60/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [60/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [60/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [60/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [60/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [60/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [60/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [60/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [60/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [60/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [60/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [60/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [60/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [60/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [60/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [60/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [60/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [60/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [60/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [60/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [60/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [60/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [60/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [60/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [60/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [60/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [60/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [60/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [60/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [60/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [60/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [60/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [60/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [60/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [60/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [60/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [60/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [60/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [60/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [60/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [60/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [60/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [60/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [60/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [60/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [60/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [60/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [60/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [60/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [60/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [60/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [60/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [60/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [60/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [61/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [61/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [61/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [61/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [61/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [61/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [61/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [61/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [61/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [61/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [61/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [61/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [61/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [61/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [61/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [61/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [61/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [61/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [61/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [61/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [61/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [61/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [61/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [61/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [61/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [61/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [61/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [61/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [61/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [61/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [61/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [61/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [61/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [61/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [61/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [61/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [61/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [61/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [61/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [61/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [61/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [61/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [61/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [61/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [61/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [61/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [61/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [61/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [61/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [61/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [61/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [61/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [61/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [61/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [61/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [61/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [61/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [61/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [61/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [61/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [61/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [61/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [61/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [61/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [61/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [61/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [61/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [61/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [61/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [61/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [61/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [61/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [61/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [61/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [61/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [61/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [61/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [61/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [61/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [61/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [61/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [61/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [61/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [61/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [61/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [61/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [61/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [61/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [61/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [61/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [61/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [61/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [61/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [61/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [61/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [61/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [61/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [61/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [61/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [61/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [61/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [61/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [61/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [61/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [61/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [61/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [61/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [61/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [61/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [61/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [61/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [61/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [61/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [61/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [61/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [61/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [61/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [61/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [61/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [61/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [61/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [61/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [61/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [61/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [61/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [61/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [61/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [61/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [61/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [61/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [61/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [61/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [61/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [61/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [61/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [61/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [61/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [61/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [61/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [61/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [61/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [61/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [61/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [61/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [61/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [61/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [61/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [61/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [61/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [61/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [61/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [61/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [61/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [61/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [61/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [61/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [61/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [61/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [61/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [61/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [61/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [61/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [61/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [61/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [61/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [61/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [61/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [61/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [61/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [61/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [61/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [61/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [61/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [61/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [61/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [61/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [61/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [61/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [61/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [61/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [61/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [61/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [61/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [61/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [61/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [61/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [61/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [61/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [61/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [61/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [61/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [61/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [61/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [61/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [61/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [61/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [61/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [61/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [61/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [61/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [61/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [61/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [61/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [61/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [61/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [61/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [61/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [61/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [61/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [61/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [61/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [61/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [61/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [61/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [61/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [61/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [61/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [61/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [61/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [61/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [61/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [61/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [61/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [62/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [62/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [62/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [62/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [62/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [62/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [62/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [62/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [62/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [62/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [62/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [62/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [62/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [62/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [62/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [62/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [62/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [62/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [62/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [62/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [62/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [62/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [62/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [62/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [62/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [62/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [62/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [62/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [62/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [62/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [62/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [62/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [62/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [62/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [62/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [62/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [62/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [62/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [62/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [62/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [62/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [62/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [62/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [62/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [62/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [62/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [62/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [62/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [62/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [62/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [62/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [62/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [62/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [62/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [62/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [62/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [62/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [62/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [62/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [62/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [62/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [62/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [62/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [62/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [62/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [62/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [62/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [62/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [62/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [62/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [62/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [62/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [62/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [62/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [62/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [62/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [62/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [62/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [62/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [62/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [62/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [62/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [62/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [62/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [62/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [62/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [62/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [62/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [62/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [62/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [62/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [62/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [62/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [62/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [62/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [62/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [62/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [62/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [62/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [62/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [62/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [62/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [62/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [62/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [62/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [62/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [62/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [62/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [62/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [62/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [62/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [62/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [62/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [62/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [62/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [62/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [62/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [62/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [62/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [62/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [62/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [62/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [62/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [62/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [62/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [62/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [62/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [62/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [62/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [62/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [62/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [62/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [62/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [62/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [62/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [62/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [62/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [62/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [62/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [62/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [62/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [62/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [62/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [62/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [62/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [62/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [62/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [62/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [62/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [62/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [62/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [62/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [62/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [62/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [62/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [62/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [62/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [62/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [62/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [62/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [62/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [62/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [62/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [62/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [62/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [62/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [62/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [62/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [62/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [62/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [62/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [62/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [62/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [62/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [62/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [62/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [62/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [62/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [62/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [62/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [62/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [62/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [62/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [62/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [62/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [62/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [62/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [62/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [62/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [62/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [62/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [62/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [62/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [62/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [62/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [62/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [62/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [62/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [62/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [62/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [62/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [62/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [62/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [62/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [62/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [62/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [62/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [62/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [62/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [62/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [62/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [62/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [62/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [62/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [62/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [62/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [62/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [62/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [62/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [62/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [62/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [62/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [62/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [63/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [63/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [63/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [63/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [63/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [63/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [63/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [63/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [63/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [63/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [63/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [63/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [63/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [63/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [63/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [63/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [63/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [63/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [63/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [63/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [63/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [63/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [63/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [63/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [63/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [63/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [63/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [63/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [63/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [63/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [63/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [63/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [63/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [63/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [63/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [63/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [63/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [63/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [63/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [63/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [63/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [63/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [63/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [63/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [63/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [63/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [63/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [63/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [63/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [63/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [63/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [63/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [63/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [63/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [63/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [63/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [63/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [63/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [63/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [63/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [63/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [63/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [63/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [63/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [63/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [63/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [63/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [63/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [63/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [63/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [63/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [63/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [63/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [63/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [63/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [63/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [63/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [63/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [63/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [63/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [63/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [63/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [63/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [63/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [63/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [63/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [63/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [63/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [63/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [63/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [63/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [63/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [63/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [63/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [63/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [63/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [63/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [63/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [63/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [63/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [63/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [63/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [63/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [63/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [63/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [63/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [63/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [63/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [63/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [63/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [63/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [63/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [63/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [63/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [63/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [63/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [63/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [63/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [63/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [63/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [63/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [63/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [63/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [63/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [63/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [63/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [63/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [63/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [63/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [63/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [63/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [63/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [63/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [63/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [63/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [63/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [63/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [63/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [63/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [63/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [63/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [63/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [63/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [63/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [63/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [63/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [63/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [63/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [63/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [63/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [63/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [63/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [63/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [63/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [63/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [63/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [63/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [63/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [63/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [63/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [63/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [63/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [63/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [63/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [63/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [63/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [63/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [63/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [63/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [63/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [63/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [63/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [63/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [63/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [63/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [63/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [63/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [63/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [63/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [63/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [63/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [63/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [63/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [63/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [63/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [63/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [63/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [63/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [63/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [63/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [63/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [63/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [63/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [63/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [63/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [63/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [63/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [63/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [63/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [63/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [63/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [63/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [63/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [63/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [63/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [63/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [63/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [63/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [63/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [63/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [63/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [63/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [63/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [63/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [63/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [63/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [63/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [63/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [63/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [63/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [63/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [63/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [63/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [64/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [64/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [64/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [64/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [64/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [64/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [64/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [64/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [64/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [64/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [64/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [64/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [64/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [64/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [64/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [64/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [64/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [64/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [64/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [64/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [64/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [64/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [64/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [64/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [64/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [64/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [64/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [64/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [64/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [64/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [64/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [64/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [64/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [64/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [64/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [64/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [64/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [64/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [64/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [64/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [64/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [64/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [64/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [64/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [64/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [64/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [64/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [64/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [64/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [64/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [64/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [64/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [64/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [64/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [64/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [64/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [64/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [64/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [64/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [64/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [64/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [64/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [64/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [64/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [64/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [64/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [64/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [64/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [64/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [64/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [64/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [64/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [64/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [64/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [64/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [64/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [64/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [64/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [64/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [64/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [64/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [64/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [64/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [64/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [64/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [64/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [64/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [64/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [64/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [64/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [64/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [64/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [64/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [64/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [64/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [64/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [64/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [64/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [64/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [64/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [64/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [64/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [64/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [64/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [64/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [64/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [64/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [64/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [64/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [64/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [64/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [64/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [64/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [64/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [64/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [64/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [64/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [64/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [64/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [64/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [64/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [64/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [64/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [64/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [64/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [64/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [64/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [64/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [64/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [64/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [64/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [64/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [64/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [64/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [64/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [64/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [64/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [64/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [64/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [64/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [64/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [64/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [64/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [64/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [64/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [64/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [64/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [64/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [64/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [64/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [64/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [64/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [64/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [64/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [64/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [64/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [64/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [64/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [64/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [64/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [64/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [64/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [64/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [64/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [64/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [64/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [64/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [64/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [64/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [64/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [64/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [64/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [64/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [64/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [64/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [64/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [64/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [64/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [64/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [64/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [64/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [64/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [64/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [64/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [64/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [64/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [64/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [64/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [64/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [64/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [64/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [64/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [64/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [64/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [64/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [64/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [64/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [64/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [64/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [64/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [64/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [64/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [64/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [64/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [64/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [64/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [64/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [64/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [64/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [64/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [64/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [64/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [64/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [64/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [64/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [64/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [64/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [64/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [64/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [64/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [64/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [64/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [64/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [65/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [65/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [65/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [65/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [65/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [65/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [65/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [65/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [65/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [65/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [65/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [65/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [65/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [65/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [65/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [65/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [65/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [65/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [65/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [65/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [65/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [65/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [65/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [65/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [65/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [65/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [65/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [65/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [65/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [65/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [65/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [65/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [65/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [65/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [65/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [65/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [65/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [65/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [65/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [65/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [65/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [65/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [65/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [65/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [65/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [65/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [65/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [65/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [65/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [65/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [65/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [65/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [65/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [65/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [65/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [65/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [65/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [65/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [65/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [65/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [65/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [65/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [65/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [65/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [65/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [65/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [65/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [65/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [65/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [65/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [65/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [65/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [65/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [65/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [65/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [65/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [65/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [65/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [65/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [65/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [65/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [65/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [65/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [65/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [65/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [65/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [65/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [65/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [65/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [65/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [65/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [65/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [65/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [65/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [65/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [65/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [65/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [65/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [65/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [65/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [65/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [65/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [65/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [65/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [65/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [65/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [65/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [65/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [65/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [65/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [65/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [65/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [65/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [65/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [65/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [65/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [65/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [65/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [65/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [65/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [65/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [65/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [65/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [65/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [65/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [65/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [65/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [65/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [65/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [65/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [65/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [65/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [65/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [65/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [65/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [65/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [65/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [65/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [65/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [65/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [65/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [65/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [65/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [65/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [65/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [65/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [65/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [65/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [65/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [65/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [65/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [65/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [65/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [65/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [65/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [65/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [65/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [65/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [65/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [65/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [65/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [65/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [65/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [65/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [65/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [65/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [65/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [65/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [65/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [65/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [65/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [65/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [65/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [65/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [65/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [65/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [65/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [65/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [65/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [65/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [65/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [65/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [65/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [65/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [65/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [65/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [65/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [65/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [65/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [65/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [65/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [65/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [65/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [65/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [65/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [65/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [65/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [65/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [65/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [65/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [65/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [65/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [65/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [65/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [65/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [65/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [65/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [65/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [65/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [65/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [65/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [65/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [65/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [65/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [65/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [65/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [65/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [65/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [65/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [65/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [65/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [65/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [65/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [66/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [66/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [66/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [66/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [66/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [66/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [66/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [66/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [66/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [66/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [66/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [66/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [66/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [66/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [66/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [66/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [66/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [66/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [66/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [66/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [66/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [66/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [66/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [66/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [66/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [66/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [66/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [66/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [66/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [66/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [66/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [66/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [66/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [66/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [66/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [66/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [66/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [66/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [66/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [66/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [66/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [66/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [66/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [66/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [66/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [66/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [66/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [66/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [66/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [66/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [66/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [66/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [66/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [66/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [66/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [66/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [66/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [66/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [66/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [66/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [66/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [66/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [66/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [66/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [66/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [66/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [66/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [66/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [66/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [66/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [66/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [66/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [66/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [66/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [66/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [66/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [66/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [66/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [66/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [66/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [66/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [66/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [66/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [66/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [66/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [66/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [66/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [66/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [66/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [66/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [66/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [66/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [66/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [66/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [66/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [66/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [66/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [66/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [66/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [66/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [66/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [66/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [66/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [66/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [66/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [66/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [66/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [66/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [66/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [66/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [66/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [66/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [66/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [66/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [66/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [66/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [66/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [66/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [66/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [66/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [66/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [66/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [66/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [66/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [66/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [66/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [66/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [66/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [66/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [66/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [66/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [66/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [66/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [66/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [66/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [66/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [66/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [66/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [66/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [66/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [66/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [66/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [66/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [66/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [66/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [66/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [66/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [66/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [66/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [66/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [66/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [66/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [66/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [66/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [66/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [66/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [66/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [66/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [66/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [66/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [66/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [66/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [66/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [66/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [66/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [66/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [66/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [66/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [66/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [66/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [66/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [66/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [66/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [66/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [66/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [66/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [66/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [66/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [66/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [66/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [66/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [66/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [66/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [66/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [66/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [66/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [66/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [66/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [66/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [66/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [66/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [66/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [66/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [66/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [66/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [66/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [66/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [66/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [66/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [66/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [66/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [66/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [66/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [66/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [66/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [66/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [66/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [66/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [66/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [66/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [66/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [66/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [66/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [66/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [66/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [66/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [66/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [66/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [66/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [66/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [66/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [66/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [66/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [67/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [67/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [67/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [67/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [67/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [67/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [67/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [67/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [67/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [67/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [67/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [67/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [67/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [67/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [67/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [67/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [67/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [67/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [67/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [67/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [67/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [67/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [67/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [67/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [67/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [67/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [67/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [67/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [67/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [67/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [67/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [67/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [67/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [67/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [67/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [67/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [67/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [67/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [67/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [67/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [67/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [67/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [67/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [67/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [67/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [67/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [67/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [67/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [67/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [67/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [67/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [67/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [67/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [67/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [67/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [67/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [67/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [67/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [67/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [67/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [67/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [67/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [67/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [67/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [67/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [67/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [67/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [67/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [67/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [67/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [67/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [67/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [67/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [67/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [67/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [67/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [67/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [67/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [67/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [67/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [67/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [67/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [67/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [67/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [67/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [67/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [67/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [67/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [67/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [67/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [67/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [67/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [67/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [67/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [67/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [67/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [67/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [67/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [67/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [67/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [67/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [67/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [67/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [67/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [67/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [67/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [67/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [67/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [67/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [67/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [67/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [67/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [67/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [67/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [67/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [67/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [67/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [67/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [67/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [67/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [67/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [67/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [67/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [67/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [67/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [67/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [67/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [67/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [67/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [67/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [67/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [67/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [67/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [67/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [67/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [67/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [67/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [67/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [67/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [67/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [67/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [67/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [67/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [67/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [67/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [67/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [67/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [67/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [67/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [67/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [67/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [67/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [67/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [67/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [67/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [67/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [67/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [67/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [67/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [67/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [67/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [67/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [67/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [67/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [67/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [67/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [67/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [67/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [67/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [67/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [67/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [67/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [67/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [67/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [67/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [67/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [67/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [67/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [67/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [67/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [67/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [67/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [67/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [67/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [67/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [67/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [67/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [67/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [67/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [67/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [67/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [67/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [67/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [67/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [67/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [67/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [67/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [67/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [67/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [67/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [67/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [67/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [67/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [67/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [67/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [67/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [67/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [67/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [67/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [67/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [67/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [67/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [67/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [67/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [67/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [67/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [67/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [67/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [67/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [67/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [67/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [67/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [67/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [68/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [68/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [68/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [68/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [68/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [68/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [68/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [68/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [68/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [68/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [68/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [68/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [68/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [68/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [68/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [68/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [68/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [68/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [68/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [68/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [68/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [68/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [68/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [68/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [68/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [68/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [68/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [68/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [68/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [68/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [68/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [68/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [68/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [68/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [68/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [68/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [68/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [68/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [68/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [68/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [68/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [68/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [68/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [68/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [68/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [68/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [68/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [68/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [68/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [68/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [68/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [68/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [68/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [68/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [68/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [68/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [68/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [68/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [68/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [68/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [68/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [68/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [68/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [68/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [68/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [68/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [68/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [68/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [68/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [68/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [68/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [68/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [68/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [68/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [68/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [68/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [68/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [68/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [68/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [68/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [68/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [68/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [68/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [68/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [68/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [68/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [68/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [68/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [68/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [68/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [68/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [68/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [68/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [68/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [68/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [68/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [68/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [68/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [68/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [68/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [68/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [68/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [68/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [68/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [68/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [68/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [68/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [68/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [68/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [68/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [68/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [68/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [68/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [68/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [68/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [68/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [68/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [68/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [68/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [68/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [68/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [68/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [68/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [68/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [68/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [68/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [68/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [68/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [68/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [68/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [68/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [68/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [68/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [68/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [68/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [68/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [68/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [68/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [68/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [68/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [68/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [68/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [68/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [68/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [68/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [68/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [68/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [68/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [68/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [68/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [68/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [68/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [68/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [68/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [68/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [68/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [68/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [68/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [68/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [68/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [68/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [68/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [68/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [68/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [68/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [68/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [68/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [68/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [68/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [68/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [68/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [68/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [68/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [68/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [68/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [68/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [68/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [68/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [68/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [68/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [68/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [68/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [68/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [68/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [68/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [68/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [68/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [68/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [68/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [68/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [68/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [68/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [68/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [68/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [68/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [68/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [68/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [68/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [68/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [68/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [68/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [68/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [68/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [68/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [68/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [68/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [68/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [68/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [68/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [68/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [68/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [68/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [68/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [68/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [68/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [68/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [68/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [68/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [68/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [68/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [68/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [68/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [68/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [69/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [69/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [69/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [69/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [69/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [69/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [69/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [69/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [69/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [69/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [69/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [69/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [69/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [69/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [69/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [69/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [69/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [69/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [69/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [69/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [69/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [69/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [69/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [69/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [69/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [69/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [69/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [69/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [69/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [69/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [69/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [69/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [69/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [69/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [69/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [69/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [69/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [69/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [69/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [69/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [69/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [69/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [69/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [69/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [69/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [69/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [69/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [69/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [69/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [69/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [69/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [69/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [69/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [69/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [69/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [69/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [69/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [69/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [69/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [69/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [69/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [69/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [69/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [69/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [69/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [69/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [69/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [69/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [69/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [69/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [69/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [69/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [69/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [69/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [69/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [69/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [69/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [69/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [69/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [69/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [69/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [69/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [69/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [69/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [69/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [69/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [69/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [69/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [69/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [69/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [69/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [69/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [69/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [69/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [69/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [69/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [69/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [69/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [69/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [69/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [69/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [69/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [69/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [69/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [69/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [69/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [69/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [69/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [69/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [69/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [69/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [69/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [69/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [69/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [69/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [69/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [69/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [69/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [69/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [69/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [69/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [69/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [69/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [69/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [69/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [69/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [69/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [69/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [69/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [69/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [69/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [69/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [69/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [69/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [69/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [69/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [69/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [69/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [69/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [69/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [69/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [69/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [69/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [69/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [69/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [69/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [69/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [69/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [69/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [69/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [69/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [69/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [69/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [69/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [69/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [69/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [69/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [69/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [69/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [69/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [69/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [69/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [69/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [69/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [69/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [69/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [69/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [69/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [69/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [69/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [69/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [69/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [69/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [69/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [69/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [69/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [69/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [69/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [69/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [69/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [69/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [69/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [69/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [69/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [69/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [69/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [69/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [69/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [69/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [69/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [69/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [69/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [69/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [69/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [69/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [69/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [69/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [69/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [69/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [69/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [69/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [69/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [69/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [69/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [69/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [69/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [69/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [69/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [69/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [69/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [69/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [69/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [69/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [69/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [69/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [69/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [69/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [69/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [69/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [69/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [69/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [69/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [69/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [70/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [70/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [70/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [70/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [70/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [70/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [70/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [70/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [70/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [70/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [70/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [70/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [70/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [70/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [70/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [70/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [70/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [70/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [70/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [70/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [70/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [70/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [70/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [70/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [70/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [70/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [70/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [70/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [70/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [70/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [70/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [70/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [70/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [70/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [70/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [70/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [70/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [70/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [70/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [70/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [70/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [70/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [70/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [70/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [70/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [70/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [70/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [70/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [70/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [70/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [70/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [70/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [70/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [70/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [70/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [70/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [70/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [70/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [70/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [70/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [70/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [70/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [70/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [70/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [70/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [70/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [70/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [70/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [70/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [70/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [70/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [70/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [70/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [70/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [70/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [70/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [70/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [70/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [70/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [70/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [70/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [70/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [70/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [70/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [70/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [70/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [70/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [70/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [70/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [70/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [70/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [70/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [70/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [70/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [70/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [70/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [70/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [70/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [70/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [70/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [70/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [70/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [70/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [70/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [70/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [70/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [70/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [70/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [70/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [70/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [70/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [70/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [70/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [70/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [70/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [70/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [70/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [70/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [70/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [70/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [70/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [70/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [70/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [70/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [70/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [70/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [70/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [70/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [70/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [70/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [70/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [70/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [70/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [70/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [70/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [70/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [70/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [70/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [70/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [70/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [70/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [70/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [70/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [70/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [70/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [70/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [70/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [70/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [70/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [70/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [70/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [70/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [70/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [70/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [70/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [70/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [70/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [70/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [70/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [70/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [70/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [70/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [70/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [70/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [70/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [70/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [70/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [70/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [70/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [70/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [70/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [70/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [70/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [70/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [70/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [70/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [70/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [70/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [70/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [70/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [70/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [70/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [70/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [70/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [70/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [70/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [70/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [70/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [70/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [70/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [70/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [70/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [70/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [70/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [70/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [70/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [70/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [70/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [70/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [70/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [70/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [70/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [70/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [70/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [70/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [70/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [70/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [70/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [70/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [70/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [70/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [70/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [70/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [70/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [70/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [70/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [70/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [70/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [70/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [70/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [70/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [70/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [70/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [71/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [71/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [71/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [71/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [71/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [71/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [71/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [71/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [71/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [71/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [71/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [71/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [71/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [71/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [71/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [71/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [71/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [71/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [71/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [71/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [71/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [71/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [71/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [71/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [71/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [71/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [71/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [71/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [71/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [71/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [71/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [71/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [71/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [71/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [71/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [71/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [71/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [71/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [71/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [71/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [71/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [71/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [71/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [71/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [71/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [71/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [71/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [71/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [71/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [71/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [71/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [71/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [71/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [71/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [71/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [71/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [71/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [71/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [71/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [71/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [71/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [71/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [71/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [71/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [71/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [71/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [71/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [71/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [71/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [71/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [71/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [71/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [71/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [71/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [71/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [71/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [71/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [71/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [71/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [71/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [71/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [71/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [71/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [71/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [71/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [71/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [71/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [71/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [71/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [71/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [71/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [71/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [71/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [71/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [71/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [71/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [71/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [71/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [71/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [71/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [71/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [71/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [71/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [71/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [71/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [71/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [71/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [71/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [71/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [71/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [71/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [71/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [71/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [71/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [71/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [71/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [71/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [71/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [71/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [71/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [71/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [71/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [71/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [71/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [71/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [71/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [71/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [71/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [71/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [71/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [71/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [71/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [71/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [71/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [71/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [71/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [71/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [71/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [71/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [71/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [71/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [71/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [71/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [71/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [71/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [71/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [71/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [71/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [71/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [71/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [71/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [71/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [71/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [71/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [71/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [71/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [71/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [71/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [71/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [71/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [71/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [71/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [71/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [71/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [71/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [71/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [71/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [71/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [71/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [71/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [71/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [71/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [71/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [71/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [71/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [71/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [71/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [71/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [71/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [71/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [71/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [71/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [71/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [71/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [71/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [71/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [71/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [71/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [71/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [71/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [71/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [71/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [71/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [71/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [71/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [71/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [71/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [71/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [71/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [71/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [71/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [71/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [71/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [71/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [71/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [71/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [71/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [71/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [71/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [71/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [71/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [71/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [71/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [71/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [71/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [71/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [71/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [71/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [71/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [71/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [71/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [71/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [71/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [72/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [72/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [72/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [72/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [72/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [72/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [72/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [72/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [72/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [72/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [72/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [72/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [72/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [72/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [72/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [72/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [72/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [72/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [72/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [72/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [72/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [72/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [72/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [72/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [72/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [72/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [72/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [72/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [72/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [72/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [72/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [72/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [72/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [72/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [72/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [72/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [72/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [72/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [72/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [72/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [72/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [72/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [72/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [72/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [72/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [72/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [72/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [72/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [72/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [72/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [72/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [72/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [72/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [72/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [72/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [72/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [72/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [72/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [72/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [72/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [72/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [72/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [72/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [72/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [72/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [72/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [72/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [72/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [72/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [72/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [72/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [72/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [72/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [72/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [72/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [72/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [72/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [72/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [72/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [72/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [72/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [72/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [72/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [72/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [72/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [72/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [72/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [72/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [72/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [72/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [72/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [72/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [72/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [72/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [72/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [72/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [72/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [72/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [72/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [72/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [72/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [72/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [72/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [72/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [72/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [72/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [72/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [72/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [72/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [72/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [72/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [72/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [72/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [72/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [72/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [72/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [72/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [72/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [72/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [72/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [72/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [72/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [72/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [72/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [72/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [72/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [72/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [72/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [72/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [72/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [72/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [72/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [72/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [72/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [72/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [72/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [72/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [72/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [72/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [72/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [72/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [72/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [72/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [72/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [72/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [72/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [72/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [72/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [72/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [72/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [72/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [72/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [72/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [72/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [72/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [72/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [72/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [72/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [72/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [72/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [72/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [72/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [72/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [72/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [72/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [72/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [72/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [72/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [72/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [72/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [72/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [72/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [72/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [72/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [72/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [72/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [72/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [72/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [72/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [72/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [72/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [72/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [72/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [72/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [72/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [72/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [72/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [72/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [72/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [72/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [72/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [72/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [72/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [72/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [72/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [72/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [72/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [72/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [72/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [72/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [72/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [72/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [72/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [72/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [72/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [72/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [72/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [72/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [72/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [72/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [72/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [72/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [72/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [72/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [72/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [72/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [72/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [72/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [72/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [72/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [72/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [72/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [72/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [73/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [73/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [73/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [73/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [73/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [73/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [73/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [73/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [73/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [73/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [73/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [73/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [73/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [73/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [73/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [73/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [73/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [73/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [73/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [73/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [73/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [73/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [73/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [73/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [73/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [73/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [73/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [73/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [73/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [73/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [73/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [73/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [73/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [73/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [73/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [73/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [73/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [73/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [73/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [73/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [73/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [73/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [73/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [73/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [73/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [73/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [73/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [73/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [73/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [73/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [73/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [73/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [73/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [73/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [73/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [73/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [73/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [73/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [73/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [73/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [73/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [73/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [73/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [73/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [73/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [73/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [73/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [73/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [73/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [73/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [73/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [73/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [73/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [73/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [73/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [73/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [73/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [73/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [73/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [73/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [73/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [73/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [73/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [73/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [73/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [73/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [73/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [73/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [73/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [73/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [73/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [73/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [73/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [73/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [73/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [73/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [73/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [73/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [73/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [73/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [73/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [73/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [73/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [73/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [73/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [73/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [73/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [73/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [73/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [73/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [73/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [73/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [73/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [73/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [73/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [73/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [73/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [73/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [73/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [73/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [73/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [73/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [73/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [73/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [73/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [73/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [73/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [73/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [73/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [73/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [73/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [73/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [73/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [73/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [73/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [73/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [73/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [73/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [73/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [73/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [73/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [73/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [73/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [73/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [73/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [73/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [73/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [73/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [73/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [73/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [73/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [73/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [73/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [73/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [73/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [73/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [73/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [73/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [73/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [73/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [73/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [73/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [73/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [73/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [73/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [73/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [73/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [73/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [73/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [73/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [73/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [73/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [73/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [73/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [73/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [73/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [73/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [73/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [73/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [73/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [73/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [73/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [73/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [73/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [73/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [73/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [73/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [73/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [73/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [73/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [73/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [73/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [73/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [73/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [73/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [73/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [73/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [73/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [73/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [73/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [73/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [73/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [73/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [73/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [73/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [73/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [73/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [73/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [73/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [73/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [73/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [73/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [73/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [73/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [73/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [73/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [73/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [73/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [73/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [73/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [73/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [73/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [73/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [74/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [74/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [74/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [74/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [74/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [74/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [74/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [74/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [74/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [74/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [74/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [74/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [74/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [74/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [74/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [74/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [74/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [74/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [74/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [74/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [74/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [74/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [74/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [74/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [74/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [74/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [74/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [74/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [74/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [74/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [74/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [74/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [74/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [74/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [74/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [74/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [74/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [74/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [74/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [74/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [74/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [74/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [74/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [74/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [74/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [74/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [74/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [74/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [74/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [74/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [74/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [74/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [74/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [74/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [74/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [74/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [74/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [74/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [74/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [74/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [74/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [74/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [74/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [74/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [74/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [74/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [74/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [74/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [74/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [74/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [74/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [74/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [74/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [74/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [74/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [74/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [74/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [74/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [74/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [74/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [74/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [74/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [74/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [74/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [74/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [74/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [74/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [74/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [74/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [74/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [74/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [74/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [74/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [74/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [74/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [74/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [74/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [74/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [74/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [74/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [74/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [74/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [74/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [74/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [74/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [74/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [74/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [74/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [74/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [74/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [74/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [74/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [74/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [74/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [74/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [74/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [74/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [74/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [74/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [74/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [74/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [74/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [74/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [74/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [74/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [74/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [74/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [74/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [74/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [74/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [74/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [74/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [74/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [74/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [74/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [74/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [74/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [74/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [74/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [74/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [74/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [74/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [74/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [74/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [74/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [74/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [74/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [74/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [74/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [74/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [74/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [74/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [74/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [74/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [74/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [74/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [74/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [74/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [74/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [74/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [74/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [74/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [74/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [74/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [74/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [74/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [74/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [74/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [74/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [74/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [74/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [74/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [74/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [74/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [74/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [74/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [74/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [74/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [74/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [74/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [74/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [74/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [74/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [74/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [74/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [74/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [74/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [74/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [74/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [74/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [74/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [74/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [74/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [74/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [74/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [74/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [74/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [74/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [74/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [74/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [74/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [74/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [74/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [74/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [74/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [74/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [74/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [74/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [74/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [74/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [74/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [74/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [74/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [74/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [74/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [74/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [74/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [74/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [74/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [74/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [74/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [74/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [74/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [75/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [75/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [75/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [75/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [75/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [75/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [75/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [75/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [75/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [75/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [75/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [75/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [75/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [75/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [75/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [75/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [75/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [75/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [75/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [75/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [75/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [75/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [75/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [75/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [75/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [75/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [75/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [75/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [75/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [75/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [75/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [75/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [75/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [75/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [75/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [75/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [75/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [75/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [75/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [75/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [75/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [75/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [75/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [75/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [75/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [75/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [75/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [75/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [75/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [75/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [75/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [75/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [75/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [75/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [75/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [75/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [75/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [75/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [75/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [75/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [75/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [75/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [75/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [75/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [75/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [75/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [75/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [75/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [75/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [75/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [75/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [75/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [75/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [75/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [75/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [75/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [75/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [75/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [75/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [75/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [75/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [75/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [75/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [75/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [75/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [75/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [75/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [75/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [75/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [75/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [75/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [75/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [75/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [75/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [75/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [75/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [75/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [75/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [75/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [75/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [75/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [75/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [75/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [75/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [75/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [75/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [75/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [75/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [75/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [75/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [75/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [75/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [75/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [75/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [75/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [75/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [75/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [75/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [75/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [75/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [75/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [75/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [75/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [75/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [75/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [75/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [75/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [75/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [75/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [75/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [75/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [75/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [75/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [75/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [75/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [75/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [75/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [75/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [75/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [75/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [75/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [75/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [75/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [75/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [75/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [75/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [75/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [75/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [75/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [75/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [75/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [75/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [75/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [75/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [75/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [75/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [75/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [75/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [75/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [75/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [75/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [75/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [75/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [75/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [75/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [75/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [75/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [75/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [75/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [75/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [75/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [75/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [75/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [75/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [75/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [75/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [75/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [75/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [75/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [75/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [75/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [75/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [75/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [75/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [75/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [75/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [75/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [75/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [75/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [75/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [75/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [75/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [75/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [75/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [75/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [75/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [75/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [75/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [75/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [75/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [75/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [75/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [75/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [75/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [75/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [75/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [75/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [75/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [75/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [75/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [75/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [75/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [75/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [75/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [75/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [75/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [75/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [75/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [75/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [75/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [75/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [75/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [75/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [76/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [76/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [76/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [76/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [76/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [76/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [76/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [76/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [76/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [76/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [76/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [76/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [76/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [76/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [76/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [76/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [76/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [76/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [76/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [76/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [76/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [76/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [76/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [76/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [76/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [76/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [76/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [76/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [76/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [76/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [76/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [76/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [76/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [76/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [76/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [76/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [76/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [76/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [76/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [76/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [76/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [76/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [76/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [76/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [76/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [76/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [76/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [76/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [76/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [76/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [76/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [76/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [76/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [76/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [76/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [76/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [76/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [76/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [76/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [76/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [76/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [76/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [76/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [76/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [76/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [76/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [76/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [76/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [76/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [76/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [76/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [76/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [76/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [76/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [76/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [76/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [76/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [76/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [76/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [76/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [76/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [76/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [76/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [76/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [76/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [76/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [76/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [76/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [76/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [76/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [76/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [76/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [76/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [76/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [76/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [76/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [76/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [76/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [76/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [76/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [76/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [76/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [76/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [76/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [76/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [76/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [76/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [76/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [76/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [76/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [76/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [76/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [76/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [76/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [76/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [76/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [76/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [76/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [76/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [76/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [76/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [76/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [76/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [76/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [76/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [76/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [76/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [76/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [76/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [76/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [76/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [76/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [76/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [76/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [76/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [76/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [76/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [76/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [76/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [76/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [76/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [76/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [76/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [76/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [76/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [76/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [76/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [76/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [76/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [76/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [76/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [76/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [76/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [76/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [76/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [76/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [76/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [76/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [76/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [76/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [76/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [76/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [76/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [76/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [76/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [76/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [76/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [76/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [76/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [76/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [76/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [76/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [76/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [76/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [76/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [76/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [76/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [76/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [76/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [76/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [76/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [76/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [76/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [76/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [76/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [76/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [76/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [76/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [76/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [76/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [76/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [76/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [76/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [76/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [76/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [76/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [76/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [76/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [76/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [76/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [76/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [76/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [76/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [76/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [76/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [76/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [76/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [76/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [76/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [76/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [76/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [76/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [76/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [76/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [76/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [76/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [76/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [76/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [76/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [76/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [76/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [76/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [76/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [77/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [77/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [77/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [77/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [77/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [77/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [77/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [77/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [77/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [77/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [77/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [77/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [77/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [77/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [77/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [77/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [77/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [77/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [77/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [77/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [77/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [77/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [77/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [77/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [77/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [77/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [77/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [77/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [77/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [77/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [77/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [77/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [77/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [77/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [77/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [77/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [77/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [77/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [77/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [77/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [77/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [77/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [77/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [77/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [77/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [77/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [77/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [77/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [77/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [77/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [77/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [77/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [77/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [77/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [77/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [77/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [77/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [77/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [77/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [77/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [77/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [77/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [77/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [77/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [77/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [77/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [77/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [77/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [77/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [77/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [77/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [77/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [77/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [77/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [77/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [77/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [77/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [77/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [77/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [77/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [77/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [77/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [77/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [77/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [77/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [77/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [77/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [77/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [77/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [77/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [77/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [77/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [77/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [77/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [77/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [77/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [77/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [77/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [77/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [77/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [77/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [77/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [77/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [77/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [77/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [77/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [77/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [77/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [77/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [77/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [77/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [77/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [77/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [77/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [77/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [77/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [77/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [77/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [77/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [77/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [77/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [77/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [77/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [77/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [77/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [77/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [77/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [77/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [77/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [77/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [77/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [77/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [77/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [77/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [77/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [77/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [77/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [77/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [77/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [77/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [77/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [77/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [77/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [77/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [77/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [77/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [77/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [77/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [77/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [77/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [77/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [77/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [77/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [77/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [77/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [77/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [77/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [77/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [77/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [77/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [77/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [77/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [77/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [77/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [77/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [77/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [77/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [77/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [77/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [77/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [77/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [77/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [77/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [77/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [77/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [77/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [77/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [77/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [77/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [77/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [77/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [77/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [77/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [77/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [77/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [77/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [77/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [77/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [77/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [77/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [77/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [77/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [77/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [77/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [77/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [77/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [77/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [77/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [77/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [77/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [77/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [77/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [77/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [77/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [77/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [77/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [77/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [77/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [77/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [77/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [77/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [77/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [77/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [77/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [77/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [77/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [77/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [77/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [77/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [77/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [77/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [77/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [77/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [78/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [78/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [78/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [78/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [78/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [78/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [78/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [78/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [78/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [78/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [78/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [78/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [78/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [78/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [78/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [78/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [78/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [78/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [78/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [78/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [78/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [78/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [78/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [78/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [78/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [78/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [78/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [78/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [78/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [78/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [78/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [78/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [78/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [78/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [78/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [78/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [78/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [78/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [78/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [78/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [78/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [78/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [78/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [78/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [78/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [78/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [78/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [78/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [78/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [78/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [78/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [78/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [78/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [78/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [78/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [78/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [78/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [78/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [78/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [78/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [78/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [78/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [78/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [78/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [78/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [78/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [78/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [78/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [78/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [78/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [78/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [78/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [78/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [78/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [78/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [78/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [78/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [78/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [78/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [78/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [78/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [78/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [78/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [78/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [78/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [78/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [78/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [78/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [78/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [78/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [78/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [78/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [78/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [78/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [78/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [78/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [78/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [78/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [78/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [78/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [78/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [78/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [78/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [78/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [78/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [78/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [78/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [78/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [78/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [78/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [78/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [78/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [78/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [78/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [78/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [78/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [78/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [78/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [78/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [78/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [78/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [78/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [78/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [78/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [78/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [78/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [78/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [78/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [78/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [78/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [78/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [78/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [78/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [78/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [78/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [78/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [78/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [78/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [78/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [78/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [78/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [78/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [78/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [78/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [78/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [78/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [78/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [78/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [78/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [78/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [78/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [78/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [78/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [78/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [78/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [78/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [78/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [78/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [78/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [78/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [78/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [78/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [78/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [78/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [78/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [78/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [78/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [78/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [78/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [78/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [78/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [78/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [78/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [78/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [78/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [78/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [78/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [78/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [78/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [78/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [78/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [78/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [78/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [78/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [78/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [78/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [78/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [78/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [78/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [78/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [78/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [78/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [78/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [78/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [78/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [78/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [78/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [78/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [78/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [78/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [78/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [78/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [78/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [78/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [78/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [78/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [78/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [78/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [78/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [78/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [78/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [78/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [78/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [78/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [78/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [78/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [78/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [78/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [78/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [78/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [78/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [78/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [78/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [79/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [79/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [79/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [79/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [79/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [79/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [79/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [79/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [79/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [79/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [79/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [79/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [79/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [79/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [79/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [79/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [79/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [79/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [79/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [79/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [79/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [79/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [79/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [79/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [79/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [79/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [79/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [79/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [79/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [79/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [79/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [79/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [79/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [79/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [79/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [79/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [79/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [79/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [79/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [79/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [79/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [79/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [79/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [79/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [79/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [79/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [79/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [79/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [79/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [79/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [79/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [79/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [79/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [79/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [79/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [79/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [79/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [79/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [79/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [79/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [79/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [79/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [79/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [79/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [79/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [79/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [79/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [79/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [79/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [79/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [79/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [79/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [79/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [79/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [79/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [79/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [79/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [79/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [79/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [79/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [79/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [79/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [79/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [79/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [79/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [79/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [79/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [79/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [79/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [79/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [79/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [79/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [79/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [79/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [79/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [79/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [79/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [79/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [79/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [79/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [79/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [79/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [79/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [79/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [79/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [79/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [79/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [79/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [79/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [79/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [79/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [79/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [79/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [79/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [79/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [79/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [79/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [79/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [79/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [79/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [79/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [79/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [79/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [79/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [79/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [79/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [79/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [79/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [79/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [79/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [79/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [79/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [79/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [79/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [79/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [79/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [79/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [79/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [79/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [79/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [79/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [79/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [79/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [79/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [79/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [79/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [79/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [79/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [79/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [79/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [79/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [79/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [79/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [79/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [79/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [79/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [79/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [79/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [79/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [79/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [79/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [79/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [79/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [79/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [79/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [79/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [79/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [79/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [79/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [79/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [79/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [79/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [79/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [79/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [79/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [79/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [79/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [79/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [79/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [79/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [79/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [79/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [79/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [79/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [79/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [79/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [79/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [79/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [79/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [79/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [79/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [79/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [79/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [79/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [79/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [79/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [79/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [79/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [79/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [79/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [79/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [79/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [79/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [79/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [79/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [79/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [79/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [79/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [79/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [79/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [79/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [79/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [79/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [79/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [79/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [79/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [79/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [79/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [79/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [79/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [79/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [79/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [79/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [80/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [80/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [80/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [80/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [80/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [80/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [80/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [80/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [80/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [80/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [80/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [80/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [80/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [80/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [80/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [80/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [80/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [80/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [80/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [80/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [80/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [80/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [80/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [80/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [80/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [80/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [80/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [80/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [80/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [80/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [80/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [80/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [80/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [80/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [80/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [80/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [80/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [80/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [80/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [80/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [80/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [80/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [80/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [80/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [80/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [80/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [80/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [80/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [80/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [80/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [80/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [80/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [80/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [80/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [80/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [80/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [80/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [80/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [80/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [80/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [80/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [80/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [80/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [80/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [80/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [80/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [80/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [80/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [80/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [80/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [80/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [80/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [80/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [80/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [80/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [80/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [80/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [80/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [80/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [80/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [80/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [80/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [80/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [80/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [80/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [80/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [80/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [80/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [80/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [80/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [80/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [80/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [80/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [80/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [80/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [80/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [80/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [80/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [80/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [80/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [80/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [80/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [80/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [80/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [80/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [80/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [80/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [80/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [80/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [80/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [80/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [80/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [80/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [80/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [80/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [80/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [80/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [80/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [80/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [80/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [80/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [80/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [80/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [80/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [80/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [80/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [80/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [80/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [80/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [80/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [80/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [80/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [80/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [80/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [80/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [80/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [80/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [80/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [80/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [80/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [80/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [80/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [80/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [80/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [80/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [80/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [80/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [80/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [80/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [80/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [80/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [80/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [80/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [80/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [80/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [80/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [80/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [80/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [80/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [80/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [80/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [80/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [80/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [80/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [80/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [80/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [80/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [80/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [80/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [80/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [80/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [80/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [80/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [80/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [80/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [80/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [80/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [80/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [80/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [80/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [80/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [80/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [80/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [80/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [80/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [80/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [80/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [80/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [80/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [80/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [80/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [80/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [80/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [80/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [80/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [80/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [80/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [80/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [80/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [80/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [80/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [80/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [80/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [80/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [80/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [80/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [80/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [80/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [80/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [80/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [80/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [80/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [80/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [80/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [80/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [80/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [80/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [80/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [80/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [80/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [80/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [80/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [80/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [81/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [81/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [81/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [81/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [81/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [81/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [81/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [81/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [81/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [81/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [81/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [81/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [81/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [81/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [81/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [81/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [81/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [81/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [81/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [81/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [81/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [81/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [81/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [81/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [81/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [81/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [81/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [81/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [81/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [81/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [81/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [81/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [81/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [81/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [81/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [81/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [81/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [81/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [81/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [81/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [81/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [81/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [81/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [81/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [81/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [81/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [81/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [81/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [81/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [81/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [81/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [81/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [81/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [81/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [81/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [81/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [81/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [81/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [81/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [81/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [81/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [81/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [81/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [81/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [81/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [81/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [81/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [81/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [81/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [81/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [81/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [81/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [81/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [81/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [81/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [81/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [81/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [81/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [81/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [81/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [81/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [81/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [81/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [81/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [81/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [81/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [81/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [81/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [81/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [81/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [81/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [81/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [81/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [81/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [81/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [81/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [81/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [81/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [81/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [81/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [81/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [81/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [81/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [81/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [81/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [81/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [81/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [81/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [81/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [81/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [81/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [81/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [81/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [81/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [81/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [81/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [81/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [81/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [81/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [81/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [81/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [81/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [81/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [81/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [81/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [81/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [81/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [81/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [81/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [81/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [81/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [81/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [81/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [81/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [81/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [81/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [81/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [81/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [81/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [81/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [81/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [81/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [81/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [81/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [81/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [81/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [81/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [81/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [81/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [81/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [81/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [81/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [81/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [81/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [81/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [81/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [81/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [81/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [81/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [81/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [81/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [81/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [81/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [81/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [81/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [81/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [81/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [81/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [81/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [81/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [81/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [81/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [81/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [81/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [81/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [81/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [81/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [81/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [81/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [81/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [81/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [81/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [81/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [81/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [81/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [81/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [81/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [81/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [81/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [81/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [81/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [81/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [81/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [81/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [81/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [81/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [81/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [81/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [81/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [81/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [81/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [81/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [81/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [81/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [81/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [81/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [81/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [81/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [81/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [81/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [81/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [81/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [81/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [81/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [81/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [81/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [81/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [81/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [81/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [81/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [81/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [81/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [81/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [82/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [82/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [82/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [82/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [82/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [82/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [82/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [82/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [82/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [82/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [82/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [82/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [82/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [82/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [82/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [82/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [82/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [82/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [82/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [82/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [82/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [82/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [82/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [82/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [82/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [82/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [82/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [82/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [82/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [82/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [82/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [82/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [82/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [82/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [82/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [82/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [82/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [82/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [82/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [82/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [82/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [82/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [82/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [82/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [82/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [82/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [82/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [82/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [82/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [82/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [82/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [82/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [82/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [82/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [82/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [82/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [82/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [82/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [82/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [82/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [82/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [82/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [82/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [82/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [82/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [82/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [82/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [82/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [82/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [82/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [82/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [82/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [82/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [82/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [82/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [82/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [82/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [82/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [82/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [82/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [82/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [82/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [82/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [82/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [82/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [82/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [82/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [82/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [82/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [82/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [82/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [82/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [82/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [82/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [82/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [82/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [82/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [82/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [82/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [82/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [82/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [82/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [82/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [82/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [82/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [82/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [82/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [82/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [82/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [82/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [82/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [82/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [82/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [82/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [82/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [82/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [82/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [82/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [82/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [82/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [82/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [82/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [82/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [82/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [82/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [82/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [82/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [82/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [82/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [82/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [82/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [82/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [82/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [82/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [82/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [82/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [82/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [82/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [82/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [82/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [82/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [82/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [82/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [82/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [82/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [82/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [82/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [82/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [82/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [82/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [82/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [82/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [82/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [82/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [82/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [82/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [82/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [82/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [82/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [82/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [82/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [82/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [82/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [82/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [82/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [82/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [82/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [82/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [82/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [82/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [82/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [82/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [82/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [82/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [82/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [82/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [82/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [82/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [82/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [82/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [82/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [82/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [82/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [82/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [82/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [82/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [82/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [82/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [82/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [82/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [82/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [82/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [82/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [82/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [82/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [82/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [82/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [82/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [82/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [82/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [82/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [82/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [82/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [82/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [82/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [82/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [82/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [82/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [82/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [82/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [82/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [82/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [82/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [82/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [82/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [82/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [82/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [82/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [82/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [82/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [82/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [82/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [82/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [83/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [83/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [83/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [83/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [83/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [83/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [83/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [83/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [83/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [83/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [83/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [83/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [83/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [83/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [83/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [83/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [83/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [83/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [83/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [83/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [83/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [83/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [83/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [83/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [83/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [83/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [83/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [83/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [83/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [83/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n",
      "Epoch [83/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [83/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [83/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [83/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [83/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [83/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [83/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [83/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [83/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [83/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [83/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [83/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [83/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [83/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [83/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [83/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [83/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [83/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [83/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [83/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [83/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [83/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [83/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [83/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [83/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [83/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [83/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [83/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [83/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [83/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [83/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [83/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [83/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [83/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [83/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [83/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [83/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [83/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [83/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [83/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [83/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [83/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [83/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [83/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [83/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [83/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [83/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [83/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [83/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [83/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [83/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [83/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [83/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [83/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [83/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [83/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [83/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [83/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [83/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [83/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [83/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [83/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [83/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [83/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [83/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [83/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [83/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [83/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [83/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [83/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [83/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [83/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [83/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [83/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [83/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [83/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [83/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [83/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [83/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [83/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [83/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [83/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [83/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [83/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [83/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [83/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [83/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [83/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [83/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [83/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [83/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [83/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [83/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [83/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [83/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [83/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [83/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [83/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [83/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [83/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [83/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [83/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [83/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [83/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [83/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [83/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [83/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [83/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [83/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [83/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [83/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [83/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [83/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [83/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [83/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n",
      "Epoch [83/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [83/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [83/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [83/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [83/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [83/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [83/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [83/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [83/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [83/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [83/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [83/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [83/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [83/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [83/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [83/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [83/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [83/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [83/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [83/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [83/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [83/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [83/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [83/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [83/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [83/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [83/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [83/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [83/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [83/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [83/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [83/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [83/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [83/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [83/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [83/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [83/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [83/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [83/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [83/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [83/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [83/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [83/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [83/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [83/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [83/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [83/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [83/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [83/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [83/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [83/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [83/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [83/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [83/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [83/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [83/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [83/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [83/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [83/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [83/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [83/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [83/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [83/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [83/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [83/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [83/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [83/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [83/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [83/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [83/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [83/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [83/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [83/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [83/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [83/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [83/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [83/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [83/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [84/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [84/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [84/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [84/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [84/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [84/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [84/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [84/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [84/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [84/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [84/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [84/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [84/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [84/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [84/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [84/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [84/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [84/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [84/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [84/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [84/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [84/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [84/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [84/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [84/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [84/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [84/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [84/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [84/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n",
      "Epoch [84/300], Step [30/225], Training Accuracy: 24.8438%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/300], Step [31/225], Training Accuracy: 24.6976%, Training Loss: nan%\n",
      "Epoch [84/300], Step [32/225], Training Accuracy: 24.7559%, Training Loss: nan%\n",
      "Epoch [84/300], Step [33/225], Training Accuracy: 24.7159%, Training Loss: nan%\n",
      "Epoch [84/300], Step [34/225], Training Accuracy: 24.5864%, Training Loss: nan%\n",
      "Epoch [84/300], Step [35/225], Training Accuracy: 24.5982%, Training Loss: nan%\n",
      "Epoch [84/300], Step [36/225], Training Accuracy: 24.6528%, Training Loss: nan%\n",
      "Epoch [84/300], Step [37/225], Training Accuracy: 24.6622%, Training Loss: nan%\n",
      "Epoch [84/300], Step [38/225], Training Accuracy: 24.5477%, Training Loss: nan%\n",
      "Epoch [84/300], Step [39/225], Training Accuracy: 24.3590%, Training Loss: nan%\n",
      "Epoch [84/300], Step [40/225], Training Accuracy: 24.5703%, Training Loss: nan%\n",
      "Epoch [84/300], Step [41/225], Training Accuracy: 24.7332%, Training Loss: nan%\n",
      "Epoch [84/300], Step [42/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [84/300], Step [43/225], Training Accuracy: 24.9273%, Training Loss: nan%\n",
      "Epoch [84/300], Step [44/225], Training Accuracy: 24.9290%, Training Loss: nan%\n",
      "Epoch [84/300], Step [45/225], Training Accuracy: 24.8958%, Training Loss: nan%\n",
      "Epoch [84/300], Step [46/225], Training Accuracy: 24.7962%, Training Loss: nan%\n",
      "Epoch [84/300], Step [47/225], Training Accuracy: 24.7340%, Training Loss: nan%\n",
      "Epoch [84/300], Step [48/225], Training Accuracy: 24.8372%, Training Loss: nan%\n",
      "Epoch [84/300], Step [49/225], Training Accuracy: 24.6811%, Training Loss: nan%\n",
      "Epoch [84/300], Step [50/225], Training Accuracy: 24.7812%, Training Loss: nan%\n",
      "Epoch [84/300], Step [51/225], Training Accuracy: 24.7243%, Training Loss: nan%\n",
      "Epoch [84/300], Step [52/225], Training Accuracy: 24.6995%, Training Loss: nan%\n",
      "Epoch [84/300], Step [53/225], Training Accuracy: 24.6462%, Training Loss: nan%\n",
      "Epoch [84/300], Step [54/225], Training Accuracy: 24.5370%, Training Loss: nan%\n",
      "Epoch [84/300], Step [55/225], Training Accuracy: 24.6307%, Training Loss: nan%\n",
      "Epoch [84/300], Step [56/225], Training Accuracy: 24.8047%, Training Loss: nan%\n",
      "Epoch [84/300], Step [57/225], Training Accuracy: 24.6436%, Training Loss: nan%\n",
      "Epoch [84/300], Step [58/225], Training Accuracy: 24.6767%, Training Loss: nan%\n",
      "Epoch [84/300], Step [59/225], Training Accuracy: 24.9206%, Training Loss: nan%\n",
      "Epoch [84/300], Step [60/225], Training Accuracy: 24.9219%, Training Loss: nan%\n",
      "Epoch [84/300], Step [61/225], Training Accuracy: 24.9232%, Training Loss: nan%\n",
      "Epoch [84/300], Step [62/225], Training Accuracy: 25.0756%, Training Loss: nan%\n",
      "Epoch [84/300], Step [63/225], Training Accuracy: 25.1984%, Training Loss: nan%\n",
      "Epoch [84/300], Step [64/225], Training Accuracy: 25.0732%, Training Loss: nan%\n",
      "Epoch [84/300], Step [65/225], Training Accuracy: 25.1202%, Training Loss: nan%\n",
      "Epoch [84/300], Step [66/225], Training Accuracy: 25.0947%, Training Loss: nan%\n",
      "Epoch [84/300], Step [67/225], Training Accuracy: 25.1632%, Training Loss: nan%\n",
      "Epoch [84/300], Step [68/225], Training Accuracy: 25.2528%, Training Loss: nan%\n",
      "Epoch [84/300], Step [69/225], Training Accuracy: 25.1812%, Training Loss: nan%\n",
      "Epoch [84/300], Step [70/225], Training Accuracy: 25.2009%, Training Loss: nan%\n",
      "Epoch [84/300], Step [71/225], Training Accuracy: 25.1981%, Training Loss: nan%\n",
      "Epoch [84/300], Step [72/225], Training Accuracy: 25.1519%, Training Loss: nan%\n",
      "Epoch [84/300], Step [73/225], Training Accuracy: 25.0856%, Training Loss: nan%\n",
      "Epoch [84/300], Step [74/225], Training Accuracy: 25.0845%, Training Loss: nan%\n",
      "Epoch [84/300], Step [75/225], Training Accuracy: 25.0833%, Training Loss: nan%\n",
      "Epoch [84/300], Step [76/225], Training Accuracy: 25.1439%, Training Loss: nan%\n",
      "Epoch [84/300], Step [77/225], Training Accuracy: 25.0609%, Training Loss: nan%\n",
      "Epoch [84/300], Step [78/225], Training Accuracy: 25.1402%, Training Loss: nan%\n",
      "Epoch [84/300], Step [79/225], Training Accuracy: 25.1187%, Training Loss: nan%\n",
      "Epoch [84/300], Step [80/225], Training Accuracy: 25.1758%, Training Loss: nan%\n",
      "Epoch [84/300], Step [81/225], Training Accuracy: 25.1157%, Training Loss: nan%\n",
      "Epoch [84/300], Step [82/225], Training Accuracy: 25.0953%, Training Loss: nan%\n",
      "Epoch [84/300], Step [83/225], Training Accuracy: 25.0565%, Training Loss: nan%\n",
      "Epoch [84/300], Step [84/225], Training Accuracy: 25.0930%, Training Loss: nan%\n",
      "Epoch [84/300], Step [85/225], Training Accuracy: 25.1103%, Training Loss: nan%\n",
      "Epoch [84/300], Step [86/225], Training Accuracy: 25.1453%, Training Loss: nan%\n",
      "Epoch [84/300], Step [87/225], Training Accuracy: 25.1616%, Training Loss: nan%\n",
      "Epoch [84/300], Step [88/225], Training Accuracy: 25.1776%, Training Loss: nan%\n",
      "Epoch [84/300], Step [89/225], Training Accuracy: 25.1580%, Training Loss: nan%\n",
      "Epoch [84/300], Step [90/225], Training Accuracy: 25.0694%, Training Loss: nan%\n",
      "Epoch [84/300], Step [91/225], Training Accuracy: 25.1030%, Training Loss: nan%\n",
      "Epoch [84/300], Step [92/225], Training Accuracy: 25.0510%, Training Loss: nan%\n",
      "Epoch [84/300], Step [93/225], Training Accuracy: 25.1176%, Training Loss: nan%\n",
      "Epoch [84/300], Step [94/225], Training Accuracy: 25.1662%, Training Loss: nan%\n",
      "Epoch [84/300], Step [95/225], Training Accuracy: 25.1974%, Training Loss: nan%\n",
      "Epoch [84/300], Step [96/225], Training Accuracy: 25.2279%, Training Loss: nan%\n",
      "Epoch [84/300], Step [97/225], Training Accuracy: 25.1772%, Training Loss: nan%\n",
      "Epoch [84/300], Step [98/225], Training Accuracy: 25.2073%, Training Loss: nan%\n",
      "Epoch [84/300], Step [99/225], Training Accuracy: 25.2841%, Training Loss: nan%\n",
      "Epoch [84/300], Step [100/225], Training Accuracy: 25.3281%, Training Loss: nan%\n",
      "Epoch [84/300], Step [101/225], Training Accuracy: 25.4177%, Training Loss: nan%\n",
      "Epoch [84/300], Step [102/225], Training Accuracy: 25.3370%, Training Loss: nan%\n",
      "Epoch [84/300], Step [103/225], Training Accuracy: 25.3792%, Training Loss: nan%\n",
      "Epoch [84/300], Step [104/225], Training Accuracy: 25.3906%, Training Loss: nan%\n",
      "Epoch [84/300], Step [105/225], Training Accuracy: 25.3423%, Training Loss: nan%\n",
      "Epoch [84/300], Step [106/225], Training Accuracy: 25.4717%, Training Loss: nan%\n",
      "Epoch [84/300], Step [107/225], Training Accuracy: 25.5111%, Training Loss: nan%\n",
      "Epoch [84/300], Step [108/225], Training Accuracy: 25.6076%, Training Loss: nan%\n",
      "Epoch [84/300], Step [109/225], Training Accuracy: 25.5447%, Training Loss: nan%\n",
      "Epoch [84/300], Step [110/225], Training Accuracy: 25.5398%, Training Loss: nan%\n",
      "Epoch [84/300], Step [111/225], Training Accuracy: 25.4927%, Training Loss: nan%\n",
      "Epoch [84/300], Step [112/225], Training Accuracy: 25.5441%, Training Loss: nan%\n",
      "Epoch [84/300], Step [113/225], Training Accuracy: 25.5254%, Training Loss: nan%\n",
      "Epoch [84/300], Step [114/225], Training Accuracy: 25.6031%, Training Loss: nan%\n",
      "Epoch [84/300], Step [115/225], Training Accuracy: 25.5842%, Training Loss: nan%\n",
      "Epoch [84/300], Step [116/225], Training Accuracy: 25.5388%, Training Loss: nan%\n",
      "Epoch [84/300], Step [117/225], Training Accuracy: 25.5609%, Training Loss: nan%\n",
      "Epoch [84/300], Step [118/225], Training Accuracy: 25.4767%, Training Loss: nan%\n",
      "Epoch [84/300], Step [119/225], Training Accuracy: 25.4596%, Training Loss: nan%\n",
      "Epoch [84/300], Step [120/225], Training Accuracy: 25.4427%, Training Loss: nan%\n",
      "Epoch [84/300], Step [121/225], Training Accuracy: 25.5424%, Training Loss: nan%\n",
      "Epoch [84/300], Step [122/225], Training Accuracy: 25.5251%, Training Loss: nan%\n",
      "Epoch [84/300], Step [123/225], Training Accuracy: 25.5716%, Training Loss: nan%\n",
      "Epoch [84/300], Step [124/225], Training Accuracy: 25.5418%, Training Loss: nan%\n",
      "Epoch [84/300], Step [125/225], Training Accuracy: 25.5875%, Training Loss: nan%\n",
      "Epoch [84/300], Step [126/225], Training Accuracy: 25.5580%, Training Loss: nan%\n",
      "Epoch [84/300], Step [127/225], Training Accuracy: 25.5290%, Training Loss: nan%\n",
      "Epoch [84/300], Step [128/225], Training Accuracy: 25.5737%, Training Loss: nan%\n",
      "Epoch [84/300], Step [129/225], Training Accuracy: 25.6298%, Training Loss: nan%\n",
      "Epoch [84/300], Step [130/225], Training Accuracy: 25.6370%, Training Loss: nan%\n",
      "Epoch [84/300], Step [131/225], Training Accuracy: 25.6202%, Training Loss: nan%\n",
      "Epoch [84/300], Step [132/225], Training Accuracy: 25.5682%, Training Loss: nan%\n",
      "Epoch [84/300], Step [133/225], Training Accuracy: 25.5757%, Training Loss: nan%\n",
      "Epoch [84/300], Step [134/225], Training Accuracy: 25.6297%, Training Loss: nan%\n",
      "Epoch [84/300], Step [135/225], Training Accuracy: 25.6366%, Training Loss: nan%\n",
      "Epoch [84/300], Step [136/225], Training Accuracy: 25.6893%, Training Loss: nan%\n",
      "Epoch [84/300], Step [137/225], Training Accuracy: 25.6615%, Training Loss: nan%\n",
      "Epoch [84/300], Step [138/225], Training Accuracy: 25.6567%, Training Loss: nan%\n",
      "Epoch [84/300], Step [139/225], Training Accuracy: 25.6520%, Training Loss: nan%\n",
      "Epoch [84/300], Step [140/225], Training Accuracy: 25.6138%, Training Loss: nan%\n",
      "Epoch [84/300], Step [141/225], Training Accuracy: 25.5208%, Training Loss: nan%\n",
      "Epoch [84/300], Step [142/225], Training Accuracy: 25.5942%, Training Loss: nan%\n",
      "Epoch [84/300], Step [143/225], Training Accuracy: 25.6010%, Training Loss: nan%\n",
      "Epoch [84/300], Step [144/225], Training Accuracy: 25.5968%, Training Loss: nan%\n",
      "Epoch [84/300], Step [145/225], Training Accuracy: 25.6142%, Training Loss: nan%\n",
      "Epoch [84/300], Step [146/225], Training Accuracy: 25.6314%, Training Loss: nan%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/300], Step [147/225], Training Accuracy: 25.6590%, Training Loss: nan%\n",
      "Epoch [84/300], Step [148/225], Training Accuracy: 25.6440%, Training Loss: nan%\n",
      "Epoch [84/300], Step [149/225], Training Accuracy: 25.6816%, Training Loss: nan%\n",
      "Epoch [84/300], Step [150/225], Training Accuracy: 25.6875%, Training Loss: nan%\n",
      "Epoch [84/300], Step [151/225], Training Accuracy: 25.6933%, Training Loss: nan%\n",
      "Epoch [84/300], Step [152/225], Training Accuracy: 25.6579%, Training Loss: nan%\n",
      "Epoch [84/300], Step [153/225], Training Accuracy: 25.6230%, Training Loss: nan%\n",
      "Epoch [84/300], Step [154/225], Training Accuracy: 25.6595%, Training Loss: nan%\n",
      "Epoch [84/300], Step [155/225], Training Accuracy: 25.6855%, Training Loss: nan%\n",
      "Epoch [84/300], Step [156/225], Training Accuracy: 25.7212%, Training Loss: nan%\n",
      "Epoch [84/300], Step [157/225], Training Accuracy: 25.6568%, Training Loss: nan%\n",
      "Epoch [84/300], Step [158/225], Training Accuracy: 25.6824%, Training Loss: nan%\n",
      "Epoch [84/300], Step [159/225], Training Accuracy: 25.7862%, Training Loss: nan%\n",
      "Epoch [84/300], Step [160/225], Training Accuracy: 25.7910%, Training Loss: nan%\n",
      "Epoch [84/300], Step [161/225], Training Accuracy: 25.7861%, Training Loss: nan%\n",
      "Epoch [84/300], Step [162/225], Training Accuracy: 25.7427%, Training Loss: nan%\n",
      "Epoch [84/300], Step [163/225], Training Accuracy: 25.8148%, Training Loss: nan%\n",
      "Epoch [84/300], Step [164/225], Training Accuracy: 25.7527%, Training Loss: nan%\n",
      "Epoch [84/300], Step [165/225], Training Accuracy: 25.7197%, Training Loss: nan%\n",
      "Epoch [84/300], Step [166/225], Training Accuracy: 25.7154%, Training Loss: nan%\n",
      "Epoch [84/300], Step [167/225], Training Accuracy: 25.7391%, Training Loss: nan%\n",
      "Epoch [84/300], Step [168/225], Training Accuracy: 25.7254%, Training Loss: nan%\n",
      "Epoch [84/300], Step [169/225], Training Accuracy: 25.6379%, Training Loss: nan%\n",
      "Epoch [84/300], Step [170/225], Training Accuracy: 25.6250%, Training Loss: nan%\n",
      "Epoch [84/300], Step [171/225], Training Accuracy: 25.6488%, Training Loss: nan%\n",
      "Epoch [84/300], Step [172/225], Training Accuracy: 25.6541%, Training Loss: nan%\n",
      "Epoch [84/300], Step [173/225], Training Accuracy: 25.6774%, Training Loss: nan%\n",
      "Epoch [84/300], Step [174/225], Training Accuracy: 25.7004%, Training Loss: nan%\n",
      "Epoch [84/300], Step [175/225], Training Accuracy: 25.7321%, Training Loss: nan%\n",
      "Epoch [84/300], Step [176/225], Training Accuracy: 25.7369%, Training Loss: nan%\n",
      "Epoch [84/300], Step [177/225], Training Accuracy: 25.7504%, Training Loss: nan%\n",
      "Epoch [84/300], Step [178/225], Training Accuracy: 25.7549%, Training Loss: nan%\n",
      "Epoch [84/300], Step [179/225], Training Accuracy: 25.7332%, Training Loss: nan%\n",
      "Epoch [84/300], Step [180/225], Training Accuracy: 25.6944%, Training Loss: nan%\n",
      "Epoch [84/300], Step [181/225], Training Accuracy: 25.6733%, Training Loss: nan%\n",
      "Epoch [84/300], Step [182/225], Training Accuracy: 25.6439%, Training Loss: nan%\n",
      "Epoch [84/300], Step [183/225], Training Accuracy: 25.6660%, Training Loss: nan%\n",
      "Epoch [84/300], Step [184/225], Training Accuracy: 25.6284%, Training Loss: nan%\n",
      "Epoch [84/300], Step [185/225], Training Accuracy: 25.6081%, Training Loss: nan%\n",
      "Epoch [84/300], Step [186/225], Training Accuracy: 25.6048%, Training Loss: nan%\n",
      "Epoch [84/300], Step [187/225], Training Accuracy: 25.5765%, Training Loss: nan%\n",
      "Epoch [84/300], Step [188/225], Training Accuracy: 25.5652%, Training Loss: nan%\n",
      "Epoch [84/300], Step [189/225], Training Accuracy: 25.5787%, Training Loss: nan%\n",
      "Epoch [84/300], Step [190/225], Training Accuracy: 25.5592%, Training Loss: nan%\n",
      "Epoch [84/300], Step [191/225], Training Accuracy: 25.5154%, Training Loss: nan%\n",
      "Epoch [84/300], Step [192/225], Training Accuracy: 25.4720%, Training Loss: nan%\n",
      "Epoch [84/300], Step [193/225], Training Accuracy: 25.4534%, Training Loss: nan%\n",
      "Epoch [84/300], Step [194/225], Training Accuracy: 25.4269%, Training Loss: nan%\n",
      "Epoch [84/300], Step [195/225], Training Accuracy: 25.3846%, Training Loss: nan%\n",
      "Epoch [84/300], Step [196/225], Training Accuracy: 25.3667%, Training Loss: nan%\n",
      "Epoch [84/300], Step [197/225], Training Accuracy: 25.3569%, Training Loss: nan%\n",
      "Epoch [84/300], Step [198/225], Training Accuracy: 25.3314%, Training Loss: nan%\n",
      "Epoch [84/300], Step [199/225], Training Accuracy: 25.3141%, Training Loss: nan%\n",
      "Epoch [84/300], Step [200/225], Training Accuracy: 25.2734%, Training Loss: nan%\n",
      "Epoch [84/300], Step [201/225], Training Accuracy: 25.2954%, Training Loss: nan%\n",
      "Epoch [84/300], Step [202/225], Training Accuracy: 25.2553%, Training Loss: nan%\n",
      "Epoch [84/300], Step [203/225], Training Accuracy: 25.2771%, Training Loss: nan%\n",
      "Epoch [84/300], Step [204/225], Training Accuracy: 25.3217%, Training Loss: nan%\n",
      "Epoch [84/300], Step [205/225], Training Accuracy: 25.2973%, Training Loss: nan%\n",
      "Epoch [84/300], Step [206/225], Training Accuracy: 25.3110%, Training Loss: nan%\n",
      "Epoch [84/300], Step [207/225], Training Accuracy: 25.3170%, Training Loss: nan%\n",
      "Epoch [84/300], Step [208/225], Training Accuracy: 25.3155%, Training Loss: nan%\n",
      "Epoch [84/300], Step [209/225], Training Accuracy: 25.3514%, Training Loss: nan%\n",
      "Epoch [84/300], Step [210/225], Training Accuracy: 25.3720%, Training Loss: nan%\n",
      "Epoch [84/300], Step [211/225], Training Accuracy: 25.3406%, Training Loss: nan%\n",
      "Epoch [84/300], Step [212/225], Training Accuracy: 25.3685%, Training Loss: nan%\n",
      "Epoch [84/300], Step [213/225], Training Accuracy: 25.3815%, Training Loss: nan%\n",
      "Epoch [84/300], Step [214/225], Training Accuracy: 25.3724%, Training Loss: nan%\n",
      "Epoch [84/300], Step [215/225], Training Accuracy: 25.3125%, Training Loss: nan%\n",
      "Epoch [84/300], Step [216/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [84/300], Step [217/225], Training Accuracy: 25.2592%, Training Loss: nan%\n",
      "Epoch [84/300], Step [218/225], Training Accuracy: 25.2652%, Training Loss: nan%\n",
      "Epoch [84/300], Step [219/225], Training Accuracy: 25.2854%, Training Loss: nan%\n",
      "Epoch [84/300], Step [220/225], Training Accuracy: 25.2699%, Training Loss: nan%\n",
      "Epoch [84/300], Step [221/225], Training Accuracy: 25.2899%, Training Loss: nan%\n",
      "Epoch [84/300], Step [222/225], Training Accuracy: 25.2463%, Training Loss: nan%\n",
      "Epoch [84/300], Step [223/225], Training Accuracy: 25.2733%, Training Loss: nan%\n",
      "Epoch [84/300], Step [224/225], Training Accuracy: 25.2581%, Training Loss: nan%\n",
      "Epoch [84/300], Step [225/225], Training Accuracy: 25.2779%, Training Loss: nan%\n",
      "Epoch [85/300], Step [1/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [85/300], Step [2/225], Training Accuracy: 24.2188%, Training Loss: nan%\n",
      "Epoch [85/300], Step [3/225], Training Accuracy: 26.0417%, Training Loss: nan%\n",
      "Epoch [85/300], Step [4/225], Training Accuracy: 24.6094%, Training Loss: nan%\n",
      "Epoch [85/300], Step [5/225], Training Accuracy: 25.0000%, Training Loss: nan%\n",
      "Epoch [85/300], Step [6/225], Training Accuracy: 25.2604%, Training Loss: nan%\n",
      "Epoch [85/300], Step [7/225], Training Accuracy: 24.3304%, Training Loss: nan%\n",
      "Epoch [85/300], Step [8/225], Training Accuracy: 23.8281%, Training Loss: nan%\n",
      "Epoch [85/300], Step [9/225], Training Accuracy: 23.9583%, Training Loss: nan%\n",
      "Epoch [85/300], Step [10/225], Training Accuracy: 23.9062%, Training Loss: nan%\n",
      "Epoch [85/300], Step [11/225], Training Accuracy: 23.8636%, Training Loss: nan%\n",
      "Epoch [85/300], Step [12/225], Training Accuracy: 23.4375%, Training Loss: nan%\n",
      "Epoch [85/300], Step [13/225], Training Accuracy: 22.5962%, Training Loss: nan%\n",
      "Epoch [85/300], Step [14/225], Training Accuracy: 22.7679%, Training Loss: nan%\n",
      "Epoch [85/300], Step [15/225], Training Accuracy: 23.1250%, Training Loss: nan%\n",
      "Epoch [85/300], Step [16/225], Training Accuracy: 23.3398%, Training Loss: nan%\n",
      "Epoch [85/300], Step [17/225], Training Accuracy: 23.1618%, Training Loss: nan%\n",
      "Epoch [85/300], Step [18/225], Training Accuracy: 23.6111%, Training Loss: nan%\n",
      "Epoch [85/300], Step [19/225], Training Accuracy: 24.0954%, Training Loss: nan%\n",
      "Epoch [85/300], Step [20/225], Training Accuracy: 24.5312%, Training Loss: nan%\n",
      "Epoch [85/300], Step [21/225], Training Accuracy: 24.1815%, Training Loss: nan%\n",
      "Epoch [85/300], Step [22/225], Training Accuracy: 24.4318%, Training Loss: nan%\n",
      "Epoch [85/300], Step [23/225], Training Accuracy: 24.4565%, Training Loss: nan%\n",
      "Epoch [85/300], Step [24/225], Training Accuracy: 24.7396%, Training Loss: nan%\n",
      "Epoch [85/300], Step [25/225], Training Accuracy: 24.7500%, Training Loss: nan%\n",
      "Epoch [85/300], Step [26/225], Training Accuracy: 24.9399%, Training Loss: nan%\n",
      "Epoch [85/300], Step [27/225], Training Accuracy: 24.7106%, Training Loss: nan%\n",
      "Epoch [85/300], Step [28/225], Training Accuracy: 24.4978%, Training Loss: nan%\n",
      "Epoch [85/300], Step [29/225], Training Accuracy: 24.7306%, Training Loss: nan%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss = 0\n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #scheduler.step() \n",
    "        #print(scheduler.get_last_lr()[0])\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        #print(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += Y.size(0)\n",
    "        correct += predicted.eq(Y).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        accu=100.*correct/total\n",
    "        train_loss = running_loss/(i+1)\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.4f}%, Training Loss: {:.4f}%'.format(epoch+1, num_epochs, i+1, total_step, accu, train_loss))\n",
    "\n",
    "\n",
    "        #writer.add_scalar(f'train/accuracy', accu, epoch)\n",
    "        #writer.add_scalar(f'train/loss', train_loss, epoch)\n",
    "        writer.add_scalars(f'train/accuracy_loss', {\n",
    "            'accuracy': accu,\n",
    "            'loss': train_loss,\n",
    "        }, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, Y in test_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += Y.size(0)\n",
    "        correct += (predicted == Y).sum().item()\n",
    "\n",
    "    print('Test Accuracy : {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525ecd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce71586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
