{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce5e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d89556d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5a240eef",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d1819",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ab3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"data/64_640data/train_set.csv\",header=None).to_numpy()\n",
    "train_label = pd.read_csv(\"data/64_640data/train_label.csv\",header=None).to_numpy()\n",
    "test_set = pd.read_csv(\"data/64_640data/test_set.csv\",header=None).to_numpy()\n",
    "test_label = pd.read_csv(\"data/64_640data/test_label.csv\",header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692bac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 4096) (14392, 1) (3598, 4096) (3598, 1)\n"
     ]
    }
   ],
   "source": [
    "#delet first row data\n",
    "train_set = train_set[1:]\n",
    "train_label = train_label[1:]\n",
    "test_set = test_set[1:]\n",
    "test_label = test_label[1:]\n",
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9381e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.reshape((-1,1,64,640))\n",
    "test_set = test_set.reshape((-1,1,64,640))\n",
    "train_label = train_label.reshape(-1)\n",
    "test_label = test_label.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67059ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14392, 1, 64, 64) (14392,) (3598, 1, 64, 64) (3598,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape, train_label.shape, test_set.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16df28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tensor = Tensor(train_set) \n",
    "train_label_tensor = Tensor(train_label).type(torch.LongTensor)\n",
    "\n",
    "train_dataset = TensorDataset(train_set_tensor,train_label_tensor) \n",
    "train_loader = DataLoader(train_dataset, batch_size=64) \n",
    "\n",
    "test_set_tensor = Tensor(test_set) \n",
    "test_label_tensor = Tensor(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_dataset = TensorDataset(test_set_tensor,test_label_tensor) \n",
    "test_loader = DataLoader(test_dataset, batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc99f5",
   "metadata": {},
   "source": [
    "# construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6df059",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = 8\n",
    "D = 2\n",
    "F2 = 16\n",
    "drop_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc77c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(self.weight.data, p=2, dim=0, maxnorm=self.max_norm)\n",
    "        return super(Conv2dWithConstraint, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7864a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEncoder_EEGNet(nn.Module):\n",
    "    def __init__(self, fs, num_ch, num_time):\n",
    "        super(LocalEncoder_EEGNet, self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=F1, kernel_size=(1, int(fs / 2)), stride=1, bias=False,\n",
    "                            padding=(0, (int(fs / 2) // 2) - 1))  # []\n",
    "        self.b1 = nn.BatchNorm2d(F1)\n",
    "        self.c2 = Conv2dWithConstraint(in_channels=F1, out_channels=F1 * D, kernel_size=(num_ch, 1), stride=1,\n",
    "                                       bias=False, groups=F1, padding=(0, 0), max_norm=1)\n",
    "        self.b2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.p2 = nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.d2 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h1 = self.b1(self.c1(input))\n",
    "        h2 = self.d2(self.p2(F.elu(self.b2(self.c2(h1)))))\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_enc = LocalEncoder_EEGNet(fs=160, num_ch=64, num_time = 640)\n",
    "# a = torch.randn((10,1,64,64))\n",
    "# b = local_enc(a)\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013314b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testlocal_enc = LocalEncoder_EEGNet(fs=512, num_ch=64, num_time = 1024)\n",
    "# testa = torch.randn((10,1,64,1024))\n",
    "# testb = local_enc(testa)\n",
    "# testb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83335dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decomposer(nn.Module):\n",
    "    def __init__(self, nfeat):\n",
    "        super(Decomposer, self).__init__()\n",
    "        self.nfeat = nfeat\n",
    "        self.embed_layer = nn.Sequential(nn.Conv2d(nfeat, nfeat*2, kernel_size=1, bias=False),\n",
    "                                         nn.BatchNorm2d(nfeat*2), nn.ELU(), nn.Dropout())\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed_layer(x)\n",
    "        #print(embedded.shape)\n",
    "        rele, irre = torch.split(embedded, [int(self.nfeat), int(self.nfeat)], dim=1)\n",
    "\n",
    "        return rele, irre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad52830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposer = Decomposer(16)\n",
    "# c, d = decomposer(b)\n",
    "# print(c.shape,d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ffa83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalEncoder_EEGNet(nn.Module):\n",
    "    def __init__(self, num_ch, num_time, nfeatl):\n",
    "        super(GlobalEncoder_EEGNet, self).__init__()\n",
    "        self.c3 = nn.Conv2d(in_channels=nfeatl, out_channels=F1 * D, kernel_size=(1, 16), stride=1, bias=False,\n",
    "                            groups=(nfeatl), padding=(0, 16 // 2))\n",
    "        self.b3 = nn.BatchNorm2d(F1 * D)\n",
    "        self.p3 = nn.AvgPool2d(kernel_size=(1, 8), stride=(1, 8))\n",
    "        self.d3 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h3 = self.d3(self.p3(F.elu(self.b3(self.c3(x)))))\n",
    "        h3_ = torch.flatten(h3, start_dim=1)\n",
    "        return h3_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e21758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_enc = GlobalEncoder_EEGNet()\n",
    "# e = global_enc(c)\n",
    "# e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5609ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nfeatr):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.dense1 = nn.Linear(nfeatr, 4)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        out = self.dense1(latent)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = Classifier()\n",
    "# f = classifier(e)\n",
    "# f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b245bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Extension of grad reverse layer\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, constant):\n",
    "        ctx.constant = constant\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.neg() * ctx.constant\n",
    "        return grad_output, None\n",
    "\n",
    "    def grad_reverse(x, constant):\n",
    "        return GradReverse.apply(x, constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8e78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINE(nn.Module):\n",
    "    def __init__(self, nfeatr, nfeati):\n",
    "        super(MINE, self).__init__()\n",
    "        self.fc1_x = nn.Linear(nfeatr, int(nfeatr/16))\n",
    "        self.bn1_x = nn.BatchNorm1d(int(nfeatr/16))\n",
    "        self.fc1_y = nn.Linear(nfeati, int(nfeati/16))\n",
    "        self.bn1_y = nn.BatchNorm1d(int(nfeati/16))\n",
    "\n",
    "        self.fc2 = nn.Linear(int(nfeati/16) + int(nfeatr/16),int(nfeati/16) + int(nfeatr/16))\n",
    "        self.bn2 = nn.BatchNorm1d(int(nfeati/16) + int(nfeatr/16))\n",
    "\n",
    "        self.fc3 = nn.Linear(int(nfeati/16) + int(nfeatr/16), 1)\n",
    "\n",
    "    def forward(self, x, y, lambd=1):\n",
    "        # GRL\n",
    "        x = GradReverse.grad_reverse(x, lambd)\n",
    "        y = GradReverse.grad_reverse(y, lambd)\n",
    "        #print(x.shape,y.shape)\n",
    "        #x = x.reshape(10,-1)\n",
    "        #y = y.reshape(10,-1)\n",
    "        x = F.dropout(self.bn1_x(self.fc1_x(x)))\n",
    "        y = F.dropout(self.bn1_y(self.fc1_y(y)))\n",
    "\n",
    "        h = F.elu(torch.cat((x,y), dim=-1))\n",
    "        h = F.elu(self.bn2(self.fc2(h)))\n",
    "        h = self.fc3(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f1465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine = MINE(nfeatr=int(16 * 15), nfeati=int(16 * 15))\n",
    "# g = mine(c,d)\n",
    "# g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be56821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_disc_EEGNet(nn.Module):\n",
    "    def __init__(self, nfeatl, nfeatg, num_ch):\n",
    "        super(Global_disc_EEGNet, self).__init__()\n",
    "        self.local_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=nfeatl, out_channels=F1 * D, kernel_size=(1, 16), stride=1, bias=False,\n",
    "                      groups=(nfeatl), padding=(0, 16 // 2)),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.AvgPool2d(kernel_size=(1, 8), stride=(1, 8)),\n",
    "            nn.Dropout(drop_prob)\n",
    "        )\n",
    "        self.dense1 = nn.Linear(int(nfeatg*2), 1)\n",
    "        self.drop1 = nn.Dropout()\n",
    "\n",
    "    def forward(self, localf, globalf):\n",
    "        localff = self.local_conv(localf)\n",
    "        localff = localff.view(localf.shape[0], -1)\n",
    "\n",
    "        concat = torch.cat((localff, globalf), dim=-1)\n",
    "        #print(concat.shape)\n",
    "        out = self.drop1(self.dense1(concat))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "565077d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_disc = Global_disc_EEGNet(nfeatl=16, nfeatg=32, num_ch=64)\n",
    "# h = global_disc(c,e)\n",
    "# h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddc5df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local_disc_EEGNet(nn.Module):\n",
    "    def __init__(self, nfeatl, nfeatg, nfeatl2, num_ch):\n",
    "        super(Local_disc_EEGNet, self).__init__()\n",
    "        self.num_ch = num_ch\n",
    "        self.nfeatl = nfeatl\n",
    "        self.nfeatl2 = nfeatl2\n",
    "        self.nfeatg = nfeatg\n",
    "\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.conv = nn.Conv2d(int(self.nfeatg+self.nfeatl), 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, localf, globalf):\n",
    "        #print(localf.shape,globalf.shape)\n",
    "        # Concat-and-convolve architecture\n",
    "        globalff = globalf.unsqueeze(2).unsqueeze(3)\n",
    "        #print(globalff.shape)\n",
    "        globalff = globalff.repeat(1,1,1,self.nfeatl2)\n",
    "        concat = torch.cat((localf, globalff), dim=1)\n",
    "        out = self.drop1(self.conv(concat))\n",
    "        out = out.view(out.shape[0],-1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_disc = Local_disc_EEGNet(nfeatl=16, nfeatg=32, nfeatl2=15, num_ch=64)\n",
    "# i = local_disc(c,e)\n",
    "# i.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc84dcc",
   "metadata": {},
   "source": [
    "# function and model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "394af0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameter \"\"\"\n",
    "lr = 1e-3\n",
    "n_class = 4\n",
    "drop = 0.5\n",
    "total_epoch = 100\n",
    "w_decay = 0.05\n",
    "alpha = 0.5\n",
    "beta = 0.3\n",
    "gamma = 0.5\n",
    "bs = 40\n",
    "fs = 160\n",
    "num_ch = 64\n",
    "num_time = 64\n",
    "nfeatl = 16\n",
    "nfeatg = 32\n",
    "nfeatl2 = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6326510",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_enc = LocalEncoder_EEGNet(fs=fs, num_ch=num_ch, num_time=num_time).to(device)\n",
    "global_enc = GlobalEncoder_EEGNet(num_ch=num_ch, num_time=num_time, nfeatl=nfeatl).to(device)\n",
    "local_disc = Local_disc_EEGNet(nfeatl=nfeatl, nfeatg=nfeatg, nfeatl2=nfeatl2, num_ch=num_ch).to(device)\n",
    "global_disc = Global_disc_EEGNet(nfeatl=nfeatl, nfeatg=nfeatg, num_ch=num_ch).to(device)\n",
    "decomposer = Decomposer(nfeatl).to(device)\n",
    "mine = MINE(nfeatr=int(nfeatl * nfeatl2), nfeati=int(nfeatl * nfeatl2)).to(device)\n",
    "classifier = Classifier(nfeatg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be79f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_criterion = nn.CrossEntropyLoss().cuda()\n",
    "parameters = list(local_enc.parameters()) + list(global_enc.parameters()) + list(local_disc.parameters()) + list(\n",
    "    global_disc.parameters()) + list(classifier.parameters()) + list(mine.parameters()) + list(decomposer.parameters())\n",
    "opt = torch.optim.RAdam(parameters, lr=lr, weight_decay=w_decay)\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9f8876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_JSD_MI(joint, marginal, mean=False):\n",
    "    joint = (torch.log(torch.tensor(2.0)) - F.softplus(-joint))\n",
    "    marginal = (F.softplus(-marginal)+marginal - torch.log(torch.tensor(2.0)))\n",
    "\n",
    "    out = joint - marginal\n",
    "    if mean:\n",
    "        out = out.mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8525ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/225], Training Accuracy: 25.0000%, Training Loss: 0.8518%\n",
      "Epoch [1/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.7992%\n",
      "Epoch [1/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.7942%\n",
      "Epoch [1/100], Step [4/225], Training Accuracy: 29.2969%, Training Loss: 0.7939%\n",
      "Epoch [1/100], Step [5/225], Training Accuracy: 29.3750%, Training Loss: 0.7849%\n",
      "Epoch [1/100], Step [6/225], Training Accuracy: 28.6458%, Training Loss: 0.7862%\n",
      "Epoch [1/100], Step [7/225], Training Accuracy: 29.0179%, Training Loss: 0.7929%\n",
      "Epoch [1/100], Step [8/225], Training Accuracy: 28.5156%, Training Loss: 0.7928%\n",
      "Epoch [1/100], Step [9/225], Training Accuracy: 27.0833%, Training Loss: 0.7901%\n",
      "Epoch [1/100], Step [10/225], Training Accuracy: 26.7188%, Training Loss: 0.7938%\n",
      "Epoch [1/100], Step [11/225], Training Accuracy: 26.4205%, Training Loss: 0.7921%\n",
      "Epoch [1/100], Step [12/225], Training Accuracy: 26.3021%, Training Loss: 0.7994%\n",
      "Epoch [1/100], Step [13/225], Training Accuracy: 26.2019%, Training Loss: 0.7933%\n",
      "Epoch [1/100], Step [14/225], Training Accuracy: 25.4464%, Training Loss: 0.7965%\n",
      "Epoch [1/100], Step [15/225], Training Accuracy: 25.7292%, Training Loss: 0.7973%\n",
      "Epoch [1/100], Step [16/225], Training Accuracy: 25.7812%, Training Loss: 0.7959%\n",
      "Epoch [1/100], Step [17/225], Training Accuracy: 25.7353%, Training Loss: 0.7957%\n",
      "Epoch [1/100], Step [18/225], Training Accuracy: 26.1285%, Training Loss: 0.7920%\n",
      "Epoch [1/100], Step [19/225], Training Accuracy: 26.3980%, Training Loss: 0.7924%\n",
      "Epoch [1/100], Step [20/225], Training Accuracy: 26.6406%, Training Loss: 0.7902%\n",
      "Epoch [1/100], Step [21/225], Training Accuracy: 26.5625%, Training Loss: 0.7889%\n",
      "Epoch [1/100], Step [22/225], Training Accuracy: 26.3494%, Training Loss: 0.7895%\n",
      "Epoch [1/100], Step [23/225], Training Accuracy: 25.9511%, Training Loss: 0.7906%\n",
      "Epoch [1/100], Step [24/225], Training Accuracy: 26.3021%, Training Loss: 0.7888%\n",
      "Epoch [1/100], Step [25/225], Training Accuracy: 26.0625%, Training Loss: 0.7882%\n",
      "Epoch [1/100], Step [26/225], Training Accuracy: 26.0216%, Training Loss: 0.7878%\n",
      "Epoch [1/100], Step [27/225], Training Accuracy: 25.8102%, Training Loss: 0.7869%\n",
      "Epoch [1/100], Step [28/225], Training Accuracy: 26.0603%, Training Loss: 0.7856%\n",
      "Epoch [1/100], Step [29/225], Training Accuracy: 26.0237%, Training Loss: 0.7851%\n",
      "Epoch [1/100], Step [30/225], Training Accuracy: 26.0417%, Training Loss: 0.7855%\n",
      "Epoch [1/100], Step [31/225], Training Accuracy: 26.0081%, Training Loss: 0.7857%\n",
      "Epoch [1/100], Step [32/225], Training Accuracy: 25.7324%, Training Loss: 0.7860%\n",
      "Epoch [1/100], Step [33/225], Training Accuracy: 25.6155%, Training Loss: 0.7848%\n",
      "Epoch [1/100], Step [34/225], Training Accuracy: 25.4596%, Training Loss: 0.7845%\n",
      "Epoch [1/100], Step [35/225], Training Accuracy: 25.5357%, Training Loss: 0.7849%\n",
      "Epoch [1/100], Step [36/225], Training Accuracy: 25.7378%, Training Loss: 0.7845%\n",
      "Epoch [1/100], Step [37/225], Training Accuracy: 25.6334%, Training Loss: 0.7848%\n",
      "Epoch [1/100], Step [38/225], Training Accuracy: 25.5345%, Training Loss: 0.7837%\n",
      "Epoch [1/100], Step [39/225], Training Accuracy: 25.5609%, Training Loss: 0.7831%\n",
      "Epoch [1/100], Step [40/225], Training Accuracy: 25.5078%, Training Loss: 0.7837%\n",
      "Epoch [1/100], Step [41/225], Training Accuracy: 25.3811%, Training Loss: 0.7842%\n",
      "Epoch [1/100], Step [42/225], Training Accuracy: 25.1860%, Training Loss: 0.7846%\n",
      "Epoch [1/100], Step [43/225], Training Accuracy: 25.2907%, Training Loss: 0.7838%\n",
      "Epoch [1/100], Step [44/225], Training Accuracy: 25.1420%, Training Loss: 0.7834%\n",
      "Epoch [1/100], Step [45/225], Training Accuracy: 25.3472%, Training Loss: 0.7828%\n",
      "Epoch [1/100], Step [46/225], Training Accuracy: 25.3057%, Training Loss: 0.7826%\n",
      "Epoch [1/100], Step [47/225], Training Accuracy: 25.4322%, Training Loss: 0.7813%\n",
      "Epoch [1/100], Step [48/225], Training Accuracy: 25.4557%, Training Loss: 0.7814%\n",
      "Epoch [1/100], Step [49/225], Training Accuracy: 25.5102%, Training Loss: 0.7809%\n",
      "Epoch [1/100], Step [50/225], Training Accuracy: 25.4688%, Training Loss: 0.7813%\n",
      "Epoch [1/100], Step [51/225], Training Accuracy: 25.3983%, Training Loss: 0.7811%\n",
      "Epoch [1/100], Step [52/225], Training Accuracy: 25.4507%, Training Loss: 0.7812%\n",
      "Epoch [1/100], Step [53/225], Training Accuracy: 25.4127%, Training Loss: 0.7815%\n",
      "Epoch [1/100], Step [54/225], Training Accuracy: 25.3762%, Training Loss: 0.7816%\n",
      "Epoch [1/100], Step [55/225], Training Accuracy: 25.3125%, Training Loss: 0.7818%\n",
      "Epoch [1/100], Step [56/225], Training Accuracy: 25.4185%, Training Loss: 0.7812%\n",
      "Epoch [1/100], Step [57/225], Training Accuracy: 25.4112%, Training Loss: 0.7809%\n",
      "Epoch [1/100], Step [58/225], Training Accuracy: 25.2963%, Training Loss: 0.7809%\n",
      "Epoch [1/100], Step [59/225], Training Accuracy: 25.5032%, Training Loss: 0.7799%\n",
      "Epoch [1/100], Step [60/225], Training Accuracy: 25.4688%, Training Loss: 0.7800%\n",
      "Epoch [1/100], Step [61/225], Training Accuracy: 25.5123%, Training Loss: 0.7791%\n",
      "Epoch [1/100], Step [62/225], Training Accuracy: 25.5544%, Training Loss: 0.7788%\n",
      "Epoch [1/100], Step [63/225], Training Accuracy: 25.5704%, Training Loss: 0.7784%\n",
      "Epoch [1/100], Step [64/225], Training Accuracy: 25.5371%, Training Loss: 0.7784%\n",
      "Epoch [1/100], Step [65/225], Training Accuracy: 25.5529%, Training Loss: 0.7781%\n",
      "Epoch [1/100], Step [66/225], Training Accuracy: 25.7576%, Training Loss: 0.7769%\n",
      "Epoch [1/100], Step [67/225], Training Accuracy: 25.9795%, Training Loss: 0.7762%\n",
      "Epoch [1/100], Step [68/225], Training Accuracy: 25.8732%, Training Loss: 0.7767%\n",
      "Epoch [1/100], Step [69/225], Training Accuracy: 25.8605%, Training Loss: 0.7767%\n",
      "Epoch [1/100], Step [70/225], Training Accuracy: 25.7589%, Training Loss: 0.7776%\n",
      "Epoch [1/100], Step [71/225], Training Accuracy: 25.7923%, Training Loss: 0.7774%\n",
      "Epoch [1/100], Step [72/225], Training Accuracy: 25.7595%, Training Loss: 0.7774%\n",
      "Epoch [1/100], Step [73/225], Training Accuracy: 25.7920%, Training Loss: 0.7768%\n",
      "Epoch [1/100], Step [74/225], Training Accuracy: 25.7812%, Training Loss: 0.7768%\n",
      "Epoch [1/100], Step [75/225], Training Accuracy: 25.6250%, Training Loss: 0.7769%\n",
      "Epoch [1/100], Step [76/225], Training Accuracy: 25.6579%, Training Loss: 0.7765%\n",
      "Epoch [1/100], Step [77/225], Training Accuracy: 25.6291%, Training Loss: 0.7769%\n",
      "Epoch [1/100], Step [78/225], Training Accuracy: 25.6611%, Training Loss: 0.7770%\n",
      "Epoch [1/100], Step [79/225], Training Accuracy: 25.5736%, Training Loss: 0.7770%\n",
      "Epoch [1/100], Step [80/225], Training Accuracy: 25.6250%, Training Loss: 0.7765%\n",
      "Epoch [1/100], Step [81/225], Training Accuracy: 25.5594%, Training Loss: 0.7763%\n",
      "Epoch [1/100], Step [82/225], Training Accuracy: 25.5716%, Training Loss: 0.7763%\n",
      "Epoch [1/100], Step [83/225], Training Accuracy: 25.6589%, Training Loss: 0.7767%\n",
      "Epoch [1/100], Step [84/225], Training Accuracy: 25.7068%, Training Loss: 0.7761%\n",
      "Epoch [1/100], Step [85/225], Training Accuracy: 25.6066%, Training Loss: 0.7764%\n",
      "Epoch [1/100], Step [86/225], Training Accuracy: 25.6904%, Training Loss: 0.7760%\n",
      "Epoch [1/100], Step [87/225], Training Accuracy: 25.6466%, Training Loss: 0.7759%\n",
      "Epoch [1/100], Step [88/225], Training Accuracy: 25.5682%, Training Loss: 0.7763%\n",
      "Epoch [1/100], Step [89/225], Training Accuracy: 25.5267%, Training Loss: 0.7765%\n",
      "Epoch [1/100], Step [90/225], Training Accuracy: 25.5208%, Training Loss: 0.7759%\n",
      "Epoch [1/100], Step [91/225], Training Accuracy: 25.4808%, Training Loss: 0.7759%\n",
      "Epoch [1/100], Step [92/225], Training Accuracy: 25.4925%, Training Loss: 0.7755%\n",
      "Epoch [1/100], Step [93/225], Training Accuracy: 25.4368%, Training Loss: 0.7752%\n",
      "Epoch [1/100], Step [94/225], Training Accuracy: 25.4987%, Training Loss: 0.7749%\n",
      "Epoch [1/100], Step [95/225], Training Accuracy: 25.5099%, Training Loss: 0.7745%\n",
      "Epoch [1/100], Step [96/225], Training Accuracy: 25.4883%, Training Loss: 0.7742%\n",
      "Epoch [1/100], Step [97/225], Training Accuracy: 25.4832%, Training Loss: 0.7739%\n",
      "Epoch [1/100], Step [98/225], Training Accuracy: 25.3667%, Training Loss: 0.7737%\n",
      "Epoch [1/100], Step [99/225], Training Accuracy: 25.4893%, Training Loss: 0.7731%\n",
      "Epoch [1/100], Step [100/225], Training Accuracy: 25.3750%, Training Loss: 0.7733%\n",
      "Epoch [1/100], Step [101/225], Training Accuracy: 25.4486%, Training Loss: 0.7730%\n",
      "Epoch [1/100], Step [102/225], Training Accuracy: 25.4136%, Training Loss: 0.7729%\n",
      "Epoch [1/100], Step [103/225], Training Accuracy: 25.4399%, Training Loss: 0.7725%\n",
      "Epoch [1/100], Step [104/225], Training Accuracy: 25.4507%, Training Loss: 0.7724%\n",
      "Epoch [1/100], Step [105/225], Training Accuracy: 25.4464%, Training Loss: 0.7722%\n",
      "Epoch [1/100], Step [106/225], Training Accuracy: 25.4570%, Training Loss: 0.7719%\n",
      "Epoch [1/100], Step [107/225], Training Accuracy: 25.4673%, Training Loss: 0.7715%\n",
      "Epoch [1/100], Step [108/225], Training Accuracy: 25.3906%, Training Loss: 0.7717%\n",
      "Epoch [1/100], Step [109/225], Training Accuracy: 25.3154%, Training Loss: 0.7718%\n",
      "Epoch [1/100], Step [110/225], Training Accuracy: 25.3409%, Training Loss: 0.7720%\n",
      "Epoch [1/100], Step [111/225], Training Accuracy: 25.3378%, Training Loss: 0.7718%\n",
      "Epoch [1/100], Step [112/225], Training Accuracy: 25.3906%, Training Loss: 0.7719%\n",
      "Epoch [1/100], Step [113/225], Training Accuracy: 25.4148%, Training Loss: 0.7713%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [114/225], Training Accuracy: 25.4249%, Training Loss: 0.7716%\n",
      "Epoch [1/100], Step [115/225], Training Accuracy: 25.4755%, Training Loss: 0.7714%\n",
      "Epoch [1/100], Step [116/225], Training Accuracy: 25.5119%, Training Loss: 0.7711%\n",
      "Epoch [1/100], Step [117/225], Training Accuracy: 25.5075%, Training Loss: 0.7710%\n",
      "Epoch [1/100], Step [118/225], Training Accuracy: 25.5297%, Training Loss: 0.7709%\n",
      "Epoch [1/100], Step [119/225], Training Accuracy: 25.5383%, Training Loss: 0.7708%\n",
      "Epoch [1/100], Step [120/225], Training Accuracy: 25.4948%, Training Loss: 0.7709%\n",
      "Epoch [1/100], Step [121/225], Training Accuracy: 25.5036%, Training Loss: 0.7707%\n",
      "Epoch [1/100], Step [122/225], Training Accuracy: 25.4611%, Training Loss: 0.7706%\n",
      "Epoch [1/100], Step [123/225], Training Accuracy: 25.3557%, Training Loss: 0.7706%\n",
      "Epoch [1/100], Step [124/225], Training Accuracy: 25.3528%, Training Loss: 0.7704%\n",
      "Epoch [1/100], Step [125/225], Training Accuracy: 25.4750%, Training Loss: 0.7702%\n",
      "Epoch [1/100], Step [126/225], Training Accuracy: 25.4092%, Training Loss: 0.7699%\n",
      "Epoch [1/100], Step [127/225], Training Accuracy: 25.4306%, Training Loss: 0.7696%\n",
      "Epoch [1/100], Step [128/225], Training Accuracy: 25.4517%, Training Loss: 0.7693%\n",
      "Epoch [1/100], Step [129/225], Training Accuracy: 25.4966%, Training Loss: 0.7693%\n",
      "Epoch [1/100], Step [130/225], Training Accuracy: 25.4327%, Training Loss: 0.7692%\n",
      "Epoch [1/100], Step [131/225], Training Accuracy: 25.3936%, Training Loss: 0.7693%\n",
      "Epoch [1/100], Step [132/225], Training Accuracy: 25.3433%, Training Loss: 0.7692%\n",
      "Epoch [1/100], Step [133/225], Training Accuracy: 25.3642%, Training Loss: 0.7690%\n",
      "Epoch [1/100], Step [134/225], Training Accuracy: 25.3848%, Training Loss: 0.7686%\n",
      "Epoch [1/100], Step [135/225], Training Accuracy: 25.3819%, Training Loss: 0.7683%\n",
      "Epoch [1/100], Step [136/225], Training Accuracy: 25.3791%, Training Loss: 0.7682%\n",
      "Epoch [1/100], Step [137/225], Training Accuracy: 25.4220%, Training Loss: 0.7681%\n",
      "Epoch [1/100], Step [138/225], Training Accuracy: 25.4076%, Training Loss: 0.7681%\n",
      "Epoch [1/100], Step [139/225], Training Accuracy: 25.3934%, Training Loss: 0.7681%\n",
      "Epoch [1/100], Step [140/225], Training Accuracy: 25.3906%, Training Loss: 0.7678%\n",
      "Epoch [1/100], Step [141/225], Training Accuracy: 25.3546%, Training Loss: 0.7677%\n",
      "Epoch [1/100], Step [142/225], Training Accuracy: 25.3741%, Training Loss: 0.7675%\n",
      "Epoch [1/100], Step [143/225], Training Accuracy: 25.3606%, Training Loss: 0.7672%\n",
      "Epoch [1/100], Step [144/225], Training Accuracy: 25.3689%, Training Loss: 0.7671%\n",
      "Epoch [1/100], Step [145/225], Training Accuracy: 25.3664%, Training Loss: 0.7667%\n",
      "Epoch [1/100], Step [146/225], Training Accuracy: 25.3425%, Training Loss: 0.7667%\n",
      "Epoch [1/100], Step [147/225], Training Accuracy: 25.3614%, Training Loss: 0.7666%\n",
      "Epoch [1/100], Step [148/225], Training Accuracy: 25.3378%, Training Loss: 0.7665%\n",
      "Epoch [1/100], Step [149/225], Training Accuracy: 25.3670%, Training Loss: 0.7663%\n",
      "Epoch [1/100], Step [150/225], Training Accuracy: 25.4062%, Training Loss: 0.7663%\n",
      "Epoch [1/100], Step [151/225], Training Accuracy: 25.4450%, Training Loss: 0.7660%\n",
      "Epoch [1/100], Step [152/225], Training Accuracy: 25.4420%, Training Loss: 0.7660%\n",
      "Epoch [1/100], Step [153/225], Training Accuracy: 25.4289%, Training Loss: 0.7660%\n",
      "Epoch [1/100], Step [154/225], Training Accuracy: 25.4566%, Training Loss: 0.7657%\n",
      "Epoch [1/100], Step [155/225], Training Accuracy: 25.4335%, Training Loss: 0.7655%\n",
      "Epoch [1/100], Step [156/225], Training Accuracy: 25.5108%, Training Loss: 0.7653%\n",
      "Epoch [1/100], Step [157/225], Training Accuracy: 25.5076%, Training Loss: 0.7654%\n",
      "Epoch [1/100], Step [158/225], Training Accuracy: 25.5835%, Training Loss: 0.7653%\n",
      "Epoch [1/100], Step [159/225], Training Accuracy: 25.6289%, Training Loss: 0.7650%\n",
      "Epoch [1/100], Step [160/225], Training Accuracy: 25.6836%, Training Loss: 0.7648%\n",
      "Epoch [1/100], Step [161/225], Training Accuracy: 25.6988%, Training Loss: 0.7649%\n",
      "Epoch [1/100], Step [162/225], Training Accuracy: 25.7523%, Training Loss: 0.7646%\n",
      "Epoch [1/100], Step [163/225], Training Accuracy: 25.7669%, Training Loss: 0.7644%\n",
      "Epoch [1/100], Step [164/225], Training Accuracy: 25.6764%, Training Loss: 0.7643%\n",
      "Epoch [1/100], Step [165/225], Training Accuracy: 25.6439%, Training Loss: 0.7644%\n",
      "Epoch [1/100], Step [166/225], Training Accuracy: 25.6495%, Training Loss: 0.7642%\n",
      "Epoch [1/100], Step [167/225], Training Accuracy: 25.6830%, Training Loss: 0.7639%\n",
      "Epoch [1/100], Step [168/225], Training Accuracy: 25.6510%, Training Loss: 0.7637%\n",
      "Epoch [1/100], Step [169/225], Training Accuracy: 25.5547%, Training Loss: 0.7638%\n",
      "Epoch [1/100], Step [170/225], Training Accuracy: 25.5331%, Training Loss: 0.7637%\n",
      "Epoch [1/100], Step [171/225], Training Accuracy: 25.5665%, Training Loss: 0.7635%\n",
      "Epoch [1/100], Step [172/225], Training Accuracy: 25.5814%, Training Loss: 0.7633%\n",
      "Epoch [1/100], Step [173/225], Training Accuracy: 25.6413%, Training Loss: 0.7630%\n",
      "Epoch [1/100], Step [174/225], Training Accuracy: 25.6376%, Training Loss: 0.7629%\n",
      "Epoch [1/100], Step [175/225], Training Accuracy: 25.6339%, Training Loss: 0.7625%\n",
      "Epoch [1/100], Step [176/225], Training Accuracy: 25.7102%, Training Loss: 0.7623%\n",
      "Epoch [1/100], Step [177/225], Training Accuracy: 25.7062%, Training Loss: 0.7621%\n",
      "Epoch [1/100], Step [178/225], Training Accuracy: 25.7110%, Training Loss: 0.7621%\n",
      "Epoch [1/100], Step [179/225], Training Accuracy: 25.6983%, Training Loss: 0.7619%\n",
      "Epoch [1/100], Step [180/225], Training Accuracy: 25.7205%, Training Loss: 0.7615%\n",
      "Epoch [1/100], Step [181/225], Training Accuracy: 25.6561%, Training Loss: 0.7613%\n",
      "Epoch [1/100], Step [182/225], Training Accuracy: 25.6525%, Training Loss: 0.7612%\n",
      "Epoch [1/100], Step [183/225], Training Accuracy: 25.6745%, Training Loss: 0.7610%\n",
      "Epoch [1/100], Step [184/225], Training Accuracy: 25.6878%, Training Loss: 0.7609%\n",
      "Epoch [1/100], Step [185/225], Training Accuracy: 25.6926%, Training Loss: 0.7608%\n",
      "Epoch [1/100], Step [186/225], Training Accuracy: 25.7224%, Training Loss: 0.7606%\n",
      "Epoch [1/100], Step [187/225], Training Accuracy: 25.7604%, Training Loss: 0.7604%\n",
      "Epoch [1/100], Step [188/225], Training Accuracy: 25.7563%, Training Loss: 0.7603%\n",
      "Epoch [1/100], Step [189/225], Training Accuracy: 25.7358%, Training Loss: 0.7600%\n",
      "Epoch [1/100], Step [190/225], Training Accuracy: 25.6743%, Training Loss: 0.7599%\n",
      "Epoch [1/100], Step [191/225], Training Accuracy: 25.7117%, Training Loss: 0.7597%\n",
      "Epoch [1/100], Step [192/225], Training Accuracy: 25.7161%, Training Loss: 0.7596%\n",
      "Epoch [1/100], Step [193/225], Training Accuracy: 25.7367%, Training Loss: 0.7592%\n",
      "Epoch [1/100], Step [194/225], Training Accuracy: 25.7490%, Training Loss: 0.7590%\n",
      "Epoch [1/100], Step [195/225], Training Accuracy: 25.7532%, Training Loss: 0.7589%\n",
      "Epoch [1/100], Step [196/225], Training Accuracy: 25.7494%, Training Loss: 0.7588%\n",
      "Epoch [1/100], Step [197/225], Training Accuracy: 25.7694%, Training Loss: 0.7587%\n",
      "Epoch [1/100], Step [198/225], Training Accuracy: 25.7734%, Training Loss: 0.7585%\n",
      "Epoch [1/100], Step [199/225], Training Accuracy: 25.8087%, Training Loss: 0.7582%\n",
      "Epoch [1/100], Step [200/225], Training Accuracy: 25.8203%, Training Loss: 0.7582%\n",
      "Epoch [1/100], Step [201/225], Training Accuracy: 25.8629%, Training Loss: 0.7580%\n",
      "Epoch [1/100], Step [202/225], Training Accuracy: 25.9127%, Training Loss: 0.7579%\n",
      "Epoch [1/100], Step [203/225], Training Accuracy: 25.9467%, Training Loss: 0.7577%\n",
      "Epoch [1/100], Step [204/225], Training Accuracy: 25.9498%, Training Loss: 0.7576%\n",
      "Epoch [1/100], Step [205/225], Training Accuracy: 25.9756%, Training Loss: 0.7575%\n",
      "Epoch [1/100], Step [206/225], Training Accuracy: 26.0012%, Training Loss: 0.7574%\n",
      "Epoch [1/100], Step [207/225], Training Accuracy: 25.9964%, Training Loss: 0.7573%\n",
      "Epoch [1/100], Step [208/225], Training Accuracy: 26.0592%, Training Loss: 0.7571%\n",
      "Epoch [1/100], Step [209/225], Training Accuracy: 26.0990%, Training Loss: 0.7569%\n",
      "Epoch [1/100], Step [210/225], Training Accuracy: 26.1161%, Training Loss: 0.7569%\n",
      "Epoch [1/100], Step [211/225], Training Accuracy: 26.1256%, Training Loss: 0.7567%\n",
      "Epoch [1/100], Step [212/225], Training Accuracy: 26.1203%, Training Loss: 0.7565%\n",
      "Epoch [1/100], Step [213/225], Training Accuracy: 26.0783%, Training Loss: 0.7564%\n",
      "Epoch [1/100], Step [214/225], Training Accuracy: 26.0587%, Training Loss: 0.7563%\n",
      "Epoch [1/100], Step [215/225], Training Accuracy: 26.0392%, Training Loss: 0.7562%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [216/225], Training Accuracy: 26.0272%, Training Loss: 0.7561%\n",
      "Epoch [1/100], Step [217/225], Training Accuracy: 26.0369%, Training Loss: 0.7559%\n",
      "Epoch [1/100], Step [218/225], Training Accuracy: 26.0034%, Training Loss: 0.7559%\n",
      "Epoch [1/100], Step [219/225], Training Accuracy: 26.0131%, Training Loss: 0.7557%\n",
      "Epoch [1/100], Step [220/225], Training Accuracy: 26.0227%, Training Loss: 0.7556%\n",
      "Epoch [1/100], Step [221/225], Training Accuracy: 26.0040%, Training Loss: 0.7556%\n",
      "Epoch [1/100], Step [222/225], Training Accuracy: 26.0206%, Training Loss: 0.7555%\n",
      "Epoch [1/100], Step [223/225], Training Accuracy: 25.9879%, Training Loss: 0.7554%\n",
      "Epoch [1/100], Step [224/225], Training Accuracy: 25.9835%, Training Loss: 0.7553%\n",
      "Epoch [1/100], Step [225/225], Training Accuracy: 25.9589%, Training Loss: 0.7551%\n",
      "Epoch [2/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [2/100], Step [2/225], Training Accuracy: 28.1250%, Training Loss: 0.7015%\n",
      "Epoch [2/100], Step [3/225], Training Accuracy: 26.0417%, Training Loss: 0.7082%\n",
      "Epoch [2/100], Step [4/225], Training Accuracy: 24.6094%, Training Loss: 0.7140%\n",
      "Epoch [2/100], Step [5/225], Training Accuracy: 26.5625%, Training Loss: 0.7148%\n",
      "Epoch [2/100], Step [6/225], Training Accuracy: 25.7812%, Training Loss: 0.7167%\n",
      "Epoch [2/100], Step [7/225], Training Accuracy: 25.4464%, Training Loss: 0.7198%\n",
      "Epoch [2/100], Step [8/225], Training Accuracy: 25.1953%, Training Loss: 0.7203%\n",
      "Epoch [2/100], Step [9/225], Training Accuracy: 25.3472%, Training Loss: 0.7220%\n",
      "Epoch [2/100], Step [10/225], Training Accuracy: 24.3750%, Training Loss: 0.7235%\n",
      "Epoch [2/100], Step [11/225], Training Accuracy: 24.5739%, Training Loss: 0.7218%\n",
      "Epoch [2/100], Step [12/225], Training Accuracy: 24.8698%, Training Loss: 0.7223%\n",
      "Epoch [2/100], Step [13/225], Training Accuracy: 25.6010%, Training Loss: 0.7201%\n",
      "Epoch [2/100], Step [14/225], Training Accuracy: 25.8929%, Training Loss: 0.7196%\n",
      "Epoch [2/100], Step [15/225], Training Accuracy: 26.0417%, Training Loss: 0.7187%\n",
      "Epoch [2/100], Step [16/225], Training Accuracy: 26.4648%, Training Loss: 0.7194%\n",
      "Epoch [2/100], Step [17/225], Training Accuracy: 25.9191%, Training Loss: 0.7184%\n",
      "Epoch [2/100], Step [18/225], Training Accuracy: 26.0417%, Training Loss: 0.7183%\n",
      "Epoch [2/100], Step [19/225], Training Accuracy: 26.2336%, Training Loss: 0.7176%\n",
      "Epoch [2/100], Step [20/225], Training Accuracy: 26.2500%, Training Loss: 0.7180%\n",
      "Epoch [2/100], Step [21/225], Training Accuracy: 26.2649%, Training Loss: 0.7178%\n",
      "Epoch [2/100], Step [22/225], Training Accuracy: 26.5625%, Training Loss: 0.7179%\n",
      "Epoch [2/100], Step [23/225], Training Accuracy: 26.5625%, Training Loss: 0.7183%\n",
      "Epoch [2/100], Step [24/225], Training Accuracy: 26.9531%, Training Loss: 0.7183%\n",
      "Epoch [2/100], Step [25/225], Training Accuracy: 27.3125%, Training Loss: 0.7176%\n",
      "Epoch [2/100], Step [26/225], Training Accuracy: 27.5841%, Training Loss: 0.7168%\n",
      "Epoch [2/100], Step [27/225], Training Accuracy: 27.3727%, Training Loss: 0.7170%\n",
      "Epoch [2/100], Step [28/225], Training Accuracy: 27.5112%, Training Loss: 0.7175%\n",
      "Epoch [2/100], Step [29/225], Training Accuracy: 27.5862%, Training Loss: 0.7167%\n",
      "Epoch [2/100], Step [30/225], Training Accuracy: 27.7083%, Training Loss: 0.7170%\n",
      "Epoch [2/100], Step [31/225], Training Accuracy: 27.4698%, Training Loss: 0.7176%\n",
      "Epoch [2/100], Step [32/225], Training Accuracy: 27.3926%, Training Loss: 0.7178%\n",
      "Epoch [2/100], Step [33/225], Training Accuracy: 27.3201%, Training Loss: 0.7178%\n",
      "Epoch [2/100], Step [34/225], Training Accuracy: 27.3897%, Training Loss: 0.7178%\n",
      "Epoch [2/100], Step [35/225], Training Accuracy: 27.4554%, Training Loss: 0.7176%\n",
      "Epoch [2/100], Step [36/225], Training Accuracy: 27.3872%, Training Loss: 0.7171%\n",
      "Epoch [2/100], Step [37/225], Training Accuracy: 27.3226%, Training Loss: 0.7183%\n",
      "Epoch [2/100], Step [38/225], Training Accuracy: 27.3849%, Training Loss: 0.7189%\n",
      "Epoch [2/100], Step [39/225], Training Accuracy: 27.2837%, Training Loss: 0.7185%\n",
      "Epoch [2/100], Step [40/225], Training Accuracy: 27.0312%, Training Loss: 0.7186%\n",
      "Epoch [2/100], Step [41/225], Training Accuracy: 27.2866%, Training Loss: 0.7185%\n",
      "Epoch [2/100], Step [42/225], Training Accuracy: 27.0461%, Training Loss: 0.7191%\n",
      "Epoch [2/100], Step [43/225], Training Accuracy: 27.2529%, Training Loss: 0.7187%\n",
      "Epoch [2/100], Step [44/225], Training Accuracy: 26.9886%, Training Loss: 0.7187%\n",
      "Epoch [2/100], Step [45/225], Training Accuracy: 27.0486%, Training Loss: 0.7185%\n",
      "Epoch [2/100], Step [46/225], Training Accuracy: 27.0720%, Training Loss: 0.7184%\n",
      "Epoch [2/100], Step [47/225], Training Accuracy: 27.0612%, Training Loss: 0.7187%\n",
      "Epoch [2/100], Step [48/225], Training Accuracy: 26.9857%, Training Loss: 0.7187%\n",
      "Epoch [2/100], Step [49/225], Training Accuracy: 27.0727%, Training Loss: 0.7188%\n",
      "Epoch [2/100], Step [50/225], Training Accuracy: 27.1250%, Training Loss: 0.7187%\n",
      "Epoch [2/100], Step [51/225], Training Accuracy: 27.1140%, Training Loss: 0.7186%\n",
      "Epoch [2/100], Step [52/225], Training Accuracy: 27.1034%, Training Loss: 0.7184%\n",
      "Epoch [2/100], Step [53/225], Training Accuracy: 27.0047%, Training Loss: 0.7184%\n",
      "Epoch [2/100], Step [54/225], Training Accuracy: 26.9387%, Training Loss: 0.7184%\n",
      "Epoch [2/100], Step [55/225], Training Accuracy: 26.9602%, Training Loss: 0.7181%\n",
      "Epoch [2/100], Step [56/225], Training Accuracy: 27.1763%, Training Loss: 0.7175%\n",
      "Epoch [2/100], Step [57/225], Training Accuracy: 27.2204%, Training Loss: 0.7171%\n",
      "Epoch [2/100], Step [58/225], Training Accuracy: 27.1552%, Training Loss: 0.7172%\n",
      "Epoch [2/100], Step [59/225], Training Accuracy: 27.2246%, Training Loss: 0.7173%\n",
      "Epoch [2/100], Step [60/225], Training Accuracy: 27.2656%, Training Loss: 0.7172%\n",
      "Epoch [2/100], Step [61/225], Training Accuracy: 27.1260%, Training Loss: 0.7172%\n",
      "Epoch [2/100], Step [62/225], Training Accuracy: 27.0413%, Training Loss: 0.7171%\n",
      "Epoch [2/100], Step [63/225], Training Accuracy: 26.9345%, Training Loss: 0.7172%\n",
      "Epoch [2/100], Step [64/225], Training Accuracy: 26.9531%, Training Loss: 0.7171%\n",
      "Epoch [2/100], Step [65/225], Training Accuracy: 26.9231%, Training Loss: 0.7171%\n",
      "Epoch [2/100], Step [66/225], Training Accuracy: 26.9176%, Training Loss: 0.7166%\n",
      "Epoch [2/100], Step [67/225], Training Accuracy: 26.8657%, Training Loss: 0.7165%\n",
      "Epoch [2/100], Step [68/225], Training Accuracy: 26.7923%, Training Loss: 0.7164%\n",
      "Epoch [2/100], Step [69/225], Training Accuracy: 26.6984%, Training Loss: 0.7165%\n",
      "Epoch [2/100], Step [70/225], Training Accuracy: 26.6071%, Training Loss: 0.7166%\n",
      "Epoch [2/100], Step [71/225], Training Accuracy: 26.6505%, Training Loss: 0.7168%\n",
      "Epoch [2/100], Step [72/225], Training Accuracy: 26.5625%, Training Loss: 0.7170%\n",
      "Epoch [2/100], Step [73/225], Training Accuracy: 26.6481%, Training Loss: 0.7166%\n",
      "Epoch [2/100], Step [74/225], Training Accuracy: 26.6258%, Training Loss: 0.7163%\n",
      "Epoch [2/100], Step [75/225], Training Accuracy: 26.5833%, Training Loss: 0.7164%\n",
      "Epoch [2/100], Step [76/225], Training Accuracy: 26.5625%, Training Loss: 0.7160%\n",
      "Epoch [2/100], Step [77/225], Training Accuracy: 26.3799%, Training Loss: 0.7162%\n",
      "Epoch [2/100], Step [78/225], Training Accuracy: 26.3021%, Training Loss: 0.7164%\n",
      "Epoch [2/100], Step [79/225], Training Accuracy: 26.2658%, Training Loss: 0.7163%\n",
      "Epoch [2/100], Step [80/225], Training Accuracy: 26.4062%, Training Loss: 0.7161%\n",
      "Epoch [2/100], Step [81/225], Training Accuracy: 26.2924%, Training Loss: 0.7162%\n",
      "Epoch [2/100], Step [82/225], Training Accuracy: 26.3720%, Training Loss: 0.7160%\n",
      "Epoch [2/100], Step [83/225], Training Accuracy: 26.3178%, Training Loss: 0.7158%\n",
      "Epoch [2/100], Step [84/225], Training Accuracy: 26.3207%, Training Loss: 0.7157%\n",
      "Epoch [2/100], Step [85/225], Training Accuracy: 26.2684%, Training Loss: 0.7156%\n",
      "Epoch [2/100], Step [86/225], Training Accuracy: 26.2900%, Training Loss: 0.7155%\n",
      "Epoch [2/100], Step [87/225], Training Accuracy: 26.4368%, Training Loss: 0.7153%\n",
      "Epoch [2/100], Step [88/225], Training Accuracy: 26.3317%, Training Loss: 0.7155%\n",
      "Epoch [2/100], Step [89/225], Training Accuracy: 26.2640%, Training Loss: 0.7154%\n",
      "Epoch [2/100], Step [90/225], Training Accuracy: 26.3715%, Training Loss: 0.7153%\n",
      "Epoch [2/100], Step [91/225], Training Accuracy: 26.2534%, Training Loss: 0.7154%\n",
      "Epoch [2/100], Step [92/225], Training Accuracy: 26.2738%, Training Loss: 0.7154%\n",
      "Epoch [2/100], Step [93/225], Training Accuracy: 26.2433%, Training Loss: 0.7153%\n",
      "Epoch [2/100], Step [94/225], Training Accuracy: 26.2799%, Training Loss: 0.7150%\n",
      "Epoch [2/100], Step [95/225], Training Accuracy: 26.2500%, Training Loss: 0.7149%\n",
      "Epoch [2/100], Step [96/225], Training Accuracy: 26.2044%, Training Loss: 0.7150%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Step [97/225], Training Accuracy: 26.3209%, Training Loss: 0.7149%\n",
      "Epoch [2/100], Step [98/225], Training Accuracy: 26.2436%, Training Loss: 0.7148%\n",
      "Epoch [2/100], Step [99/225], Training Accuracy: 26.3258%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [100/225], Training Accuracy: 26.2500%, Training Loss: 0.7146%\n",
      "Epoch [2/100], Step [101/225], Training Accuracy: 26.3459%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [102/225], Training Accuracy: 26.2868%, Training Loss: 0.7146%\n",
      "Epoch [2/100], Step [103/225], Training Accuracy: 26.3653%, Training Loss: 0.7144%\n",
      "Epoch [2/100], Step [104/225], Training Accuracy: 26.3672%, Training Loss: 0.7144%\n",
      "Epoch [2/100], Step [105/225], Training Accuracy: 26.3393%, Training Loss: 0.7144%\n",
      "Epoch [2/100], Step [106/225], Training Accuracy: 26.2972%, Training Loss: 0.7146%\n",
      "Epoch [2/100], Step [107/225], Training Accuracy: 26.2704%, Training Loss: 0.7144%\n",
      "Epoch [2/100], Step [108/225], Training Accuracy: 26.2731%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [109/225], Training Accuracy: 26.1325%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [110/225], Training Accuracy: 26.1932%, Training Loss: 0.7143%\n",
      "Epoch [2/100], Step [111/225], Training Accuracy: 26.0839%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [112/225], Training Accuracy: 26.0882%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [113/225], Training Accuracy: 26.1200%, Training Loss: 0.7145%\n",
      "Epoch [2/100], Step [114/225], Training Accuracy: 26.1102%, Training Loss: 0.7144%\n",
      "Epoch [2/100], Step [115/225], Training Accuracy: 26.1005%, Training Loss: 0.7141%\n",
      "Epoch [2/100], Step [116/225], Training Accuracy: 26.1180%, Training Loss: 0.7141%\n",
      "Epoch [2/100], Step [117/225], Training Accuracy: 26.1351%, Training Loss: 0.7139%\n",
      "Epoch [2/100], Step [118/225], Training Accuracy: 26.0990%, Training Loss: 0.7140%\n",
      "Epoch [2/100], Step [119/225], Training Accuracy: 26.1292%, Training Loss: 0.7140%\n",
      "Epoch [2/100], Step [120/225], Training Accuracy: 26.1198%, Training Loss: 0.7139%\n",
      "Epoch [2/100], Step [121/225], Training Accuracy: 26.1235%, Training Loss: 0.7138%\n",
      "Epoch [2/100], Step [122/225], Training Accuracy: 26.0630%, Training Loss: 0.7138%\n",
      "Epoch [2/100], Step [123/225], Training Accuracy: 26.0417%, Training Loss: 0.7138%\n",
      "Epoch [2/100], Step [124/225], Training Accuracy: 26.0711%, Training Loss: 0.7137%\n",
      "Epoch [2/100], Step [125/225], Training Accuracy: 26.0375%, Training Loss: 0.7136%\n",
      "Epoch [2/100], Step [126/225], Training Accuracy: 26.0045%, Training Loss: 0.7136%\n",
      "Epoch [2/100], Step [127/225], Training Accuracy: 26.0089%, Training Loss: 0.7135%\n",
      "Epoch [2/100], Step [128/225], Training Accuracy: 26.0498%, Training Loss: 0.7134%\n",
      "Epoch [2/100], Step [129/225], Training Accuracy: 26.1143%, Training Loss: 0.7133%\n",
      "Epoch [2/100], Step [130/225], Training Accuracy: 26.0577%, Training Loss: 0.7133%\n",
      "Epoch [2/100], Step [131/225], Training Accuracy: 26.0019%, Training Loss: 0.7133%\n",
      "Epoch [2/100], Step [132/225], Training Accuracy: 25.9470%, Training Loss: 0.7132%\n",
      "Epoch [2/100], Step [133/225], Training Accuracy: 25.9398%, Training Loss: 0.7132%\n",
      "Epoch [2/100], Step [134/225], Training Accuracy: 25.9445%, Training Loss: 0.7132%\n",
      "Epoch [2/100], Step [135/225], Training Accuracy: 25.9491%, Training Loss: 0.7131%\n",
      "Epoch [2/100], Step [136/225], Training Accuracy: 25.9881%, Training Loss: 0.7130%\n",
      "Epoch [2/100], Step [137/225], Training Accuracy: 26.0265%, Training Loss: 0.7128%\n",
      "Epoch [2/100], Step [138/225], Training Accuracy: 26.0417%, Training Loss: 0.7129%\n",
      "Epoch [2/100], Step [139/225], Training Accuracy: 26.0117%, Training Loss: 0.7128%\n",
      "Epoch [2/100], Step [140/225], Training Accuracy: 26.0156%, Training Loss: 0.7128%\n",
      "Epoch [2/100], Step [141/225], Training Accuracy: 25.9752%, Training Loss: 0.7129%\n",
      "Epoch [2/100], Step [142/225], Training Accuracy: 25.9793%, Training Loss: 0.7128%\n",
      "Epoch [2/100], Step [143/225], Training Accuracy: 25.9943%, Training Loss: 0.7127%\n",
      "Epoch [2/100], Step [144/225], Training Accuracy: 26.0091%, Training Loss: 0.7128%\n",
      "Epoch [2/100], Step [145/225], Training Accuracy: 26.0237%, Training Loss: 0.7126%\n",
      "Epoch [2/100], Step [146/225], Training Accuracy: 26.0060%, Training Loss: 0.7126%\n",
      "Epoch [2/100], Step [147/225], Training Accuracy: 26.0310%, Training Loss: 0.7124%\n",
      "Epoch [2/100], Step [148/225], Training Accuracy: 26.0557%, Training Loss: 0.7123%\n",
      "Epoch [2/100], Step [149/225], Training Accuracy: 26.0487%, Training Loss: 0.7123%\n",
      "Epoch [2/100], Step [150/225], Training Accuracy: 25.9896%, Training Loss: 0.7124%\n",
      "Epoch [2/100], Step [151/225], Training Accuracy: 26.0555%, Training Loss: 0.7123%\n",
      "Epoch [2/100], Step [152/225], Training Accuracy: 26.0177%, Training Loss: 0.7122%\n",
      "Epoch [2/100], Step [153/225], Training Accuracy: 26.0110%, Training Loss: 0.7121%\n",
      "Epoch [2/100], Step [154/225], Training Accuracy: 26.0146%, Training Loss: 0.7121%\n",
      "Epoch [2/100], Step [155/225], Training Accuracy: 26.0181%, Training Loss: 0.7120%\n",
      "Epoch [2/100], Step [156/225], Training Accuracy: 26.0817%, Training Loss: 0.7119%\n",
      "Epoch [2/100], Step [157/225], Training Accuracy: 26.0151%, Training Loss: 0.7119%\n",
      "Epoch [2/100], Step [158/225], Training Accuracy: 26.0087%, Training Loss: 0.7118%\n",
      "Epoch [2/100], Step [159/225], Training Accuracy: 26.0318%, Training Loss: 0.7116%\n",
      "Epoch [2/100], Step [160/225], Training Accuracy: 26.0352%, Training Loss: 0.7116%\n",
      "Epoch [2/100], Step [161/225], Training Accuracy: 26.0481%, Training Loss: 0.7115%\n",
      "Epoch [2/100], Step [162/225], Training Accuracy: 26.0610%, Training Loss: 0.7114%\n",
      "Epoch [2/100], Step [163/225], Training Accuracy: 26.0640%, Training Loss: 0.7113%\n",
      "Epoch [2/100], Step [164/225], Training Accuracy: 26.0575%, Training Loss: 0.7113%\n",
      "Epoch [2/100], Step [165/225], Training Accuracy: 26.0322%, Training Loss: 0.7112%\n",
      "Epoch [2/100], Step [166/225], Training Accuracy: 26.0072%, Training Loss: 0.7111%\n",
      "Epoch [2/100], Step [167/225], Training Accuracy: 26.0573%, Training Loss: 0.7110%\n",
      "Epoch [2/100], Step [168/225], Training Accuracy: 26.0324%, Training Loss: 0.7110%\n",
      "Epoch [2/100], Step [169/225], Training Accuracy: 25.9985%, Training Loss: 0.7110%\n",
      "Epoch [2/100], Step [170/225], Training Accuracy: 25.9375%, Training Loss: 0.7110%\n",
      "Epoch [2/100], Step [171/225], Training Accuracy: 25.9412%, Training Loss: 0.7110%\n",
      "Epoch [2/100], Step [172/225], Training Accuracy: 25.9720%, Training Loss: 0.7108%\n",
      "Epoch [2/100], Step [173/225], Training Accuracy: 26.0387%, Training Loss: 0.7107%\n",
      "Epoch [2/100], Step [174/225], Training Accuracy: 26.0596%, Training Loss: 0.7107%\n",
      "Epoch [2/100], Step [175/225], Training Accuracy: 26.1071%, Training Loss: 0.7106%\n",
      "Epoch [2/100], Step [176/225], Training Accuracy: 26.1097%, Training Loss: 0.7104%\n",
      "Epoch [2/100], Step [177/225], Training Accuracy: 26.1653%, Training Loss: 0.7104%\n",
      "Epoch [2/100], Step [178/225], Training Accuracy: 26.1850%, Training Loss: 0.7102%\n",
      "Epoch [2/100], Step [179/225], Training Accuracy: 26.2308%, Training Loss: 0.7101%\n",
      "Epoch [2/100], Step [180/225], Training Accuracy: 26.2326%, Training Loss: 0.7101%\n",
      "Epoch [2/100], Step [181/225], Training Accuracy: 26.2086%, Training Loss: 0.7100%\n",
      "Epoch [2/100], Step [182/225], Training Accuracy: 26.2534%, Training Loss: 0.7099%\n",
      "Epoch [2/100], Step [183/225], Training Accuracy: 26.2380%, Training Loss: 0.7098%\n",
      "Epoch [2/100], Step [184/225], Training Accuracy: 26.2398%, Training Loss: 0.7097%\n",
      "Epoch [2/100], Step [185/225], Training Accuracy: 26.2078%, Training Loss: 0.7097%\n",
      "Epoch [2/100], Step [186/225], Training Accuracy: 26.2433%, Training Loss: 0.7097%\n",
      "Epoch [2/100], Step [187/225], Training Accuracy: 26.2366%, Training Loss: 0.7096%\n",
      "Epoch [2/100], Step [188/225], Training Accuracy: 26.2633%, Training Loss: 0.7096%\n",
      "Epoch [2/100], Step [189/225], Training Accuracy: 26.2897%, Training Loss: 0.7095%\n",
      "Epoch [2/100], Step [190/225], Training Accuracy: 26.2664%, Training Loss: 0.7094%\n",
      "Epoch [2/100], Step [191/225], Training Accuracy: 26.2680%, Training Loss: 0.7094%\n",
      "Epoch [2/100], Step [192/225], Training Accuracy: 26.2614%, Training Loss: 0.7093%\n",
      "Epoch [2/100], Step [193/225], Training Accuracy: 26.2791%, Training Loss: 0.7093%\n",
      "Epoch [2/100], Step [194/225], Training Accuracy: 26.3128%, Training Loss: 0.7093%\n",
      "Epoch [2/100], Step [195/225], Training Accuracy: 26.3542%, Training Loss: 0.7091%\n",
      "Epoch [2/100], Step [196/225], Training Accuracy: 26.4031%, Training Loss: 0.7091%\n",
      "Epoch [2/100], Step [197/225], Training Accuracy: 26.3959%, Training Loss: 0.7091%\n",
      "Epoch [2/100], Step [198/225], Training Accuracy: 26.4047%, Training Loss: 0.7090%\n",
      "Epoch [2/100], Step [199/225], Training Accuracy: 26.3898%, Training Loss: 0.7089%\n",
      "Epoch [2/100], Step [200/225], Training Accuracy: 26.3906%, Training Loss: 0.7089%\n",
      "Epoch [2/100], Step [201/225], Training Accuracy: 26.3837%, Training Loss: 0.7088%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Step [202/225], Training Accuracy: 26.4078%, Training Loss: 0.7087%\n",
      "Epoch [2/100], Step [203/225], Training Accuracy: 26.4317%, Training Loss: 0.7087%\n",
      "Epoch [2/100], Step [204/225], Training Accuracy: 26.4782%, Training Loss: 0.7085%\n",
      "Epoch [2/100], Step [205/225], Training Accuracy: 26.5091%, Training Loss: 0.7085%\n",
      "Epoch [2/100], Step [206/225], Training Accuracy: 26.5397%, Training Loss: 0.7084%\n",
      "Epoch [2/100], Step [207/225], Training Accuracy: 26.5172%, Training Loss: 0.7084%\n",
      "Epoch [2/100], Step [208/225], Training Accuracy: 26.4949%, Training Loss: 0.7084%\n",
      "Epoch [2/100], Step [209/225], Training Accuracy: 26.5027%, Training Loss: 0.7083%\n",
      "Epoch [2/100], Step [210/225], Training Accuracy: 26.5253%, Training Loss: 0.7082%\n",
      "Epoch [2/100], Step [211/225], Training Accuracy: 26.5255%, Training Loss: 0.7082%\n",
      "Epoch [2/100], Step [212/225], Training Accuracy: 26.5330%, Training Loss: 0.7081%\n",
      "Epoch [2/100], Step [213/225], Training Accuracy: 26.5625%, Training Loss: 0.7081%\n",
      "Epoch [2/100], Step [214/225], Training Accuracy: 26.5625%, Training Loss: 0.7080%\n",
      "Epoch [2/100], Step [215/225], Training Accuracy: 26.5552%, Training Loss: 0.7079%\n",
      "Epoch [2/100], Step [216/225], Training Accuracy: 26.5697%, Training Loss: 0.7079%\n",
      "Epoch [2/100], Step [217/225], Training Accuracy: 26.5769%, Training Loss: 0.7079%\n",
      "Epoch [2/100], Step [218/225], Training Accuracy: 26.5768%, Training Loss: 0.7078%\n",
      "Epoch [2/100], Step [219/225], Training Accuracy: 26.6410%, Training Loss: 0.7077%\n",
      "Epoch [2/100], Step [220/225], Training Accuracy: 26.6335%, Training Loss: 0.7076%\n",
      "Epoch [2/100], Step [221/225], Training Accuracy: 26.6686%, Training Loss: 0.7076%\n",
      "Epoch [2/100], Step [222/225], Training Accuracy: 26.6610%, Training Loss: 0.7075%\n",
      "Epoch [2/100], Step [223/225], Training Accuracy: 26.6816%, Training Loss: 0.7075%\n",
      "Epoch [2/100], Step [224/225], Training Accuracy: 26.6811%, Training Loss: 0.7075%\n",
      "Epoch [2/100], Step [225/225], Training Accuracy: 26.6606%, Training Loss: 0.7074%\n",
      "Epoch [3/100], Step [1/225], Training Accuracy: 29.6875%, Training Loss: 0.6849%\n",
      "Epoch [3/100], Step [2/225], Training Accuracy: 28.1250%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [3/225], Training Accuracy: 28.6458%, Training Loss: 0.6899%\n",
      "Epoch [3/100], Step [4/225], Training Accuracy: 27.7344%, Training Loss: 0.6904%\n",
      "Epoch [3/100], Step [5/225], Training Accuracy: 28.4375%, Training Loss: 0.6922%\n",
      "Epoch [3/100], Step [6/225], Training Accuracy: 28.9062%, Training Loss: 0.6922%\n",
      "Epoch [3/100], Step [7/225], Training Accuracy: 29.9107%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [8/225], Training Accuracy: 29.1016%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [9/225], Training Accuracy: 29.6875%, Training Loss: 0.6926%\n",
      "Epoch [3/100], Step [10/225], Training Accuracy: 28.1250%, Training Loss: 0.6940%\n",
      "Epoch [3/100], Step [11/225], Training Accuracy: 28.5511%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [12/225], Training Accuracy: 27.6042%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [13/225], Training Accuracy: 27.7644%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [14/225], Training Accuracy: 27.5670%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [15/225], Training Accuracy: 27.5000%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [16/225], Training Accuracy: 27.3438%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [17/225], Training Accuracy: 27.2978%, Training Loss: 0.6940%\n",
      "Epoch [3/100], Step [18/225], Training Accuracy: 27.7778%, Training Loss: 0.6938%\n",
      "Epoch [3/100], Step [19/225], Training Accuracy: 27.9605%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [20/225], Training Accuracy: 27.7344%, Training Loss: 0.6937%\n",
      "Epoch [3/100], Step [21/225], Training Accuracy: 27.9762%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [22/225], Training Accuracy: 27.7699%, Training Loss: 0.6939%\n",
      "Epoch [3/100], Step [23/225], Training Accuracy: 27.5136%, Training Loss: 0.6946%\n",
      "Epoch [3/100], Step [24/225], Training Accuracy: 27.6693%, Training Loss: 0.6939%\n",
      "Epoch [3/100], Step [25/225], Training Accuracy: 27.6250%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [26/225], Training Accuracy: 27.7644%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [27/225], Training Accuracy: 28.0093%, Training Loss: 0.6925%\n",
      "Epoch [3/100], Step [28/225], Training Accuracy: 27.9576%, Training Loss: 0.6927%\n",
      "Epoch [3/100], Step [29/225], Training Accuracy: 28.1789%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [30/225], Training Accuracy: 28.1250%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [31/225], Training Accuracy: 28.0746%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [32/225], Training Accuracy: 28.3203%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [33/225], Training Accuracy: 28.4091%, Training Loss: 0.6926%\n",
      "Epoch [3/100], Step [34/225], Training Accuracy: 28.2169%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [35/225], Training Accuracy: 28.0804%, Training Loss: 0.6924%\n",
      "Epoch [3/100], Step [36/225], Training Accuracy: 28.1684%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [37/225], Training Accuracy: 28.3784%, Training Loss: 0.6925%\n",
      "Epoch [3/100], Step [38/225], Training Accuracy: 28.3717%, Training Loss: 0.6925%\n",
      "Epoch [3/100], Step [39/225], Training Accuracy: 28.1250%, Training Loss: 0.6926%\n",
      "Epoch [3/100], Step [40/225], Training Accuracy: 27.8125%, Training Loss: 0.6927%\n",
      "Epoch [3/100], Step [41/225], Training Accuracy: 27.6677%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [42/225], Training Accuracy: 27.6414%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [43/225], Training Accuracy: 27.7253%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [44/225], Training Accuracy: 27.8764%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [45/225], Training Accuracy: 27.8819%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [46/225], Training Accuracy: 27.9212%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [47/225], Training Accuracy: 27.9255%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [48/225], Training Accuracy: 27.9622%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [49/225], Training Accuracy: 28.0293%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [50/225], Training Accuracy: 27.9688%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [51/225], Training Accuracy: 28.1250%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [52/225], Training Accuracy: 28.1250%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [53/225], Training Accuracy: 27.9481%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [54/225], Training Accuracy: 27.7778%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [55/225], Training Accuracy: 27.7273%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [56/225], Training Accuracy: 27.7344%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [57/225], Training Accuracy: 27.7138%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [58/225], Training Accuracy: 27.4784%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [59/225], Training Accuracy: 27.5424%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [60/225], Training Accuracy: 27.4479%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [61/225], Training Accuracy: 27.2541%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [62/225], Training Accuracy: 27.2177%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [63/225], Training Accuracy: 27.1825%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [64/225], Training Accuracy: 27.4170%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [65/225], Training Accuracy: 27.4038%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [66/225], Training Accuracy: 27.4858%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [67/225], Training Accuracy: 27.5886%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [68/225], Training Accuracy: 27.7803%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [69/225], Training Accuracy: 27.7627%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [70/225], Training Accuracy: 27.7009%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [71/225], Training Accuracy: 27.6408%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [72/225], Training Accuracy: 27.5825%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [73/225], Training Accuracy: 27.5257%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [74/225], Training Accuracy: 27.6394%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [75/225], Training Accuracy: 27.6250%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [76/225], Training Accuracy: 27.6521%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [77/225], Training Accuracy: 27.6177%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [78/225], Training Accuracy: 27.6442%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [79/225], Training Accuracy: 27.6305%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [80/225], Training Accuracy: 27.6367%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [81/225], Training Accuracy: 27.6427%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [82/225], Training Accuracy: 27.7630%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [83/225], Training Accuracy: 27.7108%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [84/225], Training Accuracy: 27.8274%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [85/225], Training Accuracy: 27.7757%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [86/225], Training Accuracy: 27.8888%, Training Loss: 0.6933%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Step [87/225], Training Accuracy: 27.8376%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [88/225], Training Accuracy: 27.8232%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [89/225], Training Accuracy: 27.7563%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [90/225], Training Accuracy: 27.8993%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [91/225], Training Accuracy: 27.9533%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [92/225], Training Accuracy: 27.9552%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [93/225], Training Accuracy: 28.1250%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [94/225], Training Accuracy: 28.1250%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [95/225], Training Accuracy: 28.0921%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [96/225], Training Accuracy: 28.1250%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [97/225], Training Accuracy: 28.1894%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [98/225], Training Accuracy: 28.2047%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [99/225], Training Accuracy: 28.2355%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [100/225], Training Accuracy: 28.1875%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [101/225], Training Accuracy: 28.2642%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [102/225], Training Accuracy: 28.2016%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [103/225], Training Accuracy: 28.2615%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [104/225], Training Accuracy: 28.2602%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [105/225], Training Accuracy: 28.2589%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [106/225], Training Accuracy: 28.2577%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [107/225], Training Accuracy: 28.2564%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [108/225], Training Accuracy: 28.1973%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [109/225], Training Accuracy: 28.1823%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [110/225], Training Accuracy: 28.1818%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [111/225], Training Accuracy: 28.1532%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [112/225], Training Accuracy: 28.1110%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [113/225], Training Accuracy: 28.1665%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [114/225], Training Accuracy: 28.1387%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [115/225], Training Accuracy: 28.1250%, Training Loss: 0.6936%\n",
      "Epoch [3/100], Step [116/225], Training Accuracy: 28.1385%, Training Loss: 0.6935%\n",
      "Epoch [3/100], Step [117/225], Training Accuracy: 28.1116%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [118/225], Training Accuracy: 28.0853%, Training Loss: 0.6934%\n",
      "Epoch [3/100], Step [119/225], Training Accuracy: 28.1381%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [120/225], Training Accuracy: 28.1510%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [121/225], Training Accuracy: 28.1767%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [122/225], Training Accuracy: 28.1250%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [123/225], Training Accuracy: 28.0488%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [124/225], Training Accuracy: 28.1250%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [125/225], Training Accuracy: 28.1250%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [126/225], Training Accuracy: 28.1002%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [127/225], Training Accuracy: 28.1250%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [128/225], Training Accuracy: 28.1738%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [129/225], Training Accuracy: 28.1977%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [130/225], Training Accuracy: 28.1851%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [131/225], Training Accuracy: 28.1727%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [132/225], Training Accuracy: 28.1368%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [133/225], Training Accuracy: 28.1133%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [134/225], Training Accuracy: 28.1250%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [135/225], Training Accuracy: 28.1713%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [136/225], Training Accuracy: 28.1480%, Training Loss: 0.6933%\n",
      "Epoch [3/100], Step [137/225], Training Accuracy: 28.2048%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [138/225], Training Accuracy: 28.2609%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [139/225], Training Accuracy: 28.2487%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [140/225], Training Accuracy: 28.1696%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [141/225], Training Accuracy: 28.1139%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [142/225], Training Accuracy: 28.1250%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [143/225], Training Accuracy: 28.0813%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [144/225], Training Accuracy: 28.0382%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [145/225], Training Accuracy: 28.0280%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [146/225], Training Accuracy: 28.0287%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [147/225], Training Accuracy: 28.0400%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [148/225], Training Accuracy: 27.9666%, Training Loss: 0.6932%\n",
      "Epoch [3/100], Step [149/225], Training Accuracy: 28.0096%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [150/225], Training Accuracy: 28.0625%, Training Loss: 0.6931%\n",
      "Epoch [3/100], Step [151/225], Training Accuracy: 28.1457%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [152/225], Training Accuracy: 28.1661%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [153/225], Training Accuracy: 28.1863%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [154/225], Training Accuracy: 28.1960%, Training Loss: 0.6929%\n",
      "Epoch [3/100], Step [155/225], Training Accuracy: 28.1855%, Training Loss: 0.6930%\n",
      "Epoch [3/100], Step [156/225], Training Accuracy: 28.2151%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [157/225], Training Accuracy: 28.2046%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [158/225], Training Accuracy: 28.2338%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [159/225], Training Accuracy: 28.2626%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [160/225], Training Accuracy: 28.2910%, Training Loss: 0.6928%\n",
      "Epoch [3/100], Step [161/225], Training Accuracy: 28.3288%, Training Loss: 0.6927%\n",
      "Epoch [3/100], Step [162/225], Training Accuracy: 28.3083%, Training Loss: 0.6927%\n",
      "Epoch [3/100], Step [163/225], Training Accuracy: 28.3838%, Training Loss: 0.6926%\n",
      "Epoch [3/100], Step [164/225], Training Accuracy: 28.3727%, Training Loss: 0.6925%\n",
      "Epoch [3/100], Step [165/225], Training Accuracy: 28.3239%, Training Loss: 0.6926%\n",
      "Epoch [3/100], Step [166/225], Training Accuracy: 28.3038%, Training Loss: 0.6926%\n",
      "Epoch [3/100], Step [167/225], Training Accuracy: 28.3589%, Training Loss: 0.6925%\n",
      "Epoch [3/100], Step [168/225], Training Accuracy: 28.4040%, Training Loss: 0.6925%\n",
      "Epoch [3/100], Step [169/225], Training Accuracy: 28.3746%, Training Loss: 0.6924%\n",
      "Epoch [3/100], Step [170/225], Training Accuracy: 28.3364%, Training Loss: 0.6924%\n",
      "Epoch [3/100], Step [171/225], Training Accuracy: 28.2986%, Training Loss: 0.6924%\n",
      "Epoch [3/100], Step [172/225], Training Accuracy: 28.2794%, Training Loss: 0.6924%\n",
      "Epoch [3/100], Step [173/225], Training Accuracy: 28.2966%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [174/225], Training Accuracy: 28.2777%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [175/225], Training Accuracy: 28.2500%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [176/225], Training Accuracy: 28.2404%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [177/225], Training Accuracy: 28.2574%, Training Loss: 0.6923%\n",
      "Epoch [3/100], Step [178/225], Training Accuracy: 28.2654%, Training Loss: 0.6922%\n",
      "Epoch [3/100], Step [179/225], Training Accuracy: 28.2559%, Training Loss: 0.6922%\n",
      "Epoch [3/100], Step [180/225], Training Accuracy: 28.2899%, Training Loss: 0.6921%\n",
      "Epoch [3/100], Step [181/225], Training Accuracy: 28.2718%, Training Loss: 0.6921%\n",
      "Epoch [3/100], Step [182/225], Training Accuracy: 28.3225%, Training Loss: 0.6921%\n",
      "Epoch [3/100], Step [183/225], Training Accuracy: 28.2702%, Training Loss: 0.6920%\n",
      "Epoch [3/100], Step [184/225], Training Accuracy: 28.2439%, Training Loss: 0.6920%\n",
      "Epoch [3/100], Step [185/225], Training Accuracy: 28.2432%, Training Loss: 0.6920%\n",
      "Epoch [3/100], Step [186/225], Training Accuracy: 28.2762%, Training Loss: 0.6919%\n",
      "Epoch [3/100], Step [187/225], Training Accuracy: 28.2754%, Training Loss: 0.6918%\n",
      "Epoch [3/100], Step [188/225], Training Accuracy: 28.2995%, Training Loss: 0.6918%\n",
      "Epoch [3/100], Step [189/225], Training Accuracy: 28.3647%, Training Loss: 0.6917%\n",
      "Epoch [3/100], Step [190/225], Training Accuracy: 28.2895%, Training Loss: 0.6918%\n",
      "Epoch [3/100], Step [191/225], Training Accuracy: 28.3295%, Training Loss: 0.6918%\n",
      "Epoch [3/100], Step [192/225], Training Accuracy: 28.3366%, Training Loss: 0.6918%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Step [193/225], Training Accuracy: 28.3760%, Training Loss: 0.6917%\n",
      "Epoch [3/100], Step [194/225], Training Accuracy: 28.3666%, Training Loss: 0.6917%\n",
      "Epoch [3/100], Step [195/225], Training Accuracy: 28.3494%, Training Loss: 0.6917%\n",
      "Epoch [3/100], Step [196/225], Training Accuracy: 28.3323%, Training Loss: 0.6917%\n",
      "Epoch [3/100], Step [197/225], Training Accuracy: 28.3471%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [198/225], Training Accuracy: 28.3854%, Training Loss: 0.6917%\n",
      "Epoch [3/100], Step [199/225], Training Accuracy: 28.3998%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [200/225], Training Accuracy: 28.3438%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [201/225], Training Accuracy: 28.3738%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [202/225], Training Accuracy: 28.3493%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [203/225], Training Accuracy: 28.3328%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [204/225], Training Accuracy: 28.3395%, Training Loss: 0.6915%\n",
      "Epoch [3/100], Step [205/225], Training Accuracy: 28.3232%, Training Loss: 0.6916%\n",
      "Epoch [3/100], Step [206/225], Training Accuracy: 28.3753%, Training Loss: 0.6915%\n",
      "Epoch [3/100], Step [207/225], Training Accuracy: 28.3514%, Training Loss: 0.6915%\n",
      "Epoch [3/100], Step [208/225], Training Accuracy: 28.3954%, Training Loss: 0.6914%\n",
      "Epoch [3/100], Step [209/225], Training Accuracy: 28.4166%, Training Loss: 0.6914%\n",
      "Epoch [3/100], Step [210/225], Training Accuracy: 28.4301%, Training Loss: 0.6914%\n",
      "Epoch [3/100], Step [211/225], Training Accuracy: 28.4582%, Training Loss: 0.6914%\n",
      "Epoch [3/100], Step [212/225], Training Accuracy: 28.4861%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [213/225], Training Accuracy: 28.4698%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [214/225], Training Accuracy: 28.4755%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [215/225], Training Accuracy: 28.4302%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [216/225], Training Accuracy: 28.4216%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [217/225], Training Accuracy: 28.4058%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [218/225], Training Accuracy: 28.3759%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [219/225], Training Accuracy: 28.4175%, Training Loss: 0.6912%\n",
      "Epoch [3/100], Step [220/225], Training Accuracy: 28.4588%, Training Loss: 0.6912%\n",
      "Epoch [3/100], Step [221/225], Training Accuracy: 28.4644%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [222/225], Training Accuracy: 28.4980%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [223/225], Training Accuracy: 28.5034%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [224/225], Training Accuracy: 28.4877%, Training Loss: 0.6913%\n",
      "Epoch [3/100], Step [225/225], Training Accuracy: 28.4950%, Training Loss: 0.6913%\n",
      "Epoch [4/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6867%\n",
      "Epoch [4/100], Step [2/225], Training Accuracy: 38.2812%, Training Loss: 0.6745%\n",
      "Epoch [4/100], Step [3/225], Training Accuracy: 34.3750%, Training Loss: 0.6774%\n",
      "Epoch [4/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6780%\n",
      "Epoch [4/100], Step [5/225], Training Accuracy: 31.8750%, Training Loss: 0.6803%\n",
      "Epoch [4/100], Step [6/225], Training Accuracy: 30.7292%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [7/225], Training Accuracy: 30.3571%, Training Loss: 0.6822%\n",
      "Epoch [4/100], Step [8/225], Training Accuracy: 30.4688%, Training Loss: 0.6816%\n",
      "Epoch [4/100], Step [9/225], Training Accuracy: 30.5556%, Training Loss: 0.6829%\n",
      "Epoch [4/100], Step [10/225], Training Accuracy: 29.6875%, Training Loss: 0.6844%\n",
      "Epoch [4/100], Step [11/225], Training Accuracy: 29.8295%, Training Loss: 0.6847%\n",
      "Epoch [4/100], Step [12/225], Training Accuracy: 29.2969%, Training Loss: 0.6855%\n",
      "Epoch [4/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6861%\n",
      "Epoch [4/100], Step [14/225], Training Accuracy: 30.0223%, Training Loss: 0.6870%\n",
      "Epoch [4/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6869%\n",
      "Epoch [4/100], Step [16/225], Training Accuracy: 30.5664%, Training Loss: 0.6866%\n",
      "Epoch [4/100], Step [17/225], Training Accuracy: 30.7904%, Training Loss: 0.6874%\n",
      "Epoch [4/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6874%\n",
      "Epoch [4/100], Step [19/225], Training Accuracy: 30.7566%, Training Loss: 0.6871%\n",
      "Epoch [4/100], Step [20/225], Training Accuracy: 30.7031%, Training Loss: 0.6869%\n",
      "Epoch [4/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6863%\n",
      "Epoch [4/100], Step [22/225], Training Accuracy: 31.1790%, Training Loss: 0.6862%\n",
      "Epoch [4/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6859%\n",
      "Epoch [4/100], Step [24/225], Training Accuracy: 31.4453%, Training Loss: 0.6860%\n",
      "Epoch [4/100], Step [25/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [4/100], Step [26/225], Training Accuracy: 31.4303%, Training Loss: 0.6849%\n",
      "Epoch [4/100], Step [27/225], Training Accuracy: 31.4236%, Training Loss: 0.6845%\n",
      "Epoch [4/100], Step [28/225], Training Accuracy: 31.4174%, Training Loss: 0.6846%\n",
      "Epoch [4/100], Step [29/225], Training Accuracy: 31.5733%, Training Loss: 0.6842%\n",
      "Epoch [4/100], Step [30/225], Training Accuracy: 31.4583%, Training Loss: 0.6841%\n",
      "Epoch [4/100], Step [31/225], Training Accuracy: 31.4012%, Training Loss: 0.6842%\n",
      "Epoch [4/100], Step [32/225], Training Accuracy: 31.6406%, Training Loss: 0.6841%\n",
      "Epoch [4/100], Step [33/225], Training Accuracy: 31.6761%, Training Loss: 0.6836%\n",
      "Epoch [4/100], Step [34/225], Training Accuracy: 31.5717%, Training Loss: 0.6836%\n",
      "Epoch [4/100], Step [35/225], Training Accuracy: 31.4732%, Training Loss: 0.6839%\n",
      "Epoch [4/100], Step [36/225], Training Accuracy: 31.5538%, Training Loss: 0.6839%\n",
      "Epoch [4/100], Step [37/225], Training Accuracy: 31.5878%, Training Loss: 0.6838%\n",
      "Epoch [4/100], Step [38/225], Training Accuracy: 31.7023%, Training Loss: 0.6836%\n",
      "Epoch [4/100], Step [39/225], Training Accuracy: 31.5304%, Training Loss: 0.6836%\n",
      "Epoch [4/100], Step [40/225], Training Accuracy: 31.4844%, Training Loss: 0.6837%\n",
      "Epoch [4/100], Step [41/225], Training Accuracy: 31.4787%, Training Loss: 0.6837%\n",
      "Epoch [4/100], Step [42/225], Training Accuracy: 31.3244%, Training Loss: 0.6838%\n",
      "Epoch [4/100], Step [43/225], Training Accuracy: 31.3953%, Training Loss: 0.6836%\n",
      "Epoch [4/100], Step [44/225], Training Accuracy: 31.4631%, Training Loss: 0.6833%\n",
      "Epoch [4/100], Step [45/225], Training Accuracy: 31.6667%, Training Loss: 0.6830%\n",
      "Epoch [4/100], Step [46/225], Training Accuracy: 31.5897%, Training Loss: 0.6831%\n",
      "Epoch [4/100], Step [47/225], Training Accuracy: 31.6157%, Training Loss: 0.6828%\n",
      "Epoch [4/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6827%\n",
      "Epoch [4/100], Step [49/225], Training Accuracy: 31.6327%, Training Loss: 0.6825%\n",
      "Epoch [4/100], Step [50/225], Training Accuracy: 31.6875%, Training Loss: 0.6825%\n",
      "Epoch [4/100], Step [51/225], Training Accuracy: 31.8321%, Training Loss: 0.6824%\n",
      "Epoch [4/100], Step [52/225], Training Accuracy: 31.7308%, Training Loss: 0.6825%\n",
      "Epoch [4/100], Step [53/225], Training Accuracy: 31.7807%, Training Loss: 0.6822%\n",
      "Epoch [4/100], Step [54/225], Training Accuracy: 31.7419%, Training Loss: 0.6820%\n",
      "Epoch [4/100], Step [55/225], Training Accuracy: 31.8466%, Training Loss: 0.6820%\n",
      "Epoch [4/100], Step [56/225], Training Accuracy: 31.7801%, Training Loss: 0.6820%\n",
      "Epoch [4/100], Step [57/225], Training Accuracy: 31.8531%, Training Loss: 0.6817%\n",
      "Epoch [4/100], Step [58/225], Training Accuracy: 31.9235%, Training Loss: 0.6813%\n",
      "Epoch [4/100], Step [59/225], Training Accuracy: 32.0445%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [60/225], Training Accuracy: 32.0573%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [61/225], Training Accuracy: 31.9928%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [62/225], Training Accuracy: 31.9304%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [63/225], Training Accuracy: 31.9692%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [64/225], Training Accuracy: 31.9580%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [65/225], Training Accuracy: 31.8750%, Training Loss: 0.6813%\n",
      "Epoch [4/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6814%\n",
      "Epoch [4/100], Step [67/225], Training Accuracy: 31.9729%, Training Loss: 0.6813%\n",
      "Epoch [4/100], Step [68/225], Training Accuracy: 32.0083%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [69/225], Training Accuracy: 31.9067%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [70/225], Training Accuracy: 31.9420%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [71/225], Training Accuracy: 31.9102%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [72/225], Training Accuracy: 31.8576%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [73/225], Training Accuracy: 31.8707%, Training Loss: 0.6810%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Step [74/225], Training Accuracy: 31.9890%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [75/225], Training Accuracy: 31.8958%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [76/225], Training Accuracy: 31.8257%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [77/225], Training Accuracy: 31.7573%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [78/225], Training Accuracy: 31.8109%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [79/225], Training Accuracy: 31.6851%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [80/225], Training Accuracy: 31.7773%, Training Loss: 0.6804%\n",
      "Epoch [4/100], Step [81/225], Training Accuracy: 31.6358%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [82/225], Training Accuracy: 31.6692%, Training Loss: 0.6803%\n",
      "Epoch [4/100], Step [83/225], Training Accuracy: 31.6830%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [84/225], Training Accuracy: 31.7336%, Training Loss: 0.6803%\n",
      "Epoch [4/100], Step [85/225], Training Accuracy: 31.7463%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [86/225], Training Accuracy: 31.7951%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [87/225], Training Accuracy: 31.8606%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [88/225], Training Accuracy: 31.7827%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [89/225], Training Accuracy: 31.6889%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [90/225], Training Accuracy: 31.7014%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [91/225], Training Accuracy: 31.6621%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [92/225], Training Accuracy: 31.6746%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [93/225], Training Accuracy: 31.6868%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [94/225], Training Accuracy: 31.6822%, Training Loss: 0.6804%\n",
      "Epoch [4/100], Step [95/225], Training Accuracy: 31.5789%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [96/225], Training Accuracy: 31.6406%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [97/225], Training Accuracy: 31.6688%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [98/225], Training Accuracy: 31.6964%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [99/225], Training Accuracy: 31.6919%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [100/225], Training Accuracy: 31.6875%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [101/225], Training Accuracy: 31.7605%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [102/225], Training Accuracy: 31.6789%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [103/225], Training Accuracy: 31.6141%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [104/225], Training Accuracy: 31.6256%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [105/225], Training Accuracy: 31.5923%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [106/225], Training Accuracy: 31.5743%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [107/225], Training Accuracy: 31.5421%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [108/225], Training Accuracy: 31.4815%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [109/225], Training Accuracy: 31.4220%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [110/225], Training Accuracy: 31.4062%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [111/225], Training Accuracy: 31.3626%, Training Loss: 0.6813%\n",
      "Epoch [4/100], Step [112/225], Training Accuracy: 31.3756%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [113/225], Training Accuracy: 31.3883%, Training Loss: 0.6813%\n",
      "Epoch [4/100], Step [114/225], Training Accuracy: 31.4008%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [115/225], Training Accuracy: 31.4130%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [116/225], Training Accuracy: 31.4116%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [117/225], Training Accuracy: 31.3568%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [118/225], Training Accuracy: 31.3692%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [119/225], Training Accuracy: 31.3288%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [120/225], Training Accuracy: 31.3932%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [121/225], Training Accuracy: 31.3920%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [122/225], Training Accuracy: 31.4549%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [123/225], Training Accuracy: 31.4278%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [124/225], Training Accuracy: 31.4390%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [125/225], Training Accuracy: 31.2875%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [126/225], Training Accuracy: 31.2500%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [127/225], Training Accuracy: 31.2377%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [128/225], Training Accuracy: 31.2012%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [129/225], Training Accuracy: 31.1410%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [130/225], Training Accuracy: 31.0697%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [131/225], Training Accuracy: 31.0592%, Training Loss: 0.6813%\n",
      "Epoch [4/100], Step [132/225], Training Accuracy: 31.0724%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [133/225], Training Accuracy: 31.0150%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [134/225], Training Accuracy: 31.0868%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [135/225], Training Accuracy: 31.0532%, Training Loss: 0.6812%\n",
      "Epoch [4/100], Step [136/225], Training Accuracy: 31.0662%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [137/225], Training Accuracy: 31.1702%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [138/225], Training Accuracy: 31.1481%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [139/225], Training Accuracy: 31.0926%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [140/225], Training Accuracy: 31.0491%, Training Loss: 0.6811%\n",
      "Epoch [4/100], Step [141/225], Training Accuracy: 31.1281%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [142/225], Training Accuracy: 31.1180%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [143/225], Training Accuracy: 31.1080%, Training Loss: 0.6809%\n",
      "Epoch [4/100], Step [144/225], Training Accuracy: 31.0872%, Training Loss: 0.6810%\n",
      "Epoch [4/100], Step [145/225], Training Accuracy: 31.1746%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [146/225], Training Accuracy: 31.1430%, Training Loss: 0.6808%\n",
      "Epoch [4/100], Step [147/225], Training Accuracy: 31.1224%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [148/225], Training Accuracy: 31.1128%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [149/225], Training Accuracy: 31.0927%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [150/225], Training Accuracy: 31.1042%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [151/225], Training Accuracy: 31.1776%, Training Loss: 0.6804%\n",
      "Epoch [4/100], Step [152/225], Training Accuracy: 31.1883%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [153/225], Training Accuracy: 31.2194%, Training Loss: 0.6803%\n",
      "Epoch [4/100], Step [154/225], Training Accuracy: 31.2094%, Training Loss: 0.6804%\n",
      "Epoch [4/100], Step [155/225], Training Accuracy: 31.1794%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [156/225], Training Accuracy: 31.1899%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [157/225], Training Accuracy: 31.2201%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [158/225], Training Accuracy: 31.1907%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [159/225], Training Accuracy: 31.1517%, Training Loss: 0.6806%\n",
      "Epoch [4/100], Step [160/225], Training Accuracy: 31.1719%, Training Loss: 0.6807%\n",
      "Epoch [4/100], Step [161/225], Training Accuracy: 31.2500%, Training Loss: 0.6805%\n",
      "Epoch [4/100], Step [162/225], Training Accuracy: 31.2789%, Training Loss: 0.6804%\n",
      "Epoch [4/100], Step [163/225], Training Accuracy: 31.3363%, Training Loss: 0.6803%\n",
      "Epoch [4/100], Step [164/225], Training Accuracy: 31.3262%, Training Loss: 0.6803%\n",
      "Epoch [4/100], Step [165/225], Training Accuracy: 31.3352%, Training Loss: 0.6801%\n",
      "Epoch [4/100], Step [166/225], Training Accuracy: 31.3065%, Training Loss: 0.6800%\n",
      "Epoch [4/100], Step [167/225], Training Accuracy: 31.3249%, Training Loss: 0.6799%\n",
      "Epoch [4/100], Step [168/225], Training Accuracy: 31.2779%, Training Loss: 0.6800%\n",
      "Epoch [4/100], Step [169/225], Training Accuracy: 31.2315%, Training Loss: 0.6800%\n",
      "Epoch [4/100], Step [170/225], Training Accuracy: 31.1765%, Training Loss: 0.6801%\n",
      "Epoch [4/100], Step [171/225], Training Accuracy: 31.1769%, Training Loss: 0.6799%\n",
      "Epoch [4/100], Step [172/225], Training Accuracy: 31.2591%, Training Loss: 0.6798%\n",
      "Epoch [4/100], Step [173/225], Training Accuracy: 31.2861%, Training Loss: 0.6798%\n",
      "Epoch [4/100], Step [174/225], Training Accuracy: 31.2859%, Training Loss: 0.6798%\n",
      "Epoch [4/100], Step [175/225], Training Accuracy: 31.3125%, Training Loss: 0.6797%\n",
      "Epoch [4/100], Step [176/225], Training Accuracy: 31.3121%, Training Loss: 0.6797%\n",
      "Epoch [4/100], Step [177/225], Training Accuracy: 31.3294%, Training Loss: 0.6796%\n",
      "Epoch [4/100], Step [178/225], Training Accuracy: 31.3290%, Training Loss: 0.6796%\n",
      "Epoch [4/100], Step [179/225], Training Accuracy: 31.3635%, Training Loss: 0.6796%\n",
      "Epoch [4/100], Step [180/225], Training Accuracy: 31.3889%, Training Loss: 0.6795%\n",
      "Epoch [4/100], Step [181/225], Training Accuracy: 31.4054%, Training Loss: 0.6795%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Step [182/225], Training Accuracy: 31.4646%, Training Loss: 0.6795%\n",
      "Epoch [4/100], Step [183/225], Training Accuracy: 31.4464%, Training Loss: 0.6795%\n",
      "Epoch [4/100], Step [184/225], Training Accuracy: 31.4283%, Training Loss: 0.6795%\n",
      "Epoch [4/100], Step [185/225], Training Accuracy: 31.3682%, Training Loss: 0.6795%\n",
      "Epoch [4/100], Step [186/225], Training Accuracy: 31.3424%, Training Loss: 0.6794%\n",
      "Epoch [4/100], Step [187/225], Training Accuracy: 31.3085%, Training Loss: 0.6794%\n",
      "Epoch [4/100], Step [188/225], Training Accuracy: 31.3664%, Training Loss: 0.6793%\n",
      "Epoch [4/100], Step [189/225], Training Accuracy: 31.3823%, Training Loss: 0.6794%\n",
      "Epoch [4/100], Step [190/225], Training Accuracy: 31.3405%, Training Loss: 0.6795%\n",
      "Epoch [4/100], Step [191/225], Training Accuracy: 31.3482%, Training Loss: 0.6794%\n",
      "Epoch [4/100], Step [192/225], Training Accuracy: 31.3151%, Training Loss: 0.6794%\n",
      "Epoch [4/100], Step [193/225], Training Accuracy: 31.3148%, Training Loss: 0.6793%\n",
      "Epoch [4/100], Step [194/225], Training Accuracy: 31.3547%, Training Loss: 0.6791%\n",
      "Epoch [4/100], Step [195/225], Training Accuracy: 31.3782%, Training Loss: 0.6791%\n",
      "Epoch [4/100], Step [196/225], Training Accuracy: 31.3457%, Training Loss: 0.6790%\n",
      "Epoch [4/100], Step [197/225], Training Accuracy: 31.3531%, Training Loss: 0.6790%\n",
      "Epoch [4/100], Step [198/225], Training Accuracy: 31.3920%, Training Loss: 0.6790%\n",
      "Epoch [4/100], Step [199/225], Training Accuracy: 31.4227%, Training Loss: 0.6790%\n",
      "Epoch [4/100], Step [200/225], Training Accuracy: 31.3906%, Training Loss: 0.6790%\n",
      "Epoch [4/100], Step [201/225], Training Accuracy: 31.3899%, Training Loss: 0.6789%\n",
      "Epoch [4/100], Step [202/225], Training Accuracy: 31.4124%, Training Loss: 0.6789%\n",
      "Epoch [4/100], Step [203/225], Training Accuracy: 31.3962%, Training Loss: 0.6790%\n",
      "Epoch [4/100], Step [204/225], Training Accuracy: 31.4262%, Training Loss: 0.6789%\n",
      "Epoch [4/100], Step [205/225], Training Accuracy: 31.3872%, Training Loss: 0.6789%\n",
      "Epoch [4/100], Step [206/225], Training Accuracy: 31.3941%, Training Loss: 0.6788%\n",
      "Epoch [4/100], Step [207/225], Training Accuracy: 31.3859%, Training Loss: 0.6788%\n",
      "Epoch [4/100], Step [208/225], Training Accuracy: 31.4303%, Training Loss: 0.6786%\n",
      "Epoch [4/100], Step [209/225], Training Accuracy: 31.4743%, Training Loss: 0.6786%\n",
      "Epoch [4/100], Step [210/225], Training Accuracy: 31.5104%, Training Loss: 0.6785%\n",
      "Epoch [4/100], Step [211/225], Training Accuracy: 31.5092%, Training Loss: 0.6785%\n",
      "Epoch [4/100], Step [212/225], Training Accuracy: 31.5301%, Training Loss: 0.6783%\n",
      "Epoch [4/100], Step [213/225], Training Accuracy: 31.5214%, Training Loss: 0.6784%\n",
      "Epoch [4/100], Step [214/225], Training Accuracy: 31.5348%, Training Loss: 0.6783%\n",
      "Epoch [4/100], Step [215/225], Training Accuracy: 31.5189%, Training Loss: 0.6783%\n",
      "Epoch [4/100], Step [216/225], Training Accuracy: 31.4670%, Training Loss: 0.6784%\n",
      "Epoch [4/100], Step [217/225], Training Accuracy: 31.4732%, Training Loss: 0.6783%\n",
      "Epoch [4/100], Step [218/225], Training Accuracy: 31.4435%, Training Loss: 0.6783%\n",
      "Epoch [4/100], Step [219/225], Training Accuracy: 31.4712%, Training Loss: 0.6782%\n",
      "Epoch [4/100], Step [220/225], Training Accuracy: 31.5057%, Training Loss: 0.6781%\n",
      "Epoch [4/100], Step [221/225], Training Accuracy: 31.4904%, Training Loss: 0.6781%\n",
      "Epoch [4/100], Step [222/225], Training Accuracy: 31.4963%, Training Loss: 0.6780%\n",
      "Epoch [4/100], Step [223/225], Training Accuracy: 31.5092%, Training Loss: 0.6780%\n",
      "Epoch [4/100], Step [224/225], Training Accuracy: 31.4941%, Training Loss: 0.6780%\n",
      "Epoch [4/100], Step [225/225], Training Accuracy: 31.4967%, Training Loss: 0.6781%\n",
      "Epoch [5/100], Step [1/225], Training Accuracy: 42.1875%, Training Loss: 0.6738%\n",
      "Epoch [5/100], Step [2/225], Training Accuracy: 35.9375%, Training Loss: 0.6787%\n",
      "Epoch [5/100], Step [3/225], Training Accuracy: 33.8542%, Training Loss: 0.6812%\n",
      "Epoch [5/100], Step [4/225], Training Accuracy: 34.3750%, Training Loss: 0.6787%\n",
      "Epoch [5/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6828%\n",
      "Epoch [5/100], Step [6/225], Training Accuracy: 33.3333%, Training Loss: 0.6838%\n",
      "Epoch [5/100], Step [7/225], Training Accuracy: 33.4821%, Training Loss: 0.6827%\n",
      "Epoch [5/100], Step [8/225], Training Accuracy: 33.2031%, Training Loss: 0.6799%\n",
      "Epoch [5/100], Step [9/225], Training Accuracy: 33.3333%, Training Loss: 0.6822%\n",
      "Epoch [5/100], Step [10/225], Training Accuracy: 33.5938%, Training Loss: 0.6801%\n",
      "Epoch [5/100], Step [11/225], Training Accuracy: 33.5227%, Training Loss: 0.6793%\n",
      "Epoch [5/100], Step [12/225], Training Accuracy: 32.5521%, Training Loss: 0.6791%\n",
      "Epoch [5/100], Step [13/225], Training Accuracy: 32.8125%, Training Loss: 0.6785%\n",
      "Epoch [5/100], Step [14/225], Training Accuracy: 32.5893%, Training Loss: 0.6816%\n",
      "Epoch [5/100], Step [15/225], Training Accuracy: 32.7083%, Training Loss: 0.6815%\n",
      "Epoch [5/100], Step [16/225], Training Accuracy: 32.4219%, Training Loss: 0.6802%\n",
      "Epoch [5/100], Step [17/225], Training Accuracy: 32.1691%, Training Loss: 0.6804%\n",
      "Epoch [5/100], Step [18/225], Training Accuracy: 32.3785%, Training Loss: 0.6801%\n",
      "Epoch [5/100], Step [19/225], Training Accuracy: 32.4836%, Training Loss: 0.6802%\n",
      "Epoch [5/100], Step [20/225], Training Accuracy: 32.6562%, Training Loss: 0.6794%\n",
      "Epoch [5/100], Step [21/225], Training Accuracy: 32.6637%, Training Loss: 0.6787%\n",
      "Epoch [5/100], Step [22/225], Training Accuracy: 32.4574%, Training Loss: 0.6781%\n",
      "Epoch [5/100], Step [23/225], Training Accuracy: 32.6766%, Training Loss: 0.6773%\n",
      "Epoch [5/100], Step [24/225], Training Accuracy: 32.7474%, Training Loss: 0.6771%\n",
      "Epoch [5/100], Step [25/225], Training Accuracy: 32.8125%, Training Loss: 0.6770%\n",
      "Epoch [5/100], Step [26/225], Training Accuracy: 32.8726%, Training Loss: 0.6766%\n",
      "Epoch [5/100], Step [27/225], Training Accuracy: 32.8125%, Training Loss: 0.6759%\n",
      "Epoch [5/100], Step [28/225], Training Accuracy: 32.7009%, Training Loss: 0.6765%\n",
      "Epoch [5/100], Step [29/225], Training Accuracy: 32.8664%, Training Loss: 0.6758%\n",
      "Epoch [5/100], Step [30/225], Training Accuracy: 32.8646%, Training Loss: 0.6755%\n",
      "Epoch [5/100], Step [31/225], Training Accuracy: 32.7621%, Training Loss: 0.6753%\n",
      "Epoch [5/100], Step [32/225], Training Accuracy: 33.3496%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [33/225], Training Accuracy: 33.3807%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [34/225], Training Accuracy: 33.2261%, Training Loss: 0.6733%\n",
      "Epoch [5/100], Step [35/225], Training Accuracy: 32.8571%, Training Loss: 0.6736%\n",
      "Epoch [5/100], Step [36/225], Training Accuracy: 32.8559%, Training Loss: 0.6739%\n",
      "Epoch [5/100], Step [37/225], Training Accuracy: 32.6436%, Training Loss: 0.6744%\n",
      "Epoch [5/100], Step [38/225], Training Accuracy: 32.5247%, Training Loss: 0.6744%\n",
      "Epoch [5/100], Step [39/225], Training Accuracy: 32.4920%, Training Loss: 0.6743%\n",
      "Epoch [5/100], Step [40/225], Training Accuracy: 32.3047%, Training Loss: 0.6749%\n",
      "Epoch [5/100], Step [41/225], Training Accuracy: 32.2027%, Training Loss: 0.6748%\n",
      "Epoch [5/100], Step [42/225], Training Accuracy: 31.9568%, Training Loss: 0.6753%\n",
      "Epoch [5/100], Step [43/225], Training Accuracy: 31.9767%, Training Loss: 0.6751%\n",
      "Epoch [5/100], Step [44/225], Training Accuracy: 31.9957%, Training Loss: 0.6745%\n",
      "Epoch [5/100], Step [45/225], Training Accuracy: 32.1181%, Training Loss: 0.6742%\n",
      "Epoch [5/100], Step [46/225], Training Accuracy: 32.1671%, Training Loss: 0.6748%\n",
      "Epoch [5/100], Step [47/225], Training Accuracy: 32.1809%, Training Loss: 0.6747%\n",
      "Epoch [5/100], Step [48/225], Training Accuracy: 32.2266%, Training Loss: 0.6743%\n",
      "Epoch [5/100], Step [49/225], Training Accuracy: 32.3980%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [50/225], Training Accuracy: 32.4688%, Training Loss: 0.6742%\n",
      "Epoch [5/100], Step [51/225], Training Accuracy: 32.5674%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [52/225], Training Accuracy: 32.6322%, Training Loss: 0.6736%\n",
      "Epoch [5/100], Step [53/225], Training Accuracy: 32.7241%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [54/225], Training Accuracy: 32.6100%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [55/225], Training Accuracy: 32.6705%, Training Loss: 0.6734%\n",
      "Epoch [5/100], Step [56/225], Training Accuracy: 32.6451%, Training Loss: 0.6731%\n",
      "Epoch [5/100], Step [57/225], Training Accuracy: 32.5932%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [58/225], Training Accuracy: 32.5431%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [59/225], Training Accuracy: 32.8125%, Training Loss: 0.6721%\n",
      "Epoch [5/100], Step [60/225], Training Accuracy: 32.8646%, Training Loss: 0.6719%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Step [61/225], Training Accuracy: 32.8893%, Training Loss: 0.6720%\n",
      "Epoch [5/100], Step [62/225], Training Accuracy: 32.8377%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [63/225], Training Accuracy: 32.9365%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [64/225], Training Accuracy: 33.0566%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [65/225], Training Accuracy: 32.9808%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [66/225], Training Accuracy: 32.9545%, Training Loss: 0.6716%\n",
      "Epoch [5/100], Step [67/225], Training Accuracy: 32.9757%, Training Loss: 0.6714%\n",
      "Epoch [5/100], Step [68/225], Training Accuracy: 32.9044%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [69/225], Training Accuracy: 32.9484%, Training Loss: 0.6708%\n",
      "Epoch [5/100], Step [70/225], Training Accuracy: 32.9911%, Training Loss: 0.6707%\n",
      "Epoch [5/100], Step [71/225], Training Accuracy: 32.9665%, Training Loss: 0.6709%\n",
      "Epoch [5/100], Step [72/225], Training Accuracy: 32.8559%, Training Loss: 0.6715%\n",
      "Epoch [5/100], Step [73/225], Training Accuracy: 32.8553%, Training Loss: 0.6717%\n",
      "Epoch [5/100], Step [74/225], Training Accuracy: 33.0025%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [75/225], Training Accuracy: 33.0625%, Training Loss: 0.6712%\n",
      "Epoch [5/100], Step [76/225], Training Accuracy: 32.9359%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [77/225], Training Accuracy: 32.8937%, Training Loss: 0.6716%\n",
      "Epoch [5/100], Step [78/225], Training Accuracy: 32.9127%, Training Loss: 0.6715%\n",
      "Epoch [5/100], Step [79/225], Training Accuracy: 32.8125%, Training Loss: 0.6715%\n",
      "Epoch [5/100], Step [80/225], Training Accuracy: 32.8320%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [81/225], Training Accuracy: 32.6775%, Training Loss: 0.6718%\n",
      "Epoch [5/100], Step [82/225], Training Accuracy: 32.8316%, Training Loss: 0.6718%\n",
      "Epoch [5/100], Step [83/225], Training Accuracy: 32.8125%, Training Loss: 0.6718%\n",
      "Epoch [5/100], Step [84/225], Training Accuracy: 32.9985%, Training Loss: 0.6715%\n",
      "Epoch [5/100], Step [85/225], Training Accuracy: 32.9412%, Training Loss: 0.6720%\n",
      "Epoch [5/100], Step [86/225], Training Accuracy: 32.9033%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [87/225], Training Accuracy: 32.9023%, Training Loss: 0.6721%\n",
      "Epoch [5/100], Step [88/225], Training Accuracy: 32.8480%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [89/225], Training Accuracy: 32.8652%, Training Loss: 0.6721%\n",
      "Epoch [5/100], Step [90/225], Training Accuracy: 32.9514%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [91/225], Training Accuracy: 32.8640%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [92/225], Training Accuracy: 32.9484%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [93/225], Training Accuracy: 32.9973%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [94/225], Training Accuracy: 33.0618%, Training Loss: 0.6720%\n",
      "Epoch [5/100], Step [95/225], Training Accuracy: 32.9441%, Training Loss: 0.6726%\n",
      "Epoch [5/100], Step [96/225], Training Accuracy: 32.9590%, Training Loss: 0.6727%\n",
      "Epoch [5/100], Step [97/225], Training Accuracy: 32.9575%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [98/225], Training Accuracy: 33.0038%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [99/225], Training Accuracy: 33.0177%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [100/225], Training Accuracy: 33.0312%, Training Loss: 0.6729%\n",
      "Epoch [5/100], Step [101/225], Training Accuracy: 33.1374%, Training Loss: 0.6727%\n",
      "Epoch [5/100], Step [102/225], Training Accuracy: 33.0729%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [103/225], Training Accuracy: 33.0097%, Training Loss: 0.6729%\n",
      "Epoch [5/100], Step [104/225], Training Accuracy: 32.9928%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [105/225], Training Accuracy: 32.9315%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [106/225], Training Accuracy: 32.9452%, Training Loss: 0.6727%\n",
      "Epoch [5/100], Step [107/225], Training Accuracy: 32.8125%, Training Loss: 0.6729%\n",
      "Epoch [5/100], Step [108/225], Training Accuracy: 32.7836%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [109/225], Training Accuracy: 32.7408%, Training Loss: 0.6729%\n",
      "Epoch [5/100], Step [110/225], Training Accuracy: 32.6989%, Training Loss: 0.6729%\n",
      "Epoch [5/100], Step [111/225], Training Accuracy: 32.6858%, Training Loss: 0.6731%\n",
      "Epoch [5/100], Step [112/225], Training Accuracy: 32.7427%, Training Loss: 0.6731%\n",
      "Epoch [5/100], Step [113/225], Training Accuracy: 32.6604%, Training Loss: 0.6734%\n",
      "Epoch [5/100], Step [114/225], Training Accuracy: 32.7303%, Training Loss: 0.6732%\n",
      "Epoch [5/100], Step [115/225], Training Accuracy: 32.7174%, Training Loss: 0.6732%\n",
      "Epoch [5/100], Step [116/225], Training Accuracy: 32.7586%, Training Loss: 0.6731%\n",
      "Epoch [5/100], Step [117/225], Training Accuracy: 32.7057%, Training Loss: 0.6732%\n",
      "Epoch [5/100], Step [118/225], Training Accuracy: 32.7066%, Training Loss: 0.6733%\n",
      "Epoch [5/100], Step [119/225], Training Accuracy: 32.6812%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [120/225], Training Accuracy: 32.7604%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [121/225], Training Accuracy: 32.7479%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [122/225], Training Accuracy: 32.7485%, Training Loss: 0.6734%\n",
      "Epoch [5/100], Step [123/225], Training Accuracy: 32.6474%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [124/225], Training Accuracy: 32.6613%, Training Loss: 0.6738%\n",
      "Epoch [5/100], Step [125/225], Training Accuracy: 32.6250%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [126/225], Training Accuracy: 32.6389%, Training Loss: 0.6740%\n",
      "Epoch [5/100], Step [127/225], Training Accuracy: 32.6403%, Training Loss: 0.6740%\n",
      "Epoch [5/100], Step [128/225], Training Accuracy: 32.5928%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [129/225], Training Accuracy: 32.5460%, Training Loss: 0.6742%\n",
      "Epoch [5/100], Step [130/225], Training Accuracy: 32.4519%, Training Loss: 0.6743%\n",
      "Epoch [5/100], Step [131/225], Training Accuracy: 32.4308%, Training Loss: 0.6744%\n",
      "Epoch [5/100], Step [132/225], Training Accuracy: 32.4100%, Training Loss: 0.6743%\n",
      "Epoch [5/100], Step [133/225], Training Accuracy: 32.4366%, Training Loss: 0.6743%\n",
      "Epoch [5/100], Step [134/225], Training Accuracy: 32.4627%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [135/225], Training Accuracy: 32.4537%, Training Loss: 0.6742%\n",
      "Epoch [5/100], Step [136/225], Training Accuracy: 32.4678%, Training Loss: 0.6742%\n",
      "Epoch [5/100], Step [137/225], Training Accuracy: 32.5958%, Training Loss: 0.6740%\n",
      "Epoch [5/100], Step [138/225], Training Accuracy: 32.6087%, Training Loss: 0.6740%\n",
      "Epoch [5/100], Step [139/225], Training Accuracy: 32.5764%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [140/225], Training Accuracy: 32.5446%, Training Loss: 0.6742%\n",
      "Epoch [5/100], Step [141/225], Training Accuracy: 32.5133%, Training Loss: 0.6741%\n",
      "Epoch [5/100], Step [142/225], Training Accuracy: 32.5154%, Training Loss: 0.6739%\n",
      "Epoch [5/100], Step [143/225], Training Accuracy: 32.4628%, Training Loss: 0.6739%\n",
      "Epoch [5/100], Step [144/225], Training Accuracy: 32.4544%, Training Loss: 0.6739%\n",
      "Epoch [5/100], Step [145/225], Training Accuracy: 32.5216%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [146/225], Training Accuracy: 32.5235%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [147/225], Training Accuracy: 32.5255%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [148/225], Training Accuracy: 32.5169%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [149/225], Training Accuracy: 32.4979%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [150/225], Training Accuracy: 32.5208%, Training Loss: 0.6737%\n",
      "Epoch [5/100], Step [151/225], Training Accuracy: 32.5642%, Training Loss: 0.6736%\n",
      "Epoch [5/100], Step [152/225], Training Accuracy: 32.5863%, Training Loss: 0.6736%\n",
      "Epoch [5/100], Step [153/225], Training Accuracy: 32.5980%, Training Loss: 0.6734%\n",
      "Epoch [5/100], Step [154/225], Training Accuracy: 32.5791%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [155/225], Training Accuracy: 32.6310%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [156/225], Training Accuracy: 32.6623%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [157/225], Training Accuracy: 32.6831%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [158/225], Training Accuracy: 32.6543%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [159/225], Training Accuracy: 32.6454%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [160/225], Training Accuracy: 32.6562%, Training Loss: 0.6735%\n",
      "Epoch [5/100], Step [161/225], Training Accuracy: 32.7252%, Training Loss: 0.6733%\n",
      "Epoch [5/100], Step [162/225], Training Accuracy: 32.7257%, Training Loss: 0.6733%\n",
      "Epoch [5/100], Step [163/225], Training Accuracy: 32.7742%, Training Loss: 0.6732%\n",
      "Epoch [5/100], Step [164/225], Training Accuracy: 32.8220%, Training Loss: 0.6731%\n",
      "Epoch [5/100], Step [165/225], Training Accuracy: 32.8314%, Training Loss: 0.6730%\n",
      "Epoch [5/100], Step [166/225], Training Accuracy: 32.7843%, Training Loss: 0.6730%\n",
      "Epoch [5/100], Step [167/225], Training Accuracy: 32.8406%, Training Loss: 0.6728%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Step [168/225], Training Accuracy: 32.7939%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [169/225], Training Accuracy: 32.7663%, Training Loss: 0.6728%\n",
      "Epoch [5/100], Step [170/225], Training Accuracy: 32.7482%, Training Loss: 0.6729%\n",
      "Epoch [5/100], Step [171/225], Training Accuracy: 32.7942%, Training Loss: 0.6727%\n",
      "Epoch [5/100], Step [172/225], Training Accuracy: 32.8307%, Training Loss: 0.6727%\n",
      "Epoch [5/100], Step [173/225], Training Accuracy: 32.8215%, Training Loss: 0.6727%\n",
      "Epoch [5/100], Step [174/225], Training Accuracy: 32.8125%, Training Loss: 0.6726%\n",
      "Epoch [5/100], Step [175/225], Training Accuracy: 32.8571%, Training Loss: 0.6725%\n",
      "Epoch [5/100], Step [176/225], Training Accuracy: 32.8658%, Training Loss: 0.6725%\n",
      "Epoch [5/100], Step [177/225], Training Accuracy: 32.8655%, Training Loss: 0.6725%\n",
      "Epoch [5/100], Step [178/225], Training Accuracy: 32.8476%, Training Loss: 0.6724%\n",
      "Epoch [5/100], Step [179/225], Training Accuracy: 32.8823%, Training Loss: 0.6724%\n",
      "Epoch [5/100], Step [180/225], Training Accuracy: 32.9774%, Training Loss: 0.6723%\n",
      "Epoch [5/100], Step [181/225], Training Accuracy: 32.9506%, Training Loss: 0.6724%\n",
      "Epoch [5/100], Step [182/225], Training Accuracy: 32.9327%, Training Loss: 0.6723%\n",
      "Epoch [5/100], Step [183/225], Training Accuracy: 32.9320%, Training Loss: 0.6723%\n",
      "Epoch [5/100], Step [184/225], Training Accuracy: 32.9059%, Training Loss: 0.6723%\n",
      "Epoch [5/100], Step [185/225], Training Accuracy: 32.9054%, Training Loss: 0.6724%\n",
      "Epoch [5/100], Step [186/225], Training Accuracy: 32.9637%, Training Loss: 0.6722%\n",
      "Epoch [5/100], Step [187/225], Training Accuracy: 32.9545%, Training Loss: 0.6721%\n",
      "Epoch [5/100], Step [188/225], Training Accuracy: 32.9372%, Training Loss: 0.6720%\n",
      "Epoch [5/100], Step [189/225], Training Accuracy: 32.9778%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [190/225], Training Accuracy: 32.9605%, Training Loss: 0.6720%\n",
      "Epoch [5/100], Step [191/225], Training Accuracy: 32.9598%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [192/225], Training Accuracy: 32.9346%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [193/225], Training Accuracy: 32.9582%, Training Loss: 0.6719%\n",
      "Epoch [5/100], Step [194/225], Training Accuracy: 32.9897%, Training Loss: 0.6718%\n",
      "Epoch [5/100], Step [195/225], Training Accuracy: 32.9728%, Training Loss: 0.6717%\n",
      "Epoch [5/100], Step [196/225], Training Accuracy: 32.9799%, Training Loss: 0.6717%\n",
      "Epoch [5/100], Step [197/225], Training Accuracy: 33.0108%, Training Loss: 0.6716%\n",
      "Epoch [5/100], Step [198/225], Training Accuracy: 33.0492%, Training Loss: 0.6715%\n",
      "Epoch [5/100], Step [199/225], Training Accuracy: 33.0873%, Training Loss: 0.6714%\n",
      "Epoch [5/100], Step [200/225], Training Accuracy: 33.0781%, Training Loss: 0.6714%\n",
      "Epoch [5/100], Step [201/225], Training Accuracy: 33.0768%, Training Loss: 0.6712%\n",
      "Epoch [5/100], Step [202/225], Training Accuracy: 33.0910%, Training Loss: 0.6712%\n",
      "Epoch [5/100], Step [203/225], Training Accuracy: 33.0896%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [204/225], Training Accuracy: 33.1342%, Training Loss: 0.6712%\n",
      "Epoch [5/100], Step [205/225], Training Accuracy: 33.1326%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [206/225], Training Accuracy: 33.1159%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [207/225], Training Accuracy: 33.1144%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [208/225], Training Accuracy: 33.1581%, Training Loss: 0.6713%\n",
      "Epoch [5/100], Step [209/225], Training Accuracy: 33.1714%, Training Loss: 0.6712%\n",
      "Epoch [5/100], Step [210/225], Training Accuracy: 33.1994%, Training Loss: 0.6710%\n",
      "Epoch [5/100], Step [211/225], Training Accuracy: 33.2272%, Training Loss: 0.6710%\n",
      "Epoch [5/100], Step [212/225], Training Accuracy: 33.2473%, Training Loss: 0.6708%\n",
      "Epoch [5/100], Step [213/225], Training Accuracy: 33.2306%, Training Loss: 0.6709%\n",
      "Epoch [5/100], Step [214/225], Training Accuracy: 33.2214%, Training Loss: 0.6708%\n",
      "Epoch [5/100], Step [215/225], Training Accuracy: 33.1977%, Training Loss: 0.6708%\n",
      "Epoch [5/100], Step [216/225], Training Accuracy: 33.1308%, Training Loss: 0.6709%\n",
      "Epoch [5/100], Step [217/225], Training Accuracy: 33.1293%, Training Loss: 0.6708%\n",
      "Epoch [5/100], Step [218/225], Training Accuracy: 33.0920%, Training Loss: 0.6708%\n",
      "Epoch [5/100], Step [219/225], Training Accuracy: 33.1336%, Training Loss: 0.6707%\n",
      "Epoch [5/100], Step [220/225], Training Accuracy: 33.1889%, Training Loss: 0.6706%\n",
      "Epoch [5/100], Step [221/225], Training Accuracy: 33.1731%, Training Loss: 0.6706%\n",
      "Epoch [5/100], Step [222/225], Training Accuracy: 33.1926%, Training Loss: 0.6705%\n",
      "Epoch [5/100], Step [223/225], Training Accuracy: 33.1979%, Training Loss: 0.6704%\n",
      "Epoch [5/100], Step [224/225], Training Accuracy: 33.1613%, Training Loss: 0.6705%\n",
      "Epoch [5/100], Step [225/225], Training Accuracy: 33.1295%, Training Loss: 0.6706%\n",
      "Epoch [6/100], Step [1/225], Training Accuracy: 40.6250%, Training Loss: 0.6627%\n",
      "Epoch [6/100], Step [2/225], Training Accuracy: 39.0625%, Training Loss: 0.6620%\n",
      "Epoch [6/100], Step [3/225], Training Accuracy: 35.9375%, Training Loss: 0.6714%\n",
      "Epoch [6/100], Step [4/225], Training Accuracy: 35.1562%, Training Loss: 0.6685%\n",
      "Epoch [6/100], Step [5/225], Training Accuracy: 35.3125%, Training Loss: 0.6728%\n",
      "Epoch [6/100], Step [6/225], Training Accuracy: 34.8958%, Training Loss: 0.6731%\n",
      "Epoch [6/100], Step [7/225], Training Accuracy: 34.3750%, Training Loss: 0.6726%\n",
      "Epoch [6/100], Step [8/225], Training Accuracy: 33.7891%, Training Loss: 0.6718%\n",
      "Epoch [6/100], Step [9/225], Training Accuracy: 33.3333%, Training Loss: 0.6745%\n",
      "Epoch [6/100], Step [10/225], Training Accuracy: 33.9062%, Training Loss: 0.6736%\n",
      "Epoch [6/100], Step [11/225], Training Accuracy: 33.6648%, Training Loss: 0.6736%\n",
      "Epoch [6/100], Step [12/225], Training Accuracy: 32.6823%, Training Loss: 0.6740%\n",
      "Epoch [6/100], Step [13/225], Training Accuracy: 32.8125%, Training Loss: 0.6737%\n",
      "Epoch [6/100], Step [14/225], Training Accuracy: 32.7009%, Training Loss: 0.6760%\n",
      "Epoch [6/100], Step [15/225], Training Accuracy: 32.8125%, Training Loss: 0.6756%\n",
      "Epoch [6/100], Step [16/225], Training Accuracy: 32.7148%, Training Loss: 0.6750%\n",
      "Epoch [6/100], Step [17/225], Training Accuracy: 32.6287%, Training Loss: 0.6754%\n",
      "Epoch [6/100], Step [18/225], Training Accuracy: 32.8125%, Training Loss: 0.6753%\n",
      "Epoch [6/100], Step [19/225], Training Accuracy: 32.5658%, Training Loss: 0.6760%\n",
      "Epoch [6/100], Step [20/225], Training Accuracy: 32.8906%, Training Loss: 0.6751%\n",
      "Epoch [6/100], Step [21/225], Training Accuracy: 33.1845%, Training Loss: 0.6742%\n",
      "Epoch [6/100], Step [22/225], Training Accuracy: 32.9545%, Training Loss: 0.6734%\n",
      "Epoch [6/100], Step [23/225], Training Accuracy: 32.8125%, Training Loss: 0.6724%\n",
      "Epoch [6/100], Step [24/225], Training Accuracy: 32.8776%, Training Loss: 0.6725%\n",
      "Epoch [6/100], Step [25/225], Training Accuracy: 33.0625%, Training Loss: 0.6726%\n",
      "Epoch [6/100], Step [26/225], Training Accuracy: 33.1731%, Training Loss: 0.6724%\n",
      "Epoch [6/100], Step [27/225], Training Accuracy: 33.2176%, Training Loss: 0.6722%\n",
      "Epoch [6/100], Step [28/225], Training Accuracy: 33.0357%, Training Loss: 0.6720%\n",
      "Epoch [6/100], Step [29/225], Training Accuracy: 32.9741%, Training Loss: 0.6716%\n",
      "Epoch [6/100], Step [30/225], Training Accuracy: 33.1250%, Training Loss: 0.6708%\n",
      "Epoch [6/100], Step [31/225], Training Accuracy: 33.3165%, Training Loss: 0.6703%\n",
      "Epoch [6/100], Step [32/225], Training Accuracy: 33.5938%, Training Loss: 0.6691%\n",
      "Epoch [6/100], Step [33/225], Training Accuracy: 33.9489%, Training Loss: 0.6686%\n",
      "Epoch [6/100], Step [34/225], Training Accuracy: 33.9154%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [35/225], Training Accuracy: 33.8839%, Training Loss: 0.6685%\n",
      "Epoch [6/100], Step [36/225], Training Accuracy: 33.7674%, Training Loss: 0.6690%\n",
      "Epoch [6/100], Step [37/225], Training Accuracy: 33.6993%, Training Loss: 0.6694%\n",
      "Epoch [6/100], Step [38/225], Training Accuracy: 33.5526%, Training Loss: 0.6692%\n",
      "Epoch [6/100], Step [39/225], Training Accuracy: 33.6138%, Training Loss: 0.6698%\n",
      "Epoch [6/100], Step [40/225], Training Accuracy: 33.5938%, Training Loss: 0.6702%\n",
      "Epoch [6/100], Step [41/225], Training Accuracy: 33.3460%, Training Loss: 0.6704%\n",
      "Epoch [6/100], Step [42/225], Training Accuracy: 33.2961%, Training Loss: 0.6708%\n",
      "Epoch [6/100], Step [43/225], Training Accuracy: 33.1759%, Training Loss: 0.6708%\n",
      "Epoch [6/100], Step [44/225], Training Accuracy: 33.4517%, Training Loss: 0.6702%\n",
      "Epoch [6/100], Step [45/225], Training Accuracy: 33.5417%, Training Loss: 0.6698%\n",
      "Epoch [6/100], Step [46/225], Training Accuracy: 33.6277%, Training Loss: 0.6697%\n",
      "Epoch [6/100], Step [47/225], Training Accuracy: 33.5106%, Training Loss: 0.6695%\n",
      "Epoch [6/100], Step [48/225], Training Accuracy: 33.6589%, Training Loss: 0.6690%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Step [49/225], Training Accuracy: 33.7691%, Training Loss: 0.6685%\n",
      "Epoch [6/100], Step [50/225], Training Accuracy: 33.8750%, Training Loss: 0.6682%\n",
      "Epoch [6/100], Step [51/225], Training Accuracy: 33.9461%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [52/225], Training Accuracy: 34.0445%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [53/225], Training Accuracy: 33.9033%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [54/225], Training Accuracy: 33.7674%, Training Loss: 0.6675%\n",
      "Epoch [6/100], Step [55/225], Training Accuracy: 33.8920%, Training Loss: 0.6671%\n",
      "Epoch [6/100], Step [56/225], Training Accuracy: 33.9565%, Training Loss: 0.6671%\n",
      "Epoch [6/100], Step [57/225], Training Accuracy: 34.1557%, Training Loss: 0.6668%\n",
      "Epoch [6/100], Step [58/225], Training Accuracy: 34.1325%, Training Loss: 0.6667%\n",
      "Epoch [6/100], Step [59/225], Training Accuracy: 34.4015%, Training Loss: 0.6659%\n",
      "Epoch [6/100], Step [60/225], Training Accuracy: 34.5573%, Training Loss: 0.6656%\n",
      "Epoch [6/100], Step [61/225], Training Accuracy: 34.5031%, Training Loss: 0.6657%\n",
      "Epoch [6/100], Step [62/225], Training Accuracy: 34.5262%, Training Loss: 0.6659%\n",
      "Epoch [6/100], Step [63/225], Training Accuracy: 34.4246%, Training Loss: 0.6661%\n",
      "Epoch [6/100], Step [64/225], Training Accuracy: 34.5703%, Training Loss: 0.6659%\n",
      "Epoch [6/100], Step [65/225], Training Accuracy: 34.6394%, Training Loss: 0.6656%\n",
      "Epoch [6/100], Step [66/225], Training Accuracy: 34.6828%, Training Loss: 0.6654%\n",
      "Epoch [6/100], Step [67/225], Training Accuracy: 34.7015%, Training Loss: 0.6652%\n",
      "Epoch [6/100], Step [68/225], Training Accuracy: 34.6507%, Training Loss: 0.6650%\n",
      "Epoch [6/100], Step [69/225], Training Accuracy: 34.6014%, Training Loss: 0.6646%\n",
      "Epoch [6/100], Step [70/225], Training Accuracy: 34.4866%, Training Loss: 0.6649%\n",
      "Epoch [6/100], Step [71/225], Training Accuracy: 34.5511%, Training Loss: 0.6648%\n",
      "Epoch [6/100], Step [72/225], Training Accuracy: 34.3967%, Training Loss: 0.6656%\n",
      "Epoch [6/100], Step [73/225], Training Accuracy: 34.3536%, Training Loss: 0.6656%\n",
      "Epoch [6/100], Step [74/225], Training Accuracy: 34.3750%, Training Loss: 0.6654%\n",
      "Epoch [6/100], Step [75/225], Training Accuracy: 34.4375%, Training Loss: 0.6649%\n",
      "Epoch [6/100], Step [76/225], Training Accuracy: 34.2928%, Training Loss: 0.6651%\n",
      "Epoch [6/100], Step [77/225], Training Accuracy: 34.2938%, Training Loss: 0.6654%\n",
      "Epoch [6/100], Step [78/225], Training Accuracy: 34.2949%, Training Loss: 0.6654%\n",
      "Epoch [6/100], Step [79/225], Training Accuracy: 34.2959%, Training Loss: 0.6655%\n",
      "Epoch [6/100], Step [80/225], Training Accuracy: 34.3750%, Training Loss: 0.6652%\n",
      "Epoch [6/100], Step [81/225], Training Accuracy: 34.3364%, Training Loss: 0.6653%\n",
      "Epoch [6/100], Step [82/225], Training Accuracy: 34.2988%, Training Loss: 0.6653%\n",
      "Epoch [6/100], Step [83/225], Training Accuracy: 34.2432%, Training Loss: 0.6652%\n",
      "Epoch [6/100], Step [84/225], Training Accuracy: 34.2448%, Training Loss: 0.6652%\n",
      "Epoch [6/100], Step [85/225], Training Accuracy: 34.0809%, Training Loss: 0.6656%\n",
      "Epoch [6/100], Step [86/225], Training Accuracy: 34.1025%, Training Loss: 0.6658%\n",
      "Epoch [6/100], Step [87/225], Training Accuracy: 34.2313%, Training Loss: 0.6658%\n",
      "Epoch [6/100], Step [88/225], Training Accuracy: 34.1619%, Training Loss: 0.6660%\n",
      "Epoch [6/100], Step [89/225], Training Accuracy: 34.1819%, Training Loss: 0.6658%\n",
      "Epoch [6/100], Step [90/225], Training Accuracy: 34.1493%, Training Loss: 0.6660%\n",
      "Epoch [6/100], Step [91/225], Training Accuracy: 34.1518%, Training Loss: 0.6660%\n",
      "Epoch [6/100], Step [92/225], Training Accuracy: 34.1712%, Training Loss: 0.6659%\n",
      "Epoch [6/100], Step [93/225], Training Accuracy: 34.2238%, Training Loss: 0.6660%\n",
      "Epoch [6/100], Step [94/225], Training Accuracy: 34.3584%, Training Loss: 0.6656%\n",
      "Epoch [6/100], Step [95/225], Training Accuracy: 34.3421%, Training Loss: 0.6660%\n",
      "Epoch [6/100], Step [96/225], Training Accuracy: 34.3587%, Training Loss: 0.6661%\n",
      "Epoch [6/100], Step [97/225], Training Accuracy: 34.4394%, Training Loss: 0.6661%\n",
      "Epoch [6/100], Step [98/225], Training Accuracy: 34.3750%, Training Loss: 0.6667%\n",
      "Epoch [6/100], Step [99/225], Training Accuracy: 34.2961%, Training Loss: 0.6667%\n",
      "Epoch [6/100], Step [100/225], Training Accuracy: 34.1875%, Training Loss: 0.6666%\n",
      "Epoch [6/100], Step [101/225], Training Accuracy: 34.2203%, Training Loss: 0.6664%\n",
      "Epoch [6/100], Step [102/225], Training Accuracy: 34.2065%, Training Loss: 0.6665%\n",
      "Epoch [6/100], Step [103/225], Training Accuracy: 34.2385%, Training Loss: 0.6665%\n",
      "Epoch [6/100], Step [104/225], Training Accuracy: 34.2097%, Training Loss: 0.6666%\n",
      "Epoch [6/100], Step [105/225], Training Accuracy: 34.1369%, Training Loss: 0.6666%\n",
      "Epoch [6/100], Step [106/225], Training Accuracy: 34.1834%, Training Loss: 0.6665%\n",
      "Epoch [6/100], Step [107/225], Training Accuracy: 34.1268%, Training Loss: 0.6667%\n",
      "Epoch [6/100], Step [108/225], Training Accuracy: 34.0278%, Training Loss: 0.6667%\n",
      "Epoch [6/100], Step [109/225], Training Accuracy: 33.9736%, Training Loss: 0.6668%\n",
      "Epoch [6/100], Step [110/225], Training Accuracy: 34.0483%, Training Loss: 0.6667%\n",
      "Epoch [6/100], Step [111/225], Training Accuracy: 33.9809%, Training Loss: 0.6669%\n",
      "Epoch [6/100], Step [112/225], Training Accuracy: 34.0262%, Training Loss: 0.6669%\n",
      "Epoch [6/100], Step [113/225], Training Accuracy: 33.9878%, Training Loss: 0.6671%\n",
      "Epoch [6/100], Step [114/225], Training Accuracy: 34.0598%, Training Loss: 0.6669%\n",
      "Epoch [6/100], Step [115/225], Training Accuracy: 34.1033%, Training Loss: 0.6668%\n",
      "Epoch [6/100], Step [116/225], Training Accuracy: 34.0787%, Training Loss: 0.6669%\n",
      "Epoch [6/100], Step [117/225], Training Accuracy: 34.0011%, Training Loss: 0.6672%\n",
      "Epoch [6/100], Step [118/225], Training Accuracy: 34.0042%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [119/225], Training Accuracy: 33.9548%, Training Loss: 0.6674%\n",
      "Epoch [6/100], Step [120/225], Training Accuracy: 33.9714%, Training Loss: 0.6674%\n",
      "Epoch [6/100], Step [121/225], Training Accuracy: 33.9489%, Training Loss: 0.6675%\n",
      "Epoch [6/100], Step [122/225], Training Accuracy: 34.0164%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [123/225], Training Accuracy: 33.9558%, Training Loss: 0.6674%\n",
      "Epoch [6/100], Step [124/225], Training Accuracy: 33.9718%, Training Loss: 0.6675%\n",
      "Epoch [6/100], Step [125/225], Training Accuracy: 33.9125%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [126/225], Training Accuracy: 33.9038%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [127/225], Training Accuracy: 33.8583%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [128/225], Training Accuracy: 33.7524%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [129/225], Training Accuracy: 33.7936%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [130/225], Training Accuracy: 33.7260%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [131/225], Training Accuracy: 33.6951%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [132/225], Training Accuracy: 33.7003%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [133/225], Training Accuracy: 33.7171%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [134/225], Training Accuracy: 33.8036%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [135/225], Training Accuracy: 33.8426%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [136/225], Training Accuracy: 33.8925%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [137/225], Training Accuracy: 33.9188%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [138/225], Training Accuracy: 33.9334%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [139/225], Training Accuracy: 33.9029%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [140/225], Training Accuracy: 33.8951%, Training Loss: 0.6682%\n",
      "Epoch [6/100], Step [141/225], Training Accuracy: 33.9096%, Training Loss: 0.6682%\n",
      "Epoch [6/100], Step [142/225], Training Accuracy: 33.9239%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [143/225], Training Accuracy: 33.9161%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [144/225], Training Accuracy: 33.9627%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [145/225], Training Accuracy: 34.0409%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [146/225], Training Accuracy: 34.0432%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [147/225], Training Accuracy: 34.0136%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [148/225], Training Accuracy: 34.0372%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [149/225], Training Accuracy: 34.0604%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [150/225], Training Accuracy: 34.0625%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [151/225], Training Accuracy: 34.0646%, Training Loss: 0.6679%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Step [152/225], Training Accuracy: 34.0769%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [153/225], Training Accuracy: 34.0788%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [154/225], Training Accuracy: 34.0402%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [155/225], Training Accuracy: 34.0020%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [156/225], Training Accuracy: 34.0044%, Training Loss: 0.6682%\n",
      "Epoch [6/100], Step [157/225], Training Accuracy: 34.0068%, Training Loss: 0.6683%\n",
      "Epoch [6/100], Step [158/225], Training Accuracy: 33.9794%, Training Loss: 0.6684%\n",
      "Epoch [6/100], Step [159/225], Training Accuracy: 33.9721%, Training Loss: 0.6683%\n",
      "Epoch [6/100], Step [160/225], Training Accuracy: 33.9160%, Training Loss: 0.6685%\n",
      "Epoch [6/100], Step [161/225], Training Accuracy: 33.9674%, Training Loss: 0.6684%\n",
      "Epoch [6/100], Step [162/225], Training Accuracy: 33.9892%, Training Loss: 0.6684%\n",
      "Epoch [6/100], Step [163/225], Training Accuracy: 34.0203%, Training Loss: 0.6683%\n",
      "Epoch [6/100], Step [164/225], Training Accuracy: 34.0701%, Training Loss: 0.6682%\n",
      "Epoch [6/100], Step [165/225], Training Accuracy: 34.0625%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [166/225], Training Accuracy: 34.0550%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [167/225], Training Accuracy: 34.1130%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [168/225], Training Accuracy: 34.0495%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [169/225], Training Accuracy: 34.0422%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [170/225], Training Accuracy: 33.9798%, Training Loss: 0.6682%\n",
      "Epoch [6/100], Step [171/225], Training Accuracy: 33.9912%, Training Loss: 0.6681%\n",
      "Epoch [6/100], Step [172/225], Training Accuracy: 34.0025%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [173/225], Training Accuracy: 33.9776%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [174/225], Training Accuracy: 33.9080%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [175/225], Training Accuracy: 33.9375%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [176/225], Training Accuracy: 33.8867%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [177/225], Training Accuracy: 33.8630%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [178/225], Training Accuracy: 33.8659%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [179/225], Training Accuracy: 33.8862%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [180/225], Training Accuracy: 33.9149%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [181/225], Training Accuracy: 33.8657%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [182/225], Training Accuracy: 33.8856%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [183/225], Training Accuracy: 33.8627%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [184/225], Training Accuracy: 33.8230%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [185/225], Training Accuracy: 33.8514%, Training Loss: 0.6680%\n",
      "Epoch [6/100], Step [186/225], Training Accuracy: 33.9466%, Training Loss: 0.6679%\n",
      "Epoch [6/100], Step [187/225], Training Accuracy: 33.9405%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [188/225], Training Accuracy: 33.9844%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [189/225], Training Accuracy: 33.9616%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [190/225], Training Accuracy: 33.9309%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [191/225], Training Accuracy: 33.9169%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [192/225], Training Accuracy: 33.8867%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [193/225], Training Accuracy: 33.8973%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [194/225], Training Accuracy: 33.9562%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [195/225], Training Accuracy: 33.9343%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [196/225], Training Accuracy: 33.9445%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [197/225], Training Accuracy: 33.9070%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [198/225], Training Accuracy: 33.9568%, Training Loss: 0.6675%\n",
      "Epoch [6/100], Step [199/225], Training Accuracy: 33.9432%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [200/225], Training Accuracy: 33.8984%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [201/225], Training Accuracy: 33.9164%, Training Loss: 0.6675%\n",
      "Epoch [6/100], Step [202/225], Training Accuracy: 33.9341%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [203/225], Training Accuracy: 33.9209%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [204/225], Training Accuracy: 33.9384%, Training Loss: 0.6676%\n",
      "Epoch [6/100], Step [205/225], Training Accuracy: 33.9024%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [206/225], Training Accuracy: 33.8896%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [207/225], Training Accuracy: 33.8844%, Training Loss: 0.6678%\n",
      "Epoch [6/100], Step [208/225], Training Accuracy: 33.9318%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [209/225], Training Accuracy: 33.9489%, Training Loss: 0.6677%\n",
      "Epoch [6/100], Step [210/225], Training Accuracy: 33.9658%, Training Loss: 0.6675%\n",
      "Epoch [6/100], Step [211/225], Training Accuracy: 33.9973%, Training Loss: 0.6674%\n",
      "Epoch [6/100], Step [212/225], Training Accuracy: 33.9844%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [213/225], Training Accuracy: 33.9642%, Training Loss: 0.6674%\n",
      "Epoch [6/100], Step [214/225], Training Accuracy: 33.9734%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [215/225], Training Accuracy: 33.9680%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [216/225], Training Accuracy: 33.9120%, Training Loss: 0.6674%\n",
      "Epoch [6/100], Step [217/225], Training Accuracy: 33.9502%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [218/225], Training Accuracy: 33.9306%, Training Loss: 0.6673%\n",
      "Epoch [6/100], Step [219/225], Training Accuracy: 33.9683%, Training Loss: 0.6672%\n",
      "Epoch [6/100], Step [220/225], Training Accuracy: 34.0057%, Training Loss: 0.6671%\n",
      "Epoch [6/100], Step [221/225], Training Accuracy: 34.0003%, Training Loss: 0.6671%\n",
      "Epoch [6/100], Step [222/225], Training Accuracy: 34.0090%, Training Loss: 0.6670%\n",
      "Epoch [6/100], Step [223/225], Training Accuracy: 34.0177%, Training Loss: 0.6669%\n",
      "Epoch [6/100], Step [224/225], Training Accuracy: 34.0193%, Training Loss: 0.6669%\n",
      "Epoch [6/100], Step [225/225], Training Accuracy: 33.9911%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [1/225], Training Accuracy: 43.7500%, Training Loss: 0.6765%\n",
      "Epoch [7/100], Step [2/225], Training Accuracy: 39.8438%, Training Loss: 0.6798%\n",
      "Epoch [7/100], Step [3/225], Training Accuracy: 34.3750%, Training Loss: 0.6905%\n",
      "Epoch [7/100], Step [4/225], Training Accuracy: 33.5938%, Training Loss: 0.6823%\n",
      "Epoch [7/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6759%\n",
      "Epoch [7/100], Step [6/225], Training Accuracy: 33.3333%, Training Loss: 0.6762%\n",
      "Epoch [7/100], Step [7/225], Training Accuracy: 33.4821%, Training Loss: 0.6719%\n",
      "Epoch [7/100], Step [8/225], Training Accuracy: 33.2031%, Training Loss: 0.6706%\n",
      "Epoch [7/100], Step [9/225], Training Accuracy: 33.5069%, Training Loss: 0.6708%\n",
      "Epoch [7/100], Step [10/225], Training Accuracy: 34.2188%, Training Loss: 0.6694%\n",
      "Epoch [7/100], Step [11/225], Training Accuracy: 33.8068%, Training Loss: 0.6700%\n",
      "Epoch [7/100], Step [12/225], Training Accuracy: 33.7240%, Training Loss: 0.6695%\n",
      "Epoch [7/100], Step [13/225], Training Accuracy: 34.1346%, Training Loss: 0.6691%\n",
      "Epoch [7/100], Step [14/225], Training Accuracy: 33.5938%, Training Loss: 0.6720%\n",
      "Epoch [7/100], Step [15/225], Training Accuracy: 34.0625%, Training Loss: 0.6713%\n",
      "Epoch [7/100], Step [16/225], Training Accuracy: 34.3750%, Training Loss: 0.6698%\n",
      "Epoch [7/100], Step [17/225], Training Accuracy: 34.1912%, Training Loss: 0.6699%\n",
      "Epoch [7/100], Step [18/225], Training Accuracy: 33.8542%, Training Loss: 0.6701%\n",
      "Epoch [7/100], Step [19/225], Training Accuracy: 33.7993%, Training Loss: 0.6708%\n",
      "Epoch [7/100], Step [20/225], Training Accuracy: 33.8281%, Training Loss: 0.6699%\n",
      "Epoch [7/100], Step [21/225], Training Accuracy: 34.0030%, Training Loss: 0.6695%\n",
      "Epoch [7/100], Step [22/225], Training Accuracy: 34.3040%, Training Loss: 0.6689%\n",
      "Epoch [7/100], Step [23/225], Training Accuracy: 34.3750%, Training Loss: 0.6678%\n",
      "Epoch [7/100], Step [24/225], Training Accuracy: 34.1797%, Training Loss: 0.6679%\n",
      "Epoch [7/100], Step [25/225], Training Accuracy: 34.0625%, Training Loss: 0.6680%\n",
      "Epoch [7/100], Step [26/225], Training Accuracy: 34.3149%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [27/225], Training Accuracy: 34.4329%, Training Loss: 0.6669%\n",
      "Epoch [7/100], Step [28/225], Training Accuracy: 34.6540%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [29/225], Training Accuracy: 34.5905%, Training Loss: 0.6666%\n",
      "Epoch [7/100], Step [30/225], Training Accuracy: 34.6875%, Training Loss: 0.6666%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Step [31/225], Training Accuracy: 34.5766%, Training Loss: 0.6664%\n",
      "Epoch [7/100], Step [32/225], Training Accuracy: 34.8633%, Training Loss: 0.6656%\n",
      "Epoch [7/100], Step [33/225], Training Accuracy: 34.9905%, Training Loss: 0.6650%\n",
      "Epoch [7/100], Step [34/225], Training Accuracy: 34.8346%, Training Loss: 0.6649%\n",
      "Epoch [7/100], Step [35/225], Training Accuracy: 34.9107%, Training Loss: 0.6654%\n",
      "Epoch [7/100], Step [36/225], Training Accuracy: 34.6788%, Training Loss: 0.6662%\n",
      "Epoch [7/100], Step [37/225], Training Accuracy: 34.7128%, Training Loss: 0.6668%\n",
      "Epoch [7/100], Step [38/225], Training Accuracy: 34.7862%, Training Loss: 0.6668%\n",
      "Epoch [7/100], Step [39/225], Training Accuracy: 34.7756%, Training Loss: 0.6667%\n",
      "Epoch [7/100], Step [40/225], Training Accuracy: 34.5703%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [41/225], Training Accuracy: 34.6037%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [42/225], Training Accuracy: 34.3006%, Training Loss: 0.6676%\n",
      "Epoch [7/100], Step [43/225], Training Accuracy: 34.1933%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [44/225], Training Accuracy: 34.2330%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [45/225], Training Accuracy: 34.3056%, Training Loss: 0.6665%\n",
      "Epoch [7/100], Step [46/225], Training Accuracy: 34.4429%, Training Loss: 0.6662%\n",
      "Epoch [7/100], Step [47/225], Training Accuracy: 34.6077%, Training Loss: 0.6659%\n",
      "Epoch [7/100], Step [48/225], Training Accuracy: 34.5378%, Training Loss: 0.6654%\n",
      "Epoch [7/100], Step [49/225], Training Accuracy: 34.7258%, Training Loss: 0.6648%\n",
      "Epoch [7/100], Step [50/225], Training Accuracy: 34.7812%, Training Loss: 0.6649%\n",
      "Epoch [7/100], Step [51/225], Training Accuracy: 34.8652%, Training Loss: 0.6646%\n",
      "Epoch [7/100], Step [52/225], Training Accuracy: 34.8558%, Training Loss: 0.6644%\n",
      "Epoch [7/100], Step [53/225], Training Accuracy: 34.6403%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [54/225], Training Accuracy: 34.5775%, Training Loss: 0.6653%\n",
      "Epoch [7/100], Step [55/225], Training Accuracy: 34.6307%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [56/225], Training Accuracy: 34.4866%, Training Loss: 0.6651%\n",
      "Epoch [7/100], Step [57/225], Training Accuracy: 34.6217%, Training Loss: 0.6648%\n",
      "Epoch [7/100], Step [58/225], Training Accuracy: 34.6175%, Training Loss: 0.6648%\n",
      "Epoch [7/100], Step [59/225], Training Accuracy: 34.7722%, Training Loss: 0.6642%\n",
      "Epoch [7/100], Step [60/225], Training Accuracy: 34.7656%, Training Loss: 0.6639%\n",
      "Epoch [7/100], Step [61/225], Training Accuracy: 34.6055%, Training Loss: 0.6641%\n",
      "Epoch [7/100], Step [62/225], Training Accuracy: 34.5262%, Training Loss: 0.6641%\n",
      "Epoch [7/100], Step [63/225], Training Accuracy: 34.4494%, Training Loss: 0.6641%\n",
      "Epoch [7/100], Step [64/225], Training Accuracy: 34.4971%, Training Loss: 0.6642%\n",
      "Epoch [7/100], Step [65/225], Training Accuracy: 34.5913%, Training Loss: 0.6640%\n",
      "Epoch [7/100], Step [66/225], Training Accuracy: 34.7301%, Training Loss: 0.6637%\n",
      "Epoch [7/100], Step [67/225], Training Accuracy: 34.7248%, Training Loss: 0.6634%\n",
      "Epoch [7/100], Step [68/225], Training Accuracy: 34.6967%, Training Loss: 0.6634%\n",
      "Epoch [7/100], Step [69/225], Training Accuracy: 34.5788%, Training Loss: 0.6630%\n",
      "Epoch [7/100], Step [70/225], Training Accuracy: 34.4866%, Training Loss: 0.6633%\n",
      "Epoch [7/100], Step [71/225], Training Accuracy: 34.4850%, Training Loss: 0.6636%\n",
      "Epoch [7/100], Step [72/225], Training Accuracy: 34.3316%, Training Loss: 0.6642%\n",
      "Epoch [7/100], Step [73/225], Training Accuracy: 34.3322%, Training Loss: 0.6643%\n",
      "Epoch [7/100], Step [74/225], Training Accuracy: 34.4595%, Training Loss: 0.6640%\n",
      "Epoch [7/100], Step [75/225], Training Accuracy: 34.4167%, Training Loss: 0.6639%\n",
      "Epoch [7/100], Step [76/225], Training Accuracy: 34.2722%, Training Loss: 0.6640%\n",
      "Epoch [7/100], Step [77/225], Training Accuracy: 34.2330%, Training Loss: 0.6641%\n",
      "Epoch [7/100], Step [78/225], Training Accuracy: 34.1947%, Training Loss: 0.6641%\n",
      "Epoch [7/100], Step [79/225], Training Accuracy: 34.1772%, Training Loss: 0.6646%\n",
      "Epoch [7/100], Step [80/225], Training Accuracy: 34.2188%, Training Loss: 0.6644%\n",
      "Epoch [7/100], Step [81/225], Training Accuracy: 34.1242%, Training Loss: 0.6646%\n",
      "Epoch [7/100], Step [82/225], Training Accuracy: 34.1845%, Training Loss: 0.6647%\n",
      "Epoch [7/100], Step [83/225], Training Accuracy: 34.1867%, Training Loss: 0.6647%\n",
      "Epoch [7/100], Step [84/225], Training Accuracy: 34.1518%, Training Loss: 0.6647%\n",
      "Epoch [7/100], Step [85/225], Training Accuracy: 34.1728%, Training Loss: 0.6650%\n",
      "Epoch [7/100], Step [86/225], Training Accuracy: 34.1751%, Training Loss: 0.6651%\n",
      "Epoch [7/100], Step [87/225], Training Accuracy: 34.1415%, Training Loss: 0.6650%\n",
      "Epoch [7/100], Step [88/225], Training Accuracy: 34.1442%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [89/225], Training Accuracy: 34.0765%, Training Loss: 0.6653%\n",
      "Epoch [7/100], Step [90/225], Training Accuracy: 34.1493%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [91/225], Training Accuracy: 34.2205%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [92/225], Training Accuracy: 34.2052%, Training Loss: 0.6651%\n",
      "Epoch [7/100], Step [93/225], Training Accuracy: 34.1566%, Training Loss: 0.6653%\n",
      "Epoch [7/100], Step [94/225], Training Accuracy: 34.2753%, Training Loss: 0.6649%\n",
      "Epoch [7/100], Step [95/225], Training Accuracy: 34.2105%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [96/225], Training Accuracy: 34.2285%, Training Loss: 0.6654%\n",
      "Epoch [7/100], Step [97/225], Training Accuracy: 34.1978%, Training Loss: 0.6654%\n",
      "Epoch [7/100], Step [98/225], Training Accuracy: 34.2315%, Training Loss: 0.6655%\n",
      "Epoch [7/100], Step [99/225], Training Accuracy: 34.2803%, Training Loss: 0.6655%\n",
      "Epoch [7/100], Step [100/225], Training Accuracy: 34.2656%, Training Loss: 0.6654%\n",
      "Epoch [7/100], Step [101/225], Training Accuracy: 34.2822%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [102/225], Training Accuracy: 34.1605%, Training Loss: 0.6652%\n",
      "Epoch [7/100], Step [103/225], Training Accuracy: 34.0564%, Training Loss: 0.6655%\n",
      "Epoch [7/100], Step [104/225], Training Accuracy: 33.9844%, Training Loss: 0.6656%\n",
      "Epoch [7/100], Step [105/225], Training Accuracy: 33.9286%, Training Loss: 0.6658%\n",
      "Epoch [7/100], Step [106/225], Training Accuracy: 33.9475%, Training Loss: 0.6657%\n",
      "Epoch [7/100], Step [107/225], Training Accuracy: 33.8639%, Training Loss: 0.6657%\n",
      "Epoch [7/100], Step [108/225], Training Accuracy: 33.8252%, Training Loss: 0.6658%\n",
      "Epoch [7/100], Step [109/225], Training Accuracy: 33.7873%, Training Loss: 0.6659%\n",
      "Epoch [7/100], Step [110/225], Training Accuracy: 33.8494%, Training Loss: 0.6658%\n",
      "Epoch [7/100], Step [111/225], Training Accuracy: 33.7838%, Training Loss: 0.6659%\n",
      "Epoch [7/100], Step [112/225], Training Accuracy: 33.8030%, Training Loss: 0.6658%\n",
      "Epoch [7/100], Step [113/225], Training Accuracy: 33.7666%, Training Loss: 0.6661%\n",
      "Epoch [7/100], Step [114/225], Training Accuracy: 33.8130%, Training Loss: 0.6658%\n",
      "Epoch [7/100], Step [115/225], Training Accuracy: 33.8315%, Training Loss: 0.6658%\n",
      "Epoch [7/100], Step [116/225], Training Accuracy: 33.8362%, Training Loss: 0.6657%\n",
      "Epoch [7/100], Step [117/225], Training Accuracy: 33.7740%, Training Loss: 0.6660%\n",
      "Epoch [7/100], Step [118/225], Training Accuracy: 33.7791%, Training Loss: 0.6659%\n",
      "Epoch [7/100], Step [119/225], Training Accuracy: 33.7447%, Training Loss: 0.6661%\n",
      "Epoch [7/100], Step [120/225], Training Accuracy: 33.7630%, Training Loss: 0.6661%\n",
      "Epoch [7/100], Step [121/225], Training Accuracy: 33.7164%, Training Loss: 0.6662%\n",
      "Epoch [7/100], Step [122/225], Training Accuracy: 33.7731%, Training Loss: 0.6661%\n",
      "Epoch [7/100], Step [123/225], Training Accuracy: 33.7017%, Training Loss: 0.6662%\n",
      "Epoch [7/100], Step [124/225], Training Accuracy: 33.6820%, Training Loss: 0.6663%\n",
      "Epoch [7/100], Step [125/225], Training Accuracy: 33.6250%, Training Loss: 0.6666%\n",
      "Epoch [7/100], Step [126/225], Training Accuracy: 33.6434%, Training Loss: 0.6666%\n",
      "Epoch [7/100], Step [127/225], Training Accuracy: 33.6245%, Training Loss: 0.6666%\n",
      "Epoch [7/100], Step [128/225], Training Accuracy: 33.5571%, Training Loss: 0.6667%\n",
      "Epoch [7/100], Step [129/225], Training Accuracy: 33.5514%, Training Loss: 0.6668%\n",
      "Epoch [7/100], Step [130/225], Training Accuracy: 33.4976%, Training Loss: 0.6668%\n",
      "Epoch [7/100], Step [131/225], Training Accuracy: 33.4685%, Training Loss: 0.6667%\n",
      "Epoch [7/100], Step [132/225], Training Accuracy: 33.4399%, Training Loss: 0.6668%\n",
      "Epoch [7/100], Step [133/225], Training Accuracy: 33.4586%, Training Loss: 0.6670%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Step [134/225], Training Accuracy: 33.4422%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [135/225], Training Accuracy: 33.4722%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [136/225], Training Accuracy: 33.4903%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [137/225], Training Accuracy: 33.5652%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [138/225], Training Accuracy: 33.6504%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [139/225], Training Accuracy: 33.6331%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [140/225], Training Accuracy: 33.6719%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [141/225], Training Accuracy: 33.7766%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [142/225], Training Accuracy: 33.7588%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [143/225], Training Accuracy: 33.7740%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [144/225], Training Accuracy: 33.7565%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [145/225], Training Accuracy: 33.8254%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [146/225], Training Accuracy: 33.8827%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [147/225], Training Accuracy: 33.8648%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [148/225], Training Accuracy: 33.8894%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [149/225], Training Accuracy: 33.8926%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [150/225], Training Accuracy: 33.8854%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [151/225], Training Accuracy: 33.9507%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [152/225], Training Accuracy: 33.9433%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [153/225], Training Accuracy: 33.9154%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [154/225], Training Accuracy: 33.9184%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [155/225], Training Accuracy: 33.9012%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [156/225], Training Accuracy: 33.8842%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [157/225], Training Accuracy: 33.8973%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [158/225], Training Accuracy: 33.9102%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [159/225], Training Accuracy: 33.8738%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [160/225], Training Accuracy: 33.8281%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [161/225], Training Accuracy: 33.9092%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [162/225], Training Accuracy: 33.9410%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [163/225], Training Accuracy: 33.9916%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [164/225], Training Accuracy: 34.0320%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [165/225], Training Accuracy: 34.0057%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [166/225], Training Accuracy: 34.0173%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [167/225], Training Accuracy: 34.0475%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [168/225], Training Accuracy: 34.0309%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [169/225], Training Accuracy: 34.0144%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [170/225], Training Accuracy: 33.9614%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [171/225], Training Accuracy: 33.9638%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [172/225], Training Accuracy: 33.9480%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [173/225], Training Accuracy: 33.9595%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [174/225], Training Accuracy: 33.9440%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [175/225], Training Accuracy: 33.9375%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [176/225], Training Accuracy: 33.8867%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [177/225], Training Accuracy: 33.8630%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [178/225], Training Accuracy: 33.8483%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [179/225], Training Accuracy: 33.8251%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [180/225], Training Accuracy: 33.8542%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [181/225], Training Accuracy: 33.8570%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [182/225], Training Accuracy: 33.8170%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [183/225], Training Accuracy: 33.8029%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [184/225], Training Accuracy: 33.7721%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [185/225], Training Accuracy: 33.7416%, Training Loss: 0.6677%\n",
      "Epoch [7/100], Step [186/225], Training Accuracy: 33.8038%, Training Loss: 0.6676%\n",
      "Epoch [7/100], Step [187/225], Training Accuracy: 33.7650%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [188/225], Training Accuracy: 33.7766%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [189/225], Training Accuracy: 33.7715%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [190/225], Training Accuracy: 33.7747%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [191/225], Training Accuracy: 33.7533%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [192/225], Training Accuracy: 33.7240%, Training Loss: 0.6676%\n",
      "Epoch [7/100], Step [193/225], Training Accuracy: 33.7273%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [194/225], Training Accuracy: 33.7709%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [195/225], Training Accuracy: 33.7340%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [196/225], Training Accuracy: 33.7133%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [197/225], Training Accuracy: 33.6770%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [198/225], Training Accuracy: 33.7121%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [199/225], Training Accuracy: 33.7312%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [200/225], Training Accuracy: 33.7266%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [201/225], Training Accuracy: 33.7764%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [202/225], Training Accuracy: 33.7794%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [203/225], Training Accuracy: 33.7592%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [204/225], Training Accuracy: 33.7776%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [205/225], Training Accuracy: 33.7424%, Training Loss: 0.6676%\n",
      "Epoch [7/100], Step [206/225], Training Accuracy: 33.7454%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [207/225], Training Accuracy: 33.7636%, Training Loss: 0.6676%\n",
      "Epoch [7/100], Step [208/225], Training Accuracy: 33.7966%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [209/225], Training Accuracy: 33.8143%, Training Loss: 0.6675%\n",
      "Epoch [7/100], Step [210/225], Training Accuracy: 33.8095%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [211/225], Training Accuracy: 33.8270%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [212/225], Training Accuracy: 33.8149%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [213/225], Training Accuracy: 33.7735%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [214/225], Training Accuracy: 33.8274%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [215/225], Training Accuracy: 33.8081%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [216/225], Training Accuracy: 33.7746%, Training Loss: 0.6674%\n",
      "Epoch [7/100], Step [217/225], Training Accuracy: 33.7774%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [218/225], Training Accuracy: 33.7371%, Training Loss: 0.6673%\n",
      "Epoch [7/100], Step [219/225], Training Accuracy: 33.7757%, Training Loss: 0.6672%\n",
      "Epoch [7/100], Step [220/225], Training Accuracy: 33.8139%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [221/225], Training Accuracy: 33.8023%, Training Loss: 0.6671%\n",
      "Epoch [7/100], Step [222/225], Training Accuracy: 33.7979%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [223/225], Training Accuracy: 33.8145%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [224/225], Training Accuracy: 33.7960%, Training Loss: 0.6670%\n",
      "Epoch [7/100], Step [225/225], Training Accuracy: 33.7688%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [1/225], Training Accuracy: 43.7500%, Training Loss: 0.6607%\n",
      "Epoch [8/100], Step [2/225], Training Accuracy: 38.2812%, Training Loss: 0.6648%\n",
      "Epoch [8/100], Step [3/225], Training Accuracy: 35.9375%, Training Loss: 0.6721%\n",
      "Epoch [8/100], Step [4/225], Training Accuracy: 37.1094%, Training Loss: 0.6662%\n",
      "Epoch [8/100], Step [5/225], Training Accuracy: 38.4375%, Training Loss: 0.6653%\n",
      "Epoch [8/100], Step [6/225], Training Accuracy: 38.2812%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [7/225], Training Accuracy: 37.5000%, Training Loss: 0.6668%\n",
      "Epoch [8/100], Step [8/225], Training Accuracy: 36.5234%, Training Loss: 0.6650%\n",
      "Epoch [8/100], Step [9/225], Training Accuracy: 35.2431%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [10/225], Training Accuracy: 35.3125%, Training Loss: 0.6666%\n",
      "Epoch [8/100], Step [11/225], Training Accuracy: 34.5170%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [12/225], Training Accuracy: 34.5052%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [13/225], Training Accuracy: 34.4952%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [14/225], Training Accuracy: 34.0402%, Training Loss: 0.6712%\n",
      "Epoch [8/100], Step [15/225], Training Accuracy: 33.8542%, Training Loss: 0.6713%\n",
      "Epoch [8/100], Step [16/225], Training Accuracy: 33.7891%, Training Loss: 0.6706%\n",
      "Epoch [8/100], Step [17/225], Training Accuracy: 33.7316%, Training Loss: 0.6702%\n",
      "Epoch [8/100], Step [18/225], Training Accuracy: 33.8542%, Training Loss: 0.6698%\n",
      "Epoch [8/100], Step [19/225], Training Accuracy: 33.7993%, Training Loss: 0.6709%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Step [20/225], Training Accuracy: 33.8281%, Training Loss: 0.6705%\n",
      "Epoch [8/100], Step [21/225], Training Accuracy: 34.0774%, Training Loss: 0.6699%\n",
      "Epoch [8/100], Step [22/225], Training Accuracy: 33.5938%, Training Loss: 0.6695%\n",
      "Epoch [8/100], Step [23/225], Training Accuracy: 33.6277%, Training Loss: 0.6690%\n",
      "Epoch [8/100], Step [24/225], Training Accuracy: 33.2682%, Training Loss: 0.6694%\n",
      "Epoch [8/100], Step [25/225], Training Accuracy: 33.8125%, Training Loss: 0.6687%\n",
      "Epoch [8/100], Step [26/225], Training Accuracy: 34.1346%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [27/225], Training Accuracy: 34.0278%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [28/225], Training Accuracy: 33.7612%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [29/225], Training Accuracy: 33.8362%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [30/225], Training Accuracy: 33.9062%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [31/225], Training Accuracy: 33.7198%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [32/225], Training Accuracy: 33.8867%, Training Loss: 0.6660%\n",
      "Epoch [8/100], Step [33/225], Training Accuracy: 33.9962%, Training Loss: 0.6654%\n",
      "Epoch [8/100], Step [34/225], Training Accuracy: 33.7316%, Training Loss: 0.6655%\n",
      "Epoch [8/100], Step [35/225], Training Accuracy: 33.8839%, Training Loss: 0.6661%\n",
      "Epoch [8/100], Step [36/225], Training Accuracy: 33.5938%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [37/225], Training Accuracy: 33.9105%, Training Loss: 0.6678%\n",
      "Epoch [8/100], Step [38/225], Training Accuracy: 33.7582%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [39/225], Training Accuracy: 33.6538%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [40/225], Training Accuracy: 33.5156%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [41/225], Training Accuracy: 33.3079%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [42/225], Training Accuracy: 33.2217%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [43/225], Training Accuracy: 33.1395%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [44/225], Training Accuracy: 33.3807%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [45/225], Training Accuracy: 33.5764%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [46/225], Training Accuracy: 33.6617%, Training Loss: 0.6678%\n",
      "Epoch [8/100], Step [47/225], Training Accuracy: 33.7101%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [48/225], Training Accuracy: 33.7240%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [49/225], Training Accuracy: 33.9286%, Training Loss: 0.6669%\n",
      "Epoch [8/100], Step [50/225], Training Accuracy: 33.9688%, Training Loss: 0.6668%\n",
      "Epoch [8/100], Step [51/225], Training Accuracy: 34.0380%, Training Loss: 0.6665%\n",
      "Epoch [8/100], Step [52/225], Training Accuracy: 34.0144%, Training Loss: 0.6663%\n",
      "Epoch [8/100], Step [53/225], Training Accuracy: 33.9328%, Training Loss: 0.6663%\n",
      "Epoch [8/100], Step [54/225], Training Accuracy: 33.8252%, Training Loss: 0.6662%\n",
      "Epoch [8/100], Step [55/225], Training Accuracy: 33.9205%, Training Loss: 0.6661%\n",
      "Epoch [8/100], Step [56/225], Training Accuracy: 33.7891%, Training Loss: 0.6661%\n",
      "Epoch [8/100], Step [57/225], Training Accuracy: 33.7993%, Training Loss: 0.6657%\n",
      "Epoch [8/100], Step [58/225], Training Accuracy: 33.8362%, Training Loss: 0.6656%\n",
      "Epoch [8/100], Step [59/225], Training Accuracy: 34.0572%, Training Loss: 0.6651%\n",
      "Epoch [8/100], Step [60/225], Training Accuracy: 34.1146%, Training Loss: 0.6651%\n",
      "Epoch [8/100], Step [61/225], Training Accuracy: 33.9908%, Training Loss: 0.6650%\n",
      "Epoch [8/100], Step [62/225], Training Accuracy: 34.0222%, Training Loss: 0.6651%\n",
      "Epoch [8/100], Step [63/225], Training Accuracy: 34.0278%, Training Loss: 0.6651%\n",
      "Epoch [8/100], Step [64/225], Training Accuracy: 34.1309%, Training Loss: 0.6650%\n",
      "Epoch [8/100], Step [65/225], Training Accuracy: 34.0865%, Training Loss: 0.6649%\n",
      "Epoch [8/100], Step [66/225], Training Accuracy: 34.2093%, Training Loss: 0.6650%\n",
      "Epoch [8/100], Step [67/225], Training Accuracy: 34.4216%, Training Loss: 0.6649%\n",
      "Epoch [8/100], Step [68/225], Training Accuracy: 34.3750%, Training Loss: 0.6649%\n",
      "Epoch [8/100], Step [69/225], Training Accuracy: 34.3071%, Training Loss: 0.6644%\n",
      "Epoch [8/100], Step [70/225], Training Accuracy: 34.2857%, Training Loss: 0.6647%\n",
      "Epoch [8/100], Step [71/225], Training Accuracy: 34.3090%, Training Loss: 0.6648%\n",
      "Epoch [8/100], Step [72/225], Training Accuracy: 34.2014%, Training Loss: 0.6653%\n",
      "Epoch [8/100], Step [73/225], Training Accuracy: 34.0325%, Training Loss: 0.6654%\n",
      "Epoch [8/100], Step [74/225], Training Accuracy: 34.0160%, Training Loss: 0.6652%\n",
      "Epoch [8/100], Step [75/225], Training Accuracy: 34.0208%, Training Loss: 0.6650%\n",
      "Epoch [8/100], Step [76/225], Training Accuracy: 33.9227%, Training Loss: 0.6653%\n",
      "Epoch [8/100], Step [77/225], Training Accuracy: 34.0097%, Training Loss: 0.6654%\n",
      "Epoch [8/100], Step [78/225], Training Accuracy: 34.0144%, Training Loss: 0.6653%\n",
      "Epoch [8/100], Step [79/225], Training Accuracy: 33.9992%, Training Loss: 0.6657%\n",
      "Epoch [8/100], Step [80/225], Training Accuracy: 34.0039%, Training Loss: 0.6655%\n",
      "Epoch [8/100], Step [81/225], Training Accuracy: 33.8542%, Training Loss: 0.6658%\n",
      "Epoch [8/100], Step [82/225], Training Accuracy: 33.9367%, Training Loss: 0.6657%\n",
      "Epoch [8/100], Step [83/225], Training Accuracy: 33.8479%, Training Loss: 0.6657%\n",
      "Epoch [8/100], Step [84/225], Training Accuracy: 33.8542%, Training Loss: 0.6656%\n",
      "Epoch [8/100], Step [85/225], Training Accuracy: 33.7316%, Training Loss: 0.6659%\n",
      "Epoch [8/100], Step [86/225], Training Accuracy: 33.7936%, Training Loss: 0.6660%\n",
      "Epoch [8/100], Step [87/225], Training Accuracy: 33.7823%, Training Loss: 0.6661%\n",
      "Epoch [8/100], Step [88/225], Training Accuracy: 33.6825%, Training Loss: 0.6663%\n",
      "Epoch [8/100], Step [89/225], Training Accuracy: 33.7079%, Training Loss: 0.6664%\n",
      "Epoch [8/100], Step [90/225], Training Accuracy: 33.6979%, Training Loss: 0.6663%\n",
      "Epoch [8/100], Step [91/225], Training Accuracy: 33.7740%, Training Loss: 0.6664%\n",
      "Epoch [8/100], Step [92/225], Training Accuracy: 33.7296%, Training Loss: 0.6663%\n",
      "Epoch [8/100], Step [93/225], Training Accuracy: 33.7198%, Training Loss: 0.6664%\n",
      "Epoch [8/100], Step [94/225], Training Accuracy: 33.8431%, Training Loss: 0.6663%\n",
      "Epoch [8/100], Step [95/225], Training Accuracy: 33.7007%, Training Loss: 0.6669%\n",
      "Epoch [8/100], Step [96/225], Training Accuracy: 33.6914%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [97/225], Training Accuracy: 33.7468%, Training Loss: 0.6668%\n",
      "Epoch [8/100], Step [98/225], Training Accuracy: 33.8329%, Training Loss: 0.6669%\n",
      "Epoch [8/100], Step [99/225], Training Accuracy: 33.8068%, Training Loss: 0.6669%\n",
      "Epoch [8/100], Step [100/225], Training Accuracy: 33.8594%, Training Loss: 0.6668%\n",
      "Epoch [8/100], Step [101/225], Training Accuracy: 33.9882%, Training Loss: 0.6665%\n",
      "Epoch [8/100], Step [102/225], Training Accuracy: 33.9767%, Training Loss: 0.6666%\n",
      "Epoch [8/100], Step [103/225], Training Accuracy: 33.9199%, Training Loss: 0.6666%\n",
      "Epoch [8/100], Step [104/225], Training Accuracy: 33.9243%, Training Loss: 0.6667%\n",
      "Epoch [8/100], Step [105/225], Training Accuracy: 33.8988%, Training Loss: 0.6669%\n",
      "Epoch [8/100], Step [106/225], Training Accuracy: 33.9180%, Training Loss: 0.6669%\n",
      "Epoch [8/100], Step [107/225], Training Accuracy: 33.8493%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [108/225], Training Accuracy: 33.7240%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [109/225], Training Accuracy: 33.6439%, Training Loss: 0.6672%\n",
      "Epoch [8/100], Step [110/225], Training Accuracy: 33.6648%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [111/225], Training Accuracy: 33.6571%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [112/225], Training Accuracy: 33.7472%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [113/225], Training Accuracy: 33.7251%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [114/225], Training Accuracy: 33.8130%, Training Loss: 0.6671%\n",
      "Epoch [8/100], Step [115/225], Training Accuracy: 33.9130%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [116/225], Training Accuracy: 33.9305%, Training Loss: 0.6670%\n",
      "Epoch [8/100], Step [117/225], Training Accuracy: 33.8675%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [118/225], Training Accuracy: 33.8453%, Training Loss: 0.6672%\n",
      "Epoch [8/100], Step [119/225], Training Accuracy: 33.8104%, Training Loss: 0.6674%\n",
      "Epoch [8/100], Step [120/225], Training Accuracy: 33.8281%, Training Loss: 0.6674%\n",
      "Epoch [8/100], Step [121/225], Training Accuracy: 33.7552%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [122/225], Training Accuracy: 33.8627%, Training Loss: 0.6674%\n",
      "Epoch [8/100], Step [123/225], Training Accuracy: 33.7779%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [124/225], Training Accuracy: 33.8332%, Training Loss: 0.6676%\n",
      "Epoch [8/100], Step [125/225], Training Accuracy: 33.8000%, Training Loss: 0.6678%\n",
      "Epoch [8/100], Step [126/225], Training Accuracy: 33.7922%, Training Loss: 0.6678%\n",
      "Epoch [8/100], Step [127/225], Training Accuracy: 33.8460%, Training Loss: 0.6678%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Step [128/225], Training Accuracy: 33.7524%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [129/225], Training Accuracy: 33.8057%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [130/225], Training Accuracy: 33.7019%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [131/225], Training Accuracy: 33.7309%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [132/225], Training Accuracy: 33.7003%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [133/225], Training Accuracy: 33.7289%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [134/225], Training Accuracy: 33.7803%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [135/225], Training Accuracy: 33.8426%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [136/225], Training Accuracy: 33.8925%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [137/225], Training Accuracy: 33.9530%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [138/225], Training Accuracy: 33.9221%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [139/225], Training Accuracy: 33.8467%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [140/225], Training Accuracy: 33.8951%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [141/225], Training Accuracy: 33.9207%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [142/225], Training Accuracy: 33.9569%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [143/225], Training Accuracy: 33.9598%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [144/225], Training Accuracy: 33.9410%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [145/225], Training Accuracy: 33.9655%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [146/225], Training Accuracy: 34.0004%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [147/225], Training Accuracy: 33.9711%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [148/225], Training Accuracy: 33.9316%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [149/225], Training Accuracy: 33.8612%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [150/225], Training Accuracy: 33.8229%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [151/225], Training Accuracy: 33.8473%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [152/225], Training Accuracy: 33.8405%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [153/225], Training Accuracy: 33.8133%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [154/225], Training Accuracy: 33.7662%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [155/225], Training Accuracy: 33.7903%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [156/225], Training Accuracy: 33.8341%, Training Loss: 0.6686%\n",
      "Epoch [8/100], Step [157/225], Training Accuracy: 33.8475%, Training Loss: 0.6686%\n",
      "Epoch [8/100], Step [158/225], Training Accuracy: 33.8212%, Training Loss: 0.6687%\n",
      "Epoch [8/100], Step [159/225], Training Accuracy: 33.8640%, Training Loss: 0.6687%\n",
      "Epoch [8/100], Step [160/225], Training Accuracy: 33.8672%, Training Loss: 0.6687%\n",
      "Epoch [8/100], Step [161/225], Training Accuracy: 33.9868%, Training Loss: 0.6686%\n",
      "Epoch [8/100], Step [162/225], Training Accuracy: 33.9603%, Training Loss: 0.6686%\n",
      "Epoch [8/100], Step [163/225], Training Accuracy: 34.0203%, Training Loss: 0.6686%\n",
      "Epoch [8/100], Step [164/225], Training Accuracy: 34.0415%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [165/225], Training Accuracy: 34.0152%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [166/225], Training Accuracy: 33.9985%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [167/225], Training Accuracy: 34.0475%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [168/225], Training Accuracy: 34.0402%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [169/225], Training Accuracy: 33.9867%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [170/225], Training Accuracy: 33.9430%, Training Loss: 0.6686%\n",
      "Epoch [8/100], Step [171/225], Training Accuracy: 34.0004%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [172/225], Training Accuracy: 34.0207%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [173/225], Training Accuracy: 34.0137%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [174/225], Training Accuracy: 34.0068%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [175/225], Training Accuracy: 34.0000%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [176/225], Training Accuracy: 33.9577%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [177/225], Training Accuracy: 33.9513%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [178/225], Training Accuracy: 33.9098%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [179/225], Training Accuracy: 33.8862%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [180/225], Training Accuracy: 33.9323%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [181/225], Training Accuracy: 33.8743%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [182/225], Training Accuracy: 33.8599%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [183/225], Training Accuracy: 33.8542%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [184/225], Training Accuracy: 33.8230%, Training Loss: 0.6684%\n",
      "Epoch [8/100], Step [185/225], Training Accuracy: 33.8091%, Training Loss: 0.6685%\n",
      "Epoch [8/100], Step [186/225], Training Accuracy: 33.8794%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [187/225], Training Accuracy: 33.8570%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [188/225], Training Accuracy: 33.8597%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [189/225], Training Accuracy: 33.9120%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [190/225], Training Accuracy: 33.9062%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [191/225], Training Accuracy: 33.8923%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [192/225], Training Accuracy: 33.8867%, Training Loss: 0.6683%\n",
      "Epoch [8/100], Step [193/225], Training Accuracy: 33.8973%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [194/225], Training Accuracy: 33.8998%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [195/225], Training Accuracy: 33.8942%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [196/225], Training Accuracy: 33.8728%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [197/225], Training Accuracy: 33.8674%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [198/225], Training Accuracy: 33.9015%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [199/225], Training Accuracy: 33.9196%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [200/225], Training Accuracy: 33.8828%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [201/225], Training Accuracy: 33.9086%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [202/225], Training Accuracy: 33.9186%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [203/225], Training Accuracy: 33.8978%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [204/225], Training Accuracy: 33.9308%, Training Loss: 0.6680%\n",
      "Epoch [8/100], Step [205/225], Training Accuracy: 33.9024%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [206/225], Training Accuracy: 33.8971%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [207/225], Training Accuracy: 33.9146%, Training Loss: 0.6682%\n",
      "Epoch [8/100], Step [208/225], Training Accuracy: 33.9919%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [209/225], Training Accuracy: 34.0087%, Training Loss: 0.6681%\n",
      "Epoch [8/100], Step [210/225], Training Accuracy: 34.0476%, Training Loss: 0.6679%\n",
      "Epoch [8/100], Step [211/225], Training Accuracy: 34.0566%, Training Loss: 0.6678%\n",
      "Epoch [8/100], Step [212/225], Training Accuracy: 34.0433%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [213/225], Training Accuracy: 34.0376%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [214/225], Training Accuracy: 34.0464%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [215/225], Training Accuracy: 34.0189%, Training Loss: 0.6676%\n",
      "Epoch [8/100], Step [216/225], Training Accuracy: 33.9699%, Training Loss: 0.6677%\n",
      "Epoch [8/100], Step [217/225], Training Accuracy: 34.0006%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [218/225], Training Accuracy: 33.9736%, Training Loss: 0.6675%\n",
      "Epoch [8/100], Step [219/225], Training Accuracy: 33.9969%, Training Loss: 0.6674%\n",
      "Epoch [8/100], Step [220/225], Training Accuracy: 34.0199%, Training Loss: 0.6674%\n",
      "Epoch [8/100], Step [221/225], Training Accuracy: 34.0215%, Training Loss: 0.6674%\n",
      "Epoch [8/100], Step [222/225], Training Accuracy: 34.0442%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [223/225], Training Accuracy: 34.0667%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [224/225], Training Accuracy: 34.0472%, Training Loss: 0.6673%\n",
      "Epoch [8/100], Step [225/225], Training Accuracy: 34.0397%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [1/225], Training Accuracy: 43.7500%, Training Loss: 0.6515%\n",
      "Epoch [9/100], Step [2/225], Training Accuracy: 39.0625%, Training Loss: 0.6640%\n",
      "Epoch [9/100], Step [3/225], Training Accuracy: 36.4583%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [4/225], Training Accuracy: 35.1562%, Training Loss: 0.6659%\n",
      "Epoch [9/100], Step [5/225], Training Accuracy: 35.6250%, Training Loss: 0.6670%\n",
      "Epoch [9/100], Step [6/225], Training Accuracy: 35.6771%, Training Loss: 0.6692%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Step [7/225], Training Accuracy: 34.5982%, Training Loss: 0.6675%\n",
      "Epoch [9/100], Step [8/225], Training Accuracy: 33.7891%, Training Loss: 0.6674%\n",
      "Epoch [9/100], Step [9/225], Training Accuracy: 33.5069%, Training Loss: 0.6680%\n",
      "Epoch [9/100], Step [10/225], Training Accuracy: 34.5312%, Training Loss: 0.6666%\n",
      "Epoch [9/100], Step [11/225], Training Accuracy: 34.0909%, Training Loss: 0.6666%\n",
      "Epoch [9/100], Step [12/225], Training Accuracy: 34.1146%, Training Loss: 0.6668%\n",
      "Epoch [9/100], Step [13/225], Training Accuracy: 34.4952%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [14/225], Training Accuracy: 34.1518%, Training Loss: 0.6707%\n",
      "Epoch [9/100], Step [15/225], Training Accuracy: 33.7500%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [16/225], Training Accuracy: 34.1797%, Training Loss: 0.6705%\n",
      "Epoch [9/100], Step [17/225], Training Accuracy: 34.2831%, Training Loss: 0.6712%\n",
      "Epoch [9/100], Step [18/225], Training Accuracy: 34.2014%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [19/225], Training Accuracy: 34.2105%, Training Loss: 0.6727%\n",
      "Epoch [9/100], Step [20/225], Training Accuracy: 34.3750%, Training Loss: 0.6727%\n",
      "Epoch [9/100], Step [21/225], Training Accuracy: 34.9702%, Training Loss: 0.6722%\n",
      "Epoch [9/100], Step [22/225], Training Accuracy: 34.8722%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [23/225], Training Accuracy: 35.0543%, Training Loss: 0.6708%\n",
      "Epoch [9/100], Step [24/225], Training Accuracy: 34.5052%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [25/225], Training Accuracy: 34.6250%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [26/225], Training Accuracy: 34.6154%, Training Loss: 0.6709%\n",
      "Epoch [9/100], Step [27/225], Training Accuracy: 34.7222%, Training Loss: 0.6705%\n",
      "Epoch [9/100], Step [28/225], Training Accuracy: 34.8772%, Training Loss: 0.6698%\n",
      "Epoch [9/100], Step [29/225], Training Accuracy: 35.0216%, Training Loss: 0.6692%\n",
      "Epoch [9/100], Step [30/225], Training Accuracy: 35.0521%, Training Loss: 0.6689%\n",
      "Epoch [9/100], Step [31/225], Training Accuracy: 34.9798%, Training Loss: 0.6687%\n",
      "Epoch [9/100], Step [32/225], Training Accuracy: 35.1074%, Training Loss: 0.6678%\n",
      "Epoch [9/100], Step [33/225], Training Accuracy: 35.2746%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [34/225], Training Accuracy: 35.2022%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [35/225], Training Accuracy: 34.8214%, Training Loss: 0.6679%\n",
      "Epoch [9/100], Step [36/225], Training Accuracy: 34.8090%, Training Loss: 0.6684%\n",
      "Epoch [9/100], Step [37/225], Training Accuracy: 34.7128%, Training Loss: 0.6692%\n",
      "Epoch [9/100], Step [38/225], Training Accuracy: 34.5395%, Training Loss: 0.6692%\n",
      "Epoch [9/100], Step [39/225], Training Accuracy: 34.4151%, Training Loss: 0.6689%\n",
      "Epoch [9/100], Step [40/225], Training Accuracy: 34.2188%, Training Loss: 0.6689%\n",
      "Epoch [9/100], Step [41/225], Training Accuracy: 33.9177%, Training Loss: 0.6690%\n",
      "Epoch [9/100], Step [42/225], Training Accuracy: 33.7426%, Training Loss: 0.6692%\n",
      "Epoch [9/100], Step [43/225], Training Accuracy: 33.5392%, Training Loss: 0.6693%\n",
      "Epoch [9/100], Step [44/225], Training Accuracy: 33.7003%, Training Loss: 0.6687%\n",
      "Epoch [9/100], Step [45/225], Training Accuracy: 33.8194%, Training Loss: 0.6685%\n",
      "Epoch [9/100], Step [46/225], Training Accuracy: 33.8315%, Training Loss: 0.6686%\n",
      "Epoch [9/100], Step [47/225], Training Accuracy: 33.7101%, Training Loss: 0.6687%\n",
      "Epoch [9/100], Step [48/225], Training Accuracy: 33.6914%, Training Loss: 0.6683%\n",
      "Epoch [9/100], Step [49/225], Training Accuracy: 33.9605%, Training Loss: 0.6681%\n",
      "Epoch [9/100], Step [50/225], Training Accuracy: 33.9375%, Training Loss: 0.6682%\n",
      "Epoch [9/100], Step [51/225], Training Accuracy: 34.1605%, Training Loss: 0.6678%\n",
      "Epoch [9/100], Step [52/225], Training Accuracy: 34.1647%, Training Loss: 0.6677%\n",
      "Epoch [9/100], Step [53/225], Training Accuracy: 34.1392%, Training Loss: 0.6678%\n",
      "Epoch [9/100], Step [54/225], Training Accuracy: 34.0278%, Training Loss: 0.6679%\n",
      "Epoch [9/100], Step [55/225], Training Accuracy: 34.0341%, Training Loss: 0.6681%\n",
      "Epoch [9/100], Step [56/225], Training Accuracy: 33.8449%, Training Loss: 0.6682%\n",
      "Epoch [9/100], Step [57/225], Training Accuracy: 33.7719%, Training Loss: 0.6681%\n",
      "Epoch [9/100], Step [58/225], Training Accuracy: 33.8093%, Training Loss: 0.6680%\n",
      "Epoch [9/100], Step [59/225], Training Accuracy: 34.0307%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [60/225], Training Accuracy: 34.1406%, Training Loss: 0.6671%\n",
      "Epoch [9/100], Step [61/225], Training Accuracy: 34.0676%, Training Loss: 0.6671%\n",
      "Epoch [9/100], Step [62/225], Training Accuracy: 33.9466%, Training Loss: 0.6675%\n",
      "Epoch [9/100], Step [63/225], Training Accuracy: 33.9038%, Training Loss: 0.6678%\n",
      "Epoch [9/100], Step [64/225], Training Accuracy: 34.0088%, Training Loss: 0.6676%\n",
      "Epoch [9/100], Step [65/225], Training Accuracy: 34.1346%, Training Loss: 0.6674%\n",
      "Epoch [9/100], Step [66/225], Training Accuracy: 34.1383%, Training Loss: 0.6674%\n",
      "Epoch [9/100], Step [67/225], Training Accuracy: 34.1418%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [68/225], Training Accuracy: 34.2142%, Training Loss: 0.6673%\n",
      "Epoch [9/100], Step [69/225], Training Accuracy: 34.1938%, Training Loss: 0.6670%\n",
      "Epoch [9/100], Step [70/225], Training Accuracy: 34.1295%, Training Loss: 0.6672%\n",
      "Epoch [9/100], Step [71/225], Training Accuracy: 34.2650%, Training Loss: 0.6674%\n",
      "Epoch [9/100], Step [72/225], Training Accuracy: 34.0061%, Training Loss: 0.6680%\n",
      "Epoch [9/100], Step [73/225], Training Accuracy: 33.9255%, Training Loss: 0.6681%\n",
      "Epoch [9/100], Step [74/225], Training Accuracy: 34.0372%, Training Loss: 0.6679%\n",
      "Epoch [9/100], Step [75/225], Training Accuracy: 34.0000%, Training Loss: 0.6678%\n",
      "Epoch [9/100], Step [76/225], Training Accuracy: 33.8405%, Training Loss: 0.6679%\n",
      "Epoch [9/100], Step [77/225], Training Accuracy: 33.7459%, Training Loss: 0.6680%\n",
      "Epoch [9/100], Step [78/225], Training Accuracy: 33.7540%, Training Loss: 0.6679%\n",
      "Epoch [9/100], Step [79/225], Training Accuracy: 33.6432%, Training Loss: 0.6681%\n",
      "Epoch [9/100], Step [80/225], Training Accuracy: 33.7109%, Training Loss: 0.6680%\n",
      "Epoch [9/100], Step [81/225], Training Accuracy: 33.6613%, Training Loss: 0.6684%\n",
      "Epoch [9/100], Step [82/225], Training Accuracy: 33.7081%, Training Loss: 0.6683%\n",
      "Epoch [9/100], Step [83/225], Training Accuracy: 33.7161%, Training Loss: 0.6683%\n",
      "Epoch [9/100], Step [84/225], Training Accuracy: 33.7240%, Training Loss: 0.6682%\n",
      "Epoch [9/100], Step [85/225], Training Accuracy: 33.6949%, Training Loss: 0.6684%\n",
      "Epoch [9/100], Step [86/225], Training Accuracy: 33.7573%, Training Loss: 0.6685%\n",
      "Epoch [9/100], Step [87/225], Training Accuracy: 33.7284%, Training Loss: 0.6685%\n",
      "Epoch [9/100], Step [88/225], Training Accuracy: 33.6115%, Training Loss: 0.6688%\n",
      "Epoch [9/100], Step [89/225], Training Accuracy: 33.5499%, Training Loss: 0.6689%\n",
      "Epoch [9/100], Step [90/225], Training Accuracy: 33.5764%, Training Loss: 0.6688%\n",
      "Epoch [9/100], Step [91/225], Training Accuracy: 33.6023%, Training Loss: 0.6689%\n",
      "Epoch [9/100], Step [92/225], Training Accuracy: 33.5258%, Training Loss: 0.6688%\n",
      "Epoch [9/100], Step [93/225], Training Accuracy: 33.5517%, Training Loss: 0.6689%\n",
      "Epoch [9/100], Step [94/225], Training Accuracy: 33.5938%, Training Loss: 0.6686%\n",
      "Epoch [9/100], Step [95/225], Training Accuracy: 33.5197%, Training Loss: 0.6690%\n",
      "Epoch [9/100], Step [96/225], Training Accuracy: 33.5286%, Training Loss: 0.6690%\n",
      "Epoch [9/100], Step [97/225], Training Accuracy: 33.5213%, Training Loss: 0.6692%\n",
      "Epoch [9/100], Step [98/225], Training Accuracy: 33.5619%, Training Loss: 0.6696%\n",
      "Epoch [9/100], Step [99/225], Training Accuracy: 33.5701%, Training Loss: 0.6697%\n",
      "Epoch [9/100], Step [100/225], Training Accuracy: 33.5312%, Training Loss: 0.6696%\n",
      "Epoch [9/100], Step [101/225], Training Accuracy: 33.5705%, Training Loss: 0.6696%\n",
      "Epoch [9/100], Step [102/225], Training Accuracy: 33.5325%, Training Loss: 0.6696%\n",
      "Epoch [9/100], Step [103/225], Training Accuracy: 33.4800%, Training Loss: 0.6697%\n",
      "Epoch [9/100], Step [104/225], Training Accuracy: 33.4585%, Training Loss: 0.6698%\n",
      "Epoch [9/100], Step [105/225], Training Accuracy: 33.4226%, Training Loss: 0.6699%\n",
      "Epoch [9/100], Step [106/225], Training Accuracy: 33.4021%, Training Loss: 0.6699%\n",
      "Epoch [9/100], Step [107/225], Training Accuracy: 33.3090%, Training Loss: 0.6700%\n",
      "Epoch [9/100], Step [108/225], Training Accuracy: 33.2465%, Training Loss: 0.6702%\n",
      "Epoch [9/100], Step [109/225], Training Accuracy: 33.1279%, Training Loss: 0.6705%\n",
      "Epoch [9/100], Step [110/225], Training Accuracy: 33.1960%, Training Loss: 0.6704%\n",
      "Epoch [9/100], Step [111/225], Training Accuracy: 33.1222%, Training Loss: 0.6705%\n",
      "Epoch [9/100], Step [112/225], Training Accuracy: 33.1473%, Training Loss: 0.6704%\n",
      "Epoch [9/100], Step [113/225], Training Accuracy: 33.1444%, Training Loss: 0.6706%\n",
      "Epoch [9/100], Step [114/225], Training Accuracy: 33.1963%, Training Loss: 0.6704%\n",
      "Epoch [9/100], Step [115/225], Training Accuracy: 33.1793%, Training Loss: 0.6704%\n",
      "Epoch [9/100], Step [116/225], Training Accuracy: 33.1627%, Training Loss: 0.6702%\n",
      "Epoch [9/100], Step [117/225], Training Accuracy: 33.0662%, Training Loss: 0.6705%\n",
      "Epoch [9/100], Step [118/225], Training Accuracy: 33.0773%, Training Loss: 0.6705%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Step [119/225], Training Accuracy: 33.0226%, Training Loss: 0.6707%\n",
      "Epoch [9/100], Step [120/225], Training Accuracy: 33.0859%, Training Loss: 0.6707%\n",
      "Epoch [9/100], Step [121/225], Training Accuracy: 33.0579%, Training Loss: 0.6708%\n",
      "Epoch [9/100], Step [122/225], Training Accuracy: 33.1455%, Training Loss: 0.6707%\n",
      "Epoch [9/100], Step [123/225], Training Accuracy: 33.1555%, Training Loss: 0.6708%\n",
      "Epoch [9/100], Step [124/225], Training Accuracy: 33.1401%, Training Loss: 0.6709%\n",
      "Epoch [9/100], Step [125/225], Training Accuracy: 33.0750%, Training Loss: 0.6712%\n",
      "Epoch [9/100], Step [126/225], Training Accuracy: 33.0729%, Training Loss: 0.6711%\n",
      "Epoch [9/100], Step [127/225], Training Accuracy: 33.0709%, Training Loss: 0.6711%\n",
      "Epoch [9/100], Step [128/225], Training Accuracy: 32.9956%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [129/225], Training Accuracy: 32.9942%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [130/225], Training Accuracy: 32.9327%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [131/225], Training Accuracy: 32.9556%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [132/225], Training Accuracy: 32.9072%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [133/225], Training Accuracy: 32.9535%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [134/225], Training Accuracy: 32.9291%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [135/225], Training Accuracy: 32.9167%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [136/225], Training Accuracy: 32.9504%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [137/225], Training Accuracy: 33.0064%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [138/225], Training Accuracy: 33.0389%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [139/225], Training Accuracy: 32.9924%, Training Loss: 0.6719%\n",
      "Epoch [9/100], Step [140/225], Training Accuracy: 32.9576%, Training Loss: 0.6719%\n",
      "Epoch [9/100], Step [141/225], Training Accuracy: 32.9566%, Training Loss: 0.6719%\n",
      "Epoch [9/100], Step [142/225], Training Accuracy: 32.9996%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [143/225], Training Accuracy: 32.9873%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [144/225], Training Accuracy: 33.0187%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [145/225], Training Accuracy: 33.0603%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [146/225], Training Accuracy: 33.1015%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [147/225], Training Accuracy: 33.1101%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [148/225], Training Accuracy: 33.1081%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [149/225], Training Accuracy: 33.1271%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [150/225], Training Accuracy: 33.1458%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [151/225], Training Accuracy: 33.2057%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [152/225], Training Accuracy: 33.1826%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [153/225], Training Accuracy: 33.1495%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [154/225], Training Accuracy: 33.1372%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [155/225], Training Accuracy: 33.1552%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [156/225], Training Accuracy: 33.1330%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [157/225], Training Accuracy: 33.1509%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [158/225], Training Accuracy: 33.0597%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [159/225], Training Accuracy: 33.0975%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [160/225], Training Accuracy: 33.0859%, Training Loss: 0.6718%\n",
      "Epoch [9/100], Step [161/225], Training Accuracy: 33.1619%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [162/225], Training Accuracy: 33.1501%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [163/225], Training Accuracy: 33.1863%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [164/225], Training Accuracy: 33.1650%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [165/225], Training Accuracy: 33.1345%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [166/225], Training Accuracy: 33.1137%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [167/225], Training Accuracy: 33.1306%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [168/225], Training Accuracy: 33.0915%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [169/225], Training Accuracy: 33.0436%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [170/225], Training Accuracy: 32.9963%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [171/225], Training Accuracy: 32.9952%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [172/225], Training Accuracy: 33.0033%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [173/225], Training Accuracy: 33.0202%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [174/225], Training Accuracy: 33.0011%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [175/225], Training Accuracy: 33.0268%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [176/225], Training Accuracy: 33.0078%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [177/225], Training Accuracy: 32.9802%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [178/225], Training Accuracy: 33.0144%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [179/225], Training Accuracy: 32.9958%, Training Loss: 0.6717%\n",
      "Epoch [9/100], Step [180/225], Training Accuracy: 33.0122%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [181/225], Training Accuracy: 32.9852%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [182/225], Training Accuracy: 32.9756%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [183/225], Training Accuracy: 32.9918%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [184/225], Training Accuracy: 32.9823%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [185/225], Training Accuracy: 32.9730%, Training Loss: 0.6716%\n",
      "Epoch [9/100], Step [186/225], Training Accuracy: 33.0309%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [187/225], Training Accuracy: 33.0381%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [188/225], Training Accuracy: 33.0369%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [189/225], Training Accuracy: 33.0771%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [190/225], Training Accuracy: 33.0839%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [191/225], Training Accuracy: 33.0252%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [192/225], Training Accuracy: 32.9915%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [193/225], Training Accuracy: 32.9825%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [194/225], Training Accuracy: 33.0058%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [195/225], Training Accuracy: 33.0048%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [196/225], Training Accuracy: 32.9799%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [197/225], Training Accuracy: 32.9870%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [198/225], Training Accuracy: 33.0256%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [199/225], Training Accuracy: 33.0166%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [200/225], Training Accuracy: 32.9922%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [201/225], Training Accuracy: 33.0302%, Training Loss: 0.6712%\n",
      "Epoch [9/100], Step [202/225], Training Accuracy: 33.0600%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [203/225], Training Accuracy: 33.0357%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [204/225], Training Accuracy: 33.0423%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [205/225], Training Accuracy: 33.0259%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [206/225], Training Accuracy: 33.0249%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [207/225], Training Accuracy: 33.0616%, Training Loss: 0.6715%\n",
      "Epoch [9/100], Step [208/225], Training Accuracy: 33.1055%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [209/225], Training Accuracy: 33.1265%, Training Loss: 0.6714%\n",
      "Epoch [9/100], Step [210/225], Training Accuracy: 33.1696%, Training Loss: 0.6713%\n",
      "Epoch [9/100], Step [211/225], Training Accuracy: 33.1976%, Training Loss: 0.6712%\n",
      "Epoch [9/100], Step [212/225], Training Accuracy: 33.2179%, Training Loss: 0.6711%\n",
      "Epoch [9/100], Step [213/225], Training Accuracy: 33.1866%, Training Loss: 0.6711%\n",
      "Epoch [9/100], Step [214/225], Training Accuracy: 33.1776%, Training Loss: 0.6711%\n",
      "Epoch [9/100], Step [215/225], Training Accuracy: 33.1613%, Training Loss: 0.6710%\n",
      "Epoch [9/100], Step [216/225], Training Accuracy: 33.1236%, Training Loss: 0.6711%\n",
      "Epoch [9/100], Step [217/225], Training Accuracy: 33.1221%, Training Loss: 0.6710%\n",
      "Epoch [9/100], Step [218/225], Training Accuracy: 33.0849%, Training Loss: 0.6710%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Step [219/225], Training Accuracy: 33.1122%, Training Loss: 0.6710%\n",
      "Epoch [9/100], Step [220/225], Training Accuracy: 33.1463%, Training Loss: 0.6709%\n",
      "Epoch [9/100], Step [221/225], Training Accuracy: 33.1165%, Training Loss: 0.6710%\n",
      "Epoch [9/100], Step [222/225], Training Accuracy: 33.1363%, Training Loss: 0.6709%\n",
      "Epoch [9/100], Step [223/225], Training Accuracy: 33.1348%, Training Loss: 0.6709%\n",
      "Epoch [9/100], Step [224/225], Training Accuracy: 33.1124%, Training Loss: 0.6709%\n",
      "Epoch [9/100], Step [225/225], Training Accuracy: 33.1226%, Training Loss: 0.6710%\n",
      "Epoch [10/100], Step [1/225], Training Accuracy: 45.3125%, Training Loss: 0.6636%\n",
      "Epoch [10/100], Step [2/225], Training Accuracy: 38.2812%, Training Loss: 0.6744%\n",
      "Epoch [10/100], Step [3/225], Training Accuracy: 36.9792%, Training Loss: 0.6756%\n",
      "Epoch [10/100], Step [4/225], Training Accuracy: 36.3281%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [5/225], Training Accuracy: 36.2500%, Training Loss: 0.6730%\n",
      "Epoch [10/100], Step [6/225], Training Accuracy: 35.6771%, Training Loss: 0.6749%\n",
      "Epoch [10/100], Step [7/225], Training Accuracy: 34.5982%, Training Loss: 0.6731%\n",
      "Epoch [10/100], Step [8/225], Training Accuracy: 34.7656%, Training Loss: 0.6714%\n",
      "Epoch [10/100], Step [9/225], Training Accuracy: 34.5486%, Training Loss: 0.6723%\n",
      "Epoch [10/100], Step [10/225], Training Accuracy: 34.8438%, Training Loss: 0.6710%\n",
      "Epoch [10/100], Step [11/225], Training Accuracy: 34.9432%, Training Loss: 0.6709%\n",
      "Epoch [10/100], Step [12/225], Training Accuracy: 34.8958%, Training Loss: 0.6709%\n",
      "Epoch [10/100], Step [13/225], Training Accuracy: 34.6154%, Training Loss: 0.6707%\n",
      "Epoch [10/100], Step [14/225], Training Accuracy: 34.1518%, Training Loss: 0.6731%\n",
      "Epoch [10/100], Step [15/225], Training Accuracy: 34.1667%, Training Loss: 0.6736%\n",
      "Epoch [10/100], Step [16/225], Training Accuracy: 34.4727%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [17/225], Training Accuracy: 33.9154%, Training Loss: 0.6732%\n",
      "Epoch [10/100], Step [18/225], Training Accuracy: 33.7674%, Training Loss: 0.6734%\n",
      "Epoch [10/100], Step [19/225], Training Accuracy: 33.8816%, Training Loss: 0.6738%\n",
      "Epoch [10/100], Step [20/225], Training Accuracy: 33.6719%, Training Loss: 0.6738%\n",
      "Epoch [10/100], Step [21/225], Training Accuracy: 33.7054%, Training Loss: 0.6739%\n",
      "Epoch [10/100], Step [22/225], Training Accuracy: 33.3097%, Training Loss: 0.6740%\n",
      "Epoch [10/100], Step [23/225], Training Accuracy: 33.4239%, Training Loss: 0.6735%\n",
      "Epoch [10/100], Step [24/225], Training Accuracy: 33.3984%, Training Loss: 0.6738%\n",
      "Epoch [10/100], Step [25/225], Training Accuracy: 33.8750%, Training Loss: 0.6736%\n",
      "Epoch [10/100], Step [26/225], Training Accuracy: 33.9543%, Training Loss: 0.6735%\n",
      "Epoch [10/100], Step [27/225], Training Accuracy: 34.0856%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [28/225], Training Accuracy: 33.9844%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [29/225], Training Accuracy: 33.9978%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [30/225], Training Accuracy: 34.0625%, Training Loss: 0.6722%\n",
      "Epoch [10/100], Step [31/225], Training Accuracy: 34.0222%, Training Loss: 0.6720%\n",
      "Epoch [10/100], Step [32/225], Training Accuracy: 34.0820%, Training Loss: 0.6713%\n",
      "Epoch [10/100], Step [33/225], Training Accuracy: 34.1856%, Training Loss: 0.6708%\n",
      "Epoch [10/100], Step [34/225], Training Accuracy: 34.0074%, Training Loss: 0.6707%\n",
      "Epoch [10/100], Step [35/225], Training Accuracy: 33.7054%, Training Loss: 0.6713%\n",
      "Epoch [10/100], Step [36/225], Training Accuracy: 33.5938%, Training Loss: 0.6718%\n",
      "Epoch [10/100], Step [37/225], Training Accuracy: 33.5726%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [38/225], Training Accuracy: 33.3470%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [39/225], Training Accuracy: 33.1731%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [40/225], Training Accuracy: 33.2422%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [41/225], Training Accuracy: 33.2698%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [42/225], Training Accuracy: 32.9241%, Training Loss: 0.6731%\n",
      "Epoch [10/100], Step [43/225], Training Accuracy: 32.9578%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [44/225], Training Accuracy: 33.1321%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [45/225], Training Accuracy: 33.2292%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [46/225], Training Accuracy: 33.1182%, Training Loss: 0.6723%\n",
      "Epoch [10/100], Step [47/225], Training Accuracy: 32.9787%, Training Loss: 0.6720%\n",
      "Epoch [10/100], Step [48/225], Training Accuracy: 33.2031%, Training Loss: 0.6714%\n",
      "Epoch [10/100], Step [49/225], Training Accuracy: 33.3546%, Training Loss: 0.6712%\n",
      "Epoch [10/100], Step [50/225], Training Accuracy: 33.3125%, Training Loss: 0.6713%\n",
      "Epoch [10/100], Step [51/225], Training Accuracy: 33.4559%, Training Loss: 0.6712%\n",
      "Epoch [10/100], Step [52/225], Training Accuracy: 33.4435%, Training Loss: 0.6710%\n",
      "Epoch [10/100], Step [53/225], Training Accuracy: 33.4316%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [54/225], Training Accuracy: 33.3333%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [55/225], Training Accuracy: 33.3523%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [56/225], Training Accuracy: 33.3984%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [57/225], Training Accuracy: 33.3059%, Training Loss: 0.6709%\n",
      "Epoch [10/100], Step [58/225], Training Accuracy: 33.2166%, Training Loss: 0.6707%\n",
      "Epoch [10/100], Step [59/225], Training Accuracy: 33.5011%, Training Loss: 0.6703%\n",
      "Epoch [10/100], Step [60/225], Training Accuracy: 33.5156%, Training Loss: 0.6702%\n",
      "Epoch [10/100], Step [61/225], Training Accuracy: 33.5041%, Training Loss: 0.6701%\n",
      "Epoch [10/100], Step [62/225], Training Accuracy: 33.3669%, Training Loss: 0.6701%\n",
      "Epoch [10/100], Step [63/225], Training Accuracy: 33.4325%, Training Loss: 0.6700%\n",
      "Epoch [10/100], Step [64/225], Training Accuracy: 33.3740%, Training Loss: 0.6698%\n",
      "Epoch [10/100], Step [65/225], Training Accuracy: 33.4375%, Training Loss: 0.6697%\n",
      "Epoch [10/100], Step [66/225], Training Accuracy: 33.5701%, Training Loss: 0.6695%\n",
      "Epoch [10/100], Step [67/225], Training Accuracy: 33.6054%, Training Loss: 0.6693%\n",
      "Epoch [10/100], Step [68/225], Training Accuracy: 33.6627%, Training Loss: 0.6692%\n",
      "Epoch [10/100], Step [69/225], Training Accuracy: 33.5824%, Training Loss: 0.6688%\n",
      "Epoch [10/100], Step [70/225], Training Accuracy: 33.5938%, Training Loss: 0.6689%\n",
      "Epoch [10/100], Step [71/225], Training Accuracy: 33.5827%, Training Loss: 0.6692%\n",
      "Epoch [10/100], Step [72/225], Training Accuracy: 33.3767%, Training Loss: 0.6697%\n",
      "Epoch [10/100], Step [73/225], Training Accuracy: 33.2834%, Training Loss: 0.6698%\n",
      "Epoch [10/100], Step [74/225], Training Accuracy: 33.3826%, Training Loss: 0.6696%\n",
      "Epoch [10/100], Step [75/225], Training Accuracy: 33.3958%, Training Loss: 0.6695%\n",
      "Epoch [10/100], Step [76/225], Training Accuracy: 33.3470%, Training Loss: 0.6695%\n",
      "Epoch [10/100], Step [77/225], Training Accuracy: 33.2995%, Training Loss: 0.6696%\n",
      "Epoch [10/100], Step [78/225], Training Accuracy: 33.3934%, Training Loss: 0.6697%\n",
      "Epoch [10/100], Step [79/225], Training Accuracy: 33.3465%, Training Loss: 0.6698%\n",
      "Epoch [10/100], Step [80/225], Training Accuracy: 33.3203%, Training Loss: 0.6697%\n",
      "Epoch [10/100], Step [81/225], Training Accuracy: 33.1790%, Training Loss: 0.6701%\n",
      "Epoch [10/100], Step [82/225], Training Accuracy: 33.2127%, Training Loss: 0.6701%\n",
      "Epoch [10/100], Step [83/225], Training Accuracy: 33.1890%, Training Loss: 0.6700%\n",
      "Epoch [10/100], Step [84/225], Training Accuracy: 33.1659%, Training Loss: 0.6701%\n",
      "Epoch [10/100], Step [85/225], Training Accuracy: 33.0882%, Training Loss: 0.6704%\n",
      "Epoch [10/100], Step [86/225], Training Accuracy: 33.1214%, Training Loss: 0.6705%\n",
      "Epoch [10/100], Step [87/225], Training Accuracy: 33.1537%, Training Loss: 0.6706%\n",
      "Epoch [10/100], Step [88/225], Training Accuracy: 33.0966%, Training Loss: 0.6707%\n",
      "Epoch [10/100], Step [89/225], Training Accuracy: 33.0583%, Training Loss: 0.6706%\n",
      "Epoch [10/100], Step [90/225], Training Accuracy: 33.0382%, Training Loss: 0.6705%\n",
      "Epoch [10/100], Step [91/225], Training Accuracy: 33.0357%, Training Loss: 0.6706%\n",
      "Epoch [10/100], Step [92/225], Training Accuracy: 33.0673%, Training Loss: 0.6706%\n",
      "Epoch [10/100], Step [93/225], Training Accuracy: 33.0645%, Training Loss: 0.6707%\n",
      "Epoch [10/100], Step [94/225], Training Accuracy: 33.1782%, Training Loss: 0.6705%\n",
      "Epoch [10/100], Step [95/225], Training Accuracy: 33.1086%, Training Loss: 0.6709%\n",
      "Epoch [10/100], Step [96/225], Training Accuracy: 33.1055%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [97/225], Training Accuracy: 33.1024%, Training Loss: 0.6711%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Step [98/225], Training Accuracy: 33.0517%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [99/225], Training Accuracy: 33.0966%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [100/225], Training Accuracy: 33.1719%, Training Loss: 0.6712%\n",
      "Epoch [10/100], Step [101/225], Training Accuracy: 33.2611%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [102/225], Training Accuracy: 33.2108%, Training Loss: 0.6711%\n",
      "Epoch [10/100], Step [103/225], Training Accuracy: 33.1917%, Training Loss: 0.6713%\n",
      "Epoch [10/100], Step [104/225], Training Accuracy: 33.1280%, Training Loss: 0.6714%\n",
      "Epoch [10/100], Step [105/225], Training Accuracy: 33.0357%, Training Loss: 0.6715%\n",
      "Epoch [10/100], Step [106/225], Training Accuracy: 33.1368%, Training Loss: 0.6714%\n",
      "Epoch [10/100], Step [107/225], Training Accuracy: 33.0754%, Training Loss: 0.6715%\n",
      "Epoch [10/100], Step [108/225], Training Accuracy: 33.1163%, Training Loss: 0.6715%\n",
      "Epoch [10/100], Step [109/225], Training Accuracy: 33.0132%, Training Loss: 0.6717%\n",
      "Epoch [10/100], Step [110/225], Training Accuracy: 33.0114%, Training Loss: 0.6717%\n",
      "Epoch [10/100], Step [111/225], Training Accuracy: 32.9673%, Training Loss: 0.6718%\n",
      "Epoch [10/100], Step [112/225], Training Accuracy: 33.0776%, Training Loss: 0.6718%\n",
      "Epoch [10/100], Step [113/225], Training Accuracy: 33.0337%, Training Loss: 0.6719%\n",
      "Epoch [10/100], Step [114/225], Training Accuracy: 33.1552%, Training Loss: 0.6717%\n",
      "Epoch [10/100], Step [115/225], Training Accuracy: 33.1250%, Training Loss: 0.6717%\n",
      "Epoch [10/100], Step [116/225], Training Accuracy: 33.1492%, Training Loss: 0.6716%\n",
      "Epoch [10/100], Step [117/225], Training Accuracy: 33.0929%, Training Loss: 0.6718%\n",
      "Epoch [10/100], Step [118/225], Training Accuracy: 33.0773%, Training Loss: 0.6718%\n",
      "Epoch [10/100], Step [119/225], Training Accuracy: 33.0488%, Training Loss: 0.6719%\n",
      "Epoch [10/100], Step [120/225], Training Accuracy: 33.0729%, Training Loss: 0.6719%\n",
      "Epoch [10/100], Step [121/225], Training Accuracy: 32.9933%, Training Loss: 0.6720%\n",
      "Epoch [10/100], Step [122/225], Training Accuracy: 33.0046%, Training Loss: 0.6719%\n",
      "Epoch [10/100], Step [123/225], Training Accuracy: 32.9903%, Training Loss: 0.6719%\n",
      "Epoch [10/100], Step [124/225], Training Accuracy: 32.9889%, Training Loss: 0.6720%\n",
      "Epoch [10/100], Step [125/225], Training Accuracy: 32.9625%, Training Loss: 0.6722%\n",
      "Epoch [10/100], Step [126/225], Training Accuracy: 32.8993%, Training Loss: 0.6721%\n",
      "Epoch [10/100], Step [127/225], Training Accuracy: 32.8125%, Training Loss: 0.6722%\n",
      "Epoch [10/100], Step [128/225], Training Accuracy: 32.7881%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [129/225], Training Accuracy: 32.8004%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [130/225], Training Accuracy: 32.7043%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [131/225], Training Accuracy: 32.6813%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [132/225], Training Accuracy: 32.6349%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [133/225], Training Accuracy: 32.6715%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [134/225], Training Accuracy: 32.6842%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [135/225], Training Accuracy: 32.6736%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [136/225], Training Accuracy: 32.6631%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [137/225], Training Accuracy: 32.6642%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [138/225], Training Accuracy: 32.6653%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [139/225], Training Accuracy: 32.5877%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [140/225], Training Accuracy: 32.5893%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [141/225], Training Accuracy: 32.5244%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [142/225], Training Accuracy: 32.5924%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [143/225], Training Accuracy: 32.5721%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [144/225], Training Accuracy: 32.5629%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [145/225], Training Accuracy: 32.6509%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [146/225], Training Accuracy: 32.6520%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [147/225], Training Accuracy: 32.6743%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [148/225], Training Accuracy: 32.6436%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [149/225], Training Accuracy: 32.6552%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [150/225], Training Accuracy: 32.6458%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [151/225], Training Accuracy: 32.7297%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [152/225], Training Accuracy: 32.6789%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [153/225], Training Accuracy: 32.6287%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [154/225], Training Accuracy: 32.6705%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [155/225], Training Accuracy: 32.6411%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [156/225], Training Accuracy: 32.7023%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [157/225], Training Accuracy: 32.6533%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [158/225], Training Accuracy: 32.6444%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [159/225], Training Accuracy: 32.7339%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [160/225], Training Accuracy: 32.7148%, Training Loss: 0.6730%\n",
      "Epoch [10/100], Step [161/225], Training Accuracy: 32.7640%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [162/225], Training Accuracy: 32.7546%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [163/225], Training Accuracy: 32.8029%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [164/225], Training Accuracy: 32.8125%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [165/225], Training Accuracy: 32.7841%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [166/225], Training Accuracy: 32.7937%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [167/225], Training Accuracy: 32.8312%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [168/225], Training Accuracy: 32.7753%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [169/225], Training Accuracy: 32.7478%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [170/225], Training Accuracy: 32.6471%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [171/225], Training Accuracy: 32.6663%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [172/225], Training Accuracy: 32.6672%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [173/225], Training Accuracy: 32.6680%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [174/225], Training Accuracy: 32.6778%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [175/225], Training Accuracy: 32.6964%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [176/225], Training Accuracy: 32.6971%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [177/225], Training Accuracy: 32.7066%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [178/225], Training Accuracy: 32.6896%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [179/225], Training Accuracy: 32.6554%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [180/225], Training Accuracy: 32.6997%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [181/225], Training Accuracy: 32.6657%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [182/225], Training Accuracy: 32.6494%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [183/225], Training Accuracy: 32.6673%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [184/225], Training Accuracy: 32.6342%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [185/225], Training Accuracy: 32.6014%, Training Loss: 0.6730%\n",
      "Epoch [10/100], Step [186/225], Training Accuracy: 32.6445%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [187/225], Training Accuracy: 32.6287%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [188/225], Training Accuracy: 32.6213%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [189/225], Training Accuracy: 32.6389%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [190/225], Training Accuracy: 32.5740%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [191/225], Training Accuracy: 32.5344%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [192/225], Training Accuracy: 32.4870%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [193/225], Training Accuracy: 32.4968%, Training Loss: 0.6729%\n",
      "Epoch [10/100], Step [194/225], Training Accuracy: 32.5387%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [195/225], Training Accuracy: 32.5000%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [196/225], Training Accuracy: 32.5016%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [197/225], Training Accuracy: 32.5190%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [198/225], Training Accuracy: 32.5600%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [199/225], Training Accuracy: 32.5220%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [200/225], Training Accuracy: 32.4844%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [201/225], Training Accuracy: 32.5482%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [202/225], Training Accuracy: 32.5495%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [203/225], Training Accuracy: 32.5354%, Training Loss: 0.6727%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Step [204/225], Training Accuracy: 32.5751%, Training Loss: 0.6727%\n",
      "Epoch [10/100], Step [205/225], Training Accuracy: 32.5381%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [206/225], Training Accuracy: 32.5243%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [207/225], Training Accuracy: 32.5106%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [208/225], Training Accuracy: 32.5346%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [209/225], Training Accuracy: 32.5733%, Training Loss: 0.6728%\n",
      "Epoch [10/100], Step [210/225], Training Accuracy: 32.6116%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [211/225], Training Accuracy: 32.6200%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [212/225], Training Accuracy: 32.6430%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [213/225], Training Accuracy: 32.6144%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [214/225], Training Accuracy: 32.6227%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [215/225], Training Accuracy: 32.6017%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [216/225], Training Accuracy: 32.5593%, Training Loss: 0.6726%\n",
      "Epoch [10/100], Step [217/225], Training Accuracy: 32.5749%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [218/225], Training Accuracy: 32.5186%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [219/225], Training Accuracy: 32.5771%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [220/225], Training Accuracy: 32.6065%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [221/225], Training Accuracy: 32.5792%, Training Loss: 0.6725%\n",
      "Epoch [10/100], Step [222/225], Training Accuracy: 32.5873%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [223/225], Training Accuracy: 32.6373%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [224/225], Training Accuracy: 32.6102%, Training Loss: 0.6724%\n",
      "Epoch [10/100], Step [225/225], Training Accuracy: 32.5945%, Training Loss: 0.6725%\n",
      "Epoch [11/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6668%\n",
      "Epoch [11/100], Step [2/225], Training Accuracy: 35.1562%, Training Loss: 0.6696%\n",
      "Epoch [11/100], Step [3/225], Training Accuracy: 30.2083%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6748%\n",
      "Epoch [11/100], Step [5/225], Training Accuracy: 31.8750%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6790%\n",
      "Epoch [11/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6763%\n",
      "Epoch [11/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6764%\n",
      "Epoch [11/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6765%\n",
      "Epoch [11/100], Step [10/225], Training Accuracy: 32.0312%, Training Loss: 0.6754%\n",
      "Epoch [11/100], Step [11/225], Training Accuracy: 32.1023%, Training Loss: 0.6756%\n",
      "Epoch [11/100], Step [12/225], Training Accuracy: 31.5104%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [13/225], Training Accuracy: 31.4904%, Training Loss: 0.6756%\n",
      "Epoch [11/100], Step [14/225], Training Accuracy: 31.5848%, Training Loss: 0.6780%\n",
      "Epoch [11/100], Step [15/225], Training Accuracy: 31.6667%, Training Loss: 0.6783%\n",
      "Epoch [11/100], Step [16/225], Training Accuracy: 32.4219%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [17/225], Training Accuracy: 31.9853%, Training Loss: 0.6779%\n",
      "Epoch [11/100], Step [18/225], Training Accuracy: 32.2917%, Training Loss: 0.6775%\n",
      "Epoch [11/100], Step [19/225], Training Accuracy: 32.4836%, Training Loss: 0.6783%\n",
      "Epoch [11/100], Step [20/225], Training Accuracy: 32.7344%, Training Loss: 0.6777%\n",
      "Epoch [11/100], Step [21/225], Training Accuracy: 32.9613%, Training Loss: 0.6774%\n",
      "Epoch [11/100], Step [22/225], Training Accuracy: 33.0256%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [23/225], Training Accuracy: 33.2880%, Training Loss: 0.6765%\n",
      "Epoch [11/100], Step [24/225], Training Accuracy: 32.8125%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [25/225], Training Accuracy: 32.8125%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [26/225], Training Accuracy: 32.9928%, Training Loss: 0.6768%\n",
      "Epoch [11/100], Step [27/225], Training Accuracy: 32.5810%, Training Loss: 0.6765%\n",
      "Epoch [11/100], Step [28/225], Training Accuracy: 32.5335%, Training Loss: 0.6768%\n",
      "Epoch [11/100], Step [29/225], Training Accuracy: 32.9741%, Training Loss: 0.6762%\n",
      "Epoch [11/100], Step [30/225], Training Accuracy: 32.9688%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [31/225], Training Accuracy: 32.7621%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [32/225], Training Accuracy: 32.8125%, Training Loss: 0.6753%\n",
      "Epoch [11/100], Step [33/225], Training Accuracy: 32.8598%, Training Loss: 0.6751%\n",
      "Epoch [11/100], Step [34/225], Training Accuracy: 32.7206%, Training Loss: 0.6749%\n",
      "Epoch [11/100], Step [35/225], Training Accuracy: 32.6339%, Training Loss: 0.6750%\n",
      "Epoch [11/100], Step [36/225], Training Accuracy: 32.4653%, Training Loss: 0.6752%\n",
      "Epoch [11/100], Step [37/225], Training Accuracy: 32.5169%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [38/225], Training Accuracy: 32.3602%, Training Loss: 0.6758%\n",
      "Epoch [11/100], Step [39/225], Training Accuracy: 32.0112%, Training Loss: 0.6758%\n",
      "Epoch [11/100], Step [40/225], Training Accuracy: 32.0312%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [41/225], Training Accuracy: 32.0122%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [42/225], Training Accuracy: 31.8080%, Training Loss: 0.6763%\n",
      "Epoch [11/100], Step [43/225], Training Accuracy: 31.8677%, Training Loss: 0.6761%\n",
      "Epoch [11/100], Step [44/225], Training Accuracy: 31.8182%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [45/225], Training Accuracy: 31.8056%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [46/225], Training Accuracy: 31.6576%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [47/225], Training Accuracy: 31.6157%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6754%\n",
      "Epoch [11/100], Step [49/225], Training Accuracy: 31.8878%, Training Loss: 0.6752%\n",
      "Epoch [11/100], Step [50/225], Training Accuracy: 31.8125%, Training Loss: 0.6753%\n",
      "Epoch [11/100], Step [51/225], Training Accuracy: 32.1691%, Training Loss: 0.6750%\n",
      "Epoch [11/100], Step [52/225], Training Accuracy: 32.1514%, Training Loss: 0.6749%\n",
      "Epoch [11/100], Step [53/225], Training Accuracy: 32.0460%, Training Loss: 0.6749%\n",
      "Epoch [11/100], Step [54/225], Training Accuracy: 31.7998%, Training Loss: 0.6748%\n",
      "Epoch [11/100], Step [55/225], Training Accuracy: 31.8750%, Training Loss: 0.6748%\n",
      "Epoch [11/100], Step [56/225], Training Accuracy: 31.9475%, Training Loss: 0.6747%\n",
      "Epoch [11/100], Step [57/225], Training Accuracy: 31.9901%, Training Loss: 0.6745%\n",
      "Epoch [11/100], Step [58/225], Training Accuracy: 31.9235%, Training Loss: 0.6744%\n",
      "Epoch [11/100], Step [59/225], Training Accuracy: 32.2564%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [60/225], Training Accuracy: 32.2656%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [61/225], Training Accuracy: 32.2234%, Training Loss: 0.6740%\n",
      "Epoch [11/100], Step [62/225], Training Accuracy: 32.1825%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [63/225], Training Accuracy: 32.2917%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [64/225], Training Accuracy: 32.2998%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [65/225], Training Accuracy: 32.2356%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [66/225], Training Accuracy: 32.3627%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [67/225], Training Accuracy: 32.3461%, Training Loss: 0.6740%\n",
      "Epoch [11/100], Step [68/225], Training Accuracy: 32.4678%, Training Loss: 0.6738%\n",
      "Epoch [11/100], Step [69/225], Training Accuracy: 32.5181%, Training Loss: 0.6735%\n",
      "Epoch [11/100], Step [70/225], Training Accuracy: 32.5223%, Training Loss: 0.6736%\n",
      "Epoch [11/100], Step [71/225], Training Accuracy: 32.4824%, Training Loss: 0.6738%\n",
      "Epoch [11/100], Step [72/225], Training Accuracy: 32.2917%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [73/225], Training Accuracy: 32.2560%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [74/225], Training Accuracy: 32.3480%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [75/225], Training Accuracy: 32.2708%, Training Loss: 0.6741%\n",
      "Epoch [11/100], Step [76/225], Training Accuracy: 32.2163%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [77/225], Training Accuracy: 32.1429%, Training Loss: 0.6743%\n",
      "Epoch [11/100], Step [78/225], Training Accuracy: 32.1915%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [79/225], Training Accuracy: 32.1005%, Training Loss: 0.6743%\n",
      "Epoch [11/100], Step [80/225], Training Accuracy: 32.1289%, Training Loss: 0.6742%\n",
      "Epoch [11/100], Step [81/225], Training Accuracy: 32.0409%, Training Loss: 0.6744%\n",
      "Epoch [11/100], Step [82/225], Training Accuracy: 32.0503%, Training Loss: 0.6744%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Step [83/225], Training Accuracy: 31.9277%, Training Loss: 0.6744%\n",
      "Epoch [11/100], Step [84/225], Training Accuracy: 32.0126%, Training Loss: 0.6743%\n",
      "Epoch [11/100], Step [85/225], Training Accuracy: 31.9301%, Training Loss: 0.6745%\n",
      "Epoch [11/100], Step [86/225], Training Accuracy: 31.9949%, Training Loss: 0.6746%\n",
      "Epoch [11/100], Step [87/225], Training Accuracy: 32.0582%, Training Loss: 0.6746%\n",
      "Epoch [11/100], Step [88/225], Training Accuracy: 31.9780%, Training Loss: 0.6749%\n",
      "Epoch [11/100], Step [89/225], Training Accuracy: 31.8645%, Training Loss: 0.6750%\n",
      "Epoch [11/100], Step [90/225], Training Accuracy: 31.8403%, Training Loss: 0.6750%\n",
      "Epoch [11/100], Step [91/225], Training Accuracy: 31.8681%, Training Loss: 0.6752%\n",
      "Epoch [11/100], Step [92/225], Training Accuracy: 31.8614%, Training Loss: 0.6752%\n",
      "Epoch [11/100], Step [93/225], Training Accuracy: 31.8548%, Training Loss: 0.6753%\n",
      "Epoch [11/100], Step [94/225], Training Accuracy: 31.9481%, Training Loss: 0.6752%\n",
      "Epoch [11/100], Step [95/225], Training Accuracy: 31.8257%, Training Loss: 0.6755%\n",
      "Epoch [11/100], Step [96/225], Training Accuracy: 31.8685%, Training Loss: 0.6756%\n",
      "Epoch [11/100], Step [97/225], Training Accuracy: 31.9427%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [98/225], Training Accuracy: 31.9834%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [99/225], Training Accuracy: 32.1181%, Training Loss: 0.6756%\n",
      "Epoch [11/100], Step [100/225], Training Accuracy: 32.1562%, Training Loss: 0.6755%\n",
      "Epoch [11/100], Step [101/225], Training Accuracy: 32.2556%, Training Loss: 0.6754%\n",
      "Epoch [11/100], Step [102/225], Training Accuracy: 32.1844%, Training Loss: 0.6755%\n",
      "Epoch [11/100], Step [103/225], Training Accuracy: 32.1450%, Training Loss: 0.6756%\n",
      "Epoch [11/100], Step [104/225], Training Accuracy: 32.1064%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [105/225], Training Accuracy: 32.0536%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [106/225], Training Accuracy: 32.0460%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [107/225], Training Accuracy: 32.0239%, Training Loss: 0.6757%\n",
      "Epoch [11/100], Step [108/225], Training Accuracy: 32.1036%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [109/225], Training Accuracy: 32.0241%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [110/225], Training Accuracy: 32.0170%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [111/225], Training Accuracy: 31.9538%, Training Loss: 0.6761%\n",
      "Epoch [11/100], Step [112/225], Training Accuracy: 32.0871%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [113/225], Training Accuracy: 32.0520%, Training Loss: 0.6761%\n",
      "Epoch [11/100], Step [114/225], Training Accuracy: 32.0861%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [115/225], Training Accuracy: 32.0516%, Training Loss: 0.6760%\n",
      "Epoch [11/100], Step [116/225], Training Accuracy: 32.0178%, Training Loss: 0.6759%\n",
      "Epoch [11/100], Step [117/225], Training Accuracy: 31.9177%, Training Loss: 0.6762%\n",
      "Epoch [11/100], Step [118/225], Training Accuracy: 31.8856%, Training Loss: 0.6762%\n",
      "Epoch [11/100], Step [119/225], Training Accuracy: 31.8146%, Training Loss: 0.6763%\n",
      "Epoch [11/100], Step [120/225], Training Accuracy: 31.8490%, Training Loss: 0.6764%\n",
      "Epoch [11/100], Step [121/225], Training Accuracy: 31.8311%, Training Loss: 0.6765%\n",
      "Epoch [11/100], Step [122/225], Training Accuracy: 31.8648%, Training Loss: 0.6764%\n",
      "Epoch [11/100], Step [123/225], Training Accuracy: 31.8979%, Training Loss: 0.6764%\n",
      "Epoch [11/100], Step [124/225], Training Accuracy: 31.8674%, Training Loss: 0.6765%\n",
      "Epoch [11/100], Step [125/225], Training Accuracy: 31.8750%, Training Loss: 0.6767%\n",
      "Epoch [11/100], Step [126/225], Training Accuracy: 31.8204%, Training Loss: 0.6767%\n",
      "Epoch [11/100], Step [127/225], Training Accuracy: 31.7298%, Training Loss: 0.6767%\n",
      "Epoch [11/100], Step [128/225], Training Accuracy: 31.7139%, Training Loss: 0.6768%\n",
      "Epoch [11/100], Step [129/225], Training Accuracy: 31.7708%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [130/225], Training Accuracy: 31.7188%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [131/225], Training Accuracy: 31.7032%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [132/225], Training Accuracy: 31.6998%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [133/225], Training Accuracy: 31.7317%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [134/225], Training Accuracy: 31.7631%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [135/225], Training Accuracy: 31.7593%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [136/225], Training Accuracy: 31.7785%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [137/225], Training Accuracy: 31.8203%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [138/225], Training Accuracy: 31.8614%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [139/225], Training Accuracy: 31.8233%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [140/225], Training Accuracy: 31.7857%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [141/225], Training Accuracy: 31.7376%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [142/225], Training Accuracy: 31.8002%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [143/225], Training Accuracy: 31.7963%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [144/225], Training Accuracy: 31.8142%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [145/225], Training Accuracy: 31.8750%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [146/225], Training Accuracy: 31.9135%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [147/225], Training Accuracy: 31.9196%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [148/225], Training Accuracy: 31.8834%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [149/225], Training Accuracy: 31.9002%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [150/225], Training Accuracy: 31.9167%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [151/225], Training Accuracy: 31.9743%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [152/225], Training Accuracy: 31.9593%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [153/225], Training Accuracy: 31.9240%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [154/225], Training Accuracy: 31.9095%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [155/225], Training Accuracy: 31.8952%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [156/225], Training Accuracy: 31.9211%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [157/225], Training Accuracy: 31.8571%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [158/225], Training Accuracy: 31.8137%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [159/225], Training Accuracy: 31.9084%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [160/225], Training Accuracy: 31.8945%, Training Loss: 0.6772%\n",
      "Epoch [11/100], Step [161/225], Training Accuracy: 31.9488%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [162/225], Training Accuracy: 31.9348%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [163/225], Training Accuracy: 31.9785%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [164/225], Training Accuracy: 31.9741%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [165/225], Training Accuracy: 31.9129%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [166/225], Training Accuracy: 31.8806%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [167/225], Training Accuracy: 31.9330%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [168/225], Training Accuracy: 31.8638%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [169/225], Training Accuracy: 31.8047%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [170/225], Training Accuracy: 31.7555%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [171/225], Training Accuracy: 31.7800%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [172/225], Training Accuracy: 31.7678%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [173/225], Training Accuracy: 31.7829%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [174/225], Training Accuracy: 31.7619%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [175/225], Training Accuracy: 31.8214%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [176/225], Training Accuracy: 31.8448%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [177/225], Training Accuracy: 31.8503%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [178/225], Training Accuracy: 31.8469%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [179/225], Training Accuracy: 31.8174%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [180/225], Training Accuracy: 31.8663%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [181/225], Training Accuracy: 31.8025%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [182/225], Training Accuracy: 31.7823%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [183/225], Training Accuracy: 31.7879%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [184/225], Training Accuracy: 31.7680%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [185/225], Training Accuracy: 31.7230%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [186/225], Training Accuracy: 31.7708%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [187/225], Training Accuracy: 31.7848%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [188/225], Training Accuracy: 31.7902%, Training Loss: 0.6771%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Step [189/225], Training Accuracy: 31.8204%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [190/225], Training Accuracy: 31.7763%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [191/225], Training Accuracy: 31.7408%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [192/225], Training Accuracy: 31.6813%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [193/225], Training Accuracy: 31.6710%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [194/225], Training Accuracy: 31.7091%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [195/225], Training Accuracy: 31.6747%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [196/225], Training Accuracy: 31.6566%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [197/225], Training Accuracy: 31.6624%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [198/225], Training Accuracy: 31.6919%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [199/225], Training Accuracy: 31.6583%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [200/225], Training Accuracy: 31.6406%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [201/225], Training Accuracy: 31.6542%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [202/225], Training Accuracy: 31.6600%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [203/225], Training Accuracy: 31.6425%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [204/225], Training Accuracy: 31.7096%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [205/225], Training Accuracy: 31.7073%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [206/225], Training Accuracy: 31.7127%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [207/225], Training Accuracy: 31.6878%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [208/225], Training Accuracy: 31.7007%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [209/225], Training Accuracy: 31.7359%, Training Loss: 0.6771%\n",
      "Epoch [11/100], Step [210/225], Training Accuracy: 31.7634%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [211/225], Training Accuracy: 31.7610%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [212/225], Training Accuracy: 31.8028%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [213/225], Training Accuracy: 31.7782%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [214/225], Training Accuracy: 31.7903%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [215/225], Training Accuracy: 31.7587%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [216/225], Training Accuracy: 31.7057%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [217/225], Training Accuracy: 31.7180%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [218/225], Training Accuracy: 31.6657%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [219/225], Training Accuracy: 31.7209%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [220/225], Training Accuracy: 31.7401%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [221/225], Training Accuracy: 31.7308%, Training Loss: 0.6770%\n",
      "Epoch [11/100], Step [222/225], Training Accuracy: 31.7497%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [223/225], Training Accuracy: 31.7895%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [224/225], Training Accuracy: 31.7592%, Training Loss: 0.6769%\n",
      "Epoch [11/100], Step [225/225], Training Accuracy: 31.7468%, Training Loss: 0.6770%\n",
      "Epoch [12/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6766%\n",
      "Epoch [12/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6785%\n",
      "Epoch [12/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6835%\n",
      "Epoch [12/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6803%\n",
      "Epoch [12/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6816%\n",
      "Epoch [12/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6793%\n",
      "Epoch [12/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6797%\n",
      "Epoch [12/100], Step [11/225], Training Accuracy: 31.3920%, Training Loss: 0.6794%\n",
      "Epoch [12/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6793%\n",
      "Epoch [12/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6795%\n",
      "Epoch [12/100], Step [14/225], Training Accuracy: 30.6920%, Training Loss: 0.6808%\n",
      "Epoch [12/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6808%\n",
      "Epoch [12/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6806%\n",
      "Epoch [12/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6810%\n",
      "Epoch [12/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6813%\n",
      "Epoch [12/100], Step [20/225], Training Accuracy: 31.3281%, Training Loss: 0.6810%\n",
      "Epoch [12/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6805%\n",
      "Epoch [12/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6803%\n",
      "Epoch [12/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [24/225], Training Accuracy: 31.8359%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6797%\n",
      "Epoch [12/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6795%\n",
      "Epoch [12/100], Step [27/225], Training Accuracy: 32.2338%, Training Loss: 0.6791%\n",
      "Epoch [12/100], Step [28/225], Training Accuracy: 32.3661%, Training Loss: 0.6790%\n",
      "Epoch [12/100], Step [29/225], Training Accuracy: 32.5431%, Training Loss: 0.6786%\n",
      "Epoch [12/100], Step [30/225], Training Accuracy: 32.6042%, Training Loss: 0.6785%\n",
      "Epoch [12/100], Step [31/225], Training Accuracy: 32.7117%, Training Loss: 0.6782%\n",
      "Epoch [12/100], Step [32/225], Training Accuracy: 32.7637%, Training Loss: 0.6777%\n",
      "Epoch [12/100], Step [33/225], Training Accuracy: 32.8598%, Training Loss: 0.6775%\n",
      "Epoch [12/100], Step [34/225], Training Accuracy: 32.6746%, Training Loss: 0.6774%\n",
      "Epoch [12/100], Step [35/225], Training Accuracy: 32.4554%, Training Loss: 0.6778%\n",
      "Epoch [12/100], Step [36/225], Training Accuracy: 32.2483%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [37/225], Training Accuracy: 32.3057%, Training Loss: 0.6783%\n",
      "Epoch [12/100], Step [38/225], Training Accuracy: 32.1135%, Training Loss: 0.6785%\n",
      "Epoch [12/100], Step [39/225], Training Accuracy: 31.8109%, Training Loss: 0.6786%\n",
      "Epoch [12/100], Step [40/225], Training Accuracy: 31.7969%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [41/225], Training Accuracy: 31.8598%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [42/225], Training Accuracy: 31.6964%, Training Loss: 0.6792%\n",
      "Epoch [12/100], Step [43/225], Training Accuracy: 31.7951%, Training Loss: 0.6790%\n",
      "Epoch [12/100], Step [44/225], Training Accuracy: 31.8182%, Training Loss: 0.6790%\n",
      "Epoch [12/100], Step [45/225], Training Accuracy: 31.8403%, Training Loss: 0.6788%\n",
      "Epoch [12/100], Step [46/225], Training Accuracy: 31.6916%, Training Loss: 0.6788%\n",
      "Epoch [12/100], Step [47/225], Training Accuracy: 31.6489%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [48/225], Training Accuracy: 31.6406%, Training Loss: 0.6784%\n",
      "Epoch [12/100], Step [49/225], Training Accuracy: 31.6964%, Training Loss: 0.6784%\n",
      "Epoch [12/100], Step [50/225], Training Accuracy: 31.6562%, Training Loss: 0.6786%\n",
      "Epoch [12/100], Step [51/225], Training Accuracy: 31.8321%, Training Loss: 0.6784%\n",
      "Epoch [12/100], Step [52/225], Training Accuracy: 31.9111%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [53/225], Training Accuracy: 31.8396%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [54/225], Training Accuracy: 31.7419%, Training Loss: 0.6782%\n",
      "Epoch [12/100], Step [55/225], Training Accuracy: 31.9034%, Training Loss: 0.6782%\n",
      "Epoch [12/100], Step [56/225], Training Accuracy: 31.9475%, Training Loss: 0.6783%\n",
      "Epoch [12/100], Step [57/225], Training Accuracy: 31.9627%, Training Loss: 0.6782%\n",
      "Epoch [12/100], Step [58/225], Training Accuracy: 31.8427%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [59/225], Training Accuracy: 32.2299%, Training Loss: 0.6778%\n",
      "Epoch [12/100], Step [60/225], Training Accuracy: 32.3438%, Training Loss: 0.6779%\n",
      "Epoch [12/100], Step [61/225], Training Accuracy: 32.3258%, Training Loss: 0.6777%\n",
      "Epoch [12/100], Step [62/225], Training Accuracy: 32.3085%, Training Loss: 0.6779%\n",
      "Epoch [12/100], Step [63/225], Training Accuracy: 32.3661%, Training Loss: 0.6779%\n",
      "Epoch [12/100], Step [64/225], Training Accuracy: 32.3730%, Training Loss: 0.6777%\n",
      "Epoch [12/100], Step [65/225], Training Accuracy: 32.2596%, Training Loss: 0.6779%\n",
      "Epoch [12/100], Step [66/225], Training Accuracy: 32.3864%, Training Loss: 0.6778%\n",
      "Epoch [12/100], Step [67/225], Training Accuracy: 32.3927%, Training Loss: 0.6777%\n",
      "Epoch [12/100], Step [68/225], Training Accuracy: 32.3759%, Training Loss: 0.6777%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Step [69/225], Training Accuracy: 32.3822%, Training Loss: 0.6775%\n",
      "Epoch [12/100], Step [70/225], Training Accuracy: 32.2991%, Training Loss: 0.6775%\n",
      "Epoch [12/100], Step [71/225], Training Accuracy: 32.3283%, Training Loss: 0.6776%\n",
      "Epoch [12/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6780%\n",
      "Epoch [12/100], Step [73/225], Training Accuracy: 31.9777%, Training Loss: 0.6780%\n",
      "Epoch [12/100], Step [74/225], Training Accuracy: 32.0735%, Training Loss: 0.6779%\n",
      "Epoch [12/100], Step [75/225], Training Accuracy: 32.0208%, Training Loss: 0.6779%\n",
      "Epoch [12/100], Step [76/225], Training Accuracy: 31.9490%, Training Loss: 0.6780%\n",
      "Epoch [12/100], Step [77/225], Training Accuracy: 31.8994%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [78/225], Training Accuracy: 31.9111%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [79/225], Training Accuracy: 31.8434%, Training Loss: 0.6782%\n",
      "Epoch [12/100], Step [80/225], Training Accuracy: 31.8164%, Training Loss: 0.6781%\n",
      "Epoch [12/100], Step [81/225], Training Accuracy: 31.7515%, Training Loss: 0.6783%\n",
      "Epoch [12/100], Step [82/225], Training Accuracy: 31.7454%, Training Loss: 0.6783%\n",
      "Epoch [12/100], Step [83/225], Training Accuracy: 31.6642%, Training Loss: 0.6783%\n",
      "Epoch [12/100], Step [84/225], Training Accuracy: 31.7522%, Training Loss: 0.6783%\n",
      "Epoch [12/100], Step [85/225], Training Accuracy: 31.7279%, Training Loss: 0.6784%\n",
      "Epoch [12/100], Step [86/225], Training Accuracy: 31.8314%, Training Loss: 0.6785%\n",
      "Epoch [12/100], Step [87/225], Training Accuracy: 31.8247%, Training Loss: 0.6785%\n",
      "Epoch [12/100], Step [88/225], Training Accuracy: 31.7649%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [89/225], Training Accuracy: 31.6538%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [90/225], Training Accuracy: 31.5799%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [91/225], Training Accuracy: 31.6106%, Training Loss: 0.6788%\n",
      "Epoch [12/100], Step [92/225], Training Accuracy: 31.6236%, Training Loss: 0.6788%\n",
      "Epoch [12/100], Step [93/225], Training Accuracy: 31.5188%, Training Loss: 0.6788%\n",
      "Epoch [12/100], Step [94/225], Training Accuracy: 31.6656%, Training Loss: 0.6787%\n",
      "Epoch [12/100], Step [95/225], Training Accuracy: 31.5461%, Training Loss: 0.6789%\n",
      "Epoch [12/100], Step [96/225], Training Accuracy: 31.6406%, Training Loss: 0.6789%\n",
      "Epoch [12/100], Step [97/225], Training Accuracy: 31.6849%, Training Loss: 0.6790%\n",
      "Epoch [12/100], Step [98/225], Training Accuracy: 31.6805%, Training Loss: 0.6790%\n",
      "Epoch [12/100], Step [99/225], Training Accuracy: 31.7708%, Training Loss: 0.6789%\n",
      "Epoch [12/100], Step [100/225], Training Accuracy: 31.7812%, Training Loss: 0.6790%\n",
      "Epoch [12/100], Step [101/225], Training Accuracy: 31.8533%, Training Loss: 0.6789%\n",
      "Epoch [12/100], Step [102/225], Training Accuracy: 31.7402%, Training Loss: 0.6789%\n",
      "Epoch [12/100], Step [103/225], Training Accuracy: 31.7506%, Training Loss: 0.6791%\n",
      "Epoch [12/100], Step [104/225], Training Accuracy: 31.7608%, Training Loss: 0.6791%\n",
      "Epoch [12/100], Step [105/225], Training Accuracy: 31.7262%, Training Loss: 0.6792%\n",
      "Epoch [12/100], Step [106/225], Training Accuracy: 31.7364%, Training Loss: 0.6791%\n",
      "Epoch [12/100], Step [107/225], Training Accuracy: 31.6735%, Training Loss: 0.6792%\n",
      "Epoch [12/100], Step [108/225], Training Accuracy: 31.7130%, Training Loss: 0.6793%\n",
      "Epoch [12/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6794%\n",
      "Epoch [12/100], Step [110/225], Training Accuracy: 31.5767%, Training Loss: 0.6794%\n",
      "Epoch [12/100], Step [111/225], Training Accuracy: 31.5034%, Training Loss: 0.6795%\n",
      "Epoch [12/100], Step [112/225], Training Accuracy: 31.6267%, Training Loss: 0.6794%\n",
      "Epoch [12/100], Step [113/225], Training Accuracy: 31.6095%, Training Loss: 0.6795%\n",
      "Epoch [12/100], Step [114/225], Training Accuracy: 31.6749%, Training Loss: 0.6794%\n",
      "Epoch [12/100], Step [115/225], Training Accuracy: 31.6440%, Training Loss: 0.6794%\n",
      "Epoch [12/100], Step [116/225], Training Accuracy: 31.6406%, Training Loss: 0.6793%\n",
      "Epoch [12/100], Step [117/225], Training Accuracy: 31.5438%, Training Loss: 0.6795%\n",
      "Epoch [12/100], Step [118/225], Training Accuracy: 31.5281%, Training Loss: 0.6796%\n",
      "Epoch [12/100], Step [119/225], Training Accuracy: 31.4863%, Training Loss: 0.6796%\n",
      "Epoch [12/100], Step [120/225], Training Accuracy: 31.5104%, Training Loss: 0.6797%\n",
      "Epoch [12/100], Step [121/225], Training Accuracy: 31.5212%, Training Loss: 0.6797%\n",
      "Epoch [12/100], Step [122/225], Training Accuracy: 31.4805%, Training Loss: 0.6796%\n",
      "Epoch [12/100], Step [123/225], Training Accuracy: 31.5041%, Training Loss: 0.6797%\n",
      "Epoch [12/100], Step [124/225], Training Accuracy: 31.4894%, Training Loss: 0.6797%\n",
      "Epoch [12/100], Step [125/225], Training Accuracy: 31.4625%, Training Loss: 0.6798%\n",
      "Epoch [12/100], Step [126/225], Training Accuracy: 31.4112%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [127/225], Training Accuracy: 31.3115%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [128/225], Training Accuracy: 31.2988%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [129/225], Training Accuracy: 31.3227%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [130/225], Training Accuracy: 31.2740%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [131/225], Training Accuracy: 31.2381%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [132/225], Training Accuracy: 31.1908%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [133/225], Training Accuracy: 31.2030%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [134/225], Training Accuracy: 31.2500%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [135/225], Training Accuracy: 31.2384%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [136/225], Training Accuracy: 31.2615%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [137/225], Training Accuracy: 31.2956%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [138/225], Training Accuracy: 31.3179%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [139/225], Training Accuracy: 31.2837%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [140/225], Training Accuracy: 31.2388%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [141/225], Training Accuracy: 31.1946%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [142/225], Training Accuracy: 31.2500%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [143/225], Training Accuracy: 31.2500%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [144/225], Training Accuracy: 31.2609%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [145/225], Training Accuracy: 31.3254%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [146/225], Training Accuracy: 31.3356%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [147/225], Training Accuracy: 31.3563%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [148/225], Training Accuracy: 31.3239%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [149/225], Training Accuracy: 31.3444%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [150/225], Training Accuracy: 31.3750%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [151/225], Training Accuracy: 31.4259%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [152/225], Training Accuracy: 31.4145%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [153/225], Training Accuracy: 31.3725%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [154/225], Training Accuracy: 31.4022%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [155/225], Training Accuracy: 31.4012%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [156/225], Training Accuracy: 31.4203%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [157/225], Training Accuracy: 31.3595%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [158/225], Training Accuracy: 31.3390%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [159/225], Training Accuracy: 31.3876%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [160/225], Training Accuracy: 31.3672%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [161/225], Training Accuracy: 31.4247%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [162/225], Training Accuracy: 31.4333%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [163/225], Training Accuracy: 31.5088%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [164/225], Training Accuracy: 31.4691%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [165/225], Training Accuracy: 31.4015%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [166/225], Training Accuracy: 31.3818%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [167/225], Training Accuracy: 31.4465%, Training Loss: 0.6800%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Step [168/225], Training Accuracy: 31.3895%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [169/225], Training Accuracy: 31.3240%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [170/225], Training Accuracy: 31.2776%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [171/225], Training Accuracy: 31.3048%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [172/225], Training Accuracy: 31.3136%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [173/225], Training Accuracy: 31.3132%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [174/225], Training Accuracy: 31.3308%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [175/225], Training Accuracy: 31.3661%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [176/225], Training Accuracy: 31.3654%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [177/225], Training Accuracy: 31.3559%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [178/225], Training Accuracy: 31.3641%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [179/225], Training Accuracy: 31.3286%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [180/225], Training Accuracy: 31.3715%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [181/225], Training Accuracy: 31.3277%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [182/225], Training Accuracy: 31.3015%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [183/225], Training Accuracy: 31.3098%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [184/225], Training Accuracy: 31.2755%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [185/225], Training Accuracy: 31.2247%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [186/225], Training Accuracy: 31.2752%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [187/225], Training Accuracy: 31.2834%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [188/225], Training Accuracy: 31.2749%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [189/225], Training Accuracy: 31.2831%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [190/225], Training Accuracy: 31.2171%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [191/225], Training Accuracy: 31.1846%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [192/225], Training Accuracy: 31.1117%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [193/225], Training Accuracy: 31.1286%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [194/225], Training Accuracy: 31.1453%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [195/225], Training Accuracy: 31.1218%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [196/225], Training Accuracy: 31.0906%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [197/225], Training Accuracy: 31.1390%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [198/225], Training Accuracy: 31.1632%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [199/225], Training Accuracy: 31.1558%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [200/225], Training Accuracy: 31.1406%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [201/225], Training Accuracy: 31.1723%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [202/225], Training Accuracy: 31.1804%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [203/225], Training Accuracy: 31.1884%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [204/225], Training Accuracy: 31.2653%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [205/225], Training Accuracy: 31.2576%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [206/225], Training Accuracy: 31.2500%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [207/225], Training Accuracy: 31.2123%, Training Loss: 0.6802%\n",
      "Epoch [12/100], Step [208/225], Training Accuracy: 31.2575%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [209/225], Training Accuracy: 31.2949%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [210/225], Training Accuracy: 31.3244%, Training Loss: 0.6801%\n",
      "Epoch [12/100], Step [211/225], Training Accuracy: 31.3241%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [212/225], Training Accuracy: 31.3753%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [213/225], Training Accuracy: 31.3600%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [214/225], Training Accuracy: 31.3814%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [215/225], Training Accuracy: 31.3445%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [216/225], Training Accuracy: 31.2862%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [217/225], Training Accuracy: 31.2860%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [218/225], Training Accuracy: 31.2500%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [219/225], Training Accuracy: 31.3071%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [220/225], Training Accuracy: 31.3210%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [221/225], Training Accuracy: 31.3066%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [222/225], Training Accuracy: 31.3415%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [223/225], Training Accuracy: 31.3761%, Training Loss: 0.6800%\n",
      "Epoch [12/100], Step [224/225], Training Accuracy: 31.3756%, Training Loss: 0.6799%\n",
      "Epoch [12/100], Step [225/225], Training Accuracy: 31.3577%, Training Loss: 0.6800%\n",
      "Epoch [13/100], Step [1/225], Training Accuracy: 37.5000%, Training Loss: 0.6849%\n",
      "Epoch [13/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6835%\n",
      "Epoch [13/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6852%\n",
      "Epoch [13/100], Step [4/225], Training Accuracy: 33.2031%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6816%\n",
      "Epoch [13/100], Step [6/225], Training Accuracy: 33.3333%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [7/225], Training Accuracy: 33.0357%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [8/225], Training Accuracy: 32.6172%, Training Loss: 0.6807%\n",
      "Epoch [13/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6814%\n",
      "Epoch [13/100], Step [11/225], Training Accuracy: 31.5341%, Training Loss: 0.6818%\n",
      "Epoch [13/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6821%\n",
      "Epoch [13/100], Step [13/225], Training Accuracy: 30.8894%, Training Loss: 0.6820%\n",
      "Epoch [13/100], Step [14/225], Training Accuracy: 31.1384%, Training Loss: 0.6831%\n",
      "Epoch [13/100], Step [15/225], Training Accuracy: 31.4583%, Training Loss: 0.6831%\n",
      "Epoch [13/100], Step [16/225], Training Accuracy: 31.3477%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [17/225], Training Accuracy: 31.0662%, Training Loss: 0.6828%\n",
      "Epoch [13/100], Step [18/225], Training Accuracy: 31.1632%, Training Loss: 0.6828%\n",
      "Epoch [13/100], Step [19/225], Training Accuracy: 31.4967%, Training Loss: 0.6831%\n",
      "Epoch [13/100], Step [20/225], Training Accuracy: 31.9531%, Training Loss: 0.6829%\n",
      "Epoch [13/100], Step [21/225], Training Accuracy: 32.0685%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [22/225], Training Accuracy: 31.9602%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [23/225], Training Accuracy: 32.1332%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [24/225], Training Accuracy: 32.1615%, Training Loss: 0.6820%\n",
      "Epoch [13/100], Step [25/225], Training Accuracy: 32.2500%, Training Loss: 0.6818%\n",
      "Epoch [13/100], Step [26/225], Training Accuracy: 32.4519%, Training Loss: 0.6817%\n",
      "Epoch [13/100], Step [27/225], Training Accuracy: 32.2917%, Training Loss: 0.6813%\n",
      "Epoch [13/100], Step [28/225], Training Accuracy: 32.0312%, Training Loss: 0.6814%\n",
      "Epoch [13/100], Step [29/225], Training Accuracy: 32.2737%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [30/225], Training Accuracy: 32.2917%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [31/225], Training Accuracy: 32.1573%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [32/225], Training Accuracy: 32.5195%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [33/225], Training Accuracy: 32.6705%, Training Loss: 0.6803%\n",
      "Epoch [13/100], Step [34/225], Training Accuracy: 32.3989%, Training Loss: 0.6803%\n",
      "Epoch [13/100], Step [35/225], Training Accuracy: 32.3214%, Training Loss: 0.6805%\n",
      "Epoch [13/100], Step [36/225], Training Accuracy: 32.2049%, Training Loss: 0.6807%\n",
      "Epoch [13/100], Step [37/225], Training Accuracy: 32.1791%, Training Loss: 0.6809%\n",
      "Epoch [13/100], Step [38/225], Training Accuracy: 31.9490%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6811%\n",
      "Epoch [13/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6811%\n",
      "Epoch [13/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6811%\n",
      "Epoch [13/100], Step [42/225], Training Accuracy: 31.5848%, Training Loss: 0.6814%\n",
      "Epoch [13/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6813%\n",
      "Epoch [13/100], Step [44/225], Training Accuracy: 31.6406%, Training Loss: 0.6811%\n",
      "Epoch [13/100], Step [45/225], Training Accuracy: 31.7361%, Training Loss: 0.6811%\n",
      "Epoch [13/100], Step [46/225], Training Accuracy: 31.6236%, Training Loss: 0.6811%\n",
      "Epoch [13/100], Step [47/225], Training Accuracy: 31.4827%, Training Loss: 0.6812%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Step [48/225], Training Accuracy: 31.5755%, Training Loss: 0.6809%\n",
      "Epoch [13/100], Step [49/225], Training Accuracy: 31.6327%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [50/225], Training Accuracy: 31.6250%, Training Loss: 0.6812%\n",
      "Epoch [13/100], Step [51/225], Training Accuracy: 31.7708%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [52/225], Training Accuracy: 31.7608%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [53/225], Training Accuracy: 31.7512%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6809%\n",
      "Epoch [13/100], Step [55/225], Training Accuracy: 31.8750%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6809%\n",
      "Epoch [13/100], Step [57/225], Training Accuracy: 31.9353%, Training Loss: 0.6807%\n",
      "Epoch [13/100], Step [58/225], Training Accuracy: 31.7619%, Training Loss: 0.6806%\n",
      "Epoch [13/100], Step [59/225], Training Accuracy: 32.1769%, Training Loss: 0.6803%\n",
      "Epoch [13/100], Step [60/225], Training Accuracy: 32.2656%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [61/225], Training Accuracy: 32.1721%, Training Loss: 0.6803%\n",
      "Epoch [13/100], Step [62/225], Training Accuracy: 32.1069%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [63/225], Training Accuracy: 32.1429%, Training Loss: 0.6805%\n",
      "Epoch [13/100], Step [64/225], Training Accuracy: 32.1045%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [65/225], Training Accuracy: 31.9952%, Training Loss: 0.6805%\n",
      "Epoch [13/100], Step [66/225], Training Accuracy: 32.1259%, Training Loss: 0.6805%\n",
      "Epoch [13/100], Step [67/225], Training Accuracy: 32.0429%, Training Loss: 0.6803%\n",
      "Epoch [13/100], Step [68/225], Training Accuracy: 32.0542%, Training Loss: 0.6803%\n",
      "Epoch [13/100], Step [69/225], Training Accuracy: 32.0426%, Training Loss: 0.6801%\n",
      "Epoch [13/100], Step [70/225], Training Accuracy: 31.9866%, Training Loss: 0.6802%\n",
      "Epoch [13/100], Step [71/225], Training Accuracy: 32.0423%, Training Loss: 0.6802%\n",
      "Epoch [13/100], Step [72/225], Training Accuracy: 31.8359%, Training Loss: 0.6805%\n",
      "Epoch [13/100], Step [73/225], Training Accuracy: 31.7423%, Training Loss: 0.6805%\n",
      "Epoch [13/100], Step [74/225], Training Accuracy: 31.8412%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [76/225], Training Accuracy: 31.7845%, Training Loss: 0.6804%\n",
      "Epoch [13/100], Step [77/225], Training Accuracy: 31.7167%, Training Loss: 0.6806%\n",
      "Epoch [13/100], Step [78/225], Training Accuracy: 31.7308%, Training Loss: 0.6806%\n",
      "Epoch [13/100], Step [79/225], Training Accuracy: 31.6456%, Training Loss: 0.6807%\n",
      "Epoch [13/100], Step [80/225], Training Accuracy: 31.6016%, Training Loss: 0.6807%\n",
      "Epoch [13/100], Step [81/225], Training Accuracy: 31.5394%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [82/225], Training Accuracy: 31.5358%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [83/225], Training Accuracy: 31.4759%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [84/225], Training Accuracy: 31.5848%, Training Loss: 0.6808%\n",
      "Epoch [13/100], Step [85/225], Training Accuracy: 31.5625%, Training Loss: 0.6809%\n",
      "Epoch [13/100], Step [86/225], Training Accuracy: 31.6315%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [87/225], Training Accuracy: 31.6272%, Training Loss: 0.6810%\n",
      "Epoch [13/100], Step [88/225], Training Accuracy: 31.6051%, Training Loss: 0.6812%\n",
      "Epoch [13/100], Step [89/225], Training Accuracy: 31.5133%, Training Loss: 0.6812%\n",
      "Epoch [13/100], Step [90/225], Training Accuracy: 31.4236%, Training Loss: 0.6812%\n",
      "Epoch [13/100], Step [91/225], Training Accuracy: 31.4904%, Training Loss: 0.6812%\n",
      "Epoch [13/100], Step [92/225], Training Accuracy: 31.4538%, Training Loss: 0.6812%\n",
      "Epoch [13/100], Step [93/225], Training Accuracy: 31.3844%, Training Loss: 0.6813%\n",
      "Epoch [13/100], Step [94/225], Training Accuracy: 31.4827%, Training Loss: 0.6813%\n",
      "Epoch [13/100], Step [95/225], Training Accuracy: 31.3651%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [96/225], Training Accuracy: 31.4290%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [97/225], Training Accuracy: 31.4433%, Training Loss: 0.6816%\n",
      "Epoch [13/100], Step [98/225], Training Accuracy: 31.4573%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [99/225], Training Accuracy: 31.5814%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [100/225], Training Accuracy: 31.5781%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [101/225], Training Accuracy: 31.7141%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [102/225], Training Accuracy: 31.5870%, Training Loss: 0.6815%\n",
      "Epoch [13/100], Step [103/225], Training Accuracy: 31.6292%, Training Loss: 0.6816%\n",
      "Epoch [13/100], Step [104/225], Training Accuracy: 31.5805%, Training Loss: 0.6817%\n",
      "Epoch [13/100], Step [105/225], Training Accuracy: 31.5179%, Training Loss: 0.6818%\n",
      "Epoch [13/100], Step [106/225], Training Accuracy: 31.5153%, Training Loss: 0.6817%\n",
      "Epoch [13/100], Step [107/225], Training Accuracy: 31.4544%, Training Loss: 0.6818%\n",
      "Epoch [13/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6818%\n",
      "Epoch [13/100], Step [109/225], Training Accuracy: 31.3933%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [110/225], Training Accuracy: 31.4062%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [111/225], Training Accuracy: 31.3063%, Training Loss: 0.6820%\n",
      "Epoch [13/100], Step [112/225], Training Accuracy: 31.4314%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [113/225], Training Accuracy: 31.4298%, Training Loss: 0.6820%\n",
      "Epoch [13/100], Step [114/225], Training Accuracy: 31.4967%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [115/225], Training Accuracy: 31.4674%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [116/225], Training Accuracy: 31.4790%, Training Loss: 0.6819%\n",
      "Epoch [13/100], Step [117/225], Training Accuracy: 31.4103%, Training Loss: 0.6820%\n",
      "Epoch [13/100], Step [118/225], Training Accuracy: 31.3824%, Training Loss: 0.6820%\n",
      "Epoch [13/100], Step [119/225], Training Accuracy: 31.3419%, Training Loss: 0.6821%\n",
      "Epoch [13/100], Step [120/225], Training Accuracy: 31.3802%, Training Loss: 0.6821%\n",
      "Epoch [13/100], Step [121/225], Training Accuracy: 31.3662%, Training Loss: 0.6822%\n",
      "Epoch [13/100], Step [122/225], Training Accuracy: 31.3909%, Training Loss: 0.6821%\n",
      "Epoch [13/100], Step [123/225], Training Accuracy: 31.4151%, Training Loss: 0.6821%\n",
      "Epoch [13/100], Step [124/225], Training Accuracy: 31.3634%, Training Loss: 0.6821%\n",
      "Epoch [13/100], Step [125/225], Training Accuracy: 31.3375%, Training Loss: 0.6822%\n",
      "Epoch [13/100], Step [126/225], Training Accuracy: 31.2872%, Training Loss: 0.6822%\n",
      "Epoch [13/100], Step [127/225], Training Accuracy: 31.2008%, Training Loss: 0.6822%\n",
      "Epoch [13/100], Step [128/225], Training Accuracy: 31.2012%, Training Loss: 0.6823%\n",
      "Epoch [13/100], Step [129/225], Training Accuracy: 31.2500%, Training Loss: 0.6823%\n",
      "Epoch [13/100], Step [130/225], Training Accuracy: 31.2019%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [131/225], Training Accuracy: 31.1784%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [132/225], Training Accuracy: 31.1671%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [133/225], Training Accuracy: 31.1678%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [134/225], Training Accuracy: 31.2150%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [135/225], Training Accuracy: 31.2269%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [136/225], Training Accuracy: 31.2385%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [137/225], Training Accuracy: 31.2728%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [138/225], Training Accuracy: 31.3066%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [139/225], Training Accuracy: 31.2612%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [140/225], Training Accuracy: 31.2165%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [141/225], Training Accuracy: 31.1503%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [142/225], Training Accuracy: 31.2170%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [143/225], Training Accuracy: 31.2391%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [144/225], Training Accuracy: 31.2174%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [145/225], Training Accuracy: 31.2500%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [146/225], Training Accuracy: 31.2928%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [147/225], Training Accuracy: 31.3138%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [148/225], Training Accuracy: 31.2922%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [149/225], Training Accuracy: 31.3339%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [150/225], Training Accuracy: 31.3750%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [151/225], Training Accuracy: 31.4156%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [152/225], Training Accuracy: 31.4042%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [153/225], Training Accuracy: 31.3828%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [154/225], Training Accuracy: 31.4022%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [155/225], Training Accuracy: 31.4012%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [156/225], Training Accuracy: 31.4303%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [157/225], Training Accuracy: 31.3595%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [158/225], Training Accuracy: 31.3390%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [159/225], Training Accuracy: 31.4072%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [160/225], Training Accuracy: 31.3672%, Training Loss: 0.6827%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Step [161/225], Training Accuracy: 31.4344%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [162/225], Training Accuracy: 31.4140%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [163/225], Training Accuracy: 31.4896%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [164/225], Training Accuracy: 31.4596%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [165/225], Training Accuracy: 31.4015%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [166/225], Training Accuracy: 31.4006%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [167/225], Training Accuracy: 31.4465%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [168/225], Training Accuracy: 31.4081%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [169/225], Training Accuracy: 31.3609%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [170/225], Training Accuracy: 31.3143%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [171/225], Training Accuracy: 31.3414%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [172/225], Training Accuracy: 31.3590%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [173/225], Training Accuracy: 31.3493%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [174/225], Training Accuracy: 31.3578%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [175/225], Training Accuracy: 31.3839%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [176/225], Training Accuracy: 31.3832%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [177/225], Training Accuracy: 31.3824%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [178/225], Training Accuracy: 31.3729%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [179/225], Training Accuracy: 31.3460%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [180/225], Training Accuracy: 31.3889%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [181/225], Training Accuracy: 31.3450%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [182/225], Training Accuracy: 31.3359%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [183/225], Training Accuracy: 31.3439%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [184/225], Training Accuracy: 31.3094%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [185/225], Training Accuracy: 31.2753%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [186/225], Training Accuracy: 31.3004%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [187/225], Training Accuracy: 31.3336%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [188/225], Training Accuracy: 31.3414%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [189/225], Training Accuracy: 31.3740%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [190/225], Training Accuracy: 31.3076%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [191/225], Training Accuracy: 31.2745%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [192/225], Training Accuracy: 31.2256%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [193/225], Training Accuracy: 31.2176%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [194/225], Training Accuracy: 31.2258%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [195/225], Training Accuracy: 31.2099%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [196/225], Training Accuracy: 31.1862%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [197/225], Training Accuracy: 31.2183%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [198/225], Training Accuracy: 31.2342%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [199/225], Training Accuracy: 31.2264%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [200/225], Training Accuracy: 31.2031%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [201/225], Training Accuracy: 31.2422%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [202/225], Training Accuracy: 31.2500%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [203/225], Training Accuracy: 31.2423%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [204/225], Training Accuracy: 31.2960%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [205/225], Training Accuracy: 31.3034%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [206/225], Training Accuracy: 31.2955%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [207/225], Training Accuracy: 31.2802%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [208/225], Training Accuracy: 31.3026%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [209/225], Training Accuracy: 31.3397%, Training Loss: 0.6826%\n",
      "Epoch [13/100], Step [210/225], Training Accuracy: 31.3542%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [211/225], Training Accuracy: 31.3611%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [212/225], Training Accuracy: 31.3900%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [213/225], Training Accuracy: 31.3674%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [214/225], Training Accuracy: 31.3887%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [215/225], Training Accuracy: 31.3517%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [216/225], Training Accuracy: 31.2934%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [217/225], Training Accuracy: 31.2860%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [218/225], Training Accuracy: 31.2572%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [219/225], Training Accuracy: 31.3142%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [220/225], Training Accuracy: 31.3281%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [221/225], Training Accuracy: 31.3207%, Training Loss: 0.6825%\n",
      "Epoch [13/100], Step [222/225], Training Accuracy: 31.3345%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [223/225], Training Accuracy: 31.3691%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [224/225], Training Accuracy: 31.3546%, Training Loss: 0.6824%\n",
      "Epoch [13/100], Step [225/225], Training Accuracy: 31.3438%, Training Loss: 0.6825%\n",
      "Epoch [14/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6901%\n",
      "Epoch [14/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6858%\n",
      "Epoch [14/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6874%\n",
      "Epoch [14/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6846%\n",
      "Epoch [14/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6850%\n",
      "Epoch [14/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6856%\n",
      "Epoch [14/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6842%\n",
      "Epoch [14/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6836%\n",
      "Epoch [14/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6846%\n",
      "Epoch [14/100], Step [10/225], Training Accuracy: 30.3125%, Training Loss: 0.6843%\n",
      "Epoch [14/100], Step [11/225], Training Accuracy: 30.5398%, Training Loss: 0.6843%\n",
      "Epoch [14/100], Step [12/225], Training Accuracy: 29.8177%, Training Loss: 0.6842%\n",
      "Epoch [14/100], Step [13/225], Training Accuracy: 29.8077%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [14/225], Training Accuracy: 30.1339%, Training Loss: 0.6848%\n",
      "Epoch [14/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6846%\n",
      "Epoch [14/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6843%\n",
      "Epoch [14/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6849%\n",
      "Epoch [14/100], Step [18/225], Training Accuracy: 30.2951%, Training Loss: 0.6849%\n",
      "Epoch [14/100], Step [19/225], Training Accuracy: 30.7566%, Training Loss: 0.6849%\n",
      "Epoch [14/100], Step [20/225], Training Accuracy: 31.1719%, Training Loss: 0.6846%\n",
      "Epoch [14/100], Step [21/225], Training Accuracy: 31.2500%, Training Loss: 0.6843%\n",
      "Epoch [14/100], Step [22/225], Training Accuracy: 31.1790%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [23/225], Training Accuracy: 31.1141%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [24/225], Training Accuracy: 31.1849%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [25/225], Training Accuracy: 31.4375%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [26/225], Training Accuracy: 31.6707%, Training Loss: 0.6837%\n",
      "Epoch [14/100], Step [27/225], Training Accuracy: 31.4236%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [28/225], Training Accuracy: 31.4174%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [29/225], Training Accuracy: 31.7888%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [30/225], Training Accuracy: 31.7188%, Training Loss: 0.6831%\n",
      "Epoch [14/100], Step [31/225], Training Accuracy: 31.7036%, Training Loss: 0.6830%\n",
      "Epoch [14/100], Step [32/225], Training Accuracy: 31.8848%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [33/225], Training Accuracy: 32.1023%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [34/225], Training Accuracy: 31.8934%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [35/225], Training Accuracy: 31.7411%, Training Loss: 0.6828%\n",
      "Epoch [14/100], Step [36/225], Training Accuracy: 31.5972%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [37/225], Training Accuracy: 31.6301%, Training Loss: 0.6830%\n",
      "Epoch [14/100], Step [38/225], Training Accuracy: 31.4967%, Training Loss: 0.6831%\n",
      "Epoch [14/100], Step [39/225], Training Accuracy: 31.2901%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [40/225], Training Accuracy: 31.3281%, Training Loss: 0.6832%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Step [41/225], Training Accuracy: 31.3643%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [42/225], Training Accuracy: 31.2872%, Training Loss: 0.6836%\n",
      "Epoch [14/100], Step [43/225], Training Accuracy: 31.4317%, Training Loss: 0.6834%\n",
      "Epoch [14/100], Step [44/225], Training Accuracy: 31.4276%, Training Loss: 0.6833%\n",
      "Epoch [14/100], Step [45/225], Training Accuracy: 31.5278%, Training Loss: 0.6833%\n",
      "Epoch [14/100], Step [46/225], Training Accuracy: 31.3859%, Training Loss: 0.6833%\n",
      "Epoch [14/100], Step [47/225], Training Accuracy: 31.3830%, Training Loss: 0.6833%\n",
      "Epoch [14/100], Step [48/225], Training Accuracy: 31.4779%, Training Loss: 0.6831%\n",
      "Epoch [14/100], Step [49/225], Training Accuracy: 31.5370%, Training Loss: 0.6831%\n",
      "Epoch [14/100], Step [50/225], Training Accuracy: 31.5312%, Training Loss: 0.6833%\n",
      "Epoch [14/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6831%\n",
      "Epoch [14/100], Step [52/225], Training Accuracy: 31.6106%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [53/225], Training Accuracy: 31.5448%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [54/225], Training Accuracy: 31.4815%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [55/225], Training Accuracy: 31.6193%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [56/225], Training Accuracy: 31.6685%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [57/225], Training Accuracy: 31.6886%, Training Loss: 0.6828%\n",
      "Epoch [14/100], Step [58/225], Training Accuracy: 31.5463%, Training Loss: 0.6828%\n",
      "Epoch [14/100], Step [59/225], Training Accuracy: 31.8326%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [60/225], Training Accuracy: 31.9271%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [61/225], Training Accuracy: 31.8391%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [62/225], Training Accuracy: 31.7288%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [63/225], Training Accuracy: 31.8204%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [64/225], Training Accuracy: 31.7871%, Training Loss: 0.6825%\n",
      "Epoch [14/100], Step [65/225], Training Accuracy: 31.6587%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [66/225], Training Accuracy: 31.7708%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [67/225], Training Accuracy: 31.7631%, Training Loss: 0.6825%\n",
      "Epoch [14/100], Step [68/225], Training Accuracy: 31.8244%, Training Loss: 0.6824%\n",
      "Epoch [14/100], Step [69/225], Training Accuracy: 31.7935%, Training Loss: 0.6822%\n",
      "Epoch [14/100], Step [70/225], Training Accuracy: 31.7411%, Training Loss: 0.6823%\n",
      "Epoch [14/100], Step [71/225], Training Accuracy: 31.7562%, Training Loss: 0.6824%\n",
      "Epoch [14/100], Step [72/225], Training Accuracy: 31.5538%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [73/225], Training Accuracy: 31.5068%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [74/225], Training Accuracy: 31.6090%, Training Loss: 0.6825%\n",
      "Epoch [14/100], Step [75/225], Training Accuracy: 31.5208%, Training Loss: 0.6825%\n",
      "Epoch [14/100], Step [76/225], Training Accuracy: 31.4556%, Training Loss: 0.6826%\n",
      "Epoch [14/100], Step [77/225], Training Accuracy: 31.3920%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [78/225], Training Accuracy: 31.4103%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [79/225], Training Accuracy: 31.3291%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [80/225], Training Accuracy: 31.3086%, Training Loss: 0.6827%\n",
      "Epoch [14/100], Step [81/225], Training Accuracy: 31.2500%, Training Loss: 0.6828%\n",
      "Epoch [14/100], Step [82/225], Training Accuracy: 31.2691%, Training Loss: 0.6828%\n",
      "Epoch [14/100], Step [83/225], Training Accuracy: 31.2123%, Training Loss: 0.6829%\n",
      "Epoch [14/100], Step [84/225], Training Accuracy: 31.3058%, Training Loss: 0.6828%\n",
      "Epoch [14/100], Step [85/225], Training Accuracy: 31.2500%, Training Loss: 0.6830%\n",
      "Epoch [14/100], Step [86/225], Training Accuracy: 31.3227%, Training Loss: 0.6830%\n",
      "Epoch [14/100], Step [87/225], Training Accuracy: 31.3218%, Training Loss: 0.6830%\n",
      "Epoch [14/100], Step [88/225], Training Accuracy: 31.3033%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [89/225], Training Accuracy: 31.2324%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [90/225], Training Accuracy: 31.1458%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [91/225], Training Accuracy: 31.1813%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [92/225], Training Accuracy: 31.1311%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [93/225], Training Accuracy: 31.1324%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [94/225], Training Accuracy: 31.2001%, Training Loss: 0.6832%\n",
      "Epoch [14/100], Step [95/225], Training Accuracy: 31.0855%, Training Loss: 0.6834%\n",
      "Epoch [14/100], Step [96/225], Training Accuracy: 31.2012%, Training Loss: 0.6834%\n",
      "Epoch [14/100], Step [97/225], Training Accuracy: 31.2500%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [98/225], Training Accuracy: 31.2181%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [99/225], Training Accuracy: 31.3447%, Training Loss: 0.6834%\n",
      "Epoch [14/100], Step [100/225], Training Accuracy: 31.3438%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [101/225], Training Accuracy: 31.4666%, Training Loss: 0.6834%\n",
      "Epoch [14/100], Step [102/225], Training Accuracy: 31.3419%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [103/225], Training Accuracy: 31.3865%, Training Loss: 0.6835%\n",
      "Epoch [14/100], Step [104/225], Training Accuracy: 31.3702%, Training Loss: 0.6836%\n",
      "Epoch [14/100], Step [105/225], Training Accuracy: 31.3244%, Training Loss: 0.6836%\n",
      "Epoch [14/100], Step [106/225], Training Accuracy: 31.3090%, Training Loss: 0.6836%\n",
      "Epoch [14/100], Step [107/225], Training Accuracy: 31.2062%, Training Loss: 0.6836%\n",
      "Epoch [14/100], Step [108/225], Training Accuracy: 31.2934%, Training Loss: 0.6837%\n",
      "Epoch [14/100], Step [109/225], Training Accuracy: 31.1640%, Training Loss: 0.6837%\n",
      "Epoch [14/100], Step [110/225], Training Accuracy: 31.1790%, Training Loss: 0.6837%\n",
      "Epoch [14/100], Step [111/225], Training Accuracy: 31.0811%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [112/225], Training Accuracy: 31.1523%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [113/225], Training Accuracy: 31.1394%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [114/225], Training Accuracy: 31.2226%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [115/225], Training Accuracy: 31.1549%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [116/225], Training Accuracy: 31.1692%, Training Loss: 0.6837%\n",
      "Epoch [14/100], Step [117/225], Training Accuracy: 31.1165%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [118/225], Training Accuracy: 31.0779%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [119/225], Training Accuracy: 31.0268%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [120/225], Training Accuracy: 31.0807%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [121/225], Training Accuracy: 31.0434%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [122/225], Training Accuracy: 31.0579%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [123/225], Training Accuracy: 31.0976%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [124/225], Training Accuracy: 31.0736%, Training Loss: 0.6838%\n",
      "Epoch [14/100], Step [125/225], Training Accuracy: 31.0750%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [126/225], Training Accuracy: 31.0144%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [127/225], Training Accuracy: 30.9916%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [128/225], Training Accuracy: 30.9692%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [129/225], Training Accuracy: 31.0199%, Training Loss: 0.6839%\n",
      "Epoch [14/100], Step [130/225], Training Accuracy: 30.9856%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [131/225], Training Accuracy: 30.9399%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [132/225], Training Accuracy: 30.9067%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [133/225], Training Accuracy: 30.9328%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [134/225], Training Accuracy: 31.0168%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [135/225], Training Accuracy: 31.0185%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [136/225], Training Accuracy: 31.0432%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [137/225], Training Accuracy: 31.0903%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [138/225], Training Accuracy: 31.1028%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [139/225], Training Accuracy: 31.0926%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [140/225], Training Accuracy: 31.0603%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [141/225], Training Accuracy: 31.0062%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [142/225], Training Accuracy: 31.0629%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [143/225], Training Accuracy: 31.0642%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [144/225], Training Accuracy: 31.0655%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [145/225], Training Accuracy: 31.1207%, Training Loss: 0.6840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Step [146/225], Training Accuracy: 31.1323%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [147/225], Training Accuracy: 31.1650%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [148/225], Training Accuracy: 31.1444%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [149/225], Training Accuracy: 31.1871%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [150/225], Training Accuracy: 31.2083%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [151/225], Training Accuracy: 31.2603%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [152/225], Training Accuracy: 31.2294%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [153/225], Training Accuracy: 31.1989%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [154/225], Training Accuracy: 31.2196%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [155/225], Training Accuracy: 31.2097%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [156/225], Training Accuracy: 31.2600%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [157/225], Training Accuracy: 31.2002%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [158/225], Training Accuracy: 31.2006%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [159/225], Training Accuracy: 31.2598%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [161/225], Training Accuracy: 31.2694%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [162/225], Training Accuracy: 31.2307%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [163/225], Training Accuracy: 31.3075%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [164/225], Training Accuracy: 31.2786%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [165/225], Training Accuracy: 31.2311%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [166/225], Training Accuracy: 31.2312%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [167/225], Training Accuracy: 31.2781%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [168/225], Training Accuracy: 31.2500%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [169/225], Training Accuracy: 31.1760%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [170/225], Training Accuracy: 31.1305%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [171/225], Training Accuracy: 31.1678%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [172/225], Training Accuracy: 31.1864%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [173/225], Training Accuracy: 31.1507%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [174/225], Training Accuracy: 31.1602%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [175/225], Training Accuracy: 31.1875%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [176/225], Training Accuracy: 31.2056%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [177/225], Training Accuracy: 31.1970%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [178/225], Training Accuracy: 31.1886%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [179/225], Training Accuracy: 31.1540%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [181/225], Training Accuracy: 31.1637%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [182/225], Training Accuracy: 31.1384%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [183/225], Training Accuracy: 31.1475%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [184/225], Training Accuracy: 31.1311%, Training Loss: 0.6842%\n",
      "Epoch [14/100], Step [185/225], Training Accuracy: 31.0980%, Training Loss: 0.6842%\n",
      "Epoch [14/100], Step [186/225], Training Accuracy: 31.1408%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [187/225], Training Accuracy: 31.1581%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [188/225], Training Accuracy: 31.1669%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [189/225], Training Accuracy: 31.1921%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [190/225], Training Accuracy: 31.1513%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [191/225], Training Accuracy: 31.1273%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [192/225], Training Accuracy: 31.0547%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [193/225], Training Accuracy: 31.0719%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [194/225], Training Accuracy: 31.0889%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [195/225], Training Accuracy: 31.0817%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [196/225], Training Accuracy: 31.0507%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [197/225], Training Accuracy: 31.0834%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [198/225], Training Accuracy: 31.1158%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [199/225], Training Accuracy: 31.1008%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [200/225], Training Accuracy: 31.0859%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [201/225], Training Accuracy: 31.1178%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [202/225], Training Accuracy: 31.1340%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [203/225], Training Accuracy: 31.1268%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [204/225], Training Accuracy: 31.1964%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [205/225], Training Accuracy: 31.2195%, Training Loss: 0.6842%\n",
      "Epoch [14/100], Step [206/225], Training Accuracy: 31.2272%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [207/225], Training Accuracy: 31.2047%, Training Loss: 0.6842%\n",
      "Epoch [14/100], Step [208/225], Training Accuracy: 31.2275%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [209/225], Training Accuracy: 31.2500%, Training Loss: 0.6841%\n",
      "Epoch [14/100], Step [210/225], Training Accuracy: 31.2649%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [211/225], Training Accuracy: 31.2648%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [212/225], Training Accuracy: 31.3163%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [213/225], Training Accuracy: 31.2940%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [214/225], Training Accuracy: 31.3303%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [215/225], Training Accuracy: 31.2936%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [216/225], Training Accuracy: 31.2355%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [217/225], Training Accuracy: 31.2356%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [218/225], Training Accuracy: 31.1998%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [219/225], Training Accuracy: 31.2571%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [220/225], Training Accuracy: 31.2784%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [221/225], Training Accuracy: 31.2641%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [222/225], Training Accuracy: 31.2852%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [223/225], Training Accuracy: 31.3271%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [224/225], Training Accuracy: 31.3198%, Training Loss: 0.6840%\n",
      "Epoch [14/100], Step [225/225], Training Accuracy: 31.2952%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6846%\n",
      "Epoch [15/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6828%\n",
      "Epoch [15/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6826%\n",
      "Epoch [15/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6830%\n",
      "Epoch [15/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6830%\n",
      "Epoch [15/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [10/225], Training Accuracy: 30.6250%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6843%\n",
      "Epoch [15/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6843%\n",
      "Epoch [15/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [15/225], Training Accuracy: 30.5208%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [17/225], Training Accuracy: 30.2390%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6852%\n",
      "Epoch [15/100], Step [19/225], Training Accuracy: 30.4276%, Training Loss: 0.6853%\n",
      "Epoch [15/100], Step [20/225], Training Accuracy: 30.9375%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [21/225], Training Accuracy: 30.9524%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [22/225], Training Accuracy: 31.1790%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [23/225], Training Accuracy: 31.1821%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [25/225], Training Accuracy: 31.5625%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [27/225], Training Accuracy: 31.6551%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [31/225], Training Accuracy: 31.9052%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [33/225], Training Accuracy: 32.3390%, Training Loss: 0.6835%\n",
      "Epoch [15/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6835%\n",
      "Epoch [15/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [36/225], Training Accuracy: 32.0312%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [38/225], Training Accuracy: 31.8257%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [39/225], Training Accuracy: 31.5705%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [43/225], Training Accuracy: 31.6134%, Training Loss: 0.6843%\n",
      "Epoch [15/100], Step [44/225], Training Accuracy: 31.6761%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [45/225], Training Accuracy: 31.7361%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [46/225], Training Accuracy: 31.5557%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [48/225], Training Accuracy: 31.6732%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [49/225], Training Accuracy: 31.7283%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [50/225], Training Accuracy: 31.6875%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [51/225], Training Accuracy: 31.8015%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [52/225], Training Accuracy: 31.8209%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [53/225], Training Accuracy: 31.7512%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [55/225], Training Accuracy: 31.8466%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [57/225], Training Accuracy: 31.9079%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [58/225], Training Accuracy: 31.7888%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [59/225], Training Accuracy: 32.1504%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [61/225], Training Accuracy: 32.2234%, Training Loss: 0.6835%\n",
      "Epoch [15/100], Step [62/225], Training Accuracy: 32.2077%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [63/225], Training Accuracy: 32.2669%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [64/225], Training Accuracy: 32.2510%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [65/225], Training Accuracy: 32.1635%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [66/225], Training Accuracy: 32.2443%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [67/225], Training Accuracy: 32.2062%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [68/225], Training Accuracy: 32.3070%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [69/225], Training Accuracy: 32.2464%, Training Loss: 0.6835%\n",
      "Epoch [15/100], Step [70/225], Training Accuracy: 32.2098%, Training Loss: 0.6835%\n",
      "Epoch [15/100], Step [71/225], Training Accuracy: 32.2403%, Training Loss: 0.6836%\n",
      "Epoch [15/100], Step [72/225], Training Accuracy: 32.0312%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [73/225], Training Accuracy: 31.9563%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [74/225], Training Accuracy: 32.0312%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [75/225], Training Accuracy: 31.9583%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [76/225], Training Accuracy: 31.8873%, Training Loss: 0.6837%\n",
      "Epoch [15/100], Step [77/225], Training Accuracy: 31.7979%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [78/225], Training Accuracy: 31.8309%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [79/225], Training Accuracy: 31.7840%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [80/225], Training Accuracy: 31.7383%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [81/225], Training Accuracy: 31.6937%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [82/225], Training Accuracy: 31.7073%, Training Loss: 0.6838%\n",
      "Epoch [15/100], Step [83/225], Training Accuracy: 31.6265%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [84/225], Training Accuracy: 31.6592%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [85/225], Training Accuracy: 31.6360%, Training Loss: 0.6839%\n",
      "Epoch [15/100], Step [86/225], Training Accuracy: 31.6497%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [87/225], Training Accuracy: 31.6810%, Training Loss: 0.6840%\n",
      "Epoch [15/100], Step [88/225], Training Accuracy: 31.6761%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [89/225], Training Accuracy: 31.6011%, Training Loss: 0.6841%\n",
      "Epoch [15/100], Step [90/225], Training Accuracy: 31.4583%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [91/225], Training Accuracy: 31.4904%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [92/225], Training Accuracy: 31.4878%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [93/225], Training Accuracy: 31.4852%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [94/225], Training Accuracy: 31.5824%, Training Loss: 0.6842%\n",
      "Epoch [15/100], Step [95/225], Training Accuracy: 31.4638%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [96/225], Training Accuracy: 31.5918%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [97/225], Training Accuracy: 31.6044%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [98/225], Training Accuracy: 31.5848%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [99/225], Training Accuracy: 31.7077%, Training Loss: 0.6843%\n",
      "Epoch [15/100], Step [100/225], Training Accuracy: 31.6875%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [101/225], Training Accuracy: 31.8224%, Training Loss: 0.6843%\n",
      "Epoch [15/100], Step [102/225], Training Accuracy: 31.6942%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [103/225], Training Accuracy: 31.7354%, Training Loss: 0.6844%\n",
      "Epoch [15/100], Step [104/225], Training Accuracy: 31.7007%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [105/225], Training Accuracy: 31.6518%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [106/225], Training Accuracy: 31.6333%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [107/225], Training Accuracy: 31.5275%, Training Loss: 0.6846%\n",
      "Epoch [15/100], Step [108/225], Training Accuracy: 31.6117%, Training Loss: 0.6845%\n",
      "Epoch [15/100], Step [109/225], Training Accuracy: 31.4794%, Training Loss: 0.6846%\n",
      "Epoch [15/100], Step [110/225], Training Accuracy: 31.4631%, Training Loss: 0.6846%\n",
      "Epoch [15/100], Step [111/225], Training Accuracy: 31.3767%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [112/225], Training Accuracy: 31.4314%, Training Loss: 0.6846%\n",
      "Epoch [15/100], Step [113/225], Training Accuracy: 31.4159%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [114/225], Training Accuracy: 31.4282%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [115/225], Training Accuracy: 31.4130%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [116/225], Training Accuracy: 31.4251%, Training Loss: 0.6846%\n",
      "Epoch [15/100], Step [117/225], Training Accuracy: 31.3702%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [118/225], Training Accuracy: 31.3427%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [119/225], Training Accuracy: 31.3025%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [120/225], Training Accuracy: 31.3542%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [121/225], Training Accuracy: 31.3404%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [122/225], Training Accuracy: 31.3268%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [123/225], Training Accuracy: 31.3516%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [124/225], Training Accuracy: 31.3382%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [125/225], Training Accuracy: 31.3250%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [126/225], Training Accuracy: 31.2624%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [127/225], Training Accuracy: 31.2254%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [128/225], Training Accuracy: 31.2134%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [129/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [130/225], Training Accuracy: 31.1779%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [131/225], Training Accuracy: 31.1307%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [132/225], Training Accuracy: 31.0961%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [133/225], Training Accuracy: 31.1090%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [134/225], Training Accuracy: 31.1567%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [135/225], Training Accuracy: 31.1574%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [136/225], Training Accuracy: 31.1926%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [137/225], Training Accuracy: 31.2272%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [138/225], Training Accuracy: 31.2613%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Step [139/225], Training Accuracy: 31.2275%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [140/225], Training Accuracy: 31.2054%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [141/225], Training Accuracy: 31.1613%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [142/225], Training Accuracy: 31.2170%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [143/225], Training Accuracy: 31.2063%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [144/225], Training Accuracy: 31.2066%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [145/225], Training Accuracy: 31.2823%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [146/225], Training Accuracy: 31.3142%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [147/225], Training Accuracy: 31.3350%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [148/225], Training Accuracy: 31.3133%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [149/225], Training Accuracy: 31.3444%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [150/225], Training Accuracy: 31.3542%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [151/225], Training Accuracy: 31.4052%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [152/225], Training Accuracy: 31.4042%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [153/225], Training Accuracy: 31.3828%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [154/225], Training Accuracy: 31.4022%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [155/225], Training Accuracy: 31.3911%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [156/225], Training Accuracy: 31.4203%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [157/225], Training Accuracy: 31.3296%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [158/225], Training Accuracy: 31.2896%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [159/225], Training Accuracy: 31.3483%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [160/225], Training Accuracy: 31.3281%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [161/225], Training Accuracy: 31.3762%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [162/225], Training Accuracy: 31.3465%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [163/225], Training Accuracy: 31.3842%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [164/225], Training Accuracy: 31.3739%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [165/225], Training Accuracy: 31.2973%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [166/225], Training Accuracy: 31.2877%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [167/225], Training Accuracy: 31.3249%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [168/225], Training Accuracy: 31.2872%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [169/225], Training Accuracy: 31.2223%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [170/225], Training Accuracy: 31.1765%, Training Loss: 0.6851%\n",
      "Epoch [15/100], Step [171/225], Training Accuracy: 31.1860%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [172/225], Training Accuracy: 31.1955%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [173/225], Training Accuracy: 31.1777%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [175/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [176/225], Training Accuracy: 31.2678%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [177/225], Training Accuracy: 31.2588%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [178/225], Training Accuracy: 31.2412%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [179/225], Training Accuracy: 31.2151%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [180/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [181/225], Training Accuracy: 31.2068%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [182/225], Training Accuracy: 31.1899%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [183/225], Training Accuracy: 31.1988%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [184/225], Training Accuracy: 31.1821%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [185/225], Training Accuracy: 31.1486%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [186/225], Training Accuracy: 31.1912%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [187/225], Training Accuracy: 31.1915%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [188/225], Training Accuracy: 31.2084%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [189/225], Training Accuracy: 31.2335%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [190/225], Training Accuracy: 31.1842%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [191/225], Training Accuracy: 31.1682%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [192/225], Training Accuracy: 31.0954%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [193/225], Training Accuracy: 31.0962%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [194/225], Training Accuracy: 31.1131%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [195/225], Training Accuracy: 31.0978%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [196/225], Training Accuracy: 31.0746%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [197/225], Training Accuracy: 31.1152%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [198/225], Training Accuracy: 31.1316%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [199/225], Training Accuracy: 31.1165%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [200/225], Training Accuracy: 31.0938%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [201/225], Training Accuracy: 31.1101%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [202/225], Training Accuracy: 31.1108%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [203/225], Training Accuracy: 31.1038%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [204/225], Training Accuracy: 31.1657%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [205/225], Training Accuracy: 31.1662%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [206/225], Training Accuracy: 31.1514%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [207/225], Training Accuracy: 31.1368%, Training Loss: 0.6850%\n",
      "Epoch [15/100], Step [208/225], Training Accuracy: 31.1599%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [209/225], Training Accuracy: 31.1902%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [210/225], Training Accuracy: 31.2054%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [211/225], Training Accuracy: 31.2056%, Training Loss: 0.6849%\n",
      "Epoch [15/100], Step [212/225], Training Accuracy: 31.2500%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [213/225], Training Accuracy: 31.2207%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [214/225], Training Accuracy: 31.2427%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [215/225], Training Accuracy: 31.1991%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [216/225], Training Accuracy: 31.1487%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [217/225], Training Accuracy: 31.1420%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [218/225], Training Accuracy: 31.0995%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [219/225], Training Accuracy: 31.1572%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [220/225], Training Accuracy: 31.1790%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [221/225], Training Accuracy: 31.1722%, Training Loss: 0.6848%\n",
      "Epoch [15/100], Step [222/225], Training Accuracy: 31.1867%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [223/225], Training Accuracy: 31.2290%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [224/225], Training Accuracy: 31.2151%, Training Loss: 0.6847%\n",
      "Epoch [15/100], Step [225/225], Training Accuracy: 31.1909%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6876%\n",
      "Epoch [16/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6862%\n",
      "Epoch [16/100], Step [4/225], Training Accuracy: 30.8594%, Training Loss: 0.6833%\n",
      "Epoch [16/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6838%\n",
      "Epoch [16/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6835%\n",
      "Epoch [16/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6833%\n",
      "Epoch [16/100], Step [9/225], Training Accuracy: 30.9028%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [10/225], Training Accuracy: 30.6250%, Training Loss: 0.6840%\n",
      "Epoch [16/100], Step [11/225], Training Accuracy: 30.3977%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6842%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [14/225], Training Accuracy: 30.0223%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6846%\n",
      "Epoch [16/100], Step [17/225], Training Accuracy: 30.2390%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [18/225], Training Accuracy: 30.2083%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [19/225], Training Accuracy: 30.3454%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [20/225], Training Accuracy: 30.7812%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [21/225], Training Accuracy: 30.8780%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [22/225], Training Accuracy: 31.1080%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [23/225], Training Accuracy: 31.0462%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [24/225], Training Accuracy: 31.1849%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [25/225], Training Accuracy: 31.3750%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [26/225], Training Accuracy: 31.6106%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [27/225], Training Accuracy: 31.3657%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [28/225], Training Accuracy: 31.1384%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [29/225], Training Accuracy: 31.4655%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [30/225], Training Accuracy: 31.4062%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [31/225], Training Accuracy: 31.3004%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [32/225], Training Accuracy: 31.3965%, Training Loss: 0.6839%\n",
      "Epoch [16/100], Step [33/225], Training Accuracy: 31.5814%, Training Loss: 0.6838%\n",
      "Epoch [16/100], Step [34/225], Training Accuracy: 31.3879%, Training Loss: 0.6838%\n",
      "Epoch [16/100], Step [35/225], Training Accuracy: 31.2054%, Training Loss: 0.6839%\n",
      "Epoch [16/100], Step [36/225], Training Accuracy: 31.1198%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [37/225], Training Accuracy: 31.2078%, Training Loss: 0.6842%\n",
      "Epoch [16/100], Step [38/225], Training Accuracy: 31.0855%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [39/225], Training Accuracy: 30.9295%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [40/225], Training Accuracy: 31.0938%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [41/225], Training Accuracy: 31.0213%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [42/225], Training Accuracy: 30.9152%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [43/225], Training Accuracy: 30.9956%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [44/225], Training Accuracy: 31.0369%, Training Loss: 0.6846%\n",
      "Epoch [16/100], Step [45/225], Training Accuracy: 31.1111%, Training Loss: 0.6846%\n",
      "Epoch [16/100], Step [46/225], Training Accuracy: 30.9443%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [47/225], Training Accuracy: 30.9176%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [48/225], Training Accuracy: 31.0872%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [49/225], Training Accuracy: 31.1862%, Training Loss: 0.6846%\n",
      "Epoch [16/100], Step [50/225], Training Accuracy: 31.0938%, Training Loss: 0.6846%\n",
      "Epoch [16/100], Step [51/225], Training Accuracy: 31.2500%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [52/225], Training Accuracy: 31.2500%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [53/225], Training Accuracy: 31.1321%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [54/225], Training Accuracy: 31.0475%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [55/225], Training Accuracy: 31.1932%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [56/225], Training Accuracy: 31.2779%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [57/225], Training Accuracy: 31.2500%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [58/225], Training Accuracy: 31.2231%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [59/225], Training Accuracy: 31.5148%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [60/225], Training Accuracy: 31.6406%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [61/225], Training Accuracy: 31.5318%, Training Loss: 0.6840%\n",
      "Epoch [16/100], Step [62/225], Training Accuracy: 31.5020%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [63/225], Training Accuracy: 31.5476%, Training Loss: 0.6842%\n",
      "Epoch [16/100], Step [64/225], Training Accuracy: 31.5674%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [65/225], Training Accuracy: 31.4904%, Training Loss: 0.6842%\n",
      "Epoch [16/100], Step [66/225], Training Accuracy: 31.6051%, Training Loss: 0.6842%\n",
      "Epoch [16/100], Step [67/225], Training Accuracy: 31.5765%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [68/225], Training Accuracy: 31.6406%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [69/225], Training Accuracy: 31.6576%, Training Loss: 0.6840%\n",
      "Epoch [16/100], Step [70/225], Training Accuracy: 31.6295%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [71/225], Training Accuracy: 31.6681%, Training Loss: 0.6841%\n",
      "Epoch [16/100], Step [72/225], Training Accuracy: 31.4453%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [73/225], Training Accuracy: 31.3570%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [74/225], Training Accuracy: 31.4400%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [75/225], Training Accuracy: 31.4375%, Training Loss: 0.6842%\n",
      "Epoch [16/100], Step [76/225], Training Accuracy: 31.3939%, Training Loss: 0.6843%\n",
      "Epoch [16/100], Step [77/225], Training Accuracy: 31.3515%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [78/225], Training Accuracy: 31.4103%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [79/225], Training Accuracy: 31.3291%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [80/225], Training Accuracy: 31.3086%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [81/225], Training Accuracy: 31.2693%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [82/225], Training Accuracy: 31.2691%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [83/225], Training Accuracy: 31.2312%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [84/225], Training Accuracy: 31.2872%, Training Loss: 0.6844%\n",
      "Epoch [16/100], Step [85/225], Training Accuracy: 31.2684%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [86/225], Training Accuracy: 31.3227%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [87/225], Training Accuracy: 31.3398%, Training Loss: 0.6845%\n",
      "Epoch [16/100], Step [88/225], Training Accuracy: 31.3388%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [89/225], Training Accuracy: 31.2676%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [90/225], Training Accuracy: 31.1806%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [91/225], Training Accuracy: 31.1985%, Training Loss: 0.6847%\n",
      "Epoch [16/100], Step [92/225], Training Accuracy: 31.1481%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [93/225], Training Accuracy: 31.1492%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [94/225], Training Accuracy: 31.2168%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [95/225], Training Accuracy: 31.1020%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [96/225], Training Accuracy: 31.2012%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [97/225], Training Accuracy: 31.2178%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [98/225], Training Accuracy: 31.2341%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [99/225], Training Accuracy: 31.3605%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [100/225], Training Accuracy: 31.3594%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [101/225], Training Accuracy: 31.4975%, Training Loss: 0.6848%\n",
      "Epoch [16/100], Step [102/225], Training Accuracy: 31.3725%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [103/225], Training Accuracy: 31.3865%, Training Loss: 0.6849%\n",
      "Epoch [16/100], Step [104/225], Training Accuracy: 31.3702%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [105/225], Training Accuracy: 31.3244%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [106/225], Training Accuracy: 31.3237%, Training Loss: 0.6850%\n",
      "Epoch [16/100], Step [107/225], Training Accuracy: 31.2354%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [108/225], Training Accuracy: 31.3079%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [109/225], Training Accuracy: 31.1783%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [110/225], Training Accuracy: 31.1648%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [111/225], Training Accuracy: 31.0529%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [112/225], Training Accuracy: 31.1523%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [113/225], Training Accuracy: 31.1532%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [114/225], Training Accuracy: 31.2226%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Step [115/225], Training Accuracy: 31.1957%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [116/225], Training Accuracy: 31.2096%, Training Loss: 0.6851%\n",
      "Epoch [16/100], Step [117/225], Training Accuracy: 31.1565%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [118/225], Training Accuracy: 31.1308%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [119/225], Training Accuracy: 31.0793%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [120/225], Training Accuracy: 31.1068%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [121/225], Training Accuracy: 31.0563%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [122/225], Training Accuracy: 31.0707%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [123/225], Training Accuracy: 31.1103%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [124/225], Training Accuracy: 31.1114%, Training Loss: 0.6852%\n",
      "Epoch [16/100], Step [125/225], Training Accuracy: 31.1000%, Training Loss: 0.6853%\n",
      "Epoch [16/100], Step [126/225], Training Accuracy: 31.0392%, Training Loss: 0.6853%\n",
      "Epoch [16/100], Step [127/225], Training Accuracy: 31.0039%, Training Loss: 0.6853%\n",
      "Epoch [16/100], Step [128/225], Training Accuracy: 31.0181%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [129/225], Training Accuracy: 31.0441%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [130/225], Training Accuracy: 30.9856%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [131/225], Training Accuracy: 30.9399%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [132/225], Training Accuracy: 30.8949%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [133/225], Training Accuracy: 30.9211%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [134/225], Training Accuracy: 30.9701%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [135/225], Training Accuracy: 30.9722%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [137/225], Training Accuracy: 31.0219%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [138/225], Training Accuracy: 31.0349%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [139/225], Training Accuracy: 30.9915%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [140/225], Training Accuracy: 30.9933%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [141/225], Training Accuracy: 30.9619%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [142/225], Training Accuracy: 31.0189%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [143/225], Training Accuracy: 31.0315%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [144/225], Training Accuracy: 31.0330%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [145/225], Training Accuracy: 31.0884%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [146/225], Training Accuracy: 31.0895%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [147/225], Training Accuracy: 31.1331%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [148/225], Training Accuracy: 31.0916%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [149/225], Training Accuracy: 31.1346%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [150/225], Training Accuracy: 31.1771%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [151/225], Training Accuracy: 31.2293%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [152/225], Training Accuracy: 31.2192%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [153/225], Training Accuracy: 31.1887%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [154/225], Training Accuracy: 31.2297%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [155/225], Training Accuracy: 31.2198%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [156/225], Training Accuracy: 31.2400%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [157/225], Training Accuracy: 31.1704%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [158/225], Training Accuracy: 31.1511%, Training Loss: 0.6857%\n",
      "Epoch [16/100], Step [159/225], Training Accuracy: 31.2205%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [160/225], Training Accuracy: 31.1914%, Training Loss: 0.6857%\n",
      "Epoch [16/100], Step [161/225], Training Accuracy: 31.2306%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [162/225], Training Accuracy: 31.2018%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [163/225], Training Accuracy: 31.2596%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [164/225], Training Accuracy: 31.2405%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [165/225], Training Accuracy: 31.1648%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [166/225], Training Accuracy: 31.1653%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [167/225], Training Accuracy: 31.2126%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [168/225], Training Accuracy: 31.1756%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [169/225], Training Accuracy: 31.1113%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [170/225], Training Accuracy: 31.0662%, Training Loss: 0.6857%\n",
      "Epoch [16/100], Step [171/225], Training Accuracy: 31.0947%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [173/225], Training Accuracy: 31.1145%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [174/225], Training Accuracy: 31.1333%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [175/225], Training Accuracy: 31.1696%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [176/225], Training Accuracy: 31.1967%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [177/225], Training Accuracy: 31.2059%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [178/225], Training Accuracy: 31.1886%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [179/225], Training Accuracy: 31.1540%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [180/225], Training Accuracy: 31.1979%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [181/225], Training Accuracy: 31.1464%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [182/225], Training Accuracy: 31.1298%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [183/225], Training Accuracy: 31.1390%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [184/225], Training Accuracy: 31.1141%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [185/225], Training Accuracy: 31.0726%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [186/225], Training Accuracy: 31.1156%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [187/225], Training Accuracy: 31.1247%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [189/225], Training Accuracy: 31.1839%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [190/225], Training Accuracy: 31.1431%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [191/225], Training Accuracy: 31.1191%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [192/225], Training Accuracy: 31.0465%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [193/225], Training Accuracy: 31.0476%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [194/225], Training Accuracy: 31.0648%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [195/225], Training Accuracy: 31.0417%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [196/225], Training Accuracy: 31.0188%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [197/225], Training Accuracy: 31.0517%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [198/225], Training Accuracy: 31.0685%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [199/225], Training Accuracy: 31.0380%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [200/225], Training Accuracy: 31.0156%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [201/225], Training Accuracy: 31.0401%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [202/225], Training Accuracy: 31.0412%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [203/225], Training Accuracy: 31.0422%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [204/225], Training Accuracy: 31.1045%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [205/225], Training Accuracy: 31.1280%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [206/225], Training Accuracy: 31.1362%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [207/225], Training Accuracy: 31.1066%, Training Loss: 0.6856%\n",
      "Epoch [16/100], Step [208/225], Training Accuracy: 31.1298%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [209/225], Training Accuracy: 31.1752%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [210/225], Training Accuracy: 31.2054%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [211/225], Training Accuracy: 31.1908%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [212/225], Training Accuracy: 31.2426%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [213/225], Training Accuracy: 31.2207%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [214/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [215/225], Training Accuracy: 31.2137%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [216/225], Training Accuracy: 31.1632%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [217/225], Training Accuracy: 31.1564%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [218/225], Training Accuracy: 31.1282%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [219/225], Training Accuracy: 31.1858%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [220/225], Training Accuracy: 31.2074%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [221/225], Training Accuracy: 31.1864%, Training Loss: 0.6855%\n",
      "Epoch [16/100], Step [222/225], Training Accuracy: 31.2007%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [223/225], Training Accuracy: 31.2360%, Training Loss: 0.6854%\n",
      "Epoch [16/100], Step [224/225], Training Accuracy: 31.2291%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Step [225/225], Training Accuracy: 31.2118%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6881%\n",
      "Epoch [17/100], Step [2/225], Training Accuracy: 30.4688%, Training Loss: 0.6868%\n",
      "Epoch [17/100], Step [3/225], Training Accuracy: 29.1667%, Training Loss: 0.6888%\n",
      "Epoch [17/100], Step [4/225], Training Accuracy: 29.6875%, Training Loss: 0.6869%\n",
      "Epoch [17/100], Step [5/225], Training Accuracy: 30.6250%, Training Loss: 0.6874%\n",
      "Epoch [17/100], Step [6/225], Training Accuracy: 29.9479%, Training Loss: 0.6876%\n",
      "Epoch [17/100], Step [7/225], Training Accuracy: 30.3571%, Training Loss: 0.6863%\n",
      "Epoch [17/100], Step [8/225], Training Accuracy: 30.6641%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6865%\n",
      "Epoch [17/100], Step [10/225], Training Accuracy: 30.1562%, Training Loss: 0.6865%\n",
      "Epoch [17/100], Step [11/225], Training Accuracy: 30.1136%, Training Loss: 0.6865%\n",
      "Epoch [17/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6864%\n",
      "Epoch [17/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6865%\n",
      "Epoch [17/100], Step [14/225], Training Accuracy: 30.0223%, Training Loss: 0.6867%\n",
      "Epoch [17/100], Step [15/225], Training Accuracy: 30.4167%, Training Loss: 0.6865%\n",
      "Epoch [17/100], Step [16/225], Training Accuracy: 30.3711%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [17/225], Training Accuracy: 29.9632%, Training Loss: 0.6864%\n",
      "Epoch [17/100], Step [18/225], Training Accuracy: 29.9479%, Training Loss: 0.6866%\n",
      "Epoch [17/100], Step [19/225], Training Accuracy: 30.2632%, Training Loss: 0.6866%\n",
      "Epoch [17/100], Step [20/225], Training Accuracy: 30.9375%, Training Loss: 0.6864%\n",
      "Epoch [17/100], Step [21/225], Training Accuracy: 30.9524%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [22/225], Training Accuracy: 31.0369%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [23/225], Training Accuracy: 30.9783%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [24/225], Training Accuracy: 30.8594%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [25/225], Training Accuracy: 31.1250%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [26/225], Training Accuracy: 31.3702%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [27/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [28/225], Training Accuracy: 31.1942%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [29/225], Training Accuracy: 31.5733%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [30/225], Training Accuracy: 31.6667%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [31/225], Training Accuracy: 31.6532%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [32/225], Training Accuracy: 31.8359%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [33/225], Training Accuracy: 31.8182%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [34/225], Training Accuracy: 31.5717%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [35/225], Training Accuracy: 31.3839%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [36/225], Training Accuracy: 31.2066%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [37/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [38/225], Training Accuracy: 31.0444%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [39/225], Training Accuracy: 30.8494%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [40/225], Training Accuracy: 30.9766%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [41/225], Training Accuracy: 30.9451%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [42/225], Training Accuracy: 30.8780%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [43/225], Training Accuracy: 30.9956%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [44/225], Training Accuracy: 31.0369%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [45/225], Training Accuracy: 31.1111%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [46/225], Training Accuracy: 30.9783%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [47/225], Training Accuracy: 30.9508%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [48/225], Training Accuracy: 31.0221%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [49/225], Training Accuracy: 31.1224%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [50/225], Training Accuracy: 31.0938%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [51/225], Training Accuracy: 31.1887%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [52/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [53/225], Training Accuracy: 31.1910%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [54/225], Training Accuracy: 31.1053%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [55/225], Training Accuracy: 31.2784%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [56/225], Training Accuracy: 31.3616%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [57/225], Training Accuracy: 31.3871%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [58/225], Training Accuracy: 31.2769%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [59/225], Training Accuracy: 31.6208%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [60/225], Training Accuracy: 31.7188%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [61/225], Training Accuracy: 31.5830%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [62/225], Training Accuracy: 31.5524%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [63/225], Training Accuracy: 31.5972%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [64/225], Training Accuracy: 31.6162%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [65/225], Training Accuracy: 31.5625%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [66/225], Training Accuracy: 31.6998%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [67/225], Training Accuracy: 31.7631%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [68/225], Training Accuracy: 31.8244%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [69/225], Training Accuracy: 31.7708%, Training Loss: 0.6849%\n",
      "Epoch [17/100], Step [70/225], Training Accuracy: 31.7634%, Training Loss: 0.6850%\n",
      "Epoch [17/100], Step [71/225], Training Accuracy: 31.7782%, Training Loss: 0.6850%\n",
      "Epoch [17/100], Step [72/225], Training Accuracy: 31.5538%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [73/225], Training Accuracy: 31.4640%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [74/225], Training Accuracy: 31.5456%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [75/225], Training Accuracy: 31.5000%, Training Loss: 0.6850%\n",
      "Epoch [17/100], Step [76/225], Training Accuracy: 31.4145%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [77/225], Training Accuracy: 31.3515%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [78/225], Training Accuracy: 31.3502%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [79/225], Training Accuracy: 31.2698%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [80/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [81/225], Training Accuracy: 31.1535%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [82/225], Training Accuracy: 31.1738%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [83/225], Training Accuracy: 31.1370%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [84/225], Training Accuracy: 31.2314%, Training Loss: 0.6851%\n",
      "Epoch [17/100], Step [85/225], Training Accuracy: 31.1949%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [86/225], Training Accuracy: 31.1955%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [87/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [17/100], Step [88/225], Training Accuracy: 31.2322%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [90/225], Training Accuracy: 31.0764%, Training Loss: 0.6853%\n",
      "Epoch [17/100], Step [91/225], Training Accuracy: 31.1470%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [92/225], Training Accuracy: 31.1311%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [93/225], Training Accuracy: 31.0988%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [94/225], Training Accuracy: 31.1669%, Training Loss: 0.6854%\n",
      "Epoch [17/100], Step [95/225], Training Accuracy: 31.0526%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [96/225], Training Accuracy: 31.1523%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [97/225], Training Accuracy: 31.1695%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [98/225], Training Accuracy: 31.1703%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [99/225], Training Accuracy: 31.3289%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [100/225], Training Accuracy: 31.2812%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [101/225], Training Accuracy: 31.4047%, Training Loss: 0.6855%\n",
      "Epoch [17/100], Step [102/225], Training Accuracy: 31.3113%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [103/225], Training Accuracy: 31.3714%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [104/225], Training Accuracy: 31.3552%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [105/225], Training Accuracy: 31.3244%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [106/225], Training Accuracy: 31.3384%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [107/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [108/225], Training Accuracy: 31.3513%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [109/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [110/225], Training Accuracy: 31.2642%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [111/225], Training Accuracy: 31.1515%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Step [112/225], Training Accuracy: 31.2081%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [113/225], Training Accuracy: 31.2223%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [114/225], Training Accuracy: 31.2911%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [115/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [116/225], Training Accuracy: 31.2231%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [117/225], Training Accuracy: 31.1565%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [118/225], Training Accuracy: 31.1308%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [119/225], Training Accuracy: 31.0924%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [120/225], Training Accuracy: 31.1328%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [121/225], Training Accuracy: 31.1209%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [123/225], Training Accuracy: 31.1611%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [124/225], Training Accuracy: 31.1492%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [125/225], Training Accuracy: 31.1125%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [126/225], Training Accuracy: 31.0392%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [127/225], Training Accuracy: 30.9916%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [128/225], Training Accuracy: 30.9937%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [129/225], Training Accuracy: 31.0320%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [130/225], Training Accuracy: 30.9736%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [131/225], Training Accuracy: 30.9399%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [132/225], Training Accuracy: 30.9186%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [133/225], Training Accuracy: 30.9328%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [134/225], Training Accuracy: 30.9935%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [135/225], Training Accuracy: 31.0069%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [136/225], Training Accuracy: 31.0202%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [137/225], Training Accuracy: 31.0561%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [138/225], Training Accuracy: 31.0688%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [139/225], Training Accuracy: 31.0364%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [140/225], Training Accuracy: 31.0045%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [141/225], Training Accuracy: 30.9619%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [142/225], Training Accuracy: 31.0189%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [143/225], Training Accuracy: 31.0205%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [144/225], Training Accuracy: 31.0113%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [145/225], Training Accuracy: 31.0776%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [146/225], Training Accuracy: 31.0681%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [147/225], Training Accuracy: 31.0906%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [148/225], Training Accuracy: 31.0705%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [149/225], Training Accuracy: 31.1032%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [150/225], Training Accuracy: 31.1458%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [151/225], Training Accuracy: 31.2086%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [152/225], Training Accuracy: 31.2089%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [153/225], Training Accuracy: 31.1785%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [154/225], Training Accuracy: 31.2094%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [155/225], Training Accuracy: 31.2198%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [156/225], Training Accuracy: 31.2400%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [157/225], Training Accuracy: 31.1604%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [158/225], Training Accuracy: 31.1214%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [159/225], Training Accuracy: 31.2107%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [160/225], Training Accuracy: 31.1816%, Training Loss: 0.6861%\n",
      "Epoch [17/100], Step [161/225], Training Accuracy: 31.2306%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [162/225], Training Accuracy: 31.2114%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [163/225], Training Accuracy: 31.2692%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [164/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [165/225], Training Accuracy: 31.1932%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [166/225], Training Accuracy: 31.1935%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [167/225], Training Accuracy: 31.2313%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [168/225], Training Accuracy: 31.1942%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [169/225], Training Accuracy: 31.1298%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [170/225], Training Accuracy: 31.0846%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [171/225], Training Accuracy: 31.1038%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [173/225], Training Accuracy: 31.1236%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [174/225], Training Accuracy: 31.1512%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [175/225], Training Accuracy: 31.1875%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [176/225], Training Accuracy: 31.2145%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [177/225], Training Accuracy: 31.2235%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [178/225], Training Accuracy: 31.2237%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [179/225], Training Accuracy: 31.1889%, Training Loss: 0.6860%\n",
      "Epoch [17/100], Step [180/225], Training Accuracy: 31.2413%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [181/225], Training Accuracy: 31.1982%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [182/225], Training Accuracy: 31.1813%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [183/225], Training Accuracy: 31.1902%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [184/225], Training Accuracy: 31.1736%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [185/225], Training Accuracy: 31.1402%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [186/225], Training Accuracy: 31.1828%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [187/225], Training Accuracy: 31.1748%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [188/225], Training Accuracy: 31.1835%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [189/225], Training Accuracy: 31.2169%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [190/225], Training Accuracy: 31.1595%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [191/225], Training Accuracy: 31.1355%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [192/225], Training Accuracy: 31.0710%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [193/225], Training Accuracy: 31.0800%, Training Loss: 0.6859%\n",
      "Epoch [17/100], Step [194/225], Training Accuracy: 31.0889%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [195/225], Training Accuracy: 31.0817%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [196/225], Training Accuracy: 31.0666%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [197/225], Training Accuracy: 31.0993%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [198/225], Training Accuracy: 31.1158%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [199/225], Training Accuracy: 31.0930%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [200/225], Training Accuracy: 31.0781%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [201/225], Training Accuracy: 31.1023%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [202/225], Training Accuracy: 31.1108%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [203/225], Training Accuracy: 31.0884%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [204/225], Training Accuracy: 31.1351%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [205/225], Training Accuracy: 31.1357%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [206/225], Training Accuracy: 31.1211%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [207/225], Training Accuracy: 31.1066%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [208/225], Training Accuracy: 31.1298%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [209/225], Training Accuracy: 31.1678%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [210/225], Training Accuracy: 31.1979%, Training Loss: 0.6858%\n",
      "Epoch [17/100], Step [211/225], Training Accuracy: 31.1834%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [213/225], Training Accuracy: 31.2060%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [214/225], Training Accuracy: 31.2354%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [215/225], Training Accuracy: 31.1846%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [216/225], Training Accuracy: 31.1270%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [217/225], Training Accuracy: 31.1276%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [218/225], Training Accuracy: 31.1067%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [219/225], Training Accuracy: 31.1644%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Step [220/225], Training Accuracy: 31.1861%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [221/225], Training Accuracy: 31.1793%, Training Loss: 0.6857%\n",
      "Epoch [17/100], Step [222/225], Training Accuracy: 31.1867%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [223/225], Training Accuracy: 31.2220%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [224/225], Training Accuracy: 31.2221%, Training Loss: 0.6856%\n",
      "Epoch [17/100], Step [225/225], Training Accuracy: 31.1909%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6912%\n",
      "Epoch [18/100], Step [2/225], Training Accuracy: 30.4688%, Training Loss: 0.6879%\n",
      "Epoch [18/100], Step [3/225], Training Accuracy: 30.2083%, Training Loss: 0.6883%\n",
      "Epoch [18/100], Step [4/225], Training Accuracy: 30.4688%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [5/225], Training Accuracy: 31.8750%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [6/225], Training Accuracy: 30.9896%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [7/225], Training Accuracy: 31.0268%, Training Loss: 0.6844%\n",
      "Epoch [18/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6842%\n",
      "Epoch [18/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [18/100], Step [10/225], Training Accuracy: 30.6250%, Training Loss: 0.6851%\n",
      "Epoch [18/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6851%\n",
      "Epoch [18/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6851%\n",
      "Epoch [18/100], Step [13/225], Training Accuracy: 29.5673%, Training Loss: 0.6851%\n",
      "Epoch [18/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [15/225], Training Accuracy: 30.3125%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [16/225], Training Accuracy: 30.2734%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [17/225], Training Accuracy: 29.9632%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [18/225], Training Accuracy: 29.9479%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [19/225], Training Accuracy: 30.4276%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [20/225], Training Accuracy: 30.9375%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [21/225], Training Accuracy: 30.9524%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [22/225], Training Accuracy: 31.0369%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [23/225], Training Accuracy: 30.8424%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [24/225], Training Accuracy: 30.9245%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [25/225], Training Accuracy: 31.1250%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [26/225], Training Accuracy: 31.4303%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [27/225], Training Accuracy: 31.3079%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [28/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [29/225], Training Accuracy: 31.6272%, Training Loss: 0.6853%\n",
      "Epoch [18/100], Step [30/225], Training Accuracy: 31.6667%, Training Loss: 0.6852%\n",
      "Epoch [18/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6853%\n",
      "Epoch [18/100], Step [32/225], Training Accuracy: 31.7383%, Training Loss: 0.6852%\n",
      "Epoch [18/100], Step [33/225], Training Accuracy: 31.7708%, Training Loss: 0.6851%\n",
      "Epoch [18/100], Step [34/225], Training Accuracy: 31.5257%, Training Loss: 0.6852%\n",
      "Epoch [18/100], Step [35/225], Training Accuracy: 31.4286%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [36/225], Training Accuracy: 31.2066%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [37/225], Training Accuracy: 31.2922%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [38/225], Training Accuracy: 31.1266%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [39/225], Training Accuracy: 30.8894%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [40/225], Training Accuracy: 30.9375%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [41/225], Training Accuracy: 30.9832%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [42/225], Training Accuracy: 30.8780%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [43/225], Training Accuracy: 31.0320%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [44/225], Training Accuracy: 31.0369%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [45/225], Training Accuracy: 31.2153%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [46/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [47/225], Training Accuracy: 31.0505%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [48/225], Training Accuracy: 31.1523%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [49/225], Training Accuracy: 31.1862%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [50/225], Training Accuracy: 31.2812%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [51/225], Training Accuracy: 31.4645%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [53/225], Training Accuracy: 31.4564%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [54/225], Training Accuracy: 31.3657%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [55/225], Training Accuracy: 31.4205%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [56/225], Training Accuracy: 31.4453%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [57/225], Training Accuracy: 31.5241%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [58/225], Training Accuracy: 31.4386%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [60/225], Training Accuracy: 31.9010%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [62/225], Training Accuracy: 31.8044%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [63/225], Training Accuracy: 31.8948%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [64/225], Training Accuracy: 31.9336%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [67/225], Training Accuracy: 31.8797%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [68/225], Training Accuracy: 31.9164%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [69/225], Training Accuracy: 31.9293%, Training Loss: 0.6852%\n",
      "Epoch [18/100], Step [70/225], Training Accuracy: 31.8527%, Training Loss: 0.6853%\n",
      "Epoch [18/100], Step [71/225], Training Accuracy: 31.9102%, Training Loss: 0.6853%\n",
      "Epoch [18/100], Step [72/225], Training Accuracy: 31.7057%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [73/225], Training Accuracy: 31.6353%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [74/225], Training Accuracy: 31.7356%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [75/225], Training Accuracy: 31.6667%, Training Loss: 0.6854%\n",
      "Epoch [18/100], Step [76/225], Training Accuracy: 31.5995%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [77/225], Training Accuracy: 31.5544%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [78/225], Training Accuracy: 31.5905%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [79/225], Training Accuracy: 31.5467%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [80/225], Training Accuracy: 31.5234%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [81/225], Training Accuracy: 31.4622%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [82/225], Training Accuracy: 31.4596%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [83/225], Training Accuracy: 31.4194%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [84/225], Training Accuracy: 31.5104%, Training Loss: 0.6855%\n",
      "Epoch [18/100], Step [85/225], Training Accuracy: 31.4890%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [86/225], Training Accuracy: 31.5589%, Training Loss: 0.6856%\n",
      "Epoch [18/100], Step [87/225], Training Accuracy: 31.5733%, Training Loss: 0.6857%\n",
      "Epoch [18/100], Step [88/225], Training Accuracy: 31.5341%, Training Loss: 0.6858%\n",
      "Epoch [18/100], Step [89/225], Training Accuracy: 31.4080%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [90/225], Training Accuracy: 31.2674%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [91/225], Training Accuracy: 31.2843%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [92/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [93/225], Training Accuracy: 31.2332%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [94/225], Training Accuracy: 31.2999%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [95/225], Training Accuracy: 31.1842%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [96/225], Training Accuracy: 31.2663%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [97/225], Training Accuracy: 31.2983%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [98/225], Training Accuracy: 31.3297%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [99/225], Training Accuracy: 31.4552%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [100/225], Training Accuracy: 31.4219%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [101/225], Training Accuracy: 31.5594%, Training Loss: 0.6859%\n",
      "Epoch [18/100], Step [102/225], Training Accuracy: 31.4338%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Step [103/225], Training Accuracy: 31.4775%, Training Loss: 0.6860%\n",
      "Epoch [18/100], Step [104/225], Training Accuracy: 31.4603%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [105/225], Training Accuracy: 31.4286%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [106/225], Training Accuracy: 31.3974%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [107/225], Training Accuracy: 31.3230%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [108/225], Training Accuracy: 31.3947%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [109/225], Training Accuracy: 31.2930%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [110/225], Training Accuracy: 31.3068%, Training Loss: 0.6861%\n",
      "Epoch [18/100], Step [111/225], Training Accuracy: 31.2078%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [112/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [113/225], Training Accuracy: 31.2362%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [114/225], Training Accuracy: 31.3322%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [115/225], Training Accuracy: 31.3315%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [116/225], Training Accuracy: 31.3039%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [117/225], Training Accuracy: 31.2767%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [118/225], Training Accuracy: 31.2368%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [119/225], Training Accuracy: 31.1712%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [120/225], Training Accuracy: 31.2109%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [121/225], Training Accuracy: 31.1854%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [122/225], Training Accuracy: 31.1860%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [123/225], Training Accuracy: 31.2246%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [124/225], Training Accuracy: 31.2248%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [125/225], Training Accuracy: 31.1875%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [126/225], Training Accuracy: 31.1260%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [127/225], Training Accuracy: 31.0655%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [128/225], Training Accuracy: 31.0547%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [129/225], Training Accuracy: 31.0925%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [130/225], Training Accuracy: 31.0577%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [131/225], Training Accuracy: 31.0234%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [132/225], Training Accuracy: 30.9659%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [133/225], Training Accuracy: 31.0033%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [134/225], Training Accuracy: 31.0634%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [135/225], Training Accuracy: 31.0764%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [136/225], Training Accuracy: 31.1006%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [137/225], Training Accuracy: 31.1588%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [138/225], Training Accuracy: 31.1594%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [139/225], Training Accuracy: 31.1263%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [140/225], Training Accuracy: 31.1049%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [141/225], Training Accuracy: 31.0616%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [142/225], Training Accuracy: 31.1180%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [143/225], Training Accuracy: 31.1080%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [144/225], Training Accuracy: 31.1198%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [145/225], Training Accuracy: 31.1638%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [146/225], Training Accuracy: 31.1858%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [147/225], Training Accuracy: 31.1969%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [148/225], Training Accuracy: 31.1655%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [149/225], Training Accuracy: 31.1871%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [150/225], Training Accuracy: 31.2188%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [151/225], Training Accuracy: 31.2707%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [152/225], Training Accuracy: 31.3014%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [153/225], Training Accuracy: 31.2806%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [154/225], Training Accuracy: 31.3109%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [155/225], Training Accuracy: 31.2903%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [156/225], Training Accuracy: 31.3301%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [157/225], Training Accuracy: 31.2500%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [158/225], Training Accuracy: 31.2302%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [159/225], Training Accuracy: 31.3286%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [160/225], Training Accuracy: 31.3184%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [161/225], Training Accuracy: 31.3568%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [162/225], Training Accuracy: 31.3175%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [163/225], Training Accuracy: 31.3746%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [164/225], Training Accuracy: 31.3262%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [165/225], Training Accuracy: 31.2595%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [166/225], Training Accuracy: 31.2406%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [167/225], Training Accuracy: 31.2968%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [168/225], Training Accuracy: 31.2593%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [169/225], Training Accuracy: 31.1760%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [170/225], Training Accuracy: 31.1305%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [171/225], Training Accuracy: 31.1404%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [172/225], Training Accuracy: 31.1592%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [173/225], Training Accuracy: 31.1416%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [174/225], Training Accuracy: 31.1871%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [175/225], Training Accuracy: 31.2143%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [176/225], Training Accuracy: 31.2589%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [177/225], Training Accuracy: 31.2588%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [178/225], Training Accuracy: 31.2412%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [179/225], Training Accuracy: 31.1976%, Training Loss: 0.6866%\n",
      "Epoch [18/100], Step [180/225], Training Accuracy: 31.2500%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [181/225], Training Accuracy: 31.2068%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [182/225], Training Accuracy: 31.1813%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [183/225], Training Accuracy: 31.1902%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [185/225], Training Accuracy: 31.1064%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [186/225], Training Accuracy: 31.1408%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [187/225], Training Accuracy: 31.1330%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [190/225], Training Accuracy: 31.1184%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [191/225], Training Accuracy: 31.1027%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [192/225], Training Accuracy: 31.0384%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [193/225], Training Accuracy: 31.0638%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [194/225], Training Accuracy: 31.0728%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [195/225], Training Accuracy: 31.0577%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [196/225], Training Accuracy: 31.0507%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [197/225], Training Accuracy: 31.0914%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [198/225], Training Accuracy: 31.1158%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [199/225], Training Accuracy: 31.1087%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [200/225], Training Accuracy: 31.0938%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [201/225], Training Accuracy: 31.1256%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [202/225], Training Accuracy: 31.1417%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [203/225], Training Accuracy: 31.1345%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [204/225], Training Accuracy: 31.2117%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [205/225], Training Accuracy: 31.2043%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [206/225], Training Accuracy: 31.1817%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [207/225], Training Accuracy: 31.1670%, Training Loss: 0.6865%\n",
      "Epoch [18/100], Step [208/225], Training Accuracy: 31.1974%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [209/225], Training Accuracy: 31.2201%, Training Loss: 0.6864%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Step [210/225], Training Accuracy: 31.2351%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [211/225], Training Accuracy: 31.2352%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [212/225], Training Accuracy: 31.2869%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [213/225], Training Accuracy: 31.2573%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [214/225], Training Accuracy: 31.2865%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [215/225], Training Accuracy: 31.2427%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [216/225], Training Accuracy: 31.1849%, Training Loss: 0.6864%\n",
      "Epoch [18/100], Step [217/225], Training Accuracy: 31.1852%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [218/225], Training Accuracy: 31.1640%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [219/225], Training Accuracy: 31.2215%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [220/225], Training Accuracy: 31.2429%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [221/225], Training Accuracy: 31.2288%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [222/225], Training Accuracy: 31.2430%, Training Loss: 0.6863%\n",
      "Epoch [18/100], Step [223/225], Training Accuracy: 31.2780%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [224/225], Training Accuracy: 31.2849%, Training Loss: 0.6862%\n",
      "Epoch [18/100], Step [225/225], Training Accuracy: 31.2535%, Training Loss: 0.6863%\n",
      "Epoch [19/100], Step [1/225], Training Accuracy: 37.5000%, Training Loss: 0.6896%\n",
      "Epoch [19/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6876%\n",
      "Epoch [19/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6883%\n",
      "Epoch [19/100], Step [4/225], Training Accuracy: 33.5938%, Training Loss: 0.6870%\n",
      "Epoch [19/100], Step [5/225], Training Accuracy: 34.3750%, Training Loss: 0.6870%\n",
      "Epoch [19/100], Step [6/225], Training Accuracy: 33.5938%, Training Loss: 0.6874%\n",
      "Epoch [19/100], Step [7/225], Training Accuracy: 33.0357%, Training Loss: 0.6866%\n",
      "Epoch [19/100], Step [8/225], Training Accuracy: 32.6172%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [9/225], Training Accuracy: 32.4653%, Training Loss: 0.6867%\n",
      "Epoch [19/100], Step [10/225], Training Accuracy: 31.5625%, Training Loss: 0.6866%\n",
      "Epoch [19/100], Step [11/225], Training Accuracy: 31.3920%, Training Loss: 0.6867%\n",
      "Epoch [19/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6868%\n",
      "Epoch [19/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6869%\n",
      "Epoch [19/100], Step [14/225], Training Accuracy: 30.9152%, Training Loss: 0.6874%\n",
      "Epoch [19/100], Step [15/225], Training Accuracy: 31.2500%, Training Loss: 0.6875%\n",
      "Epoch [19/100], Step [16/225], Training Accuracy: 31.2500%, Training Loss: 0.6875%\n",
      "Epoch [19/100], Step [17/225], Training Accuracy: 31.0662%, Training Loss: 0.6877%\n",
      "Epoch [19/100], Step [18/225], Training Accuracy: 30.9896%, Training Loss: 0.6877%\n",
      "Epoch [19/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6878%\n",
      "Epoch [19/100], Step [20/225], Training Accuracy: 31.7969%, Training Loss: 0.6877%\n",
      "Epoch [19/100], Step [21/225], Training Accuracy: 31.7708%, Training Loss: 0.6875%\n",
      "Epoch [19/100], Step [22/225], Training Accuracy: 31.9602%, Training Loss: 0.6873%\n",
      "Epoch [19/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6871%\n",
      "Epoch [19/100], Step [24/225], Training Accuracy: 31.9661%, Training Loss: 0.6870%\n",
      "Epoch [19/100], Step [25/225], Training Accuracy: 32.0625%, Training Loss: 0.6870%\n",
      "Epoch [19/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6867%\n",
      "Epoch [19/100], Step [27/225], Training Accuracy: 31.8866%, Training Loss: 0.6865%\n",
      "Epoch [19/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6865%\n",
      "Epoch [19/100], Step [29/225], Training Accuracy: 32.2198%, Training Loss: 0.6863%\n",
      "Epoch [19/100], Step [30/225], Training Accuracy: 32.2917%, Training Loss: 0.6863%\n",
      "Epoch [19/100], Step [31/225], Training Accuracy: 32.1069%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [33/225], Training Accuracy: 32.4337%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [36/225], Training Accuracy: 32.1181%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [37/225], Training Accuracy: 32.1368%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [41/225], Training Accuracy: 31.6311%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6863%\n",
      "Epoch [19/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [44/225], Training Accuracy: 31.7116%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [45/225], Training Accuracy: 31.7014%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [46/225], Training Accuracy: 31.6236%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [48/225], Training Accuracy: 31.7057%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [50/225], Training Accuracy: 31.7188%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [51/225], Training Accuracy: 31.8321%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [52/225], Training Accuracy: 31.8209%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [53/225], Training Accuracy: 31.8101%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [55/225], Training Accuracy: 31.8750%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [57/225], Training Accuracy: 31.8531%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [58/225], Training Accuracy: 31.8157%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [59/225], Training Accuracy: 32.1504%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [61/225], Training Accuracy: 32.2234%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [62/225], Training Accuracy: 32.1825%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [64/225], Training Accuracy: 32.2510%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [65/225], Training Accuracy: 32.0913%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [66/225], Training Accuracy: 32.1496%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [67/225], Training Accuracy: 32.0896%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [68/225], Training Accuracy: 32.1461%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [69/225], Training Accuracy: 32.1105%, Training Loss: 0.6852%\n",
      "Epoch [19/100], Step [70/225], Training Accuracy: 32.0312%, Training Loss: 0.6852%\n",
      "Epoch [19/100], Step [71/225], Training Accuracy: 32.1083%, Training Loss: 0.6852%\n",
      "Epoch [19/100], Step [72/225], Training Accuracy: 31.9010%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [73/225], Training Accuracy: 31.8493%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [74/225], Training Accuracy: 31.9468%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [75/225], Training Accuracy: 31.9167%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [76/225], Training Accuracy: 31.8462%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [77/225], Training Accuracy: 31.7573%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [78/225], Training Accuracy: 31.7909%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [79/225], Training Accuracy: 31.7247%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [80/225], Training Accuracy: 31.6992%, Training Loss: 0.6853%\n",
      "Epoch [19/100], Step [81/225], Training Accuracy: 31.6551%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [82/225], Training Accuracy: 31.6502%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [83/225], Training Accuracy: 31.5889%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [84/225], Training Accuracy: 31.6592%, Training Loss: 0.6854%\n",
      "Epoch [19/100], Step [85/225], Training Accuracy: 31.5625%, Training Loss: 0.6855%\n",
      "Epoch [19/100], Step [86/225], Training Accuracy: 31.5770%, Training Loss: 0.6855%\n",
      "Epoch [19/100], Step [87/225], Training Accuracy: 31.5912%, Training Loss: 0.6855%\n",
      "Epoch [19/100], Step [88/225], Training Accuracy: 31.5518%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [89/225], Training Accuracy: 31.4607%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [90/225], Training Accuracy: 31.3715%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [91/225], Training Accuracy: 31.4045%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Step [92/225], Training Accuracy: 31.3519%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [93/225], Training Accuracy: 31.3508%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [94/225], Training Accuracy: 31.3996%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [95/225], Training Accuracy: 31.2664%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [96/225], Training Accuracy: 31.3477%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [97/225], Training Accuracy: 31.3789%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [98/225], Training Accuracy: 31.3935%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [99/225], Training Accuracy: 31.5341%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [100/225], Training Accuracy: 31.5312%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [101/225], Training Accuracy: 31.6522%, Training Loss: 0.6856%\n",
      "Epoch [19/100], Step [102/225], Training Accuracy: 31.5564%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [103/225], Training Accuracy: 31.5989%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [104/225], Training Accuracy: 31.5505%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [105/225], Training Accuracy: 31.5030%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [106/225], Training Accuracy: 31.5006%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [107/225], Training Accuracy: 31.4106%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [108/225], Training Accuracy: 31.4959%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [109/225], Training Accuracy: 31.3647%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [110/225], Training Accuracy: 31.3778%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [111/225], Training Accuracy: 31.2641%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [112/225], Training Accuracy: 31.3616%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [113/225], Training Accuracy: 31.3468%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [114/225], Training Accuracy: 31.4282%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [115/225], Training Accuracy: 31.3859%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [116/225], Training Accuracy: 31.3847%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [117/225], Training Accuracy: 31.3435%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [118/225], Training Accuracy: 31.3030%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [119/225], Training Accuracy: 31.2631%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [120/225], Training Accuracy: 31.3021%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [121/225], Training Accuracy: 31.2629%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [122/225], Training Accuracy: 31.2756%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [123/225], Training Accuracy: 31.3135%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [124/225], Training Accuracy: 31.3130%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [125/225], Training Accuracy: 31.2750%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [126/225], Training Accuracy: 31.2252%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [127/225], Training Accuracy: 31.1516%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [128/225], Training Accuracy: 31.1401%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [129/225], Training Accuracy: 31.1652%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [130/225], Training Accuracy: 31.1058%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [131/225], Training Accuracy: 31.0592%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [132/225], Training Accuracy: 31.0369%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [133/225], Training Accuracy: 31.0620%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [134/225], Training Accuracy: 31.1217%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [135/225], Training Accuracy: 31.1343%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [136/225], Training Accuracy: 31.1466%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [137/225], Training Accuracy: 31.2044%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [138/225], Training Accuracy: 31.2160%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [139/225], Training Accuracy: 31.1826%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [140/225], Training Accuracy: 31.1830%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [141/225], Training Accuracy: 31.1392%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [142/225], Training Accuracy: 31.2060%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [143/225], Training Accuracy: 31.1954%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [144/225], Training Accuracy: 31.1957%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [145/225], Training Accuracy: 31.2392%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [146/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [147/225], Training Accuracy: 31.2713%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [148/225], Training Accuracy: 31.2606%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [149/225], Training Accuracy: 31.2919%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [150/225], Training Accuracy: 31.3333%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [151/225], Training Accuracy: 31.3742%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [152/225], Training Accuracy: 31.3836%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [153/225], Training Accuracy: 31.3419%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [154/225], Training Accuracy: 31.3920%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [155/225], Training Accuracy: 31.3810%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [156/225], Training Accuracy: 31.4002%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [157/225], Training Accuracy: 31.3595%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [158/225], Training Accuracy: 31.3291%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [159/225], Training Accuracy: 31.4171%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [160/225], Training Accuracy: 31.3867%, Training Loss: 0.6863%\n",
      "Epoch [19/100], Step [161/225], Training Accuracy: 31.4150%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [162/225], Training Accuracy: 31.3850%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [163/225], Training Accuracy: 31.4513%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [164/225], Training Accuracy: 31.4215%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [165/225], Training Accuracy: 31.3542%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [166/225], Training Accuracy: 31.3441%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [167/225], Training Accuracy: 31.3810%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [168/225], Training Accuracy: 31.3430%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [169/225], Training Accuracy: 31.2685%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [170/225], Training Accuracy: 31.2224%, Training Loss: 0.6862%\n",
      "Epoch [19/100], Step [171/225], Training Accuracy: 31.2683%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [172/225], Training Accuracy: 31.2863%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [173/225], Training Accuracy: 31.2861%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [174/225], Training Accuracy: 31.3039%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [175/225], Training Accuracy: 31.3393%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [176/225], Training Accuracy: 31.3832%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [177/225], Training Accuracy: 31.3736%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [178/225], Training Accuracy: 31.3729%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [179/225], Training Accuracy: 31.3635%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [180/225], Training Accuracy: 31.4062%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [181/225], Training Accuracy: 31.3709%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [182/225], Training Accuracy: 31.3444%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [183/225], Training Accuracy: 31.3525%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [184/225], Training Accuracy: 31.3264%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [185/225], Training Accuracy: 31.2838%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [186/225], Training Accuracy: 31.3172%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [187/225], Training Accuracy: 31.3168%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [188/225], Training Accuracy: 31.3165%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [189/225], Training Accuracy: 31.3409%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [190/225], Training Accuracy: 31.3158%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [191/225], Training Accuracy: 31.2909%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [192/225], Training Accuracy: 31.2256%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [193/225], Training Accuracy: 31.2338%, Training Loss: 0.6861%\n",
      "Epoch [19/100], Step [194/225], Training Accuracy: 31.2419%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [195/225], Training Accuracy: 31.2260%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [196/225], Training Accuracy: 31.1942%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [197/225], Training Accuracy: 31.2183%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [198/225], Training Accuracy: 31.2342%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [199/225], Training Accuracy: 31.2186%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [200/225], Training Accuracy: 31.1953%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [201/225], Training Accuracy: 31.2189%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Step [202/225], Training Accuracy: 31.2268%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [203/225], Training Accuracy: 31.2115%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [204/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [205/225], Training Accuracy: 31.2729%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [206/225], Training Accuracy: 31.2803%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [207/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [19/100], Step [208/225], Training Accuracy: 31.2876%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [209/225], Training Accuracy: 31.3248%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [210/225], Training Accuracy: 31.3542%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [211/225], Training Accuracy: 31.3315%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [212/225], Training Accuracy: 31.3606%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [213/225], Training Accuracy: 31.3380%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [214/225], Training Accuracy: 31.3522%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [215/225], Training Accuracy: 31.3227%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [216/225], Training Accuracy: 31.2717%, Training Loss: 0.6859%\n",
      "Epoch [19/100], Step [217/225], Training Accuracy: 31.2644%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [218/225], Training Accuracy: 31.2357%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [219/225], Training Accuracy: 31.2928%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [220/225], Training Accuracy: 31.3210%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [221/225], Training Accuracy: 31.3066%, Training Loss: 0.6858%\n",
      "Epoch [19/100], Step [222/225], Training Accuracy: 31.3133%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [223/225], Training Accuracy: 31.3341%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [224/225], Training Accuracy: 31.3267%, Training Loss: 0.6857%\n",
      "Epoch [19/100], Step [225/225], Training Accuracy: 31.3091%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6867%\n",
      "Epoch [20/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6848%\n",
      "Epoch [20/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6872%\n",
      "Epoch [20/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6848%\n",
      "Epoch [20/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6863%\n",
      "Epoch [20/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6844%\n",
      "Epoch [20/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6841%\n",
      "Epoch [20/100], Step [9/225], Training Accuracy: 31.9444%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [14/225], Training Accuracy: 30.1339%, Training Loss: 0.6862%\n",
      "Epoch [20/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6862%\n",
      "Epoch [20/100], Step [16/225], Training Accuracy: 30.5664%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [18/225], Training Accuracy: 30.0347%, Training Loss: 0.6864%\n",
      "Epoch [20/100], Step [19/225], Training Accuracy: 30.5099%, Training Loss: 0.6865%\n",
      "Epoch [20/100], Step [20/225], Training Accuracy: 31.1719%, Training Loss: 0.6864%\n",
      "Epoch [20/100], Step [21/225], Training Accuracy: 31.1756%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [22/225], Training Accuracy: 31.1790%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [23/225], Training Accuracy: 30.9783%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [24/225], Training Accuracy: 30.9896%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [25/225], Training Accuracy: 31.1875%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [26/225], Training Accuracy: 31.5505%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [27/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [28/225], Training Accuracy: 31.1384%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [29/225], Training Accuracy: 31.5733%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [30/225], Training Accuracy: 31.7188%, Training Loss: 0.6851%\n",
      "Epoch [20/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6851%\n",
      "Epoch [20/100], Step [32/225], Training Accuracy: 31.8848%, Training Loss: 0.6849%\n",
      "Epoch [20/100], Step [33/225], Training Accuracy: 32.0549%, Training Loss: 0.6847%\n",
      "Epoch [20/100], Step [34/225], Training Accuracy: 31.8015%, Training Loss: 0.6847%\n",
      "Epoch [20/100], Step [35/225], Training Accuracy: 31.7411%, Training Loss: 0.6849%\n",
      "Epoch [20/100], Step [36/225], Training Accuracy: 31.4670%, Training Loss: 0.6851%\n",
      "Epoch [20/100], Step [37/225], Training Accuracy: 31.5456%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [38/225], Training Accuracy: 31.4145%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [39/225], Training Accuracy: 31.1699%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [40/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [41/225], Training Accuracy: 31.3262%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [42/225], Training Accuracy: 31.1756%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [43/225], Training Accuracy: 31.3227%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [44/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [45/225], Training Accuracy: 31.3194%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [46/225], Training Accuracy: 31.1481%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [47/225], Training Accuracy: 31.1835%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [48/225], Training Accuracy: 31.3477%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [49/225], Training Accuracy: 31.4094%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [50/225], Training Accuracy: 31.3125%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [51/225], Training Accuracy: 31.4338%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [52/225], Training Accuracy: 31.4904%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [53/225], Training Accuracy: 31.3974%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [55/225], Training Accuracy: 31.4205%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [56/225], Training Accuracy: 31.3895%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [57/225], Training Accuracy: 31.4419%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [58/225], Training Accuracy: 31.3578%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [59/225], Training Accuracy: 31.6208%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [60/225], Training Accuracy: 31.7188%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [61/225], Training Accuracy: 31.6855%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [62/225], Training Accuracy: 31.7288%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [63/225], Training Accuracy: 31.7708%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [64/225], Training Accuracy: 31.7627%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [65/225], Training Accuracy: 31.6587%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [66/225], Training Accuracy: 31.8182%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [67/225], Training Accuracy: 31.8097%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [68/225], Training Accuracy: 31.8474%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [69/225], Training Accuracy: 31.8614%, Training Loss: 0.6851%\n",
      "Epoch [20/100], Step [70/225], Training Accuracy: 31.8080%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [71/225], Training Accuracy: 31.8442%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [72/225], Training Accuracy: 31.6189%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [73/225], Training Accuracy: 31.5711%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [74/225], Training Accuracy: 31.6512%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [75/225], Training Accuracy: 31.5625%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [77/225], Training Accuracy: 31.4529%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [78/225], Training Accuracy: 31.4704%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [79/225], Training Accuracy: 31.4280%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [80/225], Training Accuracy: 31.3672%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [81/225], Training Accuracy: 31.3465%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [82/225], Training Accuracy: 31.3453%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [83/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Step [84/225], Training Accuracy: 31.3058%, Training Loss: 0.6852%\n",
      "Epoch [20/100], Step [85/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [86/225], Training Accuracy: 31.3045%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [87/225], Training Accuracy: 31.3398%, Training Loss: 0.6853%\n",
      "Epoch [20/100], Step [88/225], Training Accuracy: 31.2855%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [90/225], Training Accuracy: 31.0938%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [91/225], Training Accuracy: 31.1126%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [92/225], Training Accuracy: 31.0802%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [93/225], Training Accuracy: 31.1156%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [94/225], Training Accuracy: 31.1669%, Training Loss: 0.6854%\n",
      "Epoch [20/100], Step [95/225], Training Accuracy: 31.0526%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [96/225], Training Accuracy: 31.1849%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [97/225], Training Accuracy: 31.2017%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [98/225], Training Accuracy: 31.2341%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [99/225], Training Accuracy: 31.3447%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [100/225], Training Accuracy: 31.3125%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [101/225], Training Accuracy: 31.4511%, Training Loss: 0.6855%\n",
      "Epoch [20/100], Step [102/225], Training Accuracy: 31.3266%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [103/225], Training Accuracy: 31.3865%, Training Loss: 0.6856%\n",
      "Epoch [20/100], Step [104/225], Training Accuracy: 31.3702%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [105/225], Training Accuracy: 31.3244%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [106/225], Training Accuracy: 31.3090%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [107/225], Training Accuracy: 31.2208%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [108/225], Training Accuracy: 31.3079%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [109/225], Training Accuracy: 31.1497%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [110/225], Training Accuracy: 31.1648%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [111/225], Training Accuracy: 31.0952%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [112/225], Training Accuracy: 31.1663%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [113/225], Training Accuracy: 31.1394%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [114/225], Training Accuracy: 31.2089%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [115/225], Training Accuracy: 31.1957%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [116/225], Training Accuracy: 31.1961%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [117/225], Training Accuracy: 31.1565%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [118/225], Training Accuracy: 31.1176%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [119/225], Training Accuracy: 31.0793%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [120/225], Training Accuracy: 31.1198%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [121/225], Training Accuracy: 31.1080%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [123/225], Training Accuracy: 31.1484%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [124/225], Training Accuracy: 31.1240%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [125/225], Training Accuracy: 31.0875%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [126/225], Training Accuracy: 31.0268%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [127/225], Training Accuracy: 30.9916%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [128/225], Training Accuracy: 30.9692%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [129/225], Training Accuracy: 30.9835%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [130/225], Training Accuracy: 30.9255%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [131/225], Training Accuracy: 30.8802%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [132/225], Training Accuracy: 30.8594%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [133/225], Training Accuracy: 30.8976%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [134/225], Training Accuracy: 30.9468%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [135/225], Training Accuracy: 30.9375%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [136/225], Training Accuracy: 30.9513%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [137/225], Training Accuracy: 30.9991%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [138/225], Training Accuracy: 31.0009%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [139/225], Training Accuracy: 30.9915%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [140/225], Training Accuracy: 30.9710%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [141/225], Training Accuracy: 30.9286%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [142/225], Training Accuracy: 30.9969%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [143/225], Training Accuracy: 30.9987%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [144/225], Training Accuracy: 31.0004%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [145/225], Training Accuracy: 31.0668%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [146/225], Training Accuracy: 31.0681%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [147/225], Training Accuracy: 31.0799%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [148/225], Training Accuracy: 31.0494%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [149/225], Training Accuracy: 31.0927%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [150/225], Training Accuracy: 31.1250%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [151/225], Training Accuracy: 31.1672%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [152/225], Training Accuracy: 31.1575%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [153/225], Training Accuracy: 31.1275%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [154/225], Training Accuracy: 31.1384%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [155/225], Training Accuracy: 31.1290%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [156/225], Training Accuracy: 31.1699%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [157/225], Training Accuracy: 31.0808%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [158/225], Training Accuracy: 31.0621%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [159/225], Training Accuracy: 31.1124%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [160/225], Training Accuracy: 31.0938%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [161/225], Training Accuracy: 31.1432%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [162/225], Training Accuracy: 31.1343%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [163/225], Training Accuracy: 31.1925%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [164/225], Training Accuracy: 31.1738%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [165/225], Training Accuracy: 31.1174%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [166/225], Training Accuracy: 31.1088%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [167/225], Training Accuracy: 31.1471%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [168/225], Training Accuracy: 31.1105%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [169/225], Training Accuracy: 31.0281%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [170/225], Training Accuracy: 30.9835%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [171/225], Training Accuracy: 31.0124%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [172/225], Training Accuracy: 31.0411%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [173/225], Training Accuracy: 31.0152%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [174/225], Training Accuracy: 31.0075%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [175/225], Training Accuracy: 31.0536%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [176/225], Training Accuracy: 31.0813%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [177/225], Training Accuracy: 31.0823%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [178/225], Training Accuracy: 31.0832%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [179/225], Training Accuracy: 31.0580%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [180/225], Training Accuracy: 31.1024%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [181/225], Training Accuracy: 31.0601%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [182/225], Training Accuracy: 31.0525%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [183/225], Training Accuracy: 31.0622%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [184/225], Training Accuracy: 31.0292%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [185/225], Training Accuracy: 31.0051%, Training Loss: 0.6861%\n",
      "Epoch [20/100], Step [186/225], Training Accuracy: 31.0400%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [187/225], Training Accuracy: 31.0411%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [188/225], Training Accuracy: 31.0422%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [189/225], Training Accuracy: 31.0681%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Step [190/225], Training Accuracy: 31.0362%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [191/225], Training Accuracy: 31.0128%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [192/225], Training Accuracy: 30.9489%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [193/225], Training Accuracy: 30.9424%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [194/225], Training Accuracy: 30.9601%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [195/225], Training Accuracy: 30.9375%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [196/225], Training Accuracy: 30.9152%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [197/225], Training Accuracy: 30.9486%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [198/225], Training Accuracy: 30.9738%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [199/225], Training Accuracy: 30.9516%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [200/225], Training Accuracy: 30.9453%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [201/225], Training Accuracy: 30.9624%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [202/225], Training Accuracy: 30.9638%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [203/225], Training Accuracy: 30.9575%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [204/225], Training Accuracy: 31.0049%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [205/225], Training Accuracy: 31.0137%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [206/225], Training Accuracy: 31.0149%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [207/225], Training Accuracy: 30.9858%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [208/225], Training Accuracy: 31.0171%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [209/225], Training Accuracy: 31.0407%, Training Loss: 0.6860%\n",
      "Epoch [20/100], Step [210/225], Training Accuracy: 31.0789%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [211/225], Training Accuracy: 31.0649%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [212/225], Training Accuracy: 31.1247%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [213/225], Training Accuracy: 31.0960%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [214/225], Training Accuracy: 31.1186%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [215/225], Training Accuracy: 31.0828%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [216/225], Training Accuracy: 31.0258%, Training Loss: 0.6859%\n",
      "Epoch [20/100], Step [217/225], Training Accuracy: 31.0268%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [218/225], Training Accuracy: 30.9991%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [219/225], Training Accuracy: 31.0716%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [220/225], Training Accuracy: 31.0795%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [221/225], Training Accuracy: 31.0520%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [222/225], Training Accuracy: 31.0670%, Training Loss: 0.6858%\n",
      "Epoch [20/100], Step [223/225], Training Accuracy: 31.0959%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [224/225], Training Accuracy: 31.0965%, Training Loss: 0.6857%\n",
      "Epoch [20/100], Step [225/225], Training Accuracy: 31.0798%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6891%\n",
      "Epoch [21/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6878%\n",
      "Epoch [21/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6862%\n",
      "Epoch [21/100], Step [6/225], Training Accuracy: 33.0729%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [7/225], Training Accuracy: 32.8125%, Training Loss: 0.6849%\n",
      "Epoch [21/100], Step [8/225], Training Accuracy: 32.4219%, Training Loss: 0.6846%\n",
      "Epoch [21/100], Step [9/225], Training Accuracy: 32.2917%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [10/225], Training Accuracy: 31.7188%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [11/225], Training Accuracy: 31.3920%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6864%\n",
      "Epoch [21/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6868%\n",
      "Epoch [21/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6865%\n",
      "Epoch [21/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6869%\n",
      "Epoch [21/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6871%\n",
      "Epoch [21/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6870%\n",
      "Epoch [21/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6869%\n",
      "Epoch [21/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6866%\n",
      "Epoch [21/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6862%\n",
      "Epoch [21/100], Step [23/225], Training Accuracy: 31.5217%, Training Loss: 0.6862%\n",
      "Epoch [21/100], Step [24/225], Training Accuracy: 31.5755%, Training Loss: 0.6863%\n",
      "Epoch [21/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6862%\n",
      "Epoch [21/100], Step [26/225], Training Accuracy: 32.1514%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [27/225], Training Accuracy: 31.9444%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [28/225], Training Accuracy: 31.8638%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [29/225], Training Accuracy: 32.2737%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [30/225], Training Accuracy: 32.3438%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [31/225], Training Accuracy: 32.2581%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [32/225], Training Accuracy: 32.3730%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [33/225], Training Accuracy: 32.4811%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [35/225], Training Accuracy: 32.2321%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [36/225], Training Accuracy: 32.1181%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [37/225], Training Accuracy: 32.0524%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [38/225], Training Accuracy: 31.8668%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [39/225], Training Accuracy: 31.6106%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [41/225], Training Accuracy: 31.6311%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [43/225], Training Accuracy: 31.6134%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [44/225], Training Accuracy: 31.5696%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [45/225], Training Accuracy: 31.4931%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [46/225], Training Accuracy: 31.3519%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [47/225], Training Accuracy: 31.3165%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [48/225], Training Accuracy: 31.5430%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [49/225], Training Accuracy: 31.5689%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [50/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [51/225], Training Accuracy: 31.8015%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [52/225], Training Accuracy: 31.8510%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [53/225], Training Accuracy: 31.7807%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [54/225], Training Accuracy: 31.7130%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [55/225], Training Accuracy: 31.8182%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [56/225], Training Accuracy: 31.8638%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [57/225], Training Accuracy: 31.8805%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [58/225], Training Accuracy: 31.7888%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [59/225], Training Accuracy: 32.1504%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [61/225], Training Accuracy: 32.2234%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [62/225], Training Accuracy: 32.2581%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [63/225], Training Accuracy: 32.2669%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [64/225], Training Accuracy: 32.2266%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [65/225], Training Accuracy: 32.0673%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [66/225], Training Accuracy: 32.1970%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [67/225], Training Accuracy: 32.2062%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [68/225], Training Accuracy: 32.2381%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [69/225], Training Accuracy: 32.2464%, Training Loss: 0.6848%\n",
      "Epoch [21/100], Step [70/225], Training Accuracy: 32.1875%, Training Loss: 0.6849%\n",
      "Epoch [21/100], Step [71/225], Training Accuracy: 32.1963%, Training Loss: 0.6849%\n",
      "Epoch [21/100], Step [72/225], Training Accuracy: 31.9878%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [73/225], Training Accuracy: 31.9135%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [74/225], Training Accuracy: 31.9890%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [75/225], Training Accuracy: 31.9167%, Training Loss: 0.6850%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Step [76/225], Training Accuracy: 31.8257%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [77/225], Training Accuracy: 31.7167%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [78/225], Training Accuracy: 31.7508%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [79/225], Training Accuracy: 31.6653%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [80/225], Training Accuracy: 31.6211%, Training Loss: 0.6849%\n",
      "Epoch [21/100], Step [81/225], Training Accuracy: 31.5008%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [82/225], Training Accuracy: 31.4977%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [83/225], Training Accuracy: 31.4194%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [84/225], Training Accuracy: 31.4918%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [85/225], Training Accuracy: 31.4154%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [86/225], Training Accuracy: 31.4680%, Training Loss: 0.6850%\n",
      "Epoch [21/100], Step [87/225], Training Accuracy: 31.4835%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [88/225], Training Accuracy: 31.4631%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [89/225], Training Accuracy: 31.3729%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [90/225], Training Accuracy: 31.2847%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [91/225], Training Accuracy: 31.3359%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [92/225], Training Accuracy: 31.2840%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [93/225], Training Accuracy: 31.3004%, Training Loss: 0.6852%\n",
      "Epoch [21/100], Step [94/225], Training Accuracy: 31.3830%, Training Loss: 0.6851%\n",
      "Epoch [21/100], Step [95/225], Training Accuracy: 31.2664%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [96/225], Training Accuracy: 31.3639%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [97/225], Training Accuracy: 31.4111%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [98/225], Training Accuracy: 31.4094%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [99/225], Training Accuracy: 31.5499%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [100/225], Training Accuracy: 31.5312%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [101/225], Training Accuracy: 31.6522%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [102/225], Training Accuracy: 31.5564%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [103/225], Training Accuracy: 31.5989%, Training Loss: 0.6853%\n",
      "Epoch [21/100], Step [104/225], Training Accuracy: 31.5655%, Training Loss: 0.6854%\n",
      "Epoch [21/100], Step [105/225], Training Accuracy: 31.5179%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [107/225], Training Accuracy: 31.4836%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [108/225], Training Accuracy: 31.5538%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [109/225], Training Accuracy: 31.4220%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [110/225], Training Accuracy: 31.4062%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [111/225], Training Accuracy: 31.2922%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [112/225], Training Accuracy: 31.3477%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [113/225], Training Accuracy: 31.3468%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [114/225], Training Accuracy: 31.4008%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [115/225], Training Accuracy: 31.3723%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [116/225], Training Accuracy: 31.3578%, Training Loss: 0.6855%\n",
      "Epoch [21/100], Step [117/225], Training Accuracy: 31.2767%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [118/225], Training Accuracy: 31.2235%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [119/225], Training Accuracy: 31.1975%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [120/225], Training Accuracy: 31.2240%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [121/225], Training Accuracy: 31.1725%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [122/225], Training Accuracy: 31.1860%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [123/225], Training Accuracy: 31.2246%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [124/225], Training Accuracy: 31.1870%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [125/225], Training Accuracy: 31.1875%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [126/225], Training Accuracy: 31.1012%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [127/225], Training Accuracy: 31.0285%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [128/225], Training Accuracy: 31.0181%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [129/225], Training Accuracy: 31.0441%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [130/225], Training Accuracy: 31.0216%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [131/225], Training Accuracy: 30.9757%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [132/225], Training Accuracy: 30.9422%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [133/225], Training Accuracy: 30.9798%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [134/225], Training Accuracy: 31.0518%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [135/225], Training Accuracy: 31.0648%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [136/225], Training Accuracy: 31.1236%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [137/225], Training Accuracy: 31.1588%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [138/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [139/225], Training Accuracy: 31.1376%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [140/225], Training Accuracy: 31.1384%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [141/225], Training Accuracy: 31.1059%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [142/225], Training Accuracy: 31.1620%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [143/225], Training Accuracy: 31.1407%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [144/225], Training Accuracy: 31.1415%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [145/225], Training Accuracy: 31.2069%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [146/225], Training Accuracy: 31.2072%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [147/225], Training Accuracy: 31.2181%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [149/225], Training Accuracy: 31.2081%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [151/225], Training Accuracy: 31.2810%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [152/225], Training Accuracy: 31.2911%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [153/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [154/225], Training Accuracy: 31.2804%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [155/225], Training Accuracy: 31.2601%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [156/225], Training Accuracy: 31.3001%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [157/225], Training Accuracy: 31.2400%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [158/225], Training Accuracy: 31.2302%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [159/225], Training Accuracy: 31.2991%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [160/225], Training Accuracy: 31.2695%, Training Loss: 0.6861%\n",
      "Epoch [21/100], Step [161/225], Training Accuracy: 31.3276%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [162/225], Training Accuracy: 31.2982%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [163/225], Training Accuracy: 31.3650%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [164/225], Training Accuracy: 31.3548%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [165/225], Training Accuracy: 31.2879%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [166/225], Training Accuracy: 31.2877%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [167/225], Training Accuracy: 31.3249%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [168/225], Training Accuracy: 31.2872%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [169/225], Training Accuracy: 31.2130%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [170/225], Training Accuracy: 31.1673%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [171/225], Training Accuracy: 31.1952%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [172/225], Training Accuracy: 31.2137%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [175/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [176/225], Training Accuracy: 31.2766%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [177/225], Training Accuracy: 31.2765%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [178/225], Training Accuracy: 31.2763%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [179/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [180/225], Training Accuracy: 31.2934%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [181/225], Training Accuracy: 31.2586%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [182/225], Training Accuracy: 31.2328%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [183/225], Training Accuracy: 31.2415%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [184/225], Training Accuracy: 31.2160%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Step [185/225], Training Accuracy: 31.1824%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [186/225], Training Accuracy: 31.2248%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [187/225], Training Accuracy: 31.2082%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [188/225], Training Accuracy: 31.2001%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [189/225], Training Accuracy: 31.2335%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [190/225], Training Accuracy: 31.1924%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [191/225], Training Accuracy: 31.1600%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [192/225], Training Accuracy: 31.0954%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [193/225], Training Accuracy: 31.1043%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [194/225], Training Accuracy: 31.1211%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [195/225], Training Accuracy: 31.1138%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [196/225], Training Accuracy: 31.0826%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [197/225], Training Accuracy: 31.1072%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [198/225], Training Accuracy: 31.1316%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [199/225], Training Accuracy: 31.1008%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [200/225], Training Accuracy: 31.0859%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [201/225], Training Accuracy: 31.1023%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [202/225], Training Accuracy: 31.1030%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [203/225], Training Accuracy: 31.0961%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [204/225], Training Accuracy: 31.1504%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [205/225], Training Accuracy: 31.1738%, Training Loss: 0.6860%\n",
      "Epoch [21/100], Step [206/225], Training Accuracy: 31.1666%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [207/225], Training Accuracy: 31.1519%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [208/225], Training Accuracy: 31.1749%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [209/225], Training Accuracy: 31.2126%, Training Loss: 0.6859%\n",
      "Epoch [21/100], Step [210/225], Training Accuracy: 31.2202%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [211/225], Training Accuracy: 31.1982%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [212/225], Training Accuracy: 31.2426%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [213/225], Training Accuracy: 31.2207%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [214/225], Training Accuracy: 31.2427%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [215/225], Training Accuracy: 31.2064%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [216/225], Training Accuracy: 31.1343%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [217/225], Training Accuracy: 31.1276%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [218/225], Training Accuracy: 31.0995%, Training Loss: 0.6858%\n",
      "Epoch [21/100], Step [219/225], Training Accuracy: 31.1572%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [220/225], Training Accuracy: 31.1719%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [221/225], Training Accuracy: 31.1581%, Training Loss: 0.6857%\n",
      "Epoch [21/100], Step [222/225], Training Accuracy: 31.1796%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [223/225], Training Accuracy: 31.2080%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [224/225], Training Accuracy: 31.2012%, Training Loss: 0.6856%\n",
      "Epoch [21/100], Step [225/225], Training Accuracy: 31.1770%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6939%\n",
      "Epoch [22/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6886%\n",
      "Epoch [22/100], Step [3/225], Training Accuracy: 30.2083%, Training Loss: 0.6898%\n",
      "Epoch [22/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6875%\n",
      "Epoch [22/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6879%\n",
      "Epoch [22/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6881%\n",
      "Epoch [22/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6868%\n",
      "Epoch [22/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6867%\n",
      "Epoch [22/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [14/225], Training Accuracy: 30.4688%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6862%\n",
      "Epoch [22/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6864%\n",
      "Epoch [22/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6864%\n",
      "Epoch [22/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6864%\n",
      "Epoch [22/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6863%\n",
      "Epoch [22/100], Step [22/225], Training Accuracy: 31.7472%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6862%\n",
      "Epoch [22/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6862%\n",
      "Epoch [22/100], Step [25/225], Training Accuracy: 31.6875%, Training Loss: 0.6862%\n",
      "Epoch [22/100], Step [26/225], Training Accuracy: 32.0913%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [28/225], Training Accuracy: 31.5290%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [29/225], Training Accuracy: 31.8427%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [31/225], Training Accuracy: 31.9052%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [33/225], Training Accuracy: 32.1970%, Training Loss: 0.6850%\n",
      "Epoch [22/100], Step [34/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [35/225], Training Accuracy: 31.9643%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [36/225], Training Accuracy: 31.7708%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [37/225], Training Accuracy: 31.7568%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [38/225], Training Accuracy: 31.5789%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [39/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [40/225], Training Accuracy: 31.3281%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [41/225], Training Accuracy: 31.2881%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [42/225], Training Accuracy: 31.1012%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [43/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [44/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [45/225], Training Accuracy: 31.3194%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [46/225], Training Accuracy: 31.1481%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [47/225], Training Accuracy: 31.1503%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [48/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [49/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [50/225], Training Accuracy: 31.2188%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [51/225], Training Accuracy: 31.3419%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [52/225], Training Accuracy: 31.3702%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [53/225], Training Accuracy: 31.3090%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [54/225], Training Accuracy: 31.1632%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [55/225], Training Accuracy: 31.3068%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [56/225], Training Accuracy: 31.3337%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [57/225], Training Accuracy: 31.3322%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [58/225], Training Accuracy: 31.3039%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [59/225], Training Accuracy: 31.7267%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [61/225], Training Accuracy: 31.7367%, Training Loss: 0.6847%\n",
      "Epoch [22/100], Step [62/225], Training Accuracy: 31.7792%, Training Loss: 0.6849%\n",
      "Epoch [22/100], Step [63/225], Training Accuracy: 31.8452%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [64/225], Training Accuracy: 31.8359%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Step [67/225], Training Accuracy: 31.8563%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [68/225], Training Accuracy: 31.9164%, Training Loss: 0.6847%\n",
      "Epoch [22/100], Step [69/225], Training Accuracy: 31.9067%, Training Loss: 0.6846%\n",
      "Epoch [22/100], Step [70/225], Training Accuracy: 31.8750%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [71/225], Training Accuracy: 31.9102%, Training Loss: 0.6848%\n",
      "Epoch [22/100], Step [72/225], Training Accuracy: 31.6623%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [73/225], Training Accuracy: 31.5711%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [74/225], Training Accuracy: 31.6723%, Training Loss: 0.6850%\n",
      "Epoch [22/100], Step [75/225], Training Accuracy: 31.6458%, Training Loss: 0.6850%\n",
      "Epoch [22/100], Step [76/225], Training Accuracy: 31.5995%, Training Loss: 0.6850%\n",
      "Epoch [22/100], Step [77/225], Training Accuracy: 31.5341%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [78/225], Training Accuracy: 31.5505%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [79/225], Training Accuracy: 31.4873%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [80/225], Training Accuracy: 31.4844%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [81/225], Training Accuracy: 31.4236%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [82/225], Training Accuracy: 31.4405%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [83/225], Training Accuracy: 31.3630%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [84/225], Training Accuracy: 31.3988%, Training Loss: 0.6851%\n",
      "Epoch [22/100], Step [85/225], Training Accuracy: 31.3603%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [86/225], Training Accuracy: 31.3772%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [87/225], Training Accuracy: 31.3757%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [88/225], Training Accuracy: 31.3210%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [89/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [90/225], Training Accuracy: 31.1458%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [91/225], Training Accuracy: 31.1985%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [92/225], Training Accuracy: 31.2160%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [93/225], Training Accuracy: 31.2668%, Training Loss: 0.6853%\n",
      "Epoch [22/100], Step [94/225], Training Accuracy: 31.2999%, Training Loss: 0.6852%\n",
      "Epoch [22/100], Step [95/225], Training Accuracy: 31.1842%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [96/225], Training Accuracy: 31.2663%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [97/225], Training Accuracy: 31.2983%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [98/225], Training Accuracy: 31.2978%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [99/225], Training Accuracy: 31.4078%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [100/225], Training Accuracy: 31.4062%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [101/225], Training Accuracy: 31.5285%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [102/225], Training Accuracy: 31.4338%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [103/225], Training Accuracy: 31.4927%, Training Loss: 0.6854%\n",
      "Epoch [22/100], Step [104/225], Training Accuracy: 31.4453%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [105/225], Training Accuracy: 31.3839%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [106/225], Training Accuracy: 31.3827%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [107/225], Training Accuracy: 31.2938%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [108/225], Training Accuracy: 31.3657%, Training Loss: 0.6855%\n",
      "Epoch [22/100], Step [109/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [110/225], Training Accuracy: 31.2358%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [111/225], Training Accuracy: 31.1092%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [112/225], Training Accuracy: 31.1663%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [113/225], Training Accuracy: 31.1532%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [114/225], Training Accuracy: 31.2089%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [115/225], Training Accuracy: 31.1821%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [116/225], Training Accuracy: 31.1692%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [117/225], Training Accuracy: 31.0897%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [118/225], Training Accuracy: 31.0779%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [119/225], Training Accuracy: 31.0137%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [120/225], Training Accuracy: 31.0677%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [121/225], Training Accuracy: 31.0434%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [122/225], Training Accuracy: 31.0579%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [123/225], Training Accuracy: 31.0849%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [124/225], Training Accuracy: 31.0736%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [125/225], Training Accuracy: 31.0500%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [126/225], Training Accuracy: 31.0020%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [127/225], Training Accuracy: 30.9547%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [128/225], Training Accuracy: 30.9448%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [129/225], Training Accuracy: 30.9835%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [130/225], Training Accuracy: 30.9495%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [131/225], Training Accuracy: 30.9160%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [132/225], Training Accuracy: 30.8830%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [133/225], Training Accuracy: 30.8976%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [134/225], Training Accuracy: 30.9701%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [135/225], Training Accuracy: 30.9722%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [137/225], Training Accuracy: 31.0675%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [138/225], Training Accuracy: 31.0915%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [139/225], Training Accuracy: 31.0701%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [140/225], Training Accuracy: 31.0491%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [141/225], Training Accuracy: 30.9951%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [142/225], Training Accuracy: 31.0629%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [143/225], Training Accuracy: 31.0533%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [144/225], Training Accuracy: 31.0764%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [145/225], Training Accuracy: 31.1315%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [146/225], Training Accuracy: 31.1323%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [147/225], Training Accuracy: 31.1543%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [148/225], Training Accuracy: 31.1339%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [149/225], Training Accuracy: 31.1766%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [150/225], Training Accuracy: 31.1875%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [151/225], Training Accuracy: 31.2293%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [152/225], Training Accuracy: 31.2294%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [153/225], Training Accuracy: 31.1887%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [154/225], Training Accuracy: 31.2094%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [155/225], Training Accuracy: 31.1895%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [156/225], Training Accuracy: 31.2300%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [157/225], Training Accuracy: 31.1704%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [158/225], Training Accuracy: 31.1511%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [159/225], Training Accuracy: 31.2303%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [160/225], Training Accuracy: 31.1914%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [161/225], Training Accuracy: 31.2403%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [162/225], Training Accuracy: 31.2018%, Training Loss: 0.6861%\n",
      "Epoch [22/100], Step [163/225], Training Accuracy: 31.2692%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [164/225], Training Accuracy: 31.2595%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [165/225], Training Accuracy: 31.1837%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Step [166/225], Training Accuracy: 31.2029%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [167/225], Training Accuracy: 31.2313%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [168/225], Training Accuracy: 31.1849%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [169/225], Training Accuracy: 31.1298%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [170/225], Training Accuracy: 31.0846%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [171/225], Training Accuracy: 31.1129%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [172/225], Training Accuracy: 31.1319%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [173/225], Training Accuracy: 31.1145%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [174/225], Training Accuracy: 31.1512%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [175/225], Training Accuracy: 31.1964%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [176/225], Training Accuracy: 31.2234%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [177/225], Training Accuracy: 31.2235%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [178/225], Training Accuracy: 31.2149%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [179/225], Training Accuracy: 31.1802%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [180/225], Training Accuracy: 31.2326%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [181/225], Training Accuracy: 31.1896%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [182/225], Training Accuracy: 31.1727%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [183/225], Training Accuracy: 31.1817%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [185/225], Training Accuracy: 31.1064%, Training Loss: 0.6860%\n",
      "Epoch [22/100], Step [186/225], Training Accuracy: 31.1408%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [187/225], Training Accuracy: 31.1497%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [190/225], Training Accuracy: 31.1266%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [191/225], Training Accuracy: 31.0946%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [192/225], Training Accuracy: 31.0221%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [193/225], Training Accuracy: 31.0314%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [194/225], Training Accuracy: 31.0486%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [195/225], Training Accuracy: 31.0176%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [196/225], Training Accuracy: 30.9790%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [197/225], Training Accuracy: 31.0041%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [198/225], Training Accuracy: 31.0211%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [199/225], Training Accuracy: 30.9830%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [200/225], Training Accuracy: 30.9688%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [201/225], Training Accuracy: 30.9857%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [202/225], Training Accuracy: 30.9870%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [203/225], Training Accuracy: 30.9806%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [204/225], Training Accuracy: 31.0662%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [205/225], Training Accuracy: 31.0823%, Training Loss: 0.6859%\n",
      "Epoch [22/100], Step [206/225], Training Accuracy: 31.0907%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [207/225], Training Accuracy: 31.0764%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [208/225], Training Accuracy: 31.1073%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [209/225], Training Accuracy: 31.1304%, Training Loss: 0.6858%\n",
      "Epoch [22/100], Step [210/225], Training Accuracy: 31.1458%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [211/225], Training Accuracy: 31.1241%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [212/225], Training Accuracy: 31.1616%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [213/225], Training Accuracy: 31.1400%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [214/225], Training Accuracy: 31.1624%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [215/225], Training Accuracy: 31.1410%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [216/225], Training Accuracy: 31.0909%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [217/225], Training Accuracy: 31.0988%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [218/225], Training Accuracy: 31.0636%, Training Loss: 0.6857%\n",
      "Epoch [22/100], Step [219/225], Training Accuracy: 31.1358%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [220/225], Training Accuracy: 31.1506%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [221/225], Training Accuracy: 31.1369%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [222/225], Training Accuracy: 31.1585%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [223/225], Training Accuracy: 31.1869%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [224/225], Training Accuracy: 31.1872%, Training Loss: 0.6856%\n",
      "Epoch [22/100], Step [225/225], Training Accuracy: 31.1701%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6930%\n",
      "Epoch [23/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6892%\n",
      "Epoch [23/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6900%\n",
      "Epoch [23/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6876%\n",
      "Epoch [23/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6875%\n",
      "Epoch [23/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6879%\n",
      "Epoch [23/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6864%\n",
      "Epoch [23/100], Step [8/225], Training Accuracy: 32.2266%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6863%\n",
      "Epoch [23/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6863%\n",
      "Epoch [23/100], Step [12/225], Training Accuracy: 30.5990%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [14/225], Training Accuracy: 30.6920%, Training Loss: 0.6870%\n",
      "Epoch [23/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6869%\n",
      "Epoch [23/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6865%\n",
      "Epoch [23/100], Step [17/225], Training Accuracy: 30.7904%, Training Loss: 0.6868%\n",
      "Epoch [23/100], Step [18/225], Training Accuracy: 30.9028%, Training Loss: 0.6870%\n",
      "Epoch [23/100], Step [19/225], Training Accuracy: 31.4967%, Training Loss: 0.6869%\n",
      "Epoch [23/100], Step [20/225], Training Accuracy: 31.8750%, Training Loss: 0.6868%\n",
      "Epoch [23/100], Step [21/225], Training Accuracy: 31.8452%, Training Loss: 0.6865%\n",
      "Epoch [23/100], Step [22/225], Training Accuracy: 32.3153%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [23/225], Training Accuracy: 32.1332%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [24/225], Training Accuracy: 32.2266%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [25/225], Training Accuracy: 32.3125%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [26/225], Training Accuracy: 32.7524%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [27/225], Training Accuracy: 32.4074%, Training Loss: 0.6857%\n",
      "Epoch [23/100], Step [28/225], Training Accuracy: 32.2545%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [29/225], Training Accuracy: 32.5431%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [30/225], Training Accuracy: 32.4479%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [31/225], Training Accuracy: 32.4597%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [32/225], Training Accuracy: 32.6660%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [33/225], Training Accuracy: 32.6231%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [34/225], Training Accuracy: 32.4449%, Training Loss: 0.6849%\n",
      "Epoch [23/100], Step [35/225], Training Accuracy: 32.2321%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [36/225], Training Accuracy: 32.0747%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [37/225], Training Accuracy: 32.0946%, Training Loss: 0.6852%\n",
      "Epoch [23/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6852%\n",
      "Epoch [23/100], Step [39/225], Training Accuracy: 31.6106%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [40/225], Training Accuracy: 31.7188%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [42/225], Training Accuracy: 31.6220%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [44/225], Training Accuracy: 31.7472%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [45/225], Training Accuracy: 31.8403%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [46/225], Training Accuracy: 31.7595%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [47/225], Training Accuracy: 31.7487%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [48/225], Training Accuracy: 31.8359%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [50/225], Training Accuracy: 31.7500%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [51/225], Training Accuracy: 31.8627%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [52/225], Training Accuracy: 31.9111%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [53/225], Training Accuracy: 31.8396%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [54/225], Training Accuracy: 31.7130%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [55/225], Training Accuracy: 31.9318%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [56/225], Training Accuracy: 31.9475%, Training Loss: 0.6854%\n",
      "Epoch [23/100], Step [57/225], Training Accuracy: 31.9353%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [58/225], Training Accuracy: 31.8966%, Training Loss: 0.6852%\n",
      "Epoch [23/100], Step [59/225], Training Accuracy: 32.3358%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [60/225], Training Accuracy: 32.4219%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [61/225], Training Accuracy: 32.3514%, Training Loss: 0.6849%\n",
      "Epoch [23/100], Step [62/225], Training Accuracy: 32.2833%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [63/225], Training Accuracy: 32.3413%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [64/225], Training Accuracy: 32.2998%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [65/225], Training Accuracy: 32.2596%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [66/225], Training Accuracy: 32.3627%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [67/225], Training Accuracy: 32.3461%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [68/225], Training Accuracy: 32.3300%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [69/225], Training Accuracy: 32.3143%, Training Loss: 0.6848%\n",
      "Epoch [23/100], Step [70/225], Training Accuracy: 32.2545%, Training Loss: 0.6848%\n",
      "Epoch [23/100], Step [71/225], Training Accuracy: 32.2623%, Training Loss: 0.6848%\n",
      "Epoch [23/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [73/225], Training Accuracy: 31.9991%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [74/225], Training Accuracy: 32.0946%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [75/225], Training Accuracy: 32.0417%, Training Loss: 0.6849%\n",
      "Epoch [23/100], Step [76/225], Training Accuracy: 31.9490%, Training Loss: 0.6849%\n",
      "Epoch [23/100], Step [77/225], Training Accuracy: 31.8385%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [78/225], Training Accuracy: 31.8710%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [79/225], Training Accuracy: 31.8038%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [80/225], Training Accuracy: 31.7383%, Training Loss: 0.6850%\n",
      "Epoch [23/100], Step [81/225], Training Accuracy: 31.6165%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [82/225], Training Accuracy: 31.6311%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [83/225], Training Accuracy: 31.5700%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [84/225], Training Accuracy: 31.6778%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [85/225], Training Accuracy: 31.6360%, Training Loss: 0.6851%\n",
      "Epoch [23/100], Step [86/225], Training Accuracy: 31.6860%, Training Loss: 0.6852%\n",
      "Epoch [23/100], Step [87/225], Training Accuracy: 31.7170%, Training Loss: 0.6852%\n",
      "Epoch [23/100], Step [88/225], Training Accuracy: 31.6939%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [89/225], Training Accuracy: 31.6187%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [90/225], Training Accuracy: 31.5278%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [91/225], Training Accuracy: 31.5762%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [92/225], Training Accuracy: 31.5557%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [93/225], Training Accuracy: 31.6028%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [94/225], Training Accuracy: 31.6988%, Training Loss: 0.6853%\n",
      "Epoch [23/100], Step [95/225], Training Accuracy: 31.5789%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [96/225], Training Accuracy: 31.6406%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [97/225], Training Accuracy: 31.6527%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [98/225], Training Accuracy: 31.6645%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [99/225], Training Accuracy: 31.7866%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [100/225], Training Accuracy: 31.7656%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [101/225], Training Accuracy: 31.8688%, Training Loss: 0.6855%\n",
      "Epoch [23/100], Step [102/225], Training Accuracy: 31.7555%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [103/225], Training Accuracy: 31.7961%, Training Loss: 0.6856%\n",
      "Epoch [23/100], Step [104/225], Training Accuracy: 31.7608%, Training Loss: 0.6857%\n",
      "Epoch [23/100], Step [105/225], Training Accuracy: 31.7113%, Training Loss: 0.6857%\n",
      "Epoch [23/100], Step [106/225], Training Accuracy: 31.7217%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [107/225], Training Accuracy: 31.6151%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [108/225], Training Accuracy: 31.6840%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [109/225], Training Accuracy: 31.5654%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [110/225], Training Accuracy: 31.5625%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [111/225], Training Accuracy: 31.4611%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [112/225], Training Accuracy: 31.5430%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [113/225], Training Accuracy: 31.5265%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [114/225], Training Accuracy: 31.5789%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [115/225], Training Accuracy: 31.5761%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [116/225], Training Accuracy: 31.5598%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [117/225], Training Accuracy: 31.4770%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [118/225], Training Accuracy: 31.4354%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [119/225], Training Accuracy: 31.3813%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [120/225], Training Accuracy: 31.3932%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [121/225], Training Accuracy: 31.3920%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [122/225], Training Accuracy: 31.3781%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [123/225], Training Accuracy: 31.4278%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [124/225], Training Accuracy: 31.4390%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [126/225], Training Accuracy: 31.3492%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [127/225], Training Accuracy: 31.3115%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [128/225], Training Accuracy: 31.2988%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [129/225], Training Accuracy: 31.3469%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [130/225], Training Accuracy: 31.3221%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [131/225], Training Accuracy: 31.2977%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [132/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [133/225], Training Accuracy: 31.2617%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [134/225], Training Accuracy: 31.3316%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [135/225], Training Accuracy: 31.3194%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [136/225], Training Accuracy: 31.3419%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [137/225], Training Accuracy: 31.3641%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [138/225], Training Accuracy: 31.3859%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [139/225], Training Accuracy: 31.3624%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [140/225], Training Accuracy: 31.3616%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [141/225], Training Accuracy: 31.3165%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [142/225], Training Accuracy: 31.3820%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [143/225], Training Accuracy: 31.3811%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [144/225], Training Accuracy: 31.3911%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [145/225], Training Accuracy: 31.4224%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [146/225], Training Accuracy: 31.4533%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [147/225], Training Accuracy: 31.4732%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [148/225], Training Accuracy: 31.4400%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [149/225], Training Accuracy: 31.4702%, Training Loss: 0.6862%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Step [150/225], Training Accuracy: 31.5104%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [151/225], Training Accuracy: 31.5604%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [152/225], Training Accuracy: 31.5687%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [153/225], Training Accuracy: 31.5462%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [154/225], Training Accuracy: 31.5544%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [155/225], Training Accuracy: 31.5222%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [156/225], Training Accuracy: 31.5304%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [157/225], Training Accuracy: 31.4590%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [158/225], Training Accuracy: 31.4478%, Training Loss: 0.6863%\n",
      "Epoch [23/100], Step [159/225], Training Accuracy: 31.5153%, Training Loss: 0.6863%\n",
      "Epoch [23/100], Step [160/225], Training Accuracy: 31.4844%, Training Loss: 0.6863%\n",
      "Epoch [23/100], Step [161/225], Training Accuracy: 31.5314%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [162/225], Training Accuracy: 31.5201%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [163/225], Training Accuracy: 31.6047%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [164/225], Training Accuracy: 31.5835%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [165/225], Training Accuracy: 31.5246%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [166/225], Training Accuracy: 31.5418%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [167/225], Training Accuracy: 31.5962%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [168/225], Training Accuracy: 31.5569%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [169/225], Training Accuracy: 31.4719%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [170/225], Training Accuracy: 31.4338%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [171/225], Training Accuracy: 31.4510%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [172/225], Training Accuracy: 31.4499%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [173/225], Training Accuracy: 31.4306%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [174/225], Training Accuracy: 31.4476%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [175/225], Training Accuracy: 31.4643%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [176/225], Training Accuracy: 31.4808%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [177/225], Training Accuracy: 31.4972%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [178/225], Training Accuracy: 31.4782%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [179/225], Training Accuracy: 31.4508%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [180/225], Training Accuracy: 31.4757%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [181/225], Training Accuracy: 31.4399%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [182/225], Training Accuracy: 31.4303%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [183/225], Training Accuracy: 31.4378%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [184/225], Training Accuracy: 31.4029%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [185/225], Training Accuracy: 31.3598%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [186/225], Training Accuracy: 31.3844%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [187/225], Training Accuracy: 31.3837%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [188/225], Training Accuracy: 31.3830%, Training Loss: 0.6862%\n",
      "Epoch [23/100], Step [189/225], Training Accuracy: 31.4153%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [190/225], Training Accuracy: 31.3651%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [191/225], Training Accuracy: 31.3400%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [192/225], Training Accuracy: 31.2826%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [193/225], Training Accuracy: 31.2905%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [194/225], Training Accuracy: 31.3064%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [195/225], Training Accuracy: 31.2740%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [196/225], Training Accuracy: 31.2420%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [197/225], Training Accuracy: 31.2738%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [198/225], Training Accuracy: 31.2895%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [199/225], Training Accuracy: 31.2736%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [200/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [201/225], Training Accuracy: 31.2578%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [202/225], Training Accuracy: 31.2655%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [203/225], Training Accuracy: 31.2654%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [204/225], Training Accuracy: 31.3266%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [205/225], Training Accuracy: 31.3415%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [206/225], Training Accuracy: 31.3334%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [207/225], Training Accuracy: 31.3104%, Training Loss: 0.6861%\n",
      "Epoch [23/100], Step [208/225], Training Accuracy: 31.3477%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [209/225], Training Accuracy: 31.3846%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [210/225], Training Accuracy: 31.3988%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [211/225], Training Accuracy: 31.3907%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [212/225], Training Accuracy: 31.4416%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [213/225], Training Accuracy: 31.4187%, Training Loss: 0.6860%\n",
      "Epoch [23/100], Step [214/225], Training Accuracy: 31.4544%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [215/225], Training Accuracy: 31.4172%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [216/225], Training Accuracy: 31.3585%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [217/225], Training Accuracy: 31.3652%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [218/225], Training Accuracy: 31.3217%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [219/225], Training Accuracy: 31.3784%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [220/225], Training Accuracy: 31.3920%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [221/225], Training Accuracy: 31.3914%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [222/225], Training Accuracy: 31.4119%, Training Loss: 0.6859%\n",
      "Epoch [23/100], Step [223/225], Training Accuracy: 31.4462%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [224/225], Training Accuracy: 31.4453%, Training Loss: 0.6858%\n",
      "Epoch [23/100], Step [225/225], Training Accuracy: 31.4202%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [2/225], Training Accuracy: 30.4688%, Training Loss: 0.6849%\n",
      "Epoch [24/100], Step [3/225], Training Accuracy: 29.6875%, Training Loss: 0.6886%\n",
      "Epoch [24/100], Step [4/225], Training Accuracy: 30.4688%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [5/225], Training Accuracy: 31.2500%, Training Loss: 0.6877%\n",
      "Epoch [24/100], Step [6/225], Training Accuracy: 30.4688%, Training Loss: 0.6879%\n",
      "Epoch [24/100], Step [7/225], Training Accuracy: 30.5804%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [8/225], Training Accuracy: 30.4688%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [9/225], Training Accuracy: 30.3819%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [10/225], Training Accuracy: 30.0000%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [11/225], Training Accuracy: 30.1136%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [12/225], Training Accuracy: 29.5573%, Training Loss: 0.6867%\n",
      "Epoch [24/100], Step [13/225], Training Accuracy: 29.5673%, Training Loss: 0.6869%\n",
      "Epoch [24/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6875%\n",
      "Epoch [24/100], Step [15/225], Training Accuracy: 30.4167%, Training Loss: 0.6874%\n",
      "Epoch [24/100], Step [16/225], Training Accuracy: 30.3711%, Training Loss: 0.6871%\n",
      "Epoch [24/100], Step [17/225], Training Accuracy: 30.0551%, Training Loss: 0.6872%\n",
      "Epoch [24/100], Step [18/225], Training Accuracy: 29.9479%, Training Loss: 0.6873%\n",
      "Epoch [24/100], Step [19/225], Training Accuracy: 30.0987%, Training Loss: 0.6872%\n",
      "Epoch [24/100], Step [20/225], Training Accuracy: 30.4688%, Training Loss: 0.6872%\n",
      "Epoch [24/100], Step [21/225], Training Accuracy: 30.4315%, Training Loss: 0.6868%\n",
      "Epoch [24/100], Step [22/225], Training Accuracy: 30.6108%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [23/225], Training Accuracy: 30.5027%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [24/225], Training Accuracy: 30.7292%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [25/225], Training Accuracy: 31.0000%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [26/225], Training Accuracy: 31.3101%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [27/225], Training Accuracy: 30.9606%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Step [28/225], Training Accuracy: 30.9152%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [29/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [30/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [31/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [32/225], Training Accuracy: 31.4941%, Training Loss: 0.6852%\n",
      "Epoch [24/100], Step [33/225], Training Accuracy: 31.5814%, Training Loss: 0.6851%\n",
      "Epoch [24/100], Step [34/225], Training Accuracy: 31.4338%, Training Loss: 0.6851%\n",
      "Epoch [24/100], Step [35/225], Training Accuracy: 31.3839%, Training Loss: 0.6853%\n",
      "Epoch [24/100], Step [36/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [24/100], Step [37/225], Training Accuracy: 31.3345%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [38/225], Training Accuracy: 31.1266%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [39/225], Training Accuracy: 30.9295%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [40/225], Training Accuracy: 31.0938%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [41/225], Training Accuracy: 31.0595%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [42/225], Training Accuracy: 30.9524%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [43/225], Training Accuracy: 31.1410%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [44/225], Training Accuracy: 31.1435%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [45/225], Training Accuracy: 31.1806%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [46/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [47/225], Training Accuracy: 31.1503%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [48/225], Training Accuracy: 31.3151%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [49/225], Training Accuracy: 31.3776%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [51/225], Training Accuracy: 31.4951%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [53/225], Training Accuracy: 31.4858%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [54/225], Training Accuracy: 31.3947%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [55/225], Training Accuracy: 31.4773%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [56/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [57/225], Training Accuracy: 31.6612%, Training Loss: 0.6854%\n",
      "Epoch [24/100], Step [58/225], Training Accuracy: 31.5194%, Training Loss: 0.6854%\n",
      "Epoch [24/100], Step [59/225], Training Accuracy: 31.7797%, Training Loss: 0.6853%\n",
      "Epoch [24/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6853%\n",
      "Epoch [24/100], Step [61/225], Training Accuracy: 31.8135%, Training Loss: 0.6853%\n",
      "Epoch [24/100], Step [62/225], Training Accuracy: 31.8044%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [63/225], Training Accuracy: 31.8452%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [64/225], Training Accuracy: 31.8359%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [65/225], Training Accuracy: 31.7548%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [67/225], Training Accuracy: 31.8797%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [69/225], Training Accuracy: 31.9293%, Training Loss: 0.6854%\n",
      "Epoch [24/100], Step [70/225], Training Accuracy: 31.8973%, Training Loss: 0.6854%\n",
      "Epoch [24/100], Step [71/225], Training Accuracy: 31.9102%, Training Loss: 0.6855%\n",
      "Epoch [24/100], Step [72/225], Training Accuracy: 31.7057%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [73/225], Training Accuracy: 31.6353%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [74/225], Training Accuracy: 31.7145%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [75/225], Training Accuracy: 31.6667%, Training Loss: 0.6856%\n",
      "Epoch [24/100], Step [76/225], Training Accuracy: 31.6201%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [77/225], Training Accuracy: 31.5138%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [78/225], Training Accuracy: 31.5505%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [79/225], Training Accuracy: 31.4676%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [80/225], Training Accuracy: 31.4258%, Training Loss: 0.6857%\n",
      "Epoch [24/100], Step [81/225], Training Accuracy: 31.2693%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [82/225], Training Accuracy: 31.2881%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [83/225], Training Accuracy: 31.2312%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [84/225], Training Accuracy: 31.3430%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [85/225], Training Accuracy: 31.3051%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [86/225], Training Accuracy: 31.3772%, Training Loss: 0.6858%\n",
      "Epoch [24/100], Step [87/225], Training Accuracy: 31.3937%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [88/225], Training Accuracy: 31.3743%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [89/225], Training Accuracy: 31.3027%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [90/225], Training Accuracy: 31.1979%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [91/225], Training Accuracy: 31.1985%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [92/225], Training Accuracy: 31.1651%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [93/225], Training Accuracy: 31.1324%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [94/225], Training Accuracy: 31.2001%, Training Loss: 0.6859%\n",
      "Epoch [24/100], Step [95/225], Training Accuracy: 31.0855%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [96/225], Training Accuracy: 31.1849%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [97/225], Training Accuracy: 31.2178%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [98/225], Training Accuracy: 31.2341%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [99/225], Training Accuracy: 31.3447%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [100/225], Training Accuracy: 31.2969%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [101/225], Training Accuracy: 31.4047%, Training Loss: 0.6860%\n",
      "Epoch [24/100], Step [102/225], Training Accuracy: 31.2960%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [103/225], Training Accuracy: 31.3714%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [104/225], Training Accuracy: 31.3401%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [105/225], Training Accuracy: 31.3095%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [106/225], Training Accuracy: 31.2942%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [107/225], Training Accuracy: 31.2208%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [108/225], Training Accuracy: 31.2789%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [109/225], Training Accuracy: 31.1640%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [110/225], Training Accuracy: 31.1648%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [111/225], Training Accuracy: 31.0670%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [112/225], Training Accuracy: 31.1663%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [113/225], Training Accuracy: 31.1670%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [114/225], Training Accuracy: 31.2226%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [115/225], Training Accuracy: 31.1821%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [116/225], Training Accuracy: 31.1827%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [117/225], Training Accuracy: 31.1298%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [118/225], Training Accuracy: 31.0911%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [119/225], Training Accuracy: 31.0399%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [120/225], Training Accuracy: 31.0807%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [121/225], Training Accuracy: 31.0305%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [122/225], Training Accuracy: 31.0451%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [123/225], Training Accuracy: 31.0722%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [124/225], Training Accuracy: 31.0484%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [125/225], Training Accuracy: 31.0125%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [126/225], Training Accuracy: 30.9648%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [127/225], Training Accuracy: 30.9055%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [128/225], Training Accuracy: 30.9082%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [129/225], Training Accuracy: 30.9351%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [130/225], Training Accuracy: 30.9014%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [131/225], Training Accuracy: 30.8564%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [132/225], Training Accuracy: 30.8357%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [133/225], Training Accuracy: 30.8506%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [134/225], Training Accuracy: 30.9235%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [135/225], Training Accuracy: 30.9259%, Training Loss: 0.6865%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Step [136/225], Training Accuracy: 30.9628%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [137/225], Training Accuracy: 31.0219%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [138/225], Training Accuracy: 31.0462%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [139/225], Training Accuracy: 31.0139%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [140/225], Training Accuracy: 31.0045%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [141/225], Training Accuracy: 30.9730%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [142/225], Training Accuracy: 31.0299%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [143/225], Training Accuracy: 31.0096%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [144/225], Training Accuracy: 31.0221%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [145/225], Training Accuracy: 31.0776%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [146/225], Training Accuracy: 31.1002%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [147/225], Training Accuracy: 31.1118%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [148/225], Training Accuracy: 31.1022%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [149/225], Training Accuracy: 31.1346%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [150/225], Training Accuracy: 31.1562%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [151/225], Training Accuracy: 31.1983%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [152/225], Training Accuracy: 31.2089%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [153/225], Training Accuracy: 31.1683%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [154/225], Training Accuracy: 31.2094%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [155/225], Training Accuracy: 31.1895%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [156/225], Training Accuracy: 31.2300%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [157/225], Training Accuracy: 31.1505%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [158/225], Training Accuracy: 31.1313%, Training Loss: 0.6867%\n",
      "Epoch [24/100], Step [159/225], Training Accuracy: 31.2303%, Training Loss: 0.6867%\n",
      "Epoch [24/100], Step [160/225], Training Accuracy: 31.2012%, Training Loss: 0.6867%\n",
      "Epoch [24/100], Step [161/225], Training Accuracy: 31.2597%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [162/225], Training Accuracy: 31.2307%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [163/225], Training Accuracy: 31.3075%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [164/225], Training Accuracy: 31.2786%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [165/225], Training Accuracy: 31.2121%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [166/225], Training Accuracy: 31.2218%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [167/225], Training Accuracy: 31.2594%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [168/225], Training Accuracy: 31.2128%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [169/225], Training Accuracy: 31.1298%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [170/225], Training Accuracy: 31.0846%, Training Loss: 0.6866%\n",
      "Epoch [24/100], Step [171/225], Training Accuracy: 31.1038%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [173/225], Training Accuracy: 31.1055%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [174/225], Training Accuracy: 31.1333%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [175/225], Training Accuracy: 31.1607%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [176/225], Training Accuracy: 31.1790%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [177/225], Training Accuracy: 31.1794%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [178/225], Training Accuracy: 31.1886%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [179/225], Training Accuracy: 31.1627%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [181/225], Training Accuracy: 31.1723%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [182/225], Training Accuracy: 31.1556%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [183/225], Training Accuracy: 31.1646%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [184/225], Training Accuracy: 31.1396%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [185/225], Training Accuracy: 31.0980%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [186/225], Training Accuracy: 31.1324%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [187/225], Training Accuracy: 31.1414%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [188/225], Training Accuracy: 31.1586%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [189/225], Training Accuracy: 31.1839%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [190/225], Training Accuracy: 31.1431%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [191/225], Training Accuracy: 31.1273%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [192/225], Training Accuracy: 31.0628%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [193/225], Training Accuracy: 31.0557%, Training Loss: 0.6865%\n",
      "Epoch [24/100], Step [194/225], Training Accuracy: 31.0648%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [195/225], Training Accuracy: 31.0337%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [196/225], Training Accuracy: 31.0188%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [197/225], Training Accuracy: 31.0438%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [198/225], Training Accuracy: 31.0606%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [199/225], Training Accuracy: 31.0380%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [200/225], Training Accuracy: 31.0234%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [201/225], Training Accuracy: 31.0401%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [202/225], Training Accuracy: 31.0489%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [203/225], Training Accuracy: 31.0422%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [204/225], Training Accuracy: 31.1045%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [205/225], Training Accuracy: 31.0976%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [207/225], Training Accuracy: 31.0613%, Training Loss: 0.6864%\n",
      "Epoch [24/100], Step [208/225], Training Accuracy: 31.0998%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [209/225], Training Accuracy: 31.1453%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [210/225], Training Accuracy: 31.1756%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [211/225], Training Accuracy: 31.1537%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [212/225], Training Accuracy: 31.1984%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [213/225], Training Accuracy: 31.1840%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [214/225], Training Accuracy: 31.2062%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [215/225], Training Accuracy: 31.1846%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [216/225], Training Accuracy: 31.1198%, Training Loss: 0.6863%\n",
      "Epoch [24/100], Step [217/225], Training Accuracy: 31.1204%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [218/225], Training Accuracy: 31.0923%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [219/225], Training Accuracy: 31.1572%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [220/225], Training Accuracy: 31.1719%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [221/225], Training Accuracy: 31.1581%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [222/225], Training Accuracy: 31.1726%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [223/225], Training Accuracy: 31.2080%, Training Loss: 0.6862%\n",
      "Epoch [24/100], Step [224/225], Training Accuracy: 31.2221%, Training Loss: 0.6861%\n",
      "Epoch [24/100], Step [225/225], Training Accuracy: 31.2048%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6908%\n",
      "Epoch [25/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6882%\n",
      "Epoch [25/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6894%\n",
      "Epoch [25/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6872%\n",
      "Epoch [25/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6877%\n",
      "Epoch [25/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6877%\n",
      "Epoch [25/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6867%\n",
      "Epoch [25/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6872%\n",
      "Epoch [25/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6867%\n",
      "Epoch [25/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6869%\n",
      "Epoch [25/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6867%\n",
      "Epoch [25/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6869%\n",
      "Epoch [25/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6877%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6876%\n",
      "Epoch [25/100], Step [16/225], Training Accuracy: 31.1523%, Training Loss: 0.6871%\n",
      "Epoch [25/100], Step [17/225], Training Accuracy: 30.9743%, Training Loss: 0.6874%\n",
      "Epoch [25/100], Step [18/225], Training Accuracy: 31.0764%, Training Loss: 0.6874%\n",
      "Epoch [25/100], Step [19/225], Training Accuracy: 31.4967%, Training Loss: 0.6874%\n",
      "Epoch [25/100], Step [20/225], Training Accuracy: 31.8750%, Training Loss: 0.6872%\n",
      "Epoch [25/100], Step [21/225], Training Accuracy: 31.8452%, Training Loss: 0.6869%\n",
      "Epoch [25/100], Step [22/225], Training Accuracy: 32.0312%, Training Loss: 0.6866%\n",
      "Epoch [25/100], Step [23/225], Training Accuracy: 31.8614%, Training Loss: 0.6866%\n",
      "Epoch [25/100], Step [24/225], Training Accuracy: 32.0312%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [25/225], Training Accuracy: 32.3125%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [26/225], Training Accuracy: 32.6923%, Training Loss: 0.6857%\n",
      "Epoch [25/100], Step [27/225], Training Accuracy: 32.4653%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [28/225], Training Accuracy: 32.1987%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [29/225], Training Accuracy: 32.4892%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [30/225], Training Accuracy: 32.5000%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [31/225], Training Accuracy: 32.2077%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [32/225], Training Accuracy: 32.4707%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [33/225], Training Accuracy: 32.5284%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6851%\n",
      "Epoch [25/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [36/225], Training Accuracy: 32.0312%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [38/225], Training Accuracy: 31.8668%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [43/225], Training Accuracy: 31.6860%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [44/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [45/225], Training Accuracy: 31.5972%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [46/225], Training Accuracy: 31.4198%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [47/225], Training Accuracy: 31.3830%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [48/225], Training Accuracy: 31.4779%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [49/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [50/225], Training Accuracy: 31.4688%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [51/225], Training Accuracy: 31.5870%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [52/225], Training Accuracy: 31.7007%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [53/225], Training Accuracy: 31.6333%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [54/225], Training Accuracy: 31.5394%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [55/225], Training Accuracy: 31.6193%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [56/225], Training Accuracy: 31.6964%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [57/225], Training Accuracy: 31.7434%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [58/225], Training Accuracy: 31.6272%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [59/225], Training Accuracy: 31.9386%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [60/225], Training Accuracy: 32.0833%, Training Loss: 0.6849%\n",
      "Epoch [25/100], Step [61/225], Training Accuracy: 31.9672%, Training Loss: 0.6849%\n",
      "Epoch [25/100], Step [62/225], Training Accuracy: 31.9052%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [63/225], Training Accuracy: 31.9692%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [64/225], Training Accuracy: 31.9580%, Training Loss: 0.6849%\n",
      "Epoch [25/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [66/225], Training Accuracy: 31.9602%, Training Loss: 0.6851%\n",
      "Epoch [25/100], Step [67/225], Training Accuracy: 31.9496%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [68/225], Training Accuracy: 31.9853%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [69/225], Training Accuracy: 31.9973%, Training Loss: 0.6848%\n",
      "Epoch [25/100], Step [70/225], Training Accuracy: 31.9420%, Training Loss: 0.6849%\n",
      "Epoch [25/100], Step [71/225], Training Accuracy: 31.9542%, Training Loss: 0.6850%\n",
      "Epoch [25/100], Step [72/225], Training Accuracy: 31.7491%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [73/225], Training Accuracy: 31.6781%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [74/225], Training Accuracy: 31.7779%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [75/225], Training Accuracy: 31.7292%, Training Loss: 0.6851%\n",
      "Epoch [25/100], Step [76/225], Training Accuracy: 31.6817%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [77/225], Training Accuracy: 31.5950%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [78/225], Training Accuracy: 31.5905%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [79/225], Training Accuracy: 31.5071%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [80/225], Training Accuracy: 31.4648%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [81/225], Training Accuracy: 31.3850%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [82/225], Training Accuracy: 31.4024%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [83/225], Training Accuracy: 31.3630%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [84/225], Training Accuracy: 31.4732%, Training Loss: 0.6852%\n",
      "Epoch [25/100], Step [85/225], Training Accuracy: 31.4154%, Training Loss: 0.6853%\n",
      "Epoch [25/100], Step [86/225], Training Accuracy: 31.4317%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [87/225], Training Accuracy: 31.4296%, Training Loss: 0.6854%\n",
      "Epoch [25/100], Step [88/225], Training Accuracy: 31.4276%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [89/225], Training Accuracy: 31.3378%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [90/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [91/225], Training Accuracy: 31.2843%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [92/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [93/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [94/225], Training Accuracy: 31.3497%, Training Loss: 0.6855%\n",
      "Epoch [25/100], Step [95/225], Training Accuracy: 31.2336%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [96/225], Training Accuracy: 31.3151%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [97/225], Training Accuracy: 31.2983%, Training Loss: 0.6857%\n",
      "Epoch [25/100], Step [98/225], Training Accuracy: 31.3138%, Training Loss: 0.6857%\n",
      "Epoch [25/100], Step [99/225], Training Accuracy: 31.4394%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [100/225], Training Accuracy: 31.4062%, Training Loss: 0.6857%\n",
      "Epoch [25/100], Step [101/225], Training Accuracy: 31.5130%, Training Loss: 0.6856%\n",
      "Epoch [25/100], Step [102/225], Training Accuracy: 31.4032%, Training Loss: 0.6857%\n",
      "Epoch [25/100], Step [103/225], Training Accuracy: 31.4472%, Training Loss: 0.6858%\n",
      "Epoch [25/100], Step [104/225], Training Accuracy: 31.4303%, Training Loss: 0.6858%\n",
      "Epoch [25/100], Step [105/225], Training Accuracy: 31.3690%, Training Loss: 0.6858%\n",
      "Epoch [25/100], Step [106/225], Training Accuracy: 31.3679%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [107/225], Training Accuracy: 31.2792%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [108/225], Training Accuracy: 31.3368%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [109/225], Training Accuracy: 31.2213%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [110/225], Training Accuracy: 31.2074%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [111/225], Training Accuracy: 31.0952%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [112/225], Training Accuracy: 31.1802%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [113/225], Training Accuracy: 31.1809%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [114/225], Training Accuracy: 31.2774%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [115/225], Training Accuracy: 31.2636%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [116/225], Training Accuracy: 31.2231%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [117/225], Training Accuracy: 31.1565%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [118/225], Training Accuracy: 31.1308%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [119/225], Training Accuracy: 31.0793%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Step [120/225], Training Accuracy: 31.1198%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [121/225], Training Accuracy: 31.1080%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [123/225], Training Accuracy: 31.1484%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [124/225], Training Accuracy: 31.1618%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [125/225], Training Accuracy: 31.1375%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [126/225], Training Accuracy: 31.1012%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [127/225], Training Accuracy: 31.0531%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [128/225], Training Accuracy: 31.0303%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [129/225], Training Accuracy: 31.0804%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [130/225], Training Accuracy: 31.0096%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [131/225], Training Accuracy: 30.9757%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [132/225], Training Accuracy: 30.9304%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [133/225], Training Accuracy: 30.9445%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [134/225], Training Accuracy: 31.0051%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [135/225], Training Accuracy: 31.0069%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [136/225], Training Accuracy: 31.0317%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [137/225], Training Accuracy: 31.0789%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [138/225], Training Accuracy: 31.0915%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [139/225], Training Accuracy: 31.0477%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [140/225], Training Accuracy: 31.0491%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [141/225], Training Accuracy: 30.9840%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [142/225], Training Accuracy: 31.0629%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [143/225], Training Accuracy: 31.0752%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [144/225], Training Accuracy: 31.0981%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [145/225], Training Accuracy: 31.1422%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [146/225], Training Accuracy: 31.1858%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [147/225], Training Accuracy: 31.2075%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [149/225], Training Accuracy: 31.2081%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [151/225], Training Accuracy: 31.2707%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [152/225], Training Accuracy: 31.2706%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [153/225], Training Accuracy: 31.2398%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [154/225], Training Accuracy: 31.2703%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [155/225], Training Accuracy: 31.2601%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [156/225], Training Accuracy: 31.3001%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [157/225], Training Accuracy: 31.2400%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [158/225], Training Accuracy: 31.2104%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [159/225], Training Accuracy: 31.2795%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [160/225], Training Accuracy: 31.2500%, Training Loss: 0.6863%\n",
      "Epoch [25/100], Step [161/225], Training Accuracy: 31.2985%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [162/225], Training Accuracy: 31.2789%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [163/225], Training Accuracy: 31.3363%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [164/225], Training Accuracy: 31.3262%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [165/225], Training Accuracy: 31.2689%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [166/225], Training Accuracy: 31.2877%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [167/225], Training Accuracy: 31.3249%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [168/225], Training Accuracy: 31.2686%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [169/225], Training Accuracy: 31.2130%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [170/225], Training Accuracy: 31.1673%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [171/225], Training Accuracy: 31.2043%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [172/225], Training Accuracy: 31.2318%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [173/225], Training Accuracy: 31.2139%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [174/225], Training Accuracy: 31.2231%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [175/225], Training Accuracy: 31.2589%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [176/225], Training Accuracy: 31.2766%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [177/225], Training Accuracy: 31.2588%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [178/225], Training Accuracy: 31.2412%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [179/225], Training Accuracy: 31.2151%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [180/225], Training Accuracy: 31.2587%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [181/225], Training Accuracy: 31.2155%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [182/225], Training Accuracy: 31.2071%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [183/225], Training Accuracy: 31.2244%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [184/225], Training Accuracy: 31.1990%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [185/225], Training Accuracy: 31.1655%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [186/225], Training Accuracy: 31.1912%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [187/225], Training Accuracy: 31.1832%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [188/225], Training Accuracy: 31.1835%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [189/225], Training Accuracy: 31.2087%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [190/225], Training Accuracy: 31.1678%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [191/225], Training Accuracy: 31.1518%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [192/225], Training Accuracy: 31.0872%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [193/225], Training Accuracy: 31.0800%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [194/225], Training Accuracy: 31.0970%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [195/225], Training Accuracy: 31.0897%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [196/225], Training Accuracy: 31.0666%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [197/225], Training Accuracy: 31.0993%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [198/225], Training Accuracy: 31.1237%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [199/225], Training Accuracy: 31.0851%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [200/225], Training Accuracy: 31.0781%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [201/225], Training Accuracy: 31.1023%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [202/225], Training Accuracy: 31.1108%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [203/225], Training Accuracy: 31.1038%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [204/225], Training Accuracy: 31.1657%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [205/225], Training Accuracy: 31.1738%, Training Loss: 0.6862%\n",
      "Epoch [25/100], Step [206/225], Training Accuracy: 31.1666%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [207/225], Training Accuracy: 31.1368%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [208/225], Training Accuracy: 31.1599%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [209/225], Training Accuracy: 31.1827%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [210/225], Training Accuracy: 31.2128%, Training Loss: 0.6861%\n",
      "Epoch [25/100], Step [211/225], Training Accuracy: 31.2056%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [212/225], Training Accuracy: 31.2795%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [213/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [214/225], Training Accuracy: 31.2719%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [215/225], Training Accuracy: 31.2427%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [216/225], Training Accuracy: 31.1921%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [217/225], Training Accuracy: 31.1924%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [218/225], Training Accuracy: 31.1497%, Training Loss: 0.6860%\n",
      "Epoch [25/100], Step [219/225], Training Accuracy: 31.2072%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [220/225], Training Accuracy: 31.2287%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [221/225], Training Accuracy: 31.2005%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [222/225], Training Accuracy: 31.2148%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [223/225], Training Accuracy: 31.2570%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [224/225], Training Accuracy: 31.2570%, Training Loss: 0.6859%\n",
      "Epoch [25/100], Step [225/225], Training Accuracy: 31.2326%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6890%\n",
      "Epoch [26/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6876%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6851%\n",
      "Epoch [26/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6849%\n",
      "Epoch [26/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6852%\n",
      "Epoch [26/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6853%\n",
      "Epoch [26/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6851%\n",
      "Epoch [26/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6849%\n",
      "Epoch [26/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [17/225], Training Accuracy: 30.0551%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [18/225], Training Accuracy: 30.0347%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [19/225], Training Accuracy: 30.3454%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [20/225], Training Accuracy: 30.9375%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [21/225], Training Accuracy: 30.8780%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [22/225], Training Accuracy: 31.1080%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [23/225], Training Accuracy: 30.9103%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [24/225], Training Accuracy: 30.9896%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [25/225], Training Accuracy: 31.3125%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [26/225], Training Accuracy: 31.6106%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [27/225], Training Accuracy: 31.3079%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [28/225], Training Accuracy: 31.1942%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [29/225], Training Accuracy: 31.5733%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [30/225], Training Accuracy: 31.4583%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [31/225], Training Accuracy: 31.3004%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [32/225], Training Accuracy: 31.5430%, Training Loss: 0.6851%\n",
      "Epoch [26/100], Step [33/225], Training Accuracy: 31.8182%, Training Loss: 0.6851%\n",
      "Epoch [26/100], Step [34/225], Training Accuracy: 31.5717%, Training Loss: 0.6851%\n",
      "Epoch [26/100], Step [35/225], Training Accuracy: 31.4732%, Training Loss: 0.6853%\n",
      "Epoch [26/100], Step [36/225], Training Accuracy: 31.3802%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [37/225], Training Accuracy: 31.3345%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [38/225], Training Accuracy: 31.1266%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [39/225], Training Accuracy: 30.8894%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [40/225], Training Accuracy: 30.9375%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [41/225], Training Accuracy: 30.9070%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [42/225], Training Accuracy: 30.7292%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [43/225], Training Accuracy: 30.8140%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [44/225], Training Accuracy: 30.7173%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [45/225], Training Accuracy: 30.7986%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [46/225], Training Accuracy: 30.7065%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [47/225], Training Accuracy: 30.6516%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [48/225], Training Accuracy: 30.8268%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [49/225], Training Accuracy: 30.8992%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [50/225], Training Accuracy: 30.8438%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [51/225], Training Accuracy: 30.9743%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [52/225], Training Accuracy: 31.0096%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [53/225], Training Accuracy: 30.9257%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [54/225], Training Accuracy: 30.8738%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [55/225], Training Accuracy: 30.9943%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [56/225], Training Accuracy: 30.9710%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [57/225], Training Accuracy: 31.0307%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [58/225], Training Accuracy: 30.9537%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [59/225], Training Accuracy: 31.1970%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [60/225], Training Accuracy: 31.3542%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [61/225], Training Accuracy: 31.2756%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [62/225], Training Accuracy: 31.3256%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [63/225], Training Accuracy: 31.3740%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [64/225], Training Accuracy: 31.3721%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [65/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [66/225], Training Accuracy: 31.3920%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [67/225], Training Accuracy: 31.3899%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [68/225], Training Accuracy: 31.5028%, Training Loss: 0.6853%\n",
      "Epoch [26/100], Step [69/225], Training Accuracy: 31.4991%, Training Loss: 0.6851%\n",
      "Epoch [26/100], Step [70/225], Training Accuracy: 31.5179%, Training Loss: 0.6852%\n",
      "Epoch [26/100], Step [71/225], Training Accuracy: 31.5361%, Training Loss: 0.6852%\n",
      "Epoch [26/100], Step [72/225], Training Accuracy: 31.3585%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [73/225], Training Accuracy: 31.2928%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [74/225], Training Accuracy: 31.3978%, Training Loss: 0.6853%\n",
      "Epoch [26/100], Step [75/225], Training Accuracy: 31.3750%, Training Loss: 0.6853%\n",
      "Epoch [26/100], Step [76/225], Training Accuracy: 31.3117%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [77/225], Training Accuracy: 31.2094%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [78/225], Training Accuracy: 31.2300%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [79/225], Training Accuracy: 31.1511%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [80/225], Training Accuracy: 31.1328%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [81/225], Training Accuracy: 31.0764%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [82/225], Training Accuracy: 31.0976%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [83/225], Training Accuracy: 31.0053%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [84/225], Training Accuracy: 31.1198%, Training Loss: 0.6854%\n",
      "Epoch [26/100], Step [85/225], Training Accuracy: 31.0662%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [86/225], Training Accuracy: 31.1228%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [87/225], Training Accuracy: 31.1422%, Training Loss: 0.6855%\n",
      "Epoch [26/100], Step [88/225], Training Accuracy: 31.1435%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [89/225], Training Accuracy: 31.0920%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [90/225], Training Accuracy: 31.0069%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [91/225], Training Accuracy: 31.0440%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [92/225], Training Accuracy: 31.0122%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [93/225], Training Accuracy: 31.0316%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [94/225], Training Accuracy: 31.0838%, Training Loss: 0.6856%\n",
      "Epoch [26/100], Step [95/225], Training Accuracy: 30.9704%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [96/225], Training Accuracy: 31.0547%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [97/225], Training Accuracy: 31.1050%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [98/225], Training Accuracy: 31.1384%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [99/225], Training Accuracy: 31.2816%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [100/225], Training Accuracy: 31.2344%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [101/225], Training Accuracy: 31.3583%, Training Loss: 0.6857%\n",
      "Epoch [26/100], Step [102/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [103/225], Training Accuracy: 31.2803%, Training Loss: 0.6858%\n",
      "Epoch [26/100], Step [104/225], Training Accuracy: 31.2650%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [105/225], Training Accuracy: 31.2202%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [106/225], Training Accuracy: 31.2058%, Training Loss: 0.6859%\n",
      "Epoch [26/100], Step [107/225], Training Accuracy: 31.1040%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [108/225], Training Accuracy: 31.1777%, Training Loss: 0.6860%\n",
      "Epoch [26/100], Step [109/225], Training Accuracy: 31.0350%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [110/225], Training Accuracy: 31.0511%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [111/225], Training Accuracy: 30.9262%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [112/225], Training Accuracy: 30.9849%, Training Loss: 0.6861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Step [113/225], Training Accuracy: 30.9596%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [114/225], Training Accuracy: 31.0033%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [115/225], Training Accuracy: 30.9783%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [116/225], Training Accuracy: 30.9806%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [117/225], Training Accuracy: 30.9161%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [118/225], Training Accuracy: 30.8792%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [119/225], Training Accuracy: 30.8036%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [120/225], Training Accuracy: 30.8333%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [121/225], Training Accuracy: 30.8755%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [122/225], Training Accuracy: 30.8914%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [123/225], Training Accuracy: 30.9324%, Training Loss: 0.6861%\n",
      "Epoch [26/100], Step [124/225], Training Accuracy: 30.9098%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [125/225], Training Accuracy: 30.8750%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [126/225], Training Accuracy: 30.8160%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [127/225], Training Accuracy: 30.7948%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [128/225], Training Accuracy: 30.7739%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [129/225], Training Accuracy: 30.8261%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [130/225], Training Accuracy: 30.7812%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [131/225], Training Accuracy: 30.7252%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [132/225], Training Accuracy: 30.7055%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [133/225], Training Accuracy: 30.7096%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [134/225], Training Accuracy: 30.7603%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [135/225], Training Accuracy: 30.7755%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [136/225], Training Accuracy: 30.8249%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [137/225], Training Accuracy: 30.8964%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [138/225], Training Accuracy: 30.8990%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [139/225], Training Accuracy: 30.8790%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [140/225], Training Accuracy: 30.8594%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [141/225], Training Accuracy: 30.8178%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [142/225], Training Accuracy: 30.8979%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [143/225], Training Accuracy: 30.9003%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [144/225], Training Accuracy: 30.9028%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [145/225], Training Accuracy: 30.9698%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [146/225], Training Accuracy: 30.9932%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [147/225], Training Accuracy: 31.0055%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [148/225], Training Accuracy: 30.9755%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [149/225], Training Accuracy: 30.9983%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [150/225], Training Accuracy: 31.0208%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [151/225], Training Accuracy: 31.0534%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [152/225], Training Accuracy: 31.0444%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [153/225], Training Accuracy: 31.0253%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [154/225], Training Accuracy: 31.0572%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [155/225], Training Accuracy: 31.0383%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [156/225], Training Accuracy: 31.0697%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [157/225], Training Accuracy: 30.9912%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [158/225], Training Accuracy: 30.9830%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [159/225], Training Accuracy: 31.0436%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [160/225], Training Accuracy: 31.0156%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [161/225], Training Accuracy: 31.0656%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [162/225], Training Accuracy: 31.0282%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [163/225], Training Accuracy: 31.0870%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [164/225], Training Accuracy: 31.0404%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [165/225], Training Accuracy: 30.9754%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [166/225], Training Accuracy: 30.9770%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [167/225], Training Accuracy: 31.0254%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [168/225], Training Accuracy: 30.9896%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [169/225], Training Accuracy: 30.9079%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [170/225], Training Accuracy: 30.8732%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [171/225], Training Accuracy: 30.9028%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [172/225], Training Accuracy: 30.9230%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [173/225], Training Accuracy: 30.9068%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [174/225], Training Accuracy: 30.9267%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [175/225], Training Accuracy: 30.9464%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [176/225], Training Accuracy: 30.9837%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [177/225], Training Accuracy: 30.9852%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [178/225], Training Accuracy: 30.9515%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [179/225], Training Accuracy: 30.9270%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [180/225], Training Accuracy: 30.9635%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [181/225], Training Accuracy: 30.9133%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [182/225], Training Accuracy: 30.8980%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [183/225], Training Accuracy: 30.8999%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [184/225], Training Accuracy: 30.8679%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [185/225], Training Accuracy: 30.8361%, Training Loss: 0.6866%\n",
      "Epoch [26/100], Step [186/225], Training Accuracy: 30.8636%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [187/225], Training Accuracy: 30.8573%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [188/225], Training Accuracy: 30.8428%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [189/225], Training Accuracy: 30.8780%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [190/225], Training Accuracy: 30.8388%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [191/225], Training Accuracy: 30.8164%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [192/225], Training Accuracy: 30.7454%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [193/225], Training Accuracy: 30.7562%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [194/225], Training Accuracy: 30.7748%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [195/225], Training Accuracy: 30.7612%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [196/225], Training Accuracy: 30.7318%, Training Loss: 0.6865%\n",
      "Epoch [26/100], Step [197/225], Training Accuracy: 30.7662%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [198/225], Training Accuracy: 30.7844%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [199/225], Training Accuracy: 30.7789%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [200/225], Training Accuracy: 30.7656%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [201/225], Training Accuracy: 30.7914%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [202/225], Training Accuracy: 30.7859%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [203/225], Training Accuracy: 30.7882%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [204/225], Training Accuracy: 30.8441%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [205/225], Training Accuracy: 30.8537%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [206/225], Training Accuracy: 30.8480%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [207/225], Training Accuracy: 30.8424%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [208/225], Training Accuracy: 30.8669%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [209/225], Training Accuracy: 30.9136%, Training Loss: 0.6864%\n",
      "Epoch [26/100], Step [210/225], Training Accuracy: 30.9375%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [211/225], Training Accuracy: 30.9094%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [212/225], Training Accuracy: 30.9552%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [213/225], Training Accuracy: 30.9346%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [214/225], Training Accuracy: 30.9360%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [215/225], Training Accuracy: 30.9157%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [216/225], Training Accuracy: 30.8666%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [217/225], Training Accuracy: 30.8540%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [218/225], Training Accuracy: 30.8200%, Training Loss: 0.6863%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Step [219/225], Training Accuracy: 30.8719%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [220/225], Training Accuracy: 30.8949%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [221/225], Training Accuracy: 30.8753%, Training Loss: 0.6863%\n",
      "Epoch [26/100], Step [222/225], Training Accuracy: 30.8981%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [223/225], Training Accuracy: 30.9347%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [224/225], Training Accuracy: 30.9291%, Training Loss: 0.6862%\n",
      "Epoch [26/100], Step [225/225], Training Accuracy: 30.9061%, Training Loss: 0.6862%\n",
      "Epoch [27/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6924%\n",
      "Epoch [27/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6883%\n",
      "Epoch [27/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6899%\n",
      "Epoch [27/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6882%\n",
      "Epoch [27/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6882%\n",
      "Epoch [27/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6881%\n",
      "Epoch [27/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6872%\n",
      "Epoch [27/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6870%\n",
      "Epoch [27/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6878%\n",
      "Epoch [27/100], Step [10/225], Training Accuracy: 30.4688%, Training Loss: 0.6874%\n",
      "Epoch [27/100], Step [11/225], Training Accuracy: 30.5398%, Training Loss: 0.6872%\n",
      "Epoch [27/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6869%\n",
      "Epoch [27/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6869%\n",
      "Epoch [27/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6873%\n",
      "Epoch [27/100], Step [15/225], Training Accuracy: 30.8333%, Training Loss: 0.6870%\n",
      "Epoch [27/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6866%\n",
      "Epoch [27/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6868%\n",
      "Epoch [27/100], Step [18/225], Training Accuracy: 30.2951%, Training Loss: 0.6870%\n",
      "Epoch [27/100], Step [19/225], Training Accuracy: 30.6743%, Training Loss: 0.6868%\n",
      "Epoch [27/100], Step [20/225], Training Accuracy: 31.0938%, Training Loss: 0.6867%\n",
      "Epoch [27/100], Step [21/225], Training Accuracy: 31.1012%, Training Loss: 0.6866%\n",
      "Epoch [27/100], Step [22/225], Training Accuracy: 31.2500%, Training Loss: 0.6864%\n",
      "Epoch [27/100], Step [23/225], Training Accuracy: 31.1141%, Training Loss: 0.6863%\n",
      "Epoch [27/100], Step [24/225], Training Accuracy: 31.3151%, Training Loss: 0.6863%\n",
      "Epoch [27/100], Step [25/225], Training Accuracy: 31.5000%, Training Loss: 0.6863%\n",
      "Epoch [27/100], Step [26/225], Training Accuracy: 31.8510%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [27/225], Training Accuracy: 31.5972%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [28/225], Training Accuracy: 31.4732%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [29/225], Training Accuracy: 31.8966%, Training Loss: 0.6857%\n",
      "Epoch [27/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6853%\n",
      "Epoch [27/100], Step [32/225], Training Accuracy: 32.0312%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [33/225], Training Accuracy: 32.1496%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [35/225], Training Accuracy: 31.8750%, Training Loss: 0.6851%\n",
      "Epoch [27/100], Step [36/225], Training Accuracy: 31.6840%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [38/225], Training Accuracy: 31.5789%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [39/225], Training Accuracy: 31.4103%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [41/225], Training Accuracy: 31.5549%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [42/225], Training Accuracy: 31.4360%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [43/225], Training Accuracy: 31.6134%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [44/225], Training Accuracy: 31.6051%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [45/225], Training Accuracy: 31.6667%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [46/225], Training Accuracy: 31.5897%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [47/225], Training Accuracy: 31.5160%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [48/225], Training Accuracy: 31.6081%, Training Loss: 0.6853%\n",
      "Epoch [27/100], Step [49/225], Training Accuracy: 31.6645%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [50/225], Training Accuracy: 31.5938%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [52/225], Training Accuracy: 31.7308%, Training Loss: 0.6853%\n",
      "Epoch [27/100], Step [53/225], Training Accuracy: 31.6922%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [54/225], Training Accuracy: 31.5683%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [55/225], Training Accuracy: 31.7045%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [56/225], Training Accuracy: 31.7801%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [57/225], Training Accuracy: 31.8257%, Training Loss: 0.6851%\n",
      "Epoch [27/100], Step [58/225], Training Accuracy: 31.7349%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [59/225], Training Accuracy: 32.1239%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [60/225], Training Accuracy: 32.2396%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [61/225], Training Accuracy: 32.1977%, Training Loss: 0.6847%\n",
      "Epoch [27/100], Step [62/225], Training Accuracy: 32.2329%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [63/225], Training Accuracy: 32.2917%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [64/225], Training Accuracy: 32.2998%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [65/225], Training Accuracy: 32.2356%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [66/225], Training Accuracy: 32.3153%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [67/225], Training Accuracy: 32.2994%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [68/225], Training Accuracy: 32.3759%, Training Loss: 0.6848%\n",
      "Epoch [27/100], Step [69/225], Training Accuracy: 32.3822%, Training Loss: 0.6846%\n",
      "Epoch [27/100], Step [70/225], Training Accuracy: 32.3438%, Training Loss: 0.6847%\n",
      "Epoch [27/100], Step [71/225], Training Accuracy: 32.3724%, Training Loss: 0.6847%\n",
      "Epoch [27/100], Step [72/225], Training Accuracy: 32.1832%, Training Loss: 0.6849%\n",
      "Epoch [27/100], Step [73/225], Training Accuracy: 32.0848%, Training Loss: 0.6849%\n",
      "Epoch [27/100], Step [74/225], Training Accuracy: 32.1579%, Training Loss: 0.6849%\n",
      "Epoch [27/100], Step [75/225], Training Accuracy: 32.1042%, Training Loss: 0.6849%\n",
      "Epoch [27/100], Step [76/225], Training Accuracy: 32.0518%, Training Loss: 0.6849%\n",
      "Epoch [27/100], Step [77/225], Training Accuracy: 31.9196%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [78/225], Training Accuracy: 31.9311%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [79/225], Training Accuracy: 31.8631%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [80/225], Training Accuracy: 31.8359%, Training Loss: 0.6850%\n",
      "Epoch [27/100], Step [81/225], Training Accuracy: 31.7130%, Training Loss: 0.6851%\n",
      "Epoch [27/100], Step [82/225], Training Accuracy: 31.7264%, Training Loss: 0.6851%\n",
      "Epoch [27/100], Step [83/225], Training Accuracy: 31.6453%, Training Loss: 0.6851%\n",
      "Epoch [27/100], Step [84/225], Training Accuracy: 31.7336%, Training Loss: 0.6851%\n",
      "Epoch [27/100], Step [85/225], Training Accuracy: 31.6728%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [86/225], Training Accuracy: 31.7406%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [87/225], Training Accuracy: 31.7349%, Training Loss: 0.6852%\n",
      "Epoch [27/100], Step [88/225], Training Accuracy: 31.7294%, Training Loss: 0.6853%\n",
      "Epoch [27/100], Step [89/225], Training Accuracy: 31.6713%, Training Loss: 0.6853%\n",
      "Epoch [27/100], Step [90/225], Training Accuracy: 31.5799%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [91/225], Training Accuracy: 31.6449%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [92/225], Training Accuracy: 31.5557%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [93/225], Training Accuracy: 31.5188%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [94/225], Training Accuracy: 31.6157%, Training Loss: 0.6853%\n",
      "Epoch [27/100], Step [95/225], Training Accuracy: 31.4967%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [97/225], Training Accuracy: 31.5883%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [98/225], Training Accuracy: 31.6167%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [99/225], Training Accuracy: 31.7235%, Training Loss: 0.6854%\n",
      "Epoch [27/100], Step [100/225], Training Accuracy: 31.7031%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [101/225], Training Accuracy: 31.8533%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Step [102/225], Training Accuracy: 31.7249%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [103/225], Training Accuracy: 31.7809%, Training Loss: 0.6855%\n",
      "Epoch [27/100], Step [104/225], Training Accuracy: 31.7608%, Training Loss: 0.6856%\n",
      "Epoch [27/100], Step [105/225], Training Accuracy: 31.6964%, Training Loss: 0.6857%\n",
      "Epoch [27/100], Step [106/225], Training Accuracy: 31.6775%, Training Loss: 0.6857%\n",
      "Epoch [27/100], Step [107/225], Training Accuracy: 31.5713%, Training Loss: 0.6857%\n",
      "Epoch [27/100], Step [108/225], Training Accuracy: 31.6406%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [109/225], Training Accuracy: 31.5224%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [110/225], Training Accuracy: 31.5341%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [111/225], Training Accuracy: 31.4048%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [112/225], Training Accuracy: 31.4732%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [113/225], Training Accuracy: 31.4574%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [114/225], Training Accuracy: 31.5378%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [115/225], Training Accuracy: 31.5082%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [116/225], Training Accuracy: 31.4925%, Training Loss: 0.6858%\n",
      "Epoch [27/100], Step [117/225], Training Accuracy: 31.4370%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [118/225], Training Accuracy: 31.3957%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [119/225], Training Accuracy: 31.3550%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [120/225], Training Accuracy: 31.3932%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [121/225], Training Accuracy: 31.3533%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [122/225], Training Accuracy: 31.3781%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [123/225], Training Accuracy: 31.4151%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [124/225], Training Accuracy: 31.4012%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [125/225], Training Accuracy: 31.3750%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [126/225], Training Accuracy: 31.3120%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [127/225], Training Accuracy: 31.2746%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [128/225], Training Accuracy: 31.2622%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [129/225], Training Accuracy: 31.2984%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [131/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [132/225], Training Accuracy: 31.2145%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [133/225], Training Accuracy: 31.2265%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [134/225], Training Accuracy: 31.2733%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [135/225], Training Accuracy: 31.2847%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [136/225], Training Accuracy: 31.2845%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [137/225], Training Accuracy: 31.3184%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [138/225], Training Accuracy: 31.3179%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [139/225], Training Accuracy: 31.2950%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [140/225], Training Accuracy: 31.2277%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [141/225], Training Accuracy: 31.1835%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [142/225], Training Accuracy: 31.2280%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [143/225], Training Accuracy: 31.2172%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [144/225], Training Accuracy: 31.2283%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [145/225], Training Accuracy: 31.2823%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [146/225], Training Accuracy: 31.3356%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [147/225], Training Accuracy: 31.3350%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [148/225], Training Accuracy: 31.3028%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [149/225], Training Accuracy: 31.3444%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [150/225], Training Accuracy: 31.3854%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [151/225], Training Accuracy: 31.4363%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [152/225], Training Accuracy: 31.4350%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [153/225], Training Accuracy: 31.4032%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [154/225], Training Accuracy: 31.4428%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [155/225], Training Accuracy: 31.4415%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [156/225], Training Accuracy: 31.4804%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [157/225], Training Accuracy: 31.3993%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [158/225], Training Accuracy: 31.3687%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [159/225], Training Accuracy: 31.4465%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [160/225], Training Accuracy: 31.4160%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [161/225], Training Accuracy: 31.4635%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [162/225], Training Accuracy: 31.4236%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [163/225], Training Accuracy: 31.4801%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [164/225], Training Accuracy: 31.4501%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [165/225], Training Accuracy: 31.3826%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [166/225], Training Accuracy: 31.4100%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [167/225], Training Accuracy: 31.4465%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [168/225], Training Accuracy: 31.4081%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [169/225], Training Accuracy: 31.3147%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [170/225], Training Accuracy: 31.2684%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [171/225], Training Accuracy: 31.2865%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [172/225], Training Accuracy: 31.2863%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [173/225], Training Accuracy: 31.2952%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [174/225], Training Accuracy: 31.3039%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [175/225], Training Accuracy: 31.3304%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [176/225], Training Accuracy: 31.3477%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [177/225], Training Accuracy: 31.3471%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [178/225], Training Accuracy: 31.3466%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [179/225], Training Accuracy: 31.3198%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [180/225], Training Accuracy: 31.3628%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [181/225], Training Accuracy: 31.3104%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [182/225], Training Accuracy: 31.3015%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [183/225], Training Accuracy: 31.3098%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [184/225], Training Accuracy: 31.2755%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [185/225], Training Accuracy: 31.2162%, Training Loss: 0.6862%\n",
      "Epoch [27/100], Step [186/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [187/225], Training Accuracy: 31.2667%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [188/225], Training Accuracy: 31.2583%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [189/225], Training Accuracy: 31.2996%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [190/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [191/225], Training Accuracy: 31.2255%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [192/225], Training Accuracy: 31.1523%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [193/225], Training Accuracy: 31.1286%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [195/225], Training Accuracy: 31.1058%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [196/225], Training Accuracy: 31.0666%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [197/225], Training Accuracy: 31.0993%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [198/225], Training Accuracy: 31.1316%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [199/225], Training Accuracy: 31.1008%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [200/225], Training Accuracy: 31.0938%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [201/225], Training Accuracy: 31.1178%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [202/225], Training Accuracy: 31.1185%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [203/225], Training Accuracy: 31.1115%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [204/225], Training Accuracy: 31.1811%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [205/225], Training Accuracy: 31.1814%, Training Loss: 0.6861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Step [206/225], Training Accuracy: 31.1666%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [207/225], Training Accuracy: 31.1594%, Training Loss: 0.6861%\n",
      "Epoch [27/100], Step [208/225], Training Accuracy: 31.1749%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [209/225], Training Accuracy: 31.2201%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [210/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [211/225], Training Accuracy: 31.2278%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [212/225], Training Accuracy: 31.2647%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [213/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [214/225], Training Accuracy: 31.2646%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [215/225], Training Accuracy: 31.2427%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [216/225], Training Accuracy: 31.1777%, Training Loss: 0.6860%\n",
      "Epoch [27/100], Step [217/225], Training Accuracy: 31.1780%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [218/225], Training Accuracy: 31.1425%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [219/225], Training Accuracy: 31.1929%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [220/225], Training Accuracy: 31.2074%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [221/225], Training Accuracy: 31.1864%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [222/225], Training Accuracy: 31.2078%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [223/225], Training Accuracy: 31.2430%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [224/225], Training Accuracy: 31.2570%, Training Loss: 0.6859%\n",
      "Epoch [27/100], Step [225/225], Training Accuracy: 31.2257%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6930%\n",
      "Epoch [28/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6904%\n",
      "Epoch [28/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6919%\n",
      "Epoch [28/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6881%\n",
      "Epoch [28/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6878%\n",
      "Epoch [28/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6878%\n",
      "Epoch [28/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6869%\n",
      "Epoch [28/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6862%\n",
      "Epoch [28/100], Step [9/225], Training Accuracy: 30.9028%, Training Loss: 0.6875%\n",
      "Epoch [28/100], Step [10/225], Training Accuracy: 30.1562%, Training Loss: 0.6870%\n",
      "Epoch [28/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6869%\n",
      "Epoch [28/100], Step [12/225], Training Accuracy: 29.1667%, Training Loss: 0.6870%\n",
      "Epoch [28/100], Step [13/225], Training Accuracy: 29.3269%, Training Loss: 0.6869%\n",
      "Epoch [28/100], Step [14/225], Training Accuracy: 29.5759%, Training Loss: 0.6873%\n",
      "Epoch [28/100], Step [15/225], Training Accuracy: 30.1042%, Training Loss: 0.6872%\n",
      "Epoch [28/100], Step [16/225], Training Accuracy: 30.2734%, Training Loss: 0.6868%\n",
      "Epoch [28/100], Step [17/225], Training Accuracy: 30.0551%, Training Loss: 0.6870%\n",
      "Epoch [28/100], Step [18/225], Training Accuracy: 30.0347%, Training Loss: 0.6872%\n",
      "Epoch [28/100], Step [19/225], Training Accuracy: 30.5099%, Training Loss: 0.6871%\n",
      "Epoch [28/100], Step [20/225], Training Accuracy: 30.9375%, Training Loss: 0.6869%\n",
      "Epoch [28/100], Step [21/225], Training Accuracy: 30.9524%, Training Loss: 0.6867%\n",
      "Epoch [28/100], Step [22/225], Training Accuracy: 31.2500%, Training Loss: 0.6865%\n",
      "Epoch [28/100], Step [23/225], Training Accuracy: 31.0462%, Training Loss: 0.6865%\n",
      "Epoch [28/100], Step [24/225], Training Accuracy: 30.9896%, Training Loss: 0.6865%\n",
      "Epoch [28/100], Step [25/225], Training Accuracy: 31.3750%, Training Loss: 0.6864%\n",
      "Epoch [28/100], Step [26/225], Training Accuracy: 31.7308%, Training Loss: 0.6862%\n",
      "Epoch [28/100], Step [27/225], Training Accuracy: 31.4236%, Training Loss: 0.6861%\n",
      "Epoch [28/100], Step [28/225], Training Accuracy: 31.3616%, Training Loss: 0.6861%\n",
      "Epoch [28/100], Step [29/225], Training Accuracy: 31.6272%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [30/225], Training Accuracy: 31.7188%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [31/225], Training Accuracy: 31.6028%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [32/225], Training Accuracy: 31.7383%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [33/225], Training Accuracy: 31.8182%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [34/225], Training Accuracy: 31.8015%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [35/225], Training Accuracy: 31.7411%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [36/225], Training Accuracy: 31.5972%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [37/225], Training Accuracy: 31.6301%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [38/225], Training Accuracy: 31.4145%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [39/225], Training Accuracy: 31.1298%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [40/225], Training Accuracy: 31.2109%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [41/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [42/225], Training Accuracy: 31.0268%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [43/225], Training Accuracy: 31.1410%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [45/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [46/225], Training Accuracy: 31.1141%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [47/225], Training Accuracy: 31.0505%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [48/225], Training Accuracy: 31.2826%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [49/225], Training Accuracy: 31.1862%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [50/225], Training Accuracy: 31.0938%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [51/225], Training Accuracy: 31.2194%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [52/225], Training Accuracy: 31.2200%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [53/225], Training Accuracy: 31.1910%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [54/225], Training Accuracy: 31.0475%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [55/225], Training Accuracy: 31.0227%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [56/225], Training Accuracy: 30.9431%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [57/225], Training Accuracy: 31.0581%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [58/225], Training Accuracy: 30.9267%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [59/225], Training Accuracy: 31.2765%, Training Loss: 0.6849%\n",
      "Epoch [28/100], Step [60/225], Training Accuracy: 31.4323%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [61/225], Training Accuracy: 31.4037%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [62/225], Training Accuracy: 31.3508%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [63/225], Training Accuracy: 31.3740%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [64/225], Training Accuracy: 31.4209%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [65/225], Training Accuracy: 31.3221%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [66/225], Training Accuracy: 31.4394%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [67/225], Training Accuracy: 31.4599%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [68/225], Training Accuracy: 31.5487%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [69/225], Training Accuracy: 31.5217%, Training Loss: 0.6848%\n",
      "Epoch [28/100], Step [70/225], Training Accuracy: 31.4955%, Training Loss: 0.6849%\n",
      "Epoch [28/100], Step [71/225], Training Accuracy: 31.5141%, Training Loss: 0.6849%\n",
      "Epoch [28/100], Step [72/225], Training Accuracy: 31.3585%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [73/225], Training Accuracy: 31.2928%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [74/225], Training Accuracy: 31.3978%, Training Loss: 0.6849%\n",
      "Epoch [28/100], Step [75/225], Training Accuracy: 31.3750%, Training Loss: 0.6849%\n",
      "Epoch [28/100], Step [76/225], Training Accuracy: 31.2911%, Training Loss: 0.6849%\n",
      "Epoch [28/100], Step [77/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [78/225], Training Accuracy: 31.2901%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [79/225], Training Accuracy: 31.2104%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [80/225], Training Accuracy: 31.1328%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [81/225], Training Accuracy: 31.0185%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [82/225], Training Accuracy: 31.0404%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [83/225], Training Accuracy: 30.9676%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [84/225], Training Accuracy: 31.0454%, Training Loss: 0.6850%\n",
      "Epoch [28/100], Step [85/225], Training Accuracy: 31.0294%, Training Loss: 0.6850%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Step [86/225], Training Accuracy: 31.0865%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [87/225], Training Accuracy: 31.0884%, Training Loss: 0.6851%\n",
      "Epoch [28/100], Step [88/225], Training Accuracy: 31.0369%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [89/225], Training Accuracy: 30.9691%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [90/225], Training Accuracy: 30.8854%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [91/225], Training Accuracy: 30.8894%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [92/225], Training Accuracy: 30.8594%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [93/225], Training Accuracy: 30.8468%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [94/225], Training Accuracy: 30.9009%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [95/225], Training Accuracy: 30.7730%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [96/225], Training Accuracy: 30.8919%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [97/225], Training Accuracy: 30.9117%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [98/225], Training Accuracy: 30.8992%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [99/225], Training Accuracy: 31.0290%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [100/225], Training Accuracy: 31.0000%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [101/225], Training Accuracy: 31.1108%, Training Loss: 0.6852%\n",
      "Epoch [28/100], Step [102/225], Training Accuracy: 31.0202%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [103/225], Training Accuracy: 31.0831%, Training Loss: 0.6853%\n",
      "Epoch [28/100], Step [104/225], Training Accuracy: 31.0697%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [105/225], Training Accuracy: 30.9970%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [106/225], Training Accuracy: 30.9994%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [107/225], Training Accuracy: 30.8995%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [108/225], Training Accuracy: 30.9896%, Training Loss: 0.6854%\n",
      "Epoch [28/100], Step [109/225], Training Accuracy: 30.8343%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [110/225], Training Accuracy: 30.8523%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [111/225], Training Accuracy: 30.7855%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [112/225], Training Accuracy: 30.8733%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [113/225], Training Accuracy: 30.8767%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [114/225], Training Accuracy: 30.9485%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [115/225], Training Accuracy: 30.9375%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [116/225], Training Accuracy: 30.8998%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [117/225], Training Accuracy: 30.8360%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [118/225], Training Accuracy: 30.7865%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [119/225], Training Accuracy: 30.7379%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [120/225], Training Accuracy: 30.7943%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [121/225], Training Accuracy: 30.7980%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [122/225], Training Accuracy: 30.8017%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [123/225], Training Accuracy: 30.8308%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [124/225], Training Accuracy: 30.8216%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [125/225], Training Accuracy: 30.8125%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [126/225], Training Accuracy: 30.7664%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [127/225], Training Accuracy: 30.7702%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [128/225], Training Accuracy: 30.7617%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [129/225], Training Accuracy: 30.8018%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [130/225], Training Accuracy: 30.7572%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [131/225], Training Accuracy: 30.7133%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [132/225], Training Accuracy: 30.6818%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [133/225], Training Accuracy: 30.7096%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [134/225], Training Accuracy: 30.7603%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [135/225], Training Accuracy: 30.7755%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [136/225], Training Accuracy: 30.8019%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [137/225], Training Accuracy: 30.8280%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [138/225], Training Accuracy: 30.8311%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [139/225], Training Accuracy: 30.8004%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [140/225], Training Accuracy: 30.7812%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [141/225], Training Accuracy: 30.7402%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [142/225], Training Accuracy: 30.7989%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [143/225], Training Accuracy: 30.8129%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [144/225], Training Accuracy: 30.8377%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [145/225], Training Accuracy: 30.8944%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [146/225], Training Accuracy: 30.8968%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [147/225], Training Accuracy: 30.9205%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [148/225], Training Accuracy: 30.8910%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [149/225], Training Accuracy: 30.9249%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [150/225], Training Accuracy: 30.9479%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [151/225], Training Accuracy: 31.0017%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [152/225], Training Accuracy: 30.9930%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [153/225], Training Accuracy: 30.9334%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [154/225], Training Accuracy: 30.9761%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [155/225], Training Accuracy: 30.9677%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [156/225], Training Accuracy: 30.9996%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [157/225], Training Accuracy: 30.9315%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [158/225], Training Accuracy: 30.9237%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [159/225], Training Accuracy: 31.0043%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [160/225], Training Accuracy: 30.9766%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [161/225], Training Accuracy: 31.0268%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [162/225], Training Accuracy: 30.9992%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [163/225], Training Accuracy: 31.0487%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [164/225], Training Accuracy: 31.0404%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [165/225], Training Accuracy: 30.9943%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [166/225], Training Accuracy: 30.9864%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [167/225], Training Accuracy: 31.0348%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [168/225], Training Accuracy: 30.9896%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [169/225], Training Accuracy: 30.9172%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [170/225], Training Accuracy: 30.8640%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [171/225], Training Accuracy: 30.8662%, Training Loss: 0.6859%\n",
      "Epoch [28/100], Step [172/225], Training Accuracy: 30.8685%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [173/225], Training Accuracy: 30.8616%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [174/225], Training Accuracy: 30.8369%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [175/225], Training Accuracy: 30.8571%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [176/225], Training Accuracy: 30.8860%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [177/225], Training Accuracy: 30.8881%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [178/225], Training Accuracy: 30.8813%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [179/225], Training Accuracy: 30.8572%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [180/225], Training Accuracy: 30.8854%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [181/225], Training Accuracy: 30.8443%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [182/225], Training Accuracy: 30.8379%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [183/225], Training Accuracy: 30.8402%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [184/225], Training Accuracy: 30.8169%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [185/225], Training Accuracy: 30.7770%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [186/225], Training Accuracy: 30.8132%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [187/225], Training Accuracy: 30.7988%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [188/225], Training Accuracy: 30.8012%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [189/225], Training Accuracy: 30.8284%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [190/225], Training Accuracy: 30.7895%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [191/225], Training Accuracy: 30.7673%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [192/225], Training Accuracy: 30.6966%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Step [193/225], Training Accuracy: 30.7157%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [194/225], Training Accuracy: 30.7265%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [195/225], Training Accuracy: 30.7051%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [196/225], Training Accuracy: 30.6601%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [197/225], Training Accuracy: 30.6710%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [198/225], Training Accuracy: 30.6897%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [199/225], Training Accuracy: 30.6690%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [200/225], Training Accuracy: 30.6641%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [201/225], Training Accuracy: 30.6903%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [202/225], Training Accuracy: 30.6931%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [203/225], Training Accuracy: 30.6881%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [204/225], Training Accuracy: 30.7292%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [205/225], Training Accuracy: 30.7241%, Training Loss: 0.6858%\n",
      "Epoch [28/100], Step [206/225], Training Accuracy: 30.7191%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [207/225], Training Accuracy: 30.6990%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [208/225], Training Accuracy: 30.7242%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [209/225], Training Accuracy: 30.7641%, Training Loss: 0.6857%\n",
      "Epoch [28/100], Step [210/225], Training Accuracy: 30.7887%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [211/225], Training Accuracy: 30.7687%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [212/225], Training Accuracy: 30.8152%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [213/225], Training Accuracy: 30.8025%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [214/225], Training Accuracy: 30.8265%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [215/225], Training Accuracy: 30.8067%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [216/225], Training Accuracy: 30.7581%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [217/225], Training Accuracy: 30.7604%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [218/225], Training Accuracy: 30.7268%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [219/225], Training Accuracy: 30.7862%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [220/225], Training Accuracy: 30.7955%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [221/225], Training Accuracy: 30.7904%, Training Loss: 0.6856%\n",
      "Epoch [28/100], Step [222/225], Training Accuracy: 30.8066%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [223/225], Training Accuracy: 30.8296%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [224/225], Training Accuracy: 30.8384%, Training Loss: 0.6855%\n",
      "Epoch [28/100], Step [225/225], Training Accuracy: 30.8227%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6894%\n",
      "Epoch [29/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6864%\n",
      "Epoch [29/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6896%\n",
      "Epoch [29/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6862%\n",
      "Epoch [29/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6874%\n",
      "Epoch [29/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6870%\n",
      "Epoch [29/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [13/225], Training Accuracy: 29.5673%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6863%\n",
      "Epoch [29/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6864%\n",
      "Epoch [29/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6864%\n",
      "Epoch [29/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6866%\n",
      "Epoch [29/100], Step [18/225], Training Accuracy: 30.4688%, Training Loss: 0.6868%\n",
      "Epoch [29/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6869%\n",
      "Epoch [29/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6866%\n",
      "Epoch [29/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6863%\n",
      "Epoch [29/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6861%\n",
      "Epoch [29/100], Step [23/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [24/225], Training Accuracy: 31.1849%, Training Loss: 0.6862%\n",
      "Epoch [29/100], Step [25/225], Training Accuracy: 31.4375%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [26/225], Training Accuracy: 31.7308%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [27/225], Training Accuracy: 31.3657%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [28/225], Training Accuracy: 31.3058%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [29/225], Training Accuracy: 31.6810%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [30/225], Training Accuracy: 31.6667%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [32/225], Training Accuracy: 31.7871%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [33/225], Training Accuracy: 31.9129%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [34/225], Training Accuracy: 31.7096%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [35/225], Training Accuracy: 31.5179%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [36/225], Training Accuracy: 31.4670%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [37/225], Training Accuracy: 31.5878%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [38/225], Training Accuracy: 31.3322%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [39/225], Training Accuracy: 31.0897%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [40/225], Training Accuracy: 31.0156%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [41/225], Training Accuracy: 31.0213%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [42/225], Training Accuracy: 30.7664%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [43/225], Training Accuracy: 30.9956%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [44/225], Training Accuracy: 31.0369%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [45/225], Training Accuracy: 31.0764%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [46/225], Training Accuracy: 30.9103%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [47/225], Training Accuracy: 30.8511%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [48/225], Training Accuracy: 30.9570%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [49/225], Training Accuracy: 31.0268%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [50/225], Training Accuracy: 30.9688%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [51/225], Training Accuracy: 31.0968%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [52/225], Training Accuracy: 31.1298%, Training Loss: 0.6850%\n",
      "Epoch [29/100], Step [53/225], Training Accuracy: 31.1026%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [54/225], Training Accuracy: 30.9896%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [55/225], Training Accuracy: 31.1364%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [56/225], Training Accuracy: 31.3058%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [57/225], Training Accuracy: 31.3871%, Training Loss: 0.6850%\n",
      "Epoch [29/100], Step [58/225], Training Accuracy: 31.3308%, Training Loss: 0.6850%\n",
      "Epoch [29/100], Step [59/225], Training Accuracy: 31.6737%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [60/225], Training Accuracy: 31.7969%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [61/225], Training Accuracy: 31.7623%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [62/225], Training Accuracy: 31.7288%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [63/225], Training Accuracy: 31.8204%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [64/225], Training Accuracy: 31.8604%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [65/225], Training Accuracy: 31.7788%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [66/225], Training Accuracy: 31.8419%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [67/225], Training Accuracy: 31.8563%, Training Loss: 0.6848%\n",
      "Epoch [29/100], Step [68/225], Training Accuracy: 31.8934%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [69/225], Training Accuracy: 31.8614%, Training Loss: 0.6846%\n",
      "Epoch [29/100], Step [70/225], Training Accuracy: 31.8080%, Training Loss: 0.6846%\n",
      "Epoch [29/100], Step [71/225], Training Accuracy: 31.8882%, Training Loss: 0.6846%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Step [72/225], Training Accuracy: 31.7057%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [73/225], Training Accuracy: 31.6139%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [74/225], Training Accuracy: 31.7145%, Training Loss: 0.6846%\n",
      "Epoch [29/100], Step [75/225], Training Accuracy: 31.7292%, Training Loss: 0.6846%\n",
      "Epoch [29/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6846%\n",
      "Epoch [29/100], Step [77/225], Training Accuracy: 31.5747%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [78/225], Training Accuracy: 31.6306%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [79/225], Training Accuracy: 31.5665%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [80/225], Training Accuracy: 31.5234%, Training Loss: 0.6847%\n",
      "Epoch [29/100], Step [81/225], Training Accuracy: 31.4429%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [82/225], Training Accuracy: 31.4596%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [83/225], Training Accuracy: 31.3818%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [84/225], Training Accuracy: 31.3988%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [85/225], Training Accuracy: 31.3787%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [86/225], Training Accuracy: 31.4499%, Training Loss: 0.6849%\n",
      "Epoch [29/100], Step [87/225], Training Accuracy: 31.4116%, Training Loss: 0.6850%\n",
      "Epoch [29/100], Step [88/225], Training Accuracy: 31.3920%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [89/225], Training Accuracy: 31.3202%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [90/225], Training Accuracy: 31.2326%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [91/225], Training Accuracy: 31.2843%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [92/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [93/225], Training Accuracy: 31.2332%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [94/225], Training Accuracy: 31.3165%, Training Loss: 0.6850%\n",
      "Epoch [29/100], Step [95/225], Training Accuracy: 31.2007%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [96/225], Training Accuracy: 31.3314%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [97/225], Training Accuracy: 31.3466%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [98/225], Training Accuracy: 31.3457%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [99/225], Training Accuracy: 31.4552%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [100/225], Training Accuracy: 31.4219%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [101/225], Training Accuracy: 31.5439%, Training Loss: 0.6851%\n",
      "Epoch [29/100], Step [102/225], Training Accuracy: 31.4338%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [103/225], Training Accuracy: 31.4775%, Training Loss: 0.6852%\n",
      "Epoch [29/100], Step [104/225], Training Accuracy: 31.4453%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [105/225], Training Accuracy: 31.3988%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [106/225], Training Accuracy: 31.3974%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [107/225], Training Accuracy: 31.3084%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [108/225], Training Accuracy: 31.3802%, Training Loss: 0.6853%\n",
      "Epoch [29/100], Step [109/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [110/225], Training Accuracy: 31.2216%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [111/225], Training Accuracy: 31.0952%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [112/225], Training Accuracy: 31.1802%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [113/225], Training Accuracy: 31.1809%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [114/225], Training Accuracy: 31.2226%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [115/225], Training Accuracy: 31.1957%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [116/225], Training Accuracy: 31.1961%, Training Loss: 0.6854%\n",
      "Epoch [29/100], Step [117/225], Training Accuracy: 31.1298%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [118/225], Training Accuracy: 31.0911%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [119/225], Training Accuracy: 31.0399%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [120/225], Training Accuracy: 31.0547%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [121/225], Training Accuracy: 31.0563%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [122/225], Training Accuracy: 31.0451%, Training Loss: 0.6855%\n",
      "Epoch [29/100], Step [123/225], Training Accuracy: 31.0849%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [124/225], Training Accuracy: 31.0736%, Training Loss: 0.6856%\n",
      "Epoch [29/100], Step [125/225], Training Accuracy: 31.0500%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [126/225], Training Accuracy: 30.9896%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [127/225], Training Accuracy: 30.9424%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [128/225], Training Accuracy: 30.9448%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [129/225], Training Accuracy: 30.9593%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [130/225], Training Accuracy: 30.9014%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [131/225], Training Accuracy: 30.8564%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [132/225], Training Accuracy: 30.8357%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [133/225], Training Accuracy: 30.8741%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [134/225], Training Accuracy: 30.9352%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [135/225], Training Accuracy: 30.9491%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [136/225], Training Accuracy: 30.9628%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [137/225], Training Accuracy: 30.9991%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [138/225], Training Accuracy: 31.0236%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [139/225], Training Accuracy: 31.0027%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [140/225], Training Accuracy: 31.0045%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [141/225], Training Accuracy: 30.9286%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [142/225], Training Accuracy: 30.9859%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [143/225], Training Accuracy: 30.9878%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [144/225], Training Accuracy: 30.9896%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [145/225], Training Accuracy: 31.0453%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [146/225], Training Accuracy: 31.0788%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [147/225], Training Accuracy: 31.0906%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [148/225], Training Accuracy: 31.0811%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [149/225], Training Accuracy: 31.1032%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [150/225], Training Accuracy: 31.1458%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [151/225], Training Accuracy: 31.1983%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [152/225], Training Accuracy: 31.1883%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [153/225], Training Accuracy: 31.1479%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [154/225], Training Accuracy: 31.1891%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [155/225], Training Accuracy: 31.1794%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [156/225], Training Accuracy: 31.2099%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [157/225], Training Accuracy: 31.1604%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [158/225], Training Accuracy: 31.1313%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [159/225], Training Accuracy: 31.2303%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [160/225], Training Accuracy: 31.2012%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [161/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [162/225], Training Accuracy: 31.2307%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [163/225], Training Accuracy: 31.3171%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [164/225], Training Accuracy: 31.2976%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [165/225], Training Accuracy: 31.2311%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [166/225], Training Accuracy: 31.2029%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [167/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [168/225], Training Accuracy: 31.2128%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [169/225], Training Accuracy: 31.1483%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [170/225], Training Accuracy: 31.1029%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [171/225], Training Accuracy: 31.1312%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [172/225], Training Accuracy: 31.1410%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [173/225], Training Accuracy: 31.1507%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [174/225], Training Accuracy: 31.1692%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [175/225], Training Accuracy: 31.1964%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [176/225], Training Accuracy: 31.2234%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [177/225], Training Accuracy: 31.2235%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [178/225], Training Accuracy: 31.2324%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Step [179/225], Training Accuracy: 31.1976%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [180/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [181/225], Training Accuracy: 31.1982%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [182/225], Training Accuracy: 31.1813%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [183/225], Training Accuracy: 31.1902%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [185/225], Training Accuracy: 31.1064%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [186/225], Training Accuracy: 31.1324%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [187/225], Training Accuracy: 31.1414%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [188/225], Training Accuracy: 31.1336%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [189/225], Training Accuracy: 31.1591%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [190/225], Training Accuracy: 31.1102%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [191/225], Training Accuracy: 31.0946%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [192/225], Training Accuracy: 31.0384%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [193/225], Training Accuracy: 31.0395%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [194/225], Training Accuracy: 31.0567%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [195/225], Training Accuracy: 31.0256%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [196/225], Training Accuracy: 30.9949%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [197/225], Training Accuracy: 31.0200%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [198/225], Training Accuracy: 31.0448%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [199/225], Training Accuracy: 31.0302%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [200/225], Training Accuracy: 31.0156%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [201/225], Training Accuracy: 31.0401%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [202/225], Training Accuracy: 31.0334%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [203/225], Training Accuracy: 31.0345%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [204/225], Training Accuracy: 31.1045%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [205/225], Training Accuracy: 31.1128%, Training Loss: 0.6860%\n",
      "Epoch [29/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [207/225], Training Accuracy: 31.0613%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [208/225], Training Accuracy: 31.0847%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [209/225], Training Accuracy: 31.1154%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [210/225], Training Accuracy: 31.1161%, Training Loss: 0.6859%\n",
      "Epoch [29/100], Step [211/225], Training Accuracy: 31.1093%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [212/225], Training Accuracy: 31.1689%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [213/225], Training Accuracy: 31.1473%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [214/225], Training Accuracy: 31.1697%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [215/225], Training Accuracy: 31.1483%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [216/225], Training Accuracy: 31.0836%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [217/225], Training Accuracy: 31.0844%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [218/225], Training Accuracy: 31.0565%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [219/225], Training Accuracy: 31.1144%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [220/225], Training Accuracy: 31.1435%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [221/225], Training Accuracy: 31.1369%, Training Loss: 0.6858%\n",
      "Epoch [29/100], Step [222/225], Training Accuracy: 31.1515%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [223/225], Training Accuracy: 31.1869%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [224/225], Training Accuracy: 31.1942%, Training Loss: 0.6857%\n",
      "Epoch [29/100], Step [225/225], Training Accuracy: 31.1701%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6899%\n",
      "Epoch [30/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6874%\n",
      "Epoch [30/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6903%\n",
      "Epoch [30/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6871%\n",
      "Epoch [30/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6869%\n",
      "Epoch [30/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6875%\n",
      "Epoch [30/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6866%\n",
      "Epoch [30/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6869%\n",
      "Epoch [30/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6866%\n",
      "Epoch [30/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6865%\n",
      "Epoch [30/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [14/225], Training Accuracy: 30.4688%, Training Loss: 0.6870%\n",
      "Epoch [30/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6867%\n",
      "Epoch [30/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6867%\n",
      "Epoch [30/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6869%\n",
      "Epoch [30/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6869%\n",
      "Epoch [30/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6870%\n",
      "Epoch [30/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6865%\n",
      "Epoch [30/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [24/225], Training Accuracy: 31.3802%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [25/225], Training Accuracy: 31.6250%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [27/225], Training Accuracy: 31.6551%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [29/225], Training Accuracy: 31.8966%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6852%\n",
      "Epoch [30/100], Step [33/225], Training Accuracy: 32.1496%, Training Loss: 0.6851%\n",
      "Epoch [30/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6850%\n",
      "Epoch [30/100], Step [35/225], Training Accuracy: 31.8750%, Training Loss: 0.6851%\n",
      "Epoch [30/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6852%\n",
      "Epoch [30/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [38/225], Training Accuracy: 31.4967%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [39/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [40/225], Training Accuracy: 31.3281%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [41/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [42/225], Training Accuracy: 31.1012%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [43/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [45/225], Training Accuracy: 31.1806%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [46/225], Training Accuracy: 31.0122%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [47/225], Training Accuracy: 30.9508%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [48/225], Training Accuracy: 31.1523%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [49/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [50/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [51/225], Training Accuracy: 31.4032%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [52/225], Training Accuracy: 31.4303%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [53/225], Training Accuracy: 31.3679%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [54/225], Training Accuracy: 31.2789%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [55/225], Training Accuracy: 31.3636%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [56/225], Training Accuracy: 31.4174%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [57/225], Training Accuracy: 31.4693%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [58/225], Training Accuracy: 31.3847%, Training Loss: 0.6852%\n",
      "Epoch [30/100], Step [59/225], Training Accuracy: 31.7267%, Training Loss: 0.6850%\n",
      "Epoch [30/100], Step [60/225], Training Accuracy: 31.7969%, Training Loss: 0.6850%\n",
      "Epoch [30/100], Step [61/225], Training Accuracy: 31.7111%, Training Loss: 0.6850%\n",
      "Epoch [30/100], Step [62/225], Training Accuracy: 31.6532%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Step [63/225], Training Accuracy: 31.6964%, Training Loss: 0.6851%\n",
      "Epoch [30/100], Step [64/225], Training Accuracy: 31.7139%, Training Loss: 0.6851%\n",
      "Epoch [30/100], Step [65/225], Training Accuracy: 31.6106%, Training Loss: 0.6852%\n",
      "Epoch [30/100], Step [66/225], Training Accuracy: 31.6998%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [67/225], Training Accuracy: 31.6465%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [68/225], Training Accuracy: 31.7325%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [69/225], Training Accuracy: 31.7255%, Training Loss: 0.6851%\n",
      "Epoch [30/100], Step [70/225], Training Accuracy: 31.6741%, Training Loss: 0.6852%\n",
      "Epoch [30/100], Step [71/225], Training Accuracy: 31.7121%, Training Loss: 0.6852%\n",
      "Epoch [30/100], Step [72/225], Training Accuracy: 31.5104%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [73/225], Training Accuracy: 31.4212%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [74/225], Training Accuracy: 31.5245%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [75/225], Training Accuracy: 31.4792%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [76/225], Training Accuracy: 31.3939%, Training Loss: 0.6853%\n",
      "Epoch [30/100], Step [77/225], Training Accuracy: 31.3109%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [78/225], Training Accuracy: 31.3502%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [79/225], Training Accuracy: 31.2896%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [80/225], Training Accuracy: 31.2695%, Training Loss: 0.6854%\n",
      "Epoch [30/100], Step [81/225], Training Accuracy: 31.2114%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [82/225], Training Accuracy: 31.2309%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [83/225], Training Accuracy: 31.1747%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [84/225], Training Accuracy: 31.2686%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [85/225], Training Accuracy: 31.2132%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [86/225], Training Accuracy: 31.2682%, Training Loss: 0.6855%\n",
      "Epoch [30/100], Step [87/225], Training Accuracy: 31.3039%, Training Loss: 0.6856%\n",
      "Epoch [30/100], Step [88/225], Training Accuracy: 31.3033%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [89/225], Training Accuracy: 31.2676%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [90/225], Training Accuracy: 31.1806%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [92/225], Training Accuracy: 31.1821%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [93/225], Training Accuracy: 31.1828%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [94/225], Training Accuracy: 31.2666%, Training Loss: 0.6857%\n",
      "Epoch [30/100], Step [95/225], Training Accuracy: 31.1513%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [96/225], Training Accuracy: 31.2337%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [97/225], Training Accuracy: 31.2822%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [98/225], Training Accuracy: 31.2978%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [99/225], Training Accuracy: 31.4078%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [100/225], Training Accuracy: 31.3750%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [101/225], Training Accuracy: 31.4821%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [102/225], Training Accuracy: 31.3879%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [103/225], Training Accuracy: 31.4624%, Training Loss: 0.6858%\n",
      "Epoch [30/100], Step [104/225], Training Accuracy: 31.4153%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [105/225], Training Accuracy: 31.3690%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [106/225], Training Accuracy: 31.3532%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [107/225], Training Accuracy: 31.2646%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [108/225], Training Accuracy: 31.3513%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [109/225], Training Accuracy: 31.2357%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [110/225], Training Accuracy: 31.2358%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [111/225], Training Accuracy: 31.1092%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [112/225], Training Accuracy: 31.1802%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [113/225], Training Accuracy: 31.1670%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [114/225], Training Accuracy: 31.2089%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [115/225], Training Accuracy: 31.1957%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [116/225], Training Accuracy: 31.1557%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [117/225], Training Accuracy: 31.1031%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [118/225], Training Accuracy: 31.0779%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [119/225], Training Accuracy: 31.0137%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [120/225], Training Accuracy: 31.0286%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [121/225], Training Accuracy: 31.0692%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [122/225], Training Accuracy: 31.0707%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [123/225], Training Accuracy: 31.0976%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [124/225], Training Accuracy: 31.0610%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [125/225], Training Accuracy: 31.0500%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [126/225], Training Accuracy: 31.0020%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [127/225], Training Accuracy: 30.9670%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [128/225], Training Accuracy: 30.9692%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [129/225], Training Accuracy: 31.0078%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [130/225], Training Accuracy: 30.9255%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [131/225], Training Accuracy: 30.8922%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [132/225], Training Accuracy: 30.8594%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [133/225], Training Accuracy: 30.8858%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [134/225], Training Accuracy: 30.9468%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [135/225], Training Accuracy: 30.9491%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [136/225], Training Accuracy: 30.9743%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [137/225], Training Accuracy: 30.9991%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [138/225], Training Accuracy: 31.0122%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [139/225], Training Accuracy: 30.9802%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [140/225], Training Accuracy: 30.9598%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [141/225], Training Accuracy: 30.9286%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [142/225], Training Accuracy: 30.9859%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [143/225], Training Accuracy: 30.9878%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [144/225], Training Accuracy: 31.0004%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [145/225], Training Accuracy: 31.0453%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [146/225], Training Accuracy: 31.0788%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [147/225], Training Accuracy: 31.1012%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [148/225], Training Accuracy: 31.0600%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [149/225], Training Accuracy: 31.1032%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [150/225], Training Accuracy: 31.1354%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [151/225], Training Accuracy: 31.1776%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [152/225], Training Accuracy: 31.1678%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [153/225], Training Accuracy: 31.1275%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [154/225], Training Accuracy: 31.1790%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [155/225], Training Accuracy: 31.1895%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [156/225], Training Accuracy: 31.2200%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [157/225], Training Accuracy: 31.1505%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [158/225], Training Accuracy: 31.1214%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [159/225], Training Accuracy: 31.1616%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [160/225], Training Accuracy: 31.1133%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [161/225], Training Accuracy: 31.1821%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [162/225], Training Accuracy: 31.1535%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [163/225], Training Accuracy: 31.2021%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [164/225], Training Accuracy: 31.1643%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [165/225], Training Accuracy: 31.0985%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [166/225], Training Accuracy: 31.0900%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [167/225], Training Accuracy: 31.1377%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [168/225], Training Accuracy: 31.1012%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [169/225], Training Accuracy: 31.0189%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [170/225], Training Accuracy: 30.9651%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [171/225], Training Accuracy: 30.9942%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [172/225], Training Accuracy: 31.0138%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [173/225], Training Accuracy: 31.0152%, Training Loss: 0.6864%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Step [174/225], Training Accuracy: 31.0165%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [175/225], Training Accuracy: 31.0446%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [176/225], Training Accuracy: 31.0547%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [177/225], Training Accuracy: 31.0558%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [178/225], Training Accuracy: 31.0569%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [179/225], Training Accuracy: 31.0230%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [180/225], Training Accuracy: 31.0764%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [181/225], Training Accuracy: 31.0256%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [182/225], Training Accuracy: 31.0010%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [183/225], Training Accuracy: 31.0109%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [184/225], Training Accuracy: 30.9952%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [185/225], Training Accuracy: 30.9544%, Training Loss: 0.6864%\n",
      "Epoch [30/100], Step [186/225], Training Accuracy: 30.9812%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [187/225], Training Accuracy: 30.9743%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [188/225], Training Accuracy: 30.9674%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [189/225], Training Accuracy: 31.0020%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [190/225], Training Accuracy: 30.9622%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [191/225], Training Accuracy: 30.9391%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [192/225], Training Accuracy: 30.8757%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [193/225], Training Accuracy: 30.8857%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [194/225], Training Accuracy: 30.8956%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [195/225], Training Accuracy: 30.8894%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [196/225], Training Accuracy: 30.8514%, Training Loss: 0.6863%\n",
      "Epoch [30/100], Step [197/225], Training Accuracy: 30.8614%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [198/225], Training Accuracy: 30.8870%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [199/225], Training Accuracy: 30.8653%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [200/225], Training Accuracy: 30.8438%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [201/225], Training Accuracy: 30.8613%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [202/225], Training Accuracy: 30.8555%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [203/225], Training Accuracy: 30.8498%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [204/225], Training Accuracy: 30.9130%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [205/225], Training Accuracy: 30.9070%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [206/225], Training Accuracy: 30.8783%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [207/225], Training Accuracy: 30.8650%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [208/225], Training Accuracy: 30.8819%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [209/225], Training Accuracy: 30.9285%, Training Loss: 0.6862%\n",
      "Epoch [30/100], Step [210/225], Training Accuracy: 30.9449%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [211/225], Training Accuracy: 30.9316%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [212/225], Training Accuracy: 30.9773%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [213/225], Training Accuracy: 30.9566%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [214/225], Training Accuracy: 30.9871%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [215/225], Training Accuracy: 30.9520%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [216/225], Training Accuracy: 30.9100%, Training Loss: 0.6861%\n",
      "Epoch [30/100], Step [217/225], Training Accuracy: 30.9188%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [218/225], Training Accuracy: 30.8845%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [219/225], Training Accuracy: 30.9503%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [220/225], Training Accuracy: 30.9659%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [221/225], Training Accuracy: 30.9531%, Training Loss: 0.6860%\n",
      "Epoch [30/100], Step [222/225], Training Accuracy: 30.9755%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [223/225], Training Accuracy: 31.0118%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [224/225], Training Accuracy: 31.0059%, Training Loss: 0.6859%\n",
      "Epoch [30/100], Step [225/225], Training Accuracy: 30.9894%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6925%\n",
      "Epoch [31/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6867%\n",
      "Epoch [31/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6886%\n",
      "Epoch [31/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6862%\n",
      "Epoch [31/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6863%\n",
      "Epoch [31/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [8/225], Training Accuracy: 32.2266%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6863%\n",
      "Epoch [31/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6861%\n",
      "Epoch [31/100], Step [11/225], Training Accuracy: 31.2500%, Training Loss: 0.6863%\n",
      "Epoch [31/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6865%\n",
      "Epoch [31/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6864%\n",
      "Epoch [31/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6868%\n",
      "Epoch [31/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6869%\n",
      "Epoch [31/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6866%\n",
      "Epoch [31/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6868%\n",
      "Epoch [31/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6868%\n",
      "Epoch [31/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6869%\n",
      "Epoch [31/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6866%\n",
      "Epoch [31/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6863%\n",
      "Epoch [31/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6861%\n",
      "Epoch [31/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6861%\n",
      "Epoch [31/100], Step [24/225], Training Accuracy: 31.3802%, Training Loss: 0.6861%\n",
      "Epoch [31/100], Step [25/225], Training Accuracy: 31.7500%, Training Loss: 0.6861%\n",
      "Epoch [31/100], Step [26/225], Training Accuracy: 32.1514%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [27/225], Training Accuracy: 31.9444%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [28/225], Training Accuracy: 31.8080%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [29/225], Training Accuracy: 32.1659%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [30/225], Training Accuracy: 32.1875%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [31/225], Training Accuracy: 32.1573%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [32/225], Training Accuracy: 32.3730%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [33/225], Training Accuracy: 32.4337%, Training Loss: 0.6849%\n",
      "Epoch [31/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6849%\n",
      "Epoch [31/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [36/225], Training Accuracy: 32.0312%, Training Loss: 0.6853%\n",
      "Epoch [31/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [38/225], Training Accuracy: 31.7434%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [39/225], Training Accuracy: 31.3702%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [40/225], Training Accuracy: 31.5625%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [41/225], Training Accuracy: 31.4787%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [42/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [43/225], Training Accuracy: 31.4317%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [44/225], Training Accuracy: 31.3210%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [45/225], Training Accuracy: 31.3889%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [46/225], Training Accuracy: 31.2840%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [47/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [48/225], Training Accuracy: 31.4779%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [49/225], Training Accuracy: 31.4732%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [50/225], Training Accuracy: 31.4375%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [51/225], Training Accuracy: 31.6483%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [52/225], Training Accuracy: 31.6106%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [53/225], Training Accuracy: 31.5448%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [54/225], Training Accuracy: 31.3657%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [56/225], Training Accuracy: 31.5290%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [57/225], Training Accuracy: 31.5515%, Training Loss: 0.6853%\n",
      "Epoch [31/100], Step [58/225], Training Accuracy: 31.4386%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [61/225], Training Accuracy: 31.8648%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [62/225], Training Accuracy: 31.7792%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [63/225], Training Accuracy: 31.8452%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [64/225], Training Accuracy: 31.8359%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [65/225], Training Accuracy: 31.7788%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [66/225], Training Accuracy: 31.8655%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [67/225], Training Accuracy: 31.8563%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [68/225], Training Accuracy: 31.9164%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [69/225], Training Accuracy: 31.9067%, Training Loss: 0.6850%\n",
      "Epoch [31/100], Step [70/225], Training Accuracy: 31.8527%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [71/225], Training Accuracy: 31.8442%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [72/225], Training Accuracy: 31.6189%, Training Loss: 0.6853%\n",
      "Epoch [31/100], Step [73/225], Training Accuracy: 31.5711%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [74/225], Training Accuracy: 31.6723%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [75/225], Training Accuracy: 31.6458%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [76/225], Training Accuracy: 31.5789%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [77/225], Training Accuracy: 31.4529%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [78/225], Training Accuracy: 31.5104%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [79/225], Training Accuracy: 31.4478%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [80/225], Training Accuracy: 31.4258%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [81/225], Training Accuracy: 31.3465%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [82/225], Training Accuracy: 31.3643%, Training Loss: 0.6851%\n",
      "Epoch [31/100], Step [83/225], Training Accuracy: 31.2877%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [84/225], Training Accuracy: 31.3802%, Training Loss: 0.6852%\n",
      "Epoch [31/100], Step [85/225], Training Accuracy: 31.3235%, Training Loss: 0.6853%\n",
      "Epoch [31/100], Step [86/225], Training Accuracy: 31.3590%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [87/225], Training Accuracy: 31.3578%, Training Loss: 0.6854%\n",
      "Epoch [31/100], Step [88/225], Training Accuracy: 31.3565%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [89/225], Training Accuracy: 31.2851%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [90/225], Training Accuracy: 31.1806%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [92/225], Training Accuracy: 31.1990%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [93/225], Training Accuracy: 31.1828%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [94/225], Training Accuracy: 31.2666%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [95/225], Training Accuracy: 31.1678%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [96/225], Training Accuracy: 31.2826%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [97/225], Training Accuracy: 31.2661%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [98/225], Training Accuracy: 31.2659%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [99/225], Training Accuracy: 31.3763%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [100/225], Training Accuracy: 31.3438%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [101/225], Training Accuracy: 31.4821%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [102/225], Training Accuracy: 31.3572%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [103/225], Training Accuracy: 31.4169%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [104/225], Training Accuracy: 31.4002%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [105/225], Training Accuracy: 31.3393%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [106/225], Training Accuracy: 31.3090%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [107/225], Training Accuracy: 31.2208%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [108/225], Training Accuracy: 31.3223%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [109/225], Training Accuracy: 31.2070%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [110/225], Training Accuracy: 31.1790%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [111/225], Training Accuracy: 31.0670%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [112/225], Training Accuracy: 31.1384%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [113/225], Training Accuracy: 31.1394%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [114/225], Training Accuracy: 31.1952%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [115/225], Training Accuracy: 31.1685%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [116/225], Training Accuracy: 31.1288%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [117/225], Training Accuracy: 31.0630%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [118/225], Training Accuracy: 31.0117%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [119/225], Training Accuracy: 30.9611%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [120/225], Training Accuracy: 30.9896%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [121/225], Training Accuracy: 30.9917%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [122/225], Training Accuracy: 31.0067%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [123/225], Training Accuracy: 31.0467%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [124/225], Training Accuracy: 31.0484%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [125/225], Training Accuracy: 31.0000%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [126/225], Training Accuracy: 30.9648%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [127/225], Training Accuracy: 30.9301%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [128/225], Training Accuracy: 30.9326%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [129/225], Training Accuracy: 30.9593%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [130/225], Training Accuracy: 30.8774%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [131/225], Training Accuracy: 30.8445%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [132/225], Training Accuracy: 30.8120%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [133/225], Training Accuracy: 30.8388%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [134/225], Training Accuracy: 30.9352%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [135/225], Training Accuracy: 30.9491%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [136/225], Training Accuracy: 31.0087%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [137/225], Training Accuracy: 31.0561%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [138/225], Training Accuracy: 31.0575%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [139/225], Training Accuracy: 31.0252%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [140/225], Training Accuracy: 31.0156%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [141/225], Training Accuracy: 30.9619%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [142/225], Training Accuracy: 31.0189%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [143/225], Training Accuracy: 31.0315%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [144/225], Training Accuracy: 31.0547%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [145/225], Training Accuracy: 31.1530%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [146/225], Training Accuracy: 31.1858%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [147/225], Training Accuracy: 31.2075%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [149/225], Training Accuracy: 31.2081%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [151/225], Training Accuracy: 31.2707%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [152/225], Training Accuracy: 31.2706%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [153/225], Training Accuracy: 31.2296%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [154/225], Training Accuracy: 31.2399%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [155/225], Training Accuracy: 31.2298%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [156/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [157/225], Training Accuracy: 31.2002%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [158/225], Training Accuracy: 31.1610%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [159/225], Training Accuracy: 31.2205%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [160/225], Training Accuracy: 31.1816%, Training Loss: 0.6860%\n",
      "Epoch [31/100], Step [161/225], Training Accuracy: 31.2209%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [162/225], Training Accuracy: 31.1825%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [163/225], Training Accuracy: 31.2404%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [164/225], Training Accuracy: 31.2024%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [165/225], Training Accuracy: 31.1458%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [166/225], Training Accuracy: 31.1276%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [167/225], Training Accuracy: 31.1939%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [168/225], Training Accuracy: 31.1477%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [169/225], Training Accuracy: 31.0743%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [170/225], Training Accuracy: 31.0294%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [171/225], Training Accuracy: 31.0490%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [172/225], Training Accuracy: 31.0501%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [173/225], Training Accuracy: 31.0242%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [174/225], Training Accuracy: 31.0524%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [175/225], Training Accuracy: 31.0714%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [176/225], Training Accuracy: 31.0902%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [177/225], Training Accuracy: 31.0823%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [178/225], Training Accuracy: 31.0744%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [179/225], Training Accuracy: 31.0492%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [180/225], Training Accuracy: 31.0851%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [181/225], Training Accuracy: 31.0428%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [182/225], Training Accuracy: 31.0440%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [183/225], Training Accuracy: 31.0451%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [184/225], Training Accuracy: 31.0207%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [185/225], Training Accuracy: 30.9966%, Training Loss: 0.6859%\n",
      "Epoch [31/100], Step [186/225], Training Accuracy: 31.0232%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [187/225], Training Accuracy: 31.0495%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [188/225], Training Accuracy: 31.0505%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [189/225], Training Accuracy: 31.0764%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [190/225], Training Accuracy: 31.0362%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [191/225], Training Accuracy: 31.0046%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [192/225], Training Accuracy: 30.9326%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [193/225], Training Accuracy: 30.9424%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [194/225], Training Accuracy: 30.9520%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [195/225], Training Accuracy: 30.9375%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [196/225], Training Accuracy: 30.8992%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [197/225], Training Accuracy: 30.9327%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [198/225], Training Accuracy: 30.9580%, Training Loss: 0.6858%\n",
      "Epoch [31/100], Step [199/225], Training Accuracy: 30.9359%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [200/225], Training Accuracy: 30.9297%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [201/225], Training Accuracy: 30.9391%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [202/225], Training Accuracy: 30.9329%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [203/225], Training Accuracy: 30.9344%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [204/225], Training Accuracy: 30.9972%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [205/225], Training Accuracy: 31.0061%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [206/225], Training Accuracy: 30.9845%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [207/225], Training Accuracy: 30.9707%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [208/225], Training Accuracy: 31.0096%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [209/225], Training Accuracy: 31.0407%, Training Loss: 0.6857%\n",
      "Epoch [31/100], Step [210/225], Training Accuracy: 31.0789%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [211/225], Training Accuracy: 31.0575%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [212/225], Training Accuracy: 31.0879%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [213/225], Training Accuracy: 31.0666%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [214/225], Training Accuracy: 31.0894%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [215/225], Training Accuracy: 31.0465%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [216/225], Training Accuracy: 30.9823%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [217/225], Training Accuracy: 30.9836%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [218/225], Training Accuracy: 30.9490%, Training Loss: 0.6856%\n",
      "Epoch [31/100], Step [219/225], Training Accuracy: 31.0003%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [220/225], Training Accuracy: 31.0085%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [221/225], Training Accuracy: 30.9813%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [222/225], Training Accuracy: 30.9896%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [223/225], Training Accuracy: 31.0118%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [224/225], Training Accuracy: 31.0059%, Training Loss: 0.6855%\n",
      "Epoch [31/100], Step [225/225], Training Accuracy: 30.9825%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6879%\n",
      "Epoch [32/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6888%\n",
      "Epoch [32/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6892%\n",
      "Epoch [32/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6873%\n",
      "Epoch [32/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6872%\n",
      "Epoch [32/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6873%\n",
      "Epoch [32/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6864%\n",
      "Epoch [32/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6867%\n",
      "Epoch [32/100], Step [10/225], Training Accuracy: 30.0000%, Training Loss: 0.6866%\n",
      "Epoch [32/100], Step [11/225], Training Accuracy: 30.1136%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [13/225], Training Accuracy: 29.8077%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [14/225], Training Accuracy: 30.0223%, Training Loss: 0.6870%\n",
      "Epoch [32/100], Step [15/225], Training Accuracy: 30.5208%, Training Loss: 0.6869%\n",
      "Epoch [32/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6864%\n",
      "Epoch [32/100], Step [20/225], Training Accuracy: 31.4062%, Training Loss: 0.6865%\n",
      "Epoch [32/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [24/225], Training Accuracy: 31.5755%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [25/225], Training Accuracy: 31.8750%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [26/225], Training Accuracy: 32.2115%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [27/225], Training Accuracy: 31.8866%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [28/225], Training Accuracy: 31.8638%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [30/225], Training Accuracy: 32.2396%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [31/225], Training Accuracy: 32.0565%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [33/225], Training Accuracy: 32.3390%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [34/225], Training Accuracy: 32.1232%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [35/225], Training Accuracy: 32.0089%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [36/225], Training Accuracy: 31.8142%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [37/225], Training Accuracy: 31.8834%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [38/225], Training Accuracy: 31.7845%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [39/225], Training Accuracy: 31.4904%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [40/225], Training Accuracy: 31.4844%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [41/225], Training Accuracy: 31.5930%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [42/225], Training Accuracy: 31.4360%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [44/225], Training Accuracy: 31.7472%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [45/225], Training Accuracy: 31.7361%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [46/225], Training Accuracy: 31.6916%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Step [47/225], Training Accuracy: 31.6157%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [50/225], Training Accuracy: 31.7812%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [51/225], Training Accuracy: 31.9240%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [52/225], Training Accuracy: 31.9111%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [53/225], Training Accuracy: 31.8691%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [54/225], Training Accuracy: 31.7419%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [55/225], Training Accuracy: 31.8466%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [57/225], Training Accuracy: 31.9079%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [58/225], Training Accuracy: 31.7349%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [59/225], Training Accuracy: 32.0445%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [60/225], Training Accuracy: 32.1875%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [61/225], Training Accuracy: 32.0953%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [62/225], Training Accuracy: 32.0817%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [63/225], Training Accuracy: 32.1181%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [64/225], Training Accuracy: 32.1045%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [65/225], Training Accuracy: 32.0192%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [66/225], Training Accuracy: 32.0786%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [67/225], Training Accuracy: 32.0896%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [68/225], Training Accuracy: 32.1232%, Training Loss: 0.6851%\n",
      "Epoch [32/100], Step [69/225], Training Accuracy: 32.1558%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [70/225], Training Accuracy: 32.1205%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [71/225], Training Accuracy: 32.1743%, Training Loss: 0.6850%\n",
      "Epoch [32/100], Step [72/225], Training Accuracy: 31.9227%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [73/225], Training Accuracy: 31.8493%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [74/225], Training Accuracy: 31.9468%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [75/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [76/225], Training Accuracy: 31.8051%, Training Loss: 0.6852%\n",
      "Epoch [32/100], Step [77/225], Training Accuracy: 31.6964%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [78/225], Training Accuracy: 31.7308%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [79/225], Training Accuracy: 31.6653%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [80/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [81/225], Training Accuracy: 31.5586%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [82/225], Training Accuracy: 31.5739%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [83/225], Training Accuracy: 31.5136%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [84/225], Training Accuracy: 31.6034%, Training Loss: 0.6853%\n",
      "Epoch [32/100], Step [85/225], Training Accuracy: 31.5625%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [86/225], Training Accuracy: 31.5952%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [87/225], Training Accuracy: 31.6272%, Training Loss: 0.6854%\n",
      "Epoch [32/100], Step [88/225], Training Accuracy: 31.6229%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [89/225], Training Accuracy: 31.5660%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [90/225], Training Accuracy: 31.4757%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [91/225], Training Accuracy: 31.5076%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [92/225], Training Accuracy: 31.4368%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [93/225], Training Accuracy: 31.4348%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [94/225], Training Accuracy: 31.4993%, Training Loss: 0.6855%\n",
      "Epoch [32/100], Step [95/225], Training Accuracy: 31.3816%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [96/225], Training Accuracy: 31.4779%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [97/225], Training Accuracy: 31.4916%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [98/225], Training Accuracy: 31.5370%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [99/225], Training Accuracy: 31.6761%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [100/225], Training Accuracy: 31.6719%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [101/225], Training Accuracy: 31.7915%, Training Loss: 0.6856%\n",
      "Epoch [32/100], Step [102/225], Training Accuracy: 31.6636%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [103/225], Training Accuracy: 31.7051%, Training Loss: 0.6857%\n",
      "Epoch [32/100], Step [104/225], Training Accuracy: 31.6857%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [105/225], Training Accuracy: 31.6518%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [106/225], Training Accuracy: 31.6185%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [107/225], Training Accuracy: 31.5275%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [108/225], Training Accuracy: 31.5828%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [109/225], Training Accuracy: 31.4507%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [110/225], Training Accuracy: 31.4489%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [111/225], Training Accuracy: 31.3485%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [112/225], Training Accuracy: 31.4593%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [113/225], Training Accuracy: 31.4298%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [114/225], Training Accuracy: 31.4693%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [115/225], Training Accuracy: 31.4810%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [116/225], Training Accuracy: 31.4925%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [117/225], Training Accuracy: 31.4103%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [118/225], Training Accuracy: 31.3692%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [119/225], Training Accuracy: 31.2894%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [120/225], Training Accuracy: 31.3411%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [121/225], Training Accuracy: 31.3404%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [122/225], Training Accuracy: 31.3525%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [123/225], Training Accuracy: 31.4024%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [124/225], Training Accuracy: 31.3760%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [125/225], Training Accuracy: 31.3625%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [126/225], Training Accuracy: 31.3120%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [127/225], Training Accuracy: 31.2377%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [128/225], Training Accuracy: 31.2134%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [129/225], Training Accuracy: 31.2621%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [130/225], Training Accuracy: 31.2019%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [131/225], Training Accuracy: 31.1546%, Training Loss: 0.6864%\n",
      "Epoch [32/100], Step [132/225], Training Accuracy: 31.1316%, Training Loss: 0.6864%\n",
      "Epoch [32/100], Step [133/225], Training Accuracy: 31.1678%, Training Loss: 0.6864%\n",
      "Epoch [32/100], Step [134/225], Training Accuracy: 31.2267%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [135/225], Training Accuracy: 31.2500%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [136/225], Training Accuracy: 31.2615%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [137/225], Training Accuracy: 31.2956%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [138/225], Training Accuracy: 31.3293%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [139/225], Training Accuracy: 31.2950%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [140/225], Training Accuracy: 31.2946%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [141/225], Training Accuracy: 31.2611%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [142/225], Training Accuracy: 31.3160%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [143/225], Training Accuracy: 31.2937%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [144/225], Training Accuracy: 31.3151%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [145/225], Training Accuracy: 31.3685%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [146/225], Training Accuracy: 31.3891%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [147/225], Training Accuracy: 31.4094%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [148/225], Training Accuracy: 31.3872%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [149/225], Training Accuracy: 31.4283%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [150/225], Training Accuracy: 31.4375%, Training Loss: 0.6862%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Step [151/225], Training Accuracy: 31.4776%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [152/225], Training Accuracy: 31.4453%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [153/225], Training Accuracy: 31.4236%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [154/225], Training Accuracy: 31.4631%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [155/225], Training Accuracy: 31.4617%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [156/225], Training Accuracy: 31.5004%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [157/225], Training Accuracy: 31.4391%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [158/225], Training Accuracy: 31.4082%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [159/225], Training Accuracy: 31.5055%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [160/225], Training Accuracy: 31.4844%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [161/225], Training Accuracy: 31.5314%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [162/225], Training Accuracy: 31.5104%, Training Loss: 0.6863%\n",
      "Epoch [32/100], Step [163/225], Training Accuracy: 31.5567%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [164/225], Training Accuracy: 31.5549%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [165/225], Training Accuracy: 31.4962%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [166/225], Training Accuracy: 31.4571%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [167/225], Training Accuracy: 31.4933%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [168/225], Training Accuracy: 31.4360%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [169/225], Training Accuracy: 31.3609%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [170/225], Training Accuracy: 31.3143%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [171/225], Training Accuracy: 31.3322%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [172/225], Training Accuracy: 31.3408%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [173/225], Training Accuracy: 31.3403%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [174/225], Training Accuracy: 31.3488%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [175/225], Training Accuracy: 31.3839%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [176/225], Training Accuracy: 31.3832%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [177/225], Training Accuracy: 31.3824%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [178/225], Training Accuracy: 31.3904%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [179/225], Training Accuracy: 31.3460%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [180/225], Training Accuracy: 31.3542%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [181/225], Training Accuracy: 31.2932%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [182/225], Training Accuracy: 31.3015%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [183/225], Training Accuracy: 31.3098%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [184/225], Training Accuracy: 31.2755%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [185/225], Training Accuracy: 31.2247%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [186/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [187/225], Training Accuracy: 31.2751%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [188/225], Training Accuracy: 31.2749%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [189/225], Training Accuracy: 31.2831%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [190/225], Training Accuracy: 31.2418%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [191/225], Training Accuracy: 31.2173%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [192/225], Training Accuracy: 31.1442%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [193/225], Training Accuracy: 31.1609%, Training Loss: 0.6862%\n",
      "Epoch [32/100], Step [194/225], Training Accuracy: 31.1695%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [195/225], Training Accuracy: 31.1378%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [196/225], Training Accuracy: 31.1224%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [197/225], Training Accuracy: 31.1390%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [198/225], Training Accuracy: 31.1632%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [199/225], Training Accuracy: 31.1479%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [200/225], Training Accuracy: 31.1328%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [201/225], Training Accuracy: 31.1567%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [202/225], Training Accuracy: 31.1572%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [203/225], Training Accuracy: 31.1653%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [204/225], Training Accuracy: 31.2117%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [205/225], Training Accuracy: 31.2195%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [206/225], Training Accuracy: 31.2121%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [207/225], Training Accuracy: 31.1896%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [208/225], Training Accuracy: 31.2200%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [209/225], Training Accuracy: 31.2724%, Training Loss: 0.6861%\n",
      "Epoch [32/100], Step [210/225], Training Accuracy: 31.3170%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [211/225], Training Accuracy: 31.2870%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [212/225], Training Accuracy: 31.3458%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [213/225], Training Accuracy: 31.3234%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [214/225], Training Accuracy: 31.3522%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [215/225], Training Accuracy: 31.3154%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [216/225], Training Accuracy: 31.2645%, Training Loss: 0.6860%\n",
      "Epoch [32/100], Step [217/225], Training Accuracy: 31.2788%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [218/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [219/225], Training Accuracy: 31.3071%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [220/225], Training Accuracy: 31.3068%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [221/225], Training Accuracy: 31.2854%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [222/225], Training Accuracy: 31.2993%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [223/225], Training Accuracy: 31.3341%, Training Loss: 0.6859%\n",
      "Epoch [32/100], Step [224/225], Training Accuracy: 31.3407%, Training Loss: 0.6858%\n",
      "Epoch [32/100], Step [225/225], Training Accuracy: 31.3160%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6908%\n",
      "Epoch [33/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6880%\n",
      "Epoch [33/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6881%\n",
      "Epoch [33/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6861%\n",
      "Epoch [33/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6872%\n",
      "Epoch [33/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6876%\n",
      "Epoch [33/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6864%\n",
      "Epoch [33/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6861%\n",
      "Epoch [33/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6864%\n",
      "Epoch [33/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6863%\n",
      "Epoch [33/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6868%\n",
      "Epoch [33/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6865%\n",
      "Epoch [33/100], Step [13/225], Training Accuracy: 30.4087%, Training Loss: 0.6864%\n",
      "Epoch [33/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6870%\n",
      "Epoch [33/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6868%\n",
      "Epoch [33/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6865%\n",
      "Epoch [33/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6869%\n",
      "Epoch [33/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6871%\n",
      "Epoch [33/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6870%\n",
      "Epoch [33/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6868%\n",
      "Epoch [33/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6867%\n",
      "Epoch [33/100], Step [22/225], Training Accuracy: 31.8182%, Training Loss: 0.6864%\n",
      "Epoch [33/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6863%\n",
      "Epoch [33/100], Step [24/225], Training Accuracy: 31.8359%, Training Loss: 0.6861%\n",
      "Epoch [33/100], Step [25/225], Training Accuracy: 32.0625%, Training Loss: 0.6862%\n",
      "Epoch [33/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [27/225], Training Accuracy: 31.9444%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [28/225], Training Accuracy: 31.9754%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [29/225], Training Accuracy: 32.1659%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [30/225], Training Accuracy: 32.1875%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [31/225], Training Accuracy: 32.1069%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6850%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Step [34/225], Training Accuracy: 32.0772%, Training Loss: 0.6850%\n",
      "Epoch [33/100], Step [35/225], Training Accuracy: 31.9196%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [36/225], Training Accuracy: 31.8576%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [37/225], Training Accuracy: 31.9257%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [39/225], Training Accuracy: 31.4103%, Training Loss: 0.6857%\n",
      "Epoch [33/100], Step [40/225], Training Accuracy: 31.4062%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [41/225], Training Accuracy: 31.5168%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [42/225], Training Accuracy: 31.3244%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [43/225], Training Accuracy: 31.3953%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [44/225], Training Accuracy: 31.4631%, Training Loss: 0.6857%\n",
      "Epoch [33/100], Step [45/225], Training Accuracy: 31.5278%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [46/225], Training Accuracy: 31.4878%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [47/225], Training Accuracy: 31.4495%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [48/225], Training Accuracy: 31.5755%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [49/225], Training Accuracy: 31.6964%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [50/225], Training Accuracy: 31.6875%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [51/225], Training Accuracy: 31.8934%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [52/225], Training Accuracy: 31.8510%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [53/225], Training Accuracy: 31.8101%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [54/225], Training Accuracy: 31.6551%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [55/225], Training Accuracy: 31.7330%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [57/225], Training Accuracy: 31.8531%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [58/225], Training Accuracy: 31.7349%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [59/225], Training Accuracy: 32.1239%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [60/225], Training Accuracy: 32.2396%, Training Loss: 0.6848%\n",
      "Epoch [33/100], Step [61/225], Training Accuracy: 32.1209%, Training Loss: 0.6848%\n",
      "Epoch [33/100], Step [62/225], Training Accuracy: 32.1321%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [63/225], Training Accuracy: 32.1925%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [64/225], Training Accuracy: 32.1777%, Training Loss: 0.6848%\n",
      "Epoch [33/100], Step [65/225], Training Accuracy: 32.0192%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [66/225], Training Accuracy: 32.1259%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [67/225], Training Accuracy: 32.1362%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [68/225], Training Accuracy: 32.1691%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [69/225], Training Accuracy: 32.1784%, Training Loss: 0.6847%\n",
      "Epoch [33/100], Step [70/225], Training Accuracy: 32.1652%, Training Loss: 0.6847%\n",
      "Epoch [33/100], Step [71/225], Training Accuracy: 32.1743%, Training Loss: 0.6848%\n",
      "Epoch [33/100], Step [72/225], Training Accuracy: 31.9661%, Training Loss: 0.6850%\n",
      "Epoch [33/100], Step [73/225], Training Accuracy: 31.9349%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [74/225], Training Accuracy: 32.0312%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [75/225], Training Accuracy: 31.9792%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [76/225], Training Accuracy: 31.8873%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [77/225], Training Accuracy: 31.8385%, Training Loss: 0.6850%\n",
      "Epoch [33/100], Step [78/225], Training Accuracy: 31.8710%, Training Loss: 0.6848%\n",
      "Epoch [33/100], Step [79/225], Training Accuracy: 31.8038%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [80/225], Training Accuracy: 31.7773%, Training Loss: 0.6848%\n",
      "Epoch [33/100], Step [81/225], Training Accuracy: 31.6744%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [82/225], Training Accuracy: 31.6883%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [83/225], Training Accuracy: 31.6453%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [84/225], Training Accuracy: 31.7150%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [85/225], Training Accuracy: 31.7096%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [86/225], Training Accuracy: 31.7587%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [87/225], Training Accuracy: 31.7529%, Training Loss: 0.6849%\n",
      "Epoch [33/100], Step [88/225], Training Accuracy: 31.7116%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [89/225], Training Accuracy: 31.6187%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [90/225], Training Accuracy: 31.5104%, Training Loss: 0.6850%\n",
      "Epoch [33/100], Step [91/225], Training Accuracy: 31.5076%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [92/225], Training Accuracy: 31.4878%, Training Loss: 0.6850%\n",
      "Epoch [33/100], Step [93/225], Training Accuracy: 31.5188%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [94/225], Training Accuracy: 31.5658%, Training Loss: 0.6850%\n",
      "Epoch [33/100], Step [95/225], Training Accuracy: 31.4474%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [96/225], Training Accuracy: 31.5592%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [97/225], Training Accuracy: 31.5883%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [98/225], Training Accuracy: 31.5848%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [99/225], Training Accuracy: 31.6919%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [100/225], Training Accuracy: 31.6719%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [101/225], Training Accuracy: 31.7915%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [102/225], Training Accuracy: 31.6789%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [103/225], Training Accuracy: 31.7658%, Training Loss: 0.6851%\n",
      "Epoch [33/100], Step [104/225], Training Accuracy: 31.7157%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [105/225], Training Accuracy: 31.6815%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [106/225], Training Accuracy: 31.6775%, Training Loss: 0.6852%\n",
      "Epoch [33/100], Step [107/225], Training Accuracy: 31.5567%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [108/225], Training Accuracy: 31.5972%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [109/225], Training Accuracy: 31.4650%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [110/225], Training Accuracy: 31.4773%, Training Loss: 0.6853%\n",
      "Epoch [33/100], Step [111/225], Training Accuracy: 31.3485%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [112/225], Training Accuracy: 31.4035%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [113/225], Training Accuracy: 31.3883%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [114/225], Training Accuracy: 31.4145%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [115/225], Training Accuracy: 31.3859%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [116/225], Training Accuracy: 31.3443%, Training Loss: 0.6854%\n",
      "Epoch [33/100], Step [117/225], Training Accuracy: 31.2767%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [118/225], Training Accuracy: 31.2368%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [119/225], Training Accuracy: 31.1712%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [120/225], Training Accuracy: 31.1979%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [121/225], Training Accuracy: 31.1983%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [122/225], Training Accuracy: 31.2116%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [123/225], Training Accuracy: 31.2373%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [124/225], Training Accuracy: 31.2248%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [125/225], Training Accuracy: 31.1750%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [126/225], Training Accuracy: 31.1136%, Training Loss: 0.6855%\n",
      "Epoch [33/100], Step [127/225], Training Accuracy: 31.0778%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [128/225], Training Accuracy: 31.0791%, Training Loss: 0.6856%\n",
      "Epoch [33/100], Step [129/225], Training Accuracy: 31.1047%, Training Loss: 0.6857%\n",
      "Epoch [33/100], Step [130/225], Training Accuracy: 31.0096%, Training Loss: 0.6857%\n",
      "Epoch [33/100], Step [131/225], Training Accuracy: 30.9637%, Training Loss: 0.6857%\n",
      "Epoch [33/100], Step [132/225], Training Accuracy: 30.9186%, Training Loss: 0.6857%\n",
      "Epoch [33/100], Step [133/225], Training Accuracy: 30.9563%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [134/225], Training Accuracy: 30.9935%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [135/225], Training Accuracy: 30.9954%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [137/225], Training Accuracy: 31.0447%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [138/225], Training Accuracy: 31.0915%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [139/225], Training Accuracy: 31.0701%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Step [140/225], Training Accuracy: 31.0379%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [141/225], Training Accuracy: 31.0062%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [142/225], Training Accuracy: 31.0849%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [143/225], Training Accuracy: 31.0970%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [144/225], Training Accuracy: 31.1089%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [145/225], Training Accuracy: 31.1746%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [146/225], Training Accuracy: 31.1858%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [147/225], Training Accuracy: 31.2075%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [149/225], Training Accuracy: 31.2081%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [151/225], Training Accuracy: 31.2810%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [152/225], Training Accuracy: 31.3014%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [153/225], Training Accuracy: 31.2806%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [154/225], Training Accuracy: 31.2906%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [155/225], Training Accuracy: 31.2802%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [156/225], Training Accuracy: 31.3001%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [157/225], Training Accuracy: 31.2201%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [158/225], Training Accuracy: 31.2104%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [159/225], Training Accuracy: 31.2598%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [160/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [161/225], Training Accuracy: 31.2888%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [162/225], Training Accuracy: 31.2596%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [163/225], Training Accuracy: 31.3171%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [164/225], Training Accuracy: 31.2976%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [165/225], Training Accuracy: 31.2405%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [166/225], Training Accuracy: 31.2312%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [167/225], Training Accuracy: 31.2781%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [168/225], Training Accuracy: 31.2314%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [169/225], Training Accuracy: 31.1483%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [170/225], Training Accuracy: 31.1029%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [171/225], Training Accuracy: 31.1129%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [173/225], Training Accuracy: 31.1055%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [174/225], Training Accuracy: 31.1063%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [175/225], Training Accuracy: 31.1518%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [176/225], Training Accuracy: 31.1701%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [177/225], Training Accuracy: 31.1706%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [178/225], Training Accuracy: 31.1447%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [179/225], Training Accuracy: 31.1103%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [180/225], Training Accuracy: 31.1285%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [181/225], Training Accuracy: 31.0946%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [182/225], Training Accuracy: 31.1041%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [183/225], Training Accuracy: 31.1048%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [184/225], Training Accuracy: 31.0717%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [185/225], Training Accuracy: 31.0389%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [186/225], Training Accuracy: 31.0736%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [187/225], Training Accuracy: 31.0996%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [188/225], Training Accuracy: 31.0921%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [189/225], Training Accuracy: 31.1095%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [190/225], Training Accuracy: 31.0855%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [191/225], Training Accuracy: 31.0618%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [192/225], Training Accuracy: 30.9977%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [193/225], Training Accuracy: 31.0071%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [194/225], Training Accuracy: 31.0245%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [195/225], Training Accuracy: 31.0096%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [196/225], Training Accuracy: 30.9790%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [197/225], Training Accuracy: 30.9962%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [198/225], Training Accuracy: 31.0211%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [199/225], Training Accuracy: 31.0144%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [200/225], Training Accuracy: 30.9922%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [201/225], Training Accuracy: 31.0012%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [202/225], Training Accuracy: 31.0025%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [203/225], Training Accuracy: 30.9883%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [204/225], Training Accuracy: 31.0432%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [205/225], Training Accuracy: 31.0366%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [206/225], Training Accuracy: 31.0376%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [207/225], Training Accuracy: 31.0085%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [208/225], Training Accuracy: 31.0322%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [209/225], Training Accuracy: 31.0556%, Training Loss: 0.6860%\n",
      "Epoch [33/100], Step [210/225], Training Accuracy: 31.0640%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [211/225], Training Accuracy: 31.0352%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [212/225], Training Accuracy: 31.0805%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [213/225], Training Accuracy: 31.0593%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [214/225], Training Accuracy: 31.0821%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [215/225], Training Accuracy: 31.0538%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [216/225], Training Accuracy: 30.9968%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [217/225], Training Accuracy: 30.9908%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [218/225], Training Accuracy: 30.9633%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [219/225], Training Accuracy: 31.0288%, Training Loss: 0.6859%\n",
      "Epoch [33/100], Step [220/225], Training Accuracy: 31.0298%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [221/225], Training Accuracy: 31.0096%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [222/225], Training Accuracy: 31.0318%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [223/225], Training Accuracy: 31.0748%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [224/225], Training Accuracy: 31.0686%, Training Loss: 0.6858%\n",
      "Epoch [33/100], Step [225/225], Training Accuracy: 31.0589%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6873%\n",
      "Epoch [34/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6875%\n",
      "Epoch [34/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [5/225], Training Accuracy: 31.8750%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [6/225], Training Accuracy: 30.9896%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [7/225], Training Accuracy: 31.2500%, Training Loss: 0.6847%\n",
      "Epoch [34/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6846%\n",
      "Epoch [34/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6861%\n",
      "Epoch [34/100], Step [14/225], Training Accuracy: 30.1339%, Training Loss: 0.6867%\n",
      "Epoch [34/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6867%\n",
      "Epoch [34/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6864%\n",
      "Epoch [34/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6864%\n",
      "Epoch [34/100], Step [18/225], Training Accuracy: 30.4688%, Training Loss: 0.6865%\n",
      "Epoch [34/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6866%\n",
      "Epoch [34/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6865%\n",
      "Epoch [34/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6862%\n",
      "Epoch [34/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6861%\n",
      "Epoch [34/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [24/225], Training Accuracy: 31.6406%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [25/225], Training Accuracy: 31.8750%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [27/225], Training Accuracy: 31.6551%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [28/225], Training Accuracy: 31.5290%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [29/225], Training Accuracy: 31.8966%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [31/225], Training Accuracy: 31.7540%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [32/225], Training Accuracy: 31.9824%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [33/225], Training Accuracy: 32.0549%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [34/225], Training Accuracy: 31.8934%, Training Loss: 0.6850%\n",
      "Epoch [34/100], Step [35/225], Training Accuracy: 31.7857%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [36/225], Training Accuracy: 31.6840%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [37/225], Training Accuracy: 31.7990%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [38/225], Training Accuracy: 31.6201%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [39/225], Training Accuracy: 31.3702%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [40/225], Training Accuracy: 31.4062%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [41/225], Training Accuracy: 31.4787%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [42/225], Training Accuracy: 31.2872%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [43/225], Training Accuracy: 31.4317%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [44/225], Training Accuracy: 31.4986%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [45/225], Training Accuracy: 31.4931%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [46/225], Training Accuracy: 31.3519%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [47/225], Training Accuracy: 31.3165%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [48/225], Training Accuracy: 31.4128%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [49/225], Training Accuracy: 31.4732%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [50/225], Training Accuracy: 31.3750%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [51/225], Training Accuracy: 31.4951%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [53/225], Training Accuracy: 31.4564%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [56/225], Training Accuracy: 31.5290%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [57/225], Training Accuracy: 31.5241%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [58/225], Training Accuracy: 31.4116%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6848%\n",
      "Epoch [34/100], Step [60/225], Training Accuracy: 31.9010%, Training Loss: 0.6848%\n",
      "Epoch [34/100], Step [61/225], Training Accuracy: 31.8135%, Training Loss: 0.6848%\n",
      "Epoch [34/100], Step [62/225], Training Accuracy: 31.7792%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [63/225], Training Accuracy: 31.8452%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [64/225], Training Accuracy: 31.8848%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [65/225], Training Accuracy: 31.7788%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [66/225], Training Accuracy: 31.8419%, Training Loss: 0.6850%\n",
      "Epoch [34/100], Step [67/225], Training Accuracy: 31.8330%, Training Loss: 0.6850%\n",
      "Epoch [34/100], Step [68/225], Training Accuracy: 31.8015%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [69/225], Training Accuracy: 31.7935%, Training Loss: 0.6848%\n",
      "Epoch [34/100], Step [70/225], Training Accuracy: 31.7188%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [71/225], Training Accuracy: 31.8002%, Training Loss: 0.6849%\n",
      "Epoch [34/100], Step [72/225], Training Accuracy: 31.5972%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [73/225], Training Accuracy: 31.5068%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [74/225], Training Accuracy: 31.6090%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [75/225], Training Accuracy: 31.5625%, Training Loss: 0.6851%\n",
      "Epoch [34/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [77/225], Training Accuracy: 31.4123%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [78/225], Training Accuracy: 31.4503%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [79/225], Training Accuracy: 31.4082%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [80/225], Training Accuracy: 31.3672%, Training Loss: 0.6852%\n",
      "Epoch [34/100], Step [81/225], Training Accuracy: 31.2886%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [82/225], Training Accuracy: 31.3072%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [83/225], Training Accuracy: 31.2688%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [84/225], Training Accuracy: 31.3430%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [85/225], Training Accuracy: 31.3051%, Training Loss: 0.6853%\n",
      "Epoch [34/100], Step [86/225], Training Accuracy: 31.3772%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [87/225], Training Accuracy: 31.3757%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [88/225], Training Accuracy: 31.3565%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [89/225], Training Accuracy: 31.2851%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [90/225], Training Accuracy: 31.1806%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [92/225], Training Accuracy: 31.2330%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [93/225], Training Accuracy: 31.2332%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [94/225], Training Accuracy: 31.3165%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [95/225], Training Accuracy: 31.2007%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [96/225], Training Accuracy: 31.3151%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [97/225], Training Accuracy: 31.3789%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [98/225], Training Accuracy: 31.4094%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [99/225], Training Accuracy: 31.4867%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [100/225], Training Accuracy: 31.4688%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [101/225], Training Accuracy: 31.5903%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [102/225], Training Accuracy: 31.4645%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [103/225], Training Accuracy: 31.5231%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [104/225], Training Accuracy: 31.4904%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [105/225], Training Accuracy: 31.4435%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [106/225], Training Accuracy: 31.4121%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [107/225], Training Accuracy: 31.3230%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [108/225], Training Accuracy: 31.3947%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [109/225], Training Accuracy: 31.2930%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [110/225], Training Accuracy: 31.3210%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [111/225], Training Accuracy: 31.2359%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Step [112/225], Training Accuracy: 31.3198%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [113/225], Training Accuracy: 31.3053%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [114/225], Training Accuracy: 31.3596%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [115/225], Training Accuracy: 31.3315%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [116/225], Training Accuracy: 31.3173%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [117/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [118/225], Training Accuracy: 31.2235%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [119/225], Training Accuracy: 31.1450%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [120/225], Training Accuracy: 31.1849%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [121/225], Training Accuracy: 31.1596%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [122/225], Training Accuracy: 31.1732%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [123/225], Training Accuracy: 31.1738%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [124/225], Training Accuracy: 31.1618%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [125/225], Training Accuracy: 31.1500%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [126/225], Training Accuracy: 31.0888%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [127/225], Training Accuracy: 31.0655%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [128/225], Training Accuracy: 31.0425%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [129/225], Training Accuracy: 31.0683%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [130/225], Training Accuracy: 31.0096%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [131/225], Training Accuracy: 30.9637%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [132/225], Training Accuracy: 30.9186%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [133/225], Training Accuracy: 30.9445%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [134/225], Training Accuracy: 31.0168%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [135/225], Training Accuracy: 31.0185%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [136/225], Training Accuracy: 31.0317%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [137/225], Training Accuracy: 31.0675%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [138/225], Training Accuracy: 31.0915%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [139/225], Training Accuracy: 31.0477%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [140/225], Training Accuracy: 31.0268%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [141/225], Training Accuracy: 30.9619%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [142/225], Training Accuracy: 31.0299%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [143/225], Training Accuracy: 31.0315%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [144/225], Training Accuracy: 31.0438%, Training Loss: 0.6860%\n",
      "Epoch [34/100], Step [145/225], Training Accuracy: 31.0776%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [146/225], Training Accuracy: 31.1109%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [147/225], Training Accuracy: 31.1437%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [148/225], Training Accuracy: 31.1233%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [149/225], Training Accuracy: 31.1661%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [150/225], Training Accuracy: 31.1875%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [151/225], Training Accuracy: 31.2293%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [152/225], Training Accuracy: 31.2089%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [153/225], Training Accuracy: 31.1683%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [154/225], Training Accuracy: 31.1993%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [155/225], Training Accuracy: 31.1794%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [156/225], Training Accuracy: 31.1999%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [157/225], Training Accuracy: 31.1306%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [158/225], Training Accuracy: 31.1116%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [159/225], Training Accuracy: 31.1714%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [160/225], Training Accuracy: 31.1426%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [161/225], Training Accuracy: 31.1821%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [162/225], Training Accuracy: 31.1535%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [163/225], Training Accuracy: 31.2021%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [164/225], Training Accuracy: 31.1833%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [165/225], Training Accuracy: 31.1269%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [166/225], Training Accuracy: 31.1370%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [167/225], Training Accuracy: 31.1751%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [168/225], Training Accuracy: 31.1291%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [169/225], Training Accuracy: 31.0558%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [170/225], Training Accuracy: 31.0110%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [171/225], Training Accuracy: 31.0398%, Training Loss: 0.6859%\n",
      "Epoch [34/100], Step [172/225], Training Accuracy: 31.0683%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [173/225], Training Accuracy: 31.0784%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [174/225], Training Accuracy: 31.1153%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [175/225], Training Accuracy: 31.1518%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [176/225], Training Accuracy: 31.1790%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [177/225], Training Accuracy: 31.1882%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [178/225], Training Accuracy: 31.1973%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [179/225], Training Accuracy: 31.1627%, Training Loss: 0.6858%\n",
      "Epoch [34/100], Step [180/225], Training Accuracy: 31.1892%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [181/225], Training Accuracy: 31.1464%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [182/225], Training Accuracy: 31.1212%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [183/225], Training Accuracy: 31.1305%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [184/225], Training Accuracy: 31.1226%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [185/225], Training Accuracy: 31.0895%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [186/225], Training Accuracy: 31.1240%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [187/225], Training Accuracy: 31.1497%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [188/225], Training Accuracy: 31.1503%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [189/225], Training Accuracy: 31.1839%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [190/225], Training Accuracy: 31.1431%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [191/225], Training Accuracy: 31.1355%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [192/225], Training Accuracy: 31.0628%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [193/225], Training Accuracy: 31.0638%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [194/225], Training Accuracy: 31.0648%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [195/225], Training Accuracy: 31.0497%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [196/225], Training Accuracy: 31.0029%, Training Loss: 0.6857%\n",
      "Epoch [34/100], Step [197/225], Training Accuracy: 31.0359%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [198/225], Training Accuracy: 31.0448%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [199/225], Training Accuracy: 31.0144%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [200/225], Training Accuracy: 30.9922%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [201/225], Training Accuracy: 31.0012%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [202/225], Training Accuracy: 31.0102%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [203/225], Training Accuracy: 30.9960%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [204/225], Training Accuracy: 31.0585%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [205/225], Training Accuracy: 31.0671%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [206/225], Training Accuracy: 31.0528%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [207/225], Training Accuracy: 31.0311%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [208/225], Training Accuracy: 31.0472%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [209/225], Training Accuracy: 31.0855%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [210/225], Training Accuracy: 31.1161%, Training Loss: 0.6856%\n",
      "Epoch [34/100], Step [211/225], Training Accuracy: 31.1093%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [212/225], Training Accuracy: 31.1542%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [213/225], Training Accuracy: 31.1253%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [214/225], Training Accuracy: 31.1332%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [215/225], Training Accuracy: 31.0901%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [216/225], Training Accuracy: 31.0402%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Step [217/225], Training Accuracy: 31.0268%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [218/225], Training Accuracy: 30.9920%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [219/225], Training Accuracy: 31.0574%, Training Loss: 0.6855%\n",
      "Epoch [34/100], Step [220/225], Training Accuracy: 31.0795%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [221/225], Training Accuracy: 31.0591%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [222/225], Training Accuracy: 31.0740%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [223/225], Training Accuracy: 31.1029%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [224/225], Training Accuracy: 31.1035%, Training Loss: 0.6854%\n",
      "Epoch [34/100], Step [225/225], Training Accuracy: 31.0937%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6889%\n",
      "Epoch [35/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6878%\n",
      "Epoch [35/100], Step [3/225], Training Accuracy: 30.2083%, Training Loss: 0.6897%\n",
      "Epoch [35/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6865%\n",
      "Epoch [35/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6866%\n",
      "Epoch [35/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6868%\n",
      "Epoch [35/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6868%\n",
      "Epoch [35/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6868%\n",
      "Epoch [35/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6866%\n",
      "Epoch [35/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6866%\n",
      "Epoch [35/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6869%\n",
      "Epoch [35/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6869%\n",
      "Epoch [35/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6868%\n",
      "Epoch [35/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6865%\n",
      "Epoch [35/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6864%\n",
      "Epoch [35/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [24/225], Training Accuracy: 31.6406%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [25/225], Training Accuracy: 31.7500%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [27/225], Training Accuracy: 31.6551%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [28/225], Training Accuracy: 31.4174%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [29/225], Training Accuracy: 31.7349%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [30/225], Training Accuracy: 31.6146%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [31/225], Training Accuracy: 31.5020%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [32/225], Training Accuracy: 31.6406%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [33/225], Training Accuracy: 31.8182%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [34/225], Training Accuracy: 31.6636%, Training Loss: 0.6850%\n",
      "Epoch [35/100], Step [35/225], Training Accuracy: 31.4732%, Training Loss: 0.6850%\n",
      "Epoch [35/100], Step [36/225], Training Accuracy: 31.3802%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [37/225], Training Accuracy: 31.4189%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [38/225], Training Accuracy: 31.2089%, Training Loss: 0.6854%\n",
      "Epoch [35/100], Step [39/225], Training Accuracy: 30.9696%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [40/225], Training Accuracy: 30.9766%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [41/225], Training Accuracy: 31.0213%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [42/225], Training Accuracy: 30.9152%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [43/225], Training Accuracy: 31.1047%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [44/225], Training Accuracy: 31.0014%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [45/225], Training Accuracy: 31.0417%, Training Loss: 0.6854%\n",
      "Epoch [35/100], Step [46/225], Training Accuracy: 30.9783%, Training Loss: 0.6854%\n",
      "Epoch [35/100], Step [47/225], Training Accuracy: 30.9176%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [48/225], Training Accuracy: 31.0547%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [49/225], Training Accuracy: 31.1224%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [50/225], Training Accuracy: 31.0938%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [51/225], Training Accuracy: 31.3113%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [52/225], Training Accuracy: 31.2800%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [53/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [35/100], Step [54/225], Training Accuracy: 31.1632%, Training Loss: 0.6850%\n",
      "Epoch [35/100], Step [55/225], Training Accuracy: 31.2216%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [56/225], Training Accuracy: 31.2221%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [57/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [58/225], Training Accuracy: 31.1961%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [59/225], Training Accuracy: 31.5943%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [60/225], Training Accuracy: 31.7188%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [61/225], Training Accuracy: 31.6086%, Training Loss: 0.6846%\n",
      "Epoch [35/100], Step [62/225], Training Accuracy: 31.6028%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [63/225], Training Accuracy: 31.6468%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [64/225], Training Accuracy: 31.6650%, Training Loss: 0.6846%\n",
      "Epoch [35/100], Step [65/225], Training Accuracy: 31.5144%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [66/225], Training Accuracy: 31.6051%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [67/225], Training Accuracy: 31.6231%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [68/225], Training Accuracy: 31.6866%, Training Loss: 0.6846%\n",
      "Epoch [35/100], Step [69/225], Training Accuracy: 31.7255%, Training Loss: 0.6845%\n",
      "Epoch [35/100], Step [70/225], Training Accuracy: 31.7188%, Training Loss: 0.6845%\n",
      "Epoch [35/100], Step [71/225], Training Accuracy: 31.8002%, Training Loss: 0.6845%\n",
      "Epoch [35/100], Step [72/225], Training Accuracy: 31.5972%, Training Loss: 0.6848%\n",
      "Epoch [35/100], Step [73/225], Training Accuracy: 31.5497%, Training Loss: 0.6848%\n",
      "Epoch [35/100], Step [74/225], Training Accuracy: 31.6723%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [75/225], Training Accuracy: 31.6042%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [76/225], Training Accuracy: 31.5584%, Training Loss: 0.6847%\n",
      "Epoch [35/100], Step [77/225], Training Accuracy: 31.4935%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [78/225], Training Accuracy: 31.5304%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [79/225], Training Accuracy: 31.4676%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [80/225], Training Accuracy: 31.4648%, Training Loss: 0.6848%\n",
      "Epoch [35/100], Step [81/225], Training Accuracy: 31.4236%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [82/225], Training Accuracy: 31.4405%, Training Loss: 0.6848%\n",
      "Epoch [35/100], Step [83/225], Training Accuracy: 31.3441%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [84/225], Training Accuracy: 31.4360%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [85/225], Training Accuracy: 31.3603%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [86/225], Training Accuracy: 31.3772%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [87/225], Training Accuracy: 31.4116%, Training Loss: 0.6849%\n",
      "Epoch [35/100], Step [88/225], Training Accuracy: 31.3743%, Training Loss: 0.6850%\n",
      "Epoch [35/100], Step [89/225], Training Accuracy: 31.3027%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [90/225], Training Accuracy: 31.2326%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [91/225], Training Accuracy: 31.3015%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [92/225], Training Accuracy: 31.2840%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [93/225], Training Accuracy: 31.2668%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [94/225], Training Accuracy: 31.3664%, Training Loss: 0.6851%\n",
      "Epoch [35/100], Step [95/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [96/225], Training Accuracy: 31.3477%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [97/225], Training Accuracy: 31.3628%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [98/225], Training Accuracy: 31.3776%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [99/225], Training Accuracy: 31.4710%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [100/225], Training Accuracy: 31.4688%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Step [101/225], Training Accuracy: 31.5903%, Training Loss: 0.6852%\n",
      "Epoch [35/100], Step [102/225], Training Accuracy: 31.4645%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [103/225], Training Accuracy: 31.4927%, Training Loss: 0.6853%\n",
      "Epoch [35/100], Step [104/225], Training Accuracy: 31.4754%, Training Loss: 0.6854%\n",
      "Epoch [35/100], Step [105/225], Training Accuracy: 31.4286%, Training Loss: 0.6854%\n",
      "Epoch [35/100], Step [106/225], Training Accuracy: 31.4121%, Training Loss: 0.6854%\n",
      "Epoch [35/100], Step [107/225], Training Accuracy: 31.3230%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [108/225], Training Accuracy: 31.3947%, Training Loss: 0.6855%\n",
      "Epoch [35/100], Step [109/225], Training Accuracy: 31.2643%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [110/225], Training Accuracy: 31.2358%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [111/225], Training Accuracy: 31.1233%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [112/225], Training Accuracy: 31.2081%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [113/225], Training Accuracy: 31.1670%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [114/225], Training Accuracy: 31.2089%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [115/225], Training Accuracy: 31.1549%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [116/225], Training Accuracy: 31.1422%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [117/225], Training Accuracy: 31.0764%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [118/225], Training Accuracy: 31.0381%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [119/225], Training Accuracy: 30.9611%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [120/225], Training Accuracy: 31.0026%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [121/225], Training Accuracy: 31.0046%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [122/225], Training Accuracy: 31.0579%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [123/225], Training Accuracy: 31.0976%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [124/225], Training Accuracy: 31.0736%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [125/225], Training Accuracy: 31.0625%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [126/225], Training Accuracy: 30.9896%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [127/225], Training Accuracy: 30.9178%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [128/225], Training Accuracy: 30.8960%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [129/225], Training Accuracy: 30.9230%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [130/225], Training Accuracy: 30.8774%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [131/225], Training Accuracy: 30.8325%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [132/225], Training Accuracy: 30.7884%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [133/225], Training Accuracy: 30.8271%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [134/225], Training Accuracy: 30.8769%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [135/225], Training Accuracy: 30.8912%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [136/225], Training Accuracy: 30.9053%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [137/225], Training Accuracy: 30.9421%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [138/225], Training Accuracy: 30.9443%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [139/225], Training Accuracy: 30.8903%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [140/225], Training Accuracy: 30.8817%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [141/225], Training Accuracy: 30.8178%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [142/225], Training Accuracy: 30.8759%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [143/225], Training Accuracy: 30.8785%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [144/225], Training Accuracy: 30.8919%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [145/225], Training Accuracy: 30.9375%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [146/225], Training Accuracy: 30.9824%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [147/225], Training Accuracy: 31.0055%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [148/225], Training Accuracy: 30.9649%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [149/225], Training Accuracy: 30.9773%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [150/225], Training Accuracy: 30.9896%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [151/225], Training Accuracy: 31.0224%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [152/225], Training Accuracy: 31.0136%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [153/225], Training Accuracy: 30.9538%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [154/225], Training Accuracy: 30.9761%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [155/225], Training Accuracy: 30.9476%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [156/225], Training Accuracy: 30.9796%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [157/225], Training Accuracy: 30.9116%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [158/225], Training Accuracy: 30.9039%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [159/225], Training Accuracy: 30.9650%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [160/225], Training Accuracy: 30.9277%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [161/225], Training Accuracy: 30.9977%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [162/225], Training Accuracy: 30.9606%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [163/225], Training Accuracy: 31.0104%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [164/225], Training Accuracy: 30.9928%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [165/225], Training Accuracy: 30.9564%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [166/225], Training Accuracy: 30.9676%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [167/225], Training Accuracy: 31.0067%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [168/225], Training Accuracy: 30.9710%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [169/225], Training Accuracy: 30.8894%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [170/225], Training Accuracy: 30.8456%, Training Loss: 0.6860%\n",
      "Epoch [35/100], Step [171/225], Training Accuracy: 30.8571%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [172/225], Training Accuracy: 30.8866%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [173/225], Training Accuracy: 30.8707%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [174/225], Training Accuracy: 30.8998%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [175/225], Training Accuracy: 30.9464%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [176/225], Training Accuracy: 30.9748%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [177/225], Training Accuracy: 30.9675%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [178/225], Training Accuracy: 30.9515%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [179/225], Training Accuracy: 30.9096%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [180/225], Training Accuracy: 30.9288%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [181/225], Training Accuracy: 30.8874%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [182/225], Training Accuracy: 30.8637%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [183/225], Training Accuracy: 30.8743%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [184/225], Training Accuracy: 30.8594%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [185/225], Training Accuracy: 30.8277%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [186/225], Training Accuracy: 30.8468%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [187/225], Training Accuracy: 30.8322%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [188/225], Training Accuracy: 30.8344%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [189/225], Training Accuracy: 30.8614%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [190/225], Training Accuracy: 30.8141%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [191/225], Training Accuracy: 30.8001%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [192/225], Training Accuracy: 30.7454%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [193/225], Training Accuracy: 30.7562%, Training Loss: 0.6859%\n",
      "Epoch [35/100], Step [194/225], Training Accuracy: 30.7748%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [195/225], Training Accuracy: 30.7612%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [196/225], Training Accuracy: 30.7398%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [197/225], Training Accuracy: 30.7741%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [198/225], Training Accuracy: 30.8002%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [199/225], Training Accuracy: 30.7946%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [200/225], Training Accuracy: 30.7812%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [201/225], Training Accuracy: 30.7914%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [202/225], Training Accuracy: 30.7936%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [203/225], Training Accuracy: 30.7882%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [204/225], Training Accuracy: 30.8670%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [205/225], Training Accuracy: 30.8841%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [206/225], Training Accuracy: 30.8708%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [207/225], Training Accuracy: 30.8575%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Step [208/225], Training Accuracy: 30.8894%, Training Loss: 0.6858%\n",
      "Epoch [35/100], Step [209/225], Training Accuracy: 30.9285%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [210/225], Training Accuracy: 30.9524%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [211/225], Training Accuracy: 30.9464%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [212/225], Training Accuracy: 31.0068%, Training Loss: 0.6856%\n",
      "Epoch [35/100], Step [213/225], Training Accuracy: 30.9859%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [214/225], Training Accuracy: 30.9871%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [215/225], Training Accuracy: 30.9666%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [216/225], Training Accuracy: 30.9100%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [217/225], Training Accuracy: 30.9044%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [218/225], Training Accuracy: 30.8773%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [219/225], Training Accuracy: 30.9432%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [220/225], Training Accuracy: 30.9659%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [221/225], Training Accuracy: 30.9601%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [222/225], Training Accuracy: 30.9755%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [223/225], Training Accuracy: 31.0118%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [224/225], Training Accuracy: 31.0198%, Training Loss: 0.6857%\n",
      "Epoch [35/100], Step [225/225], Training Accuracy: 31.0033%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6923%\n",
      "Epoch [36/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6873%\n",
      "Epoch [36/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6881%\n",
      "Epoch [36/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6861%\n",
      "Epoch [36/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6860%\n",
      "Epoch [36/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6867%\n",
      "Epoch [36/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6868%\n",
      "Epoch [36/100], Step [20/225], Training Accuracy: 31.3281%, Training Loss: 0.6868%\n",
      "Epoch [36/100], Step [21/225], Training Accuracy: 31.2500%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [22/225], Training Accuracy: 31.3920%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [24/225], Training Accuracy: 31.3802%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [25/225], Training Accuracy: 31.7500%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [26/225], Training Accuracy: 32.1514%, Training Loss: 0.6860%\n",
      "Epoch [36/100], Step [27/225], Training Accuracy: 31.8866%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [28/225], Training Accuracy: 31.8638%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [29/225], Training Accuracy: 32.1659%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [32/225], Training Accuracy: 31.9824%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [33/225], Training Accuracy: 32.1023%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [35/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [36/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [38/225], Training Accuracy: 31.4967%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [39/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [40/225], Training Accuracy: 31.2109%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [41/225], Training Accuracy: 31.1738%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [42/225], Training Accuracy: 31.0640%, Training Loss: 0.6861%\n",
      "Epoch [36/100], Step [43/225], Training Accuracy: 31.2863%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [44/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [45/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [46/225], Training Accuracy: 31.1481%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [47/225], Training Accuracy: 31.0838%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [48/225], Training Accuracy: 31.2174%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [49/225], Training Accuracy: 31.3138%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [50/225], Training Accuracy: 31.3125%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [51/225], Training Accuracy: 31.4645%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [54/225], Training Accuracy: 31.3079%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [55/225], Training Accuracy: 31.3352%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [56/225], Training Accuracy: 31.3337%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [57/225], Training Accuracy: 31.4145%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [58/225], Training Accuracy: 31.3039%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [59/225], Training Accuracy: 31.6472%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [60/225], Training Accuracy: 31.7448%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [61/225], Training Accuracy: 31.6342%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [62/225], Training Accuracy: 31.6784%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [63/225], Training Accuracy: 31.7460%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [64/225], Training Accuracy: 31.7627%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [65/225], Training Accuracy: 31.6346%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [66/225], Training Accuracy: 31.7235%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [67/225], Training Accuracy: 31.7397%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [68/225], Training Accuracy: 31.8015%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [69/225], Training Accuracy: 31.8388%, Training Loss: 0.6852%\n",
      "Epoch [36/100], Step [70/225], Training Accuracy: 31.7857%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [71/225], Training Accuracy: 31.8222%, Training Loss: 0.6853%\n",
      "Epoch [36/100], Step [72/225], Training Accuracy: 31.5972%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [73/225], Training Accuracy: 31.5497%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [74/225], Training Accuracy: 31.6301%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [75/225], Training Accuracy: 31.6250%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [76/225], Training Accuracy: 31.5378%, Training Loss: 0.6854%\n",
      "Epoch [36/100], Step [77/225], Training Accuracy: 31.4529%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [78/225], Training Accuracy: 31.5104%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [79/225], Training Accuracy: 31.4478%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [80/225], Training Accuracy: 31.4648%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [81/225], Training Accuracy: 31.3272%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [82/225], Training Accuracy: 31.3453%, Training Loss: 0.6855%\n",
      "Epoch [36/100], Step [83/225], Training Accuracy: 31.2877%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [84/225], Training Accuracy: 31.3244%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [85/225], Training Accuracy: 31.2868%, Training Loss: 0.6856%\n",
      "Epoch [36/100], Step [86/225], Training Accuracy: 31.3408%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [87/225], Training Accuracy: 31.3039%, Training Loss: 0.6857%\n",
      "Epoch [36/100], Step [88/225], Training Accuracy: 31.3033%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [89/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Step [90/225], Training Accuracy: 31.1632%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [91/225], Training Accuracy: 31.2672%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [92/225], Training Accuracy: 31.1990%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [93/225], Training Accuracy: 31.1492%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [94/225], Training Accuracy: 31.2168%, Training Loss: 0.6858%\n",
      "Epoch [36/100], Step [95/225], Training Accuracy: 31.1184%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [96/225], Training Accuracy: 31.1849%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [97/225], Training Accuracy: 31.1856%, Training Loss: 0.6860%\n",
      "Epoch [36/100], Step [98/225], Training Accuracy: 31.1703%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [99/225], Training Accuracy: 31.3131%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [100/225], Training Accuracy: 31.2812%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [101/225], Training Accuracy: 31.3738%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [102/225], Training Accuracy: 31.2653%, Training Loss: 0.6859%\n",
      "Epoch [36/100], Step [103/225], Training Accuracy: 31.3107%, Training Loss: 0.6860%\n",
      "Epoch [36/100], Step [104/225], Training Accuracy: 31.2800%, Training Loss: 0.6861%\n",
      "Epoch [36/100], Step [105/225], Training Accuracy: 31.2351%, Training Loss: 0.6861%\n",
      "Epoch [36/100], Step [106/225], Training Accuracy: 31.2353%, Training Loss: 0.6861%\n",
      "Epoch [36/100], Step [107/225], Training Accuracy: 31.1332%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [108/225], Training Accuracy: 31.2211%, Training Loss: 0.6861%\n",
      "Epoch [36/100], Step [109/225], Training Accuracy: 31.1067%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [110/225], Training Accuracy: 31.0938%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [111/225], Training Accuracy: 30.9966%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [112/225], Training Accuracy: 31.0965%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [113/225], Training Accuracy: 31.0979%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [114/225], Training Accuracy: 31.1129%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [115/225], Training Accuracy: 31.1005%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [116/225], Training Accuracy: 31.0884%, Training Loss: 0.6862%\n",
      "Epoch [36/100], Step [117/225], Training Accuracy: 31.0230%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [118/225], Training Accuracy: 31.0117%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [119/225], Training Accuracy: 30.9480%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [120/225], Training Accuracy: 30.9766%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [121/225], Training Accuracy: 30.9530%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [122/225], Training Accuracy: 30.9298%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [123/225], Training Accuracy: 30.9832%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [124/225], Training Accuracy: 30.9602%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [125/225], Training Accuracy: 30.9500%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [126/225], Training Accuracy: 30.8904%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [127/225], Training Accuracy: 30.8194%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [128/225], Training Accuracy: 30.7983%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [129/225], Training Accuracy: 30.8503%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [130/225], Training Accuracy: 30.7812%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [131/225], Training Accuracy: 30.7371%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [132/225], Training Accuracy: 30.7173%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [133/225], Training Accuracy: 30.7448%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [134/225], Training Accuracy: 30.7952%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [135/225], Training Accuracy: 30.7870%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [136/225], Training Accuracy: 30.8364%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [137/225], Training Accuracy: 30.8622%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [138/225], Training Accuracy: 30.8877%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [139/225], Training Accuracy: 30.8678%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [140/225], Training Accuracy: 30.8705%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [141/225], Training Accuracy: 30.8289%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [142/225], Training Accuracy: 30.8979%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [143/225], Training Accuracy: 30.8785%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [144/225], Training Accuracy: 30.9028%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [145/225], Training Accuracy: 30.9591%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [146/225], Training Accuracy: 31.0039%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [147/225], Training Accuracy: 31.0055%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [148/225], Training Accuracy: 30.9649%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [149/225], Training Accuracy: 31.0193%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [150/225], Training Accuracy: 31.0312%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [151/225], Training Accuracy: 31.0637%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [152/225], Training Accuracy: 31.0650%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [153/225], Training Accuracy: 31.0355%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [154/225], Training Accuracy: 31.0978%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [155/225], Training Accuracy: 31.0887%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [156/225], Training Accuracy: 31.0998%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [157/225], Training Accuracy: 31.0311%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [158/225], Training Accuracy: 30.9929%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [159/225], Training Accuracy: 31.0829%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [160/225], Training Accuracy: 31.0547%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [161/225], Training Accuracy: 31.1141%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [162/225], Training Accuracy: 31.0667%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [163/225], Training Accuracy: 31.1350%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [164/225], Training Accuracy: 31.1166%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [165/225], Training Accuracy: 31.0606%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [166/225], Training Accuracy: 31.0617%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [167/225], Training Accuracy: 31.1097%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [168/225], Training Accuracy: 31.0640%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [169/225], Training Accuracy: 30.9819%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [170/225], Training Accuracy: 30.9283%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [171/225], Training Accuracy: 30.9485%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [172/225], Training Accuracy: 30.9684%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [173/225], Training Accuracy: 30.9790%, Training Loss: 0.6866%\n",
      "Epoch [36/100], Step [174/225], Training Accuracy: 31.0255%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [175/225], Training Accuracy: 31.0536%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [176/225], Training Accuracy: 31.0991%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [177/225], Training Accuracy: 31.1088%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [178/225], Training Accuracy: 31.1183%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [179/225], Training Accuracy: 31.0929%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [180/225], Training Accuracy: 31.1024%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [181/225], Training Accuracy: 31.0687%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [182/225], Training Accuracy: 31.0697%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [183/225], Training Accuracy: 31.0792%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [184/225], Training Accuracy: 31.0632%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [185/225], Training Accuracy: 31.0220%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [186/225], Training Accuracy: 31.0736%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [187/225], Training Accuracy: 31.0662%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [188/225], Training Accuracy: 31.0672%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [189/225], Training Accuracy: 31.0929%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [190/225], Training Accuracy: 31.0526%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [191/225], Training Accuracy: 31.0291%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [192/225], Training Accuracy: 30.9814%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [193/225], Training Accuracy: 30.9990%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [194/225], Training Accuracy: 30.9923%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [195/225], Training Accuracy: 30.9696%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [196/225], Training Accuracy: 30.9550%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [197/225], Training Accuracy: 30.9883%, Training Loss: 0.6865%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Step [198/225], Training Accuracy: 31.0133%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [199/225], Training Accuracy: 31.0144%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [200/225], Training Accuracy: 30.9922%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [201/225], Training Accuracy: 31.0012%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [202/225], Training Accuracy: 31.0025%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [203/225], Training Accuracy: 30.9806%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [204/225], Training Accuracy: 31.0355%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [205/225], Training Accuracy: 31.0671%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [206/225], Training Accuracy: 31.0604%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [207/225], Training Accuracy: 31.0311%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [208/225], Training Accuracy: 31.0622%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [209/225], Training Accuracy: 31.1080%, Training Loss: 0.6865%\n",
      "Epoch [36/100], Step [210/225], Training Accuracy: 31.1235%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [211/225], Training Accuracy: 31.1093%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [212/225], Training Accuracy: 31.1689%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [213/225], Training Accuracy: 31.1473%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [214/225], Training Accuracy: 31.1770%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [215/225], Training Accuracy: 31.1483%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [216/225], Training Accuracy: 31.0764%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [217/225], Training Accuracy: 31.0844%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [218/225], Training Accuracy: 31.0493%, Training Loss: 0.6864%\n",
      "Epoch [36/100], Step [219/225], Training Accuracy: 31.1144%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [220/225], Training Accuracy: 31.1435%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [221/225], Training Accuracy: 31.1298%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [222/225], Training Accuracy: 31.1444%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [223/225], Training Accuracy: 31.1799%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [224/225], Training Accuracy: 31.1802%, Training Loss: 0.6863%\n",
      "Epoch [36/100], Step [225/225], Training Accuracy: 31.1631%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6889%\n",
      "Epoch [37/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [3/225], Training Accuracy: 30.2083%, Training Loss: 0.6887%\n",
      "Epoch [37/100], Step [4/225], Training Accuracy: 30.8594%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [5/225], Training Accuracy: 31.5625%, Training Loss: 0.6866%\n",
      "Epoch [37/100], Step [6/225], Training Accuracy: 30.7292%, Training Loss: 0.6869%\n",
      "Epoch [37/100], Step [7/225], Training Accuracy: 30.8036%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [8/225], Training Accuracy: 30.6641%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [9/225], Training Accuracy: 30.5556%, Training Loss: 0.6867%\n",
      "Epoch [37/100], Step [10/225], Training Accuracy: 30.3125%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [12/225], Training Accuracy: 29.5573%, Training Loss: 0.6857%\n",
      "Epoch [37/100], Step [13/225], Training Accuracy: 29.4471%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [14/225], Training Accuracy: 29.4643%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [15/225], Training Accuracy: 29.7917%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [16/225], Training Accuracy: 29.6875%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [17/225], Training Accuracy: 29.6875%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [18/225], Training Accuracy: 29.7743%, Training Loss: 0.6866%\n",
      "Epoch [37/100], Step [19/225], Training Accuracy: 30.0987%, Training Loss: 0.6866%\n",
      "Epoch [37/100], Step [20/225], Training Accuracy: 30.6250%, Training Loss: 0.6867%\n",
      "Epoch [37/100], Step [21/225], Training Accuracy: 30.5804%, Training Loss: 0.6865%\n",
      "Epoch [37/100], Step [22/225], Training Accuracy: 30.8949%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [23/225], Training Accuracy: 30.9103%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [24/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [25/225], Training Accuracy: 31.4375%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [26/225], Training Accuracy: 31.8510%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6857%\n",
      "Epoch [37/100], Step [29/225], Training Accuracy: 31.9504%, Training Loss: 0.6854%\n",
      "Epoch [37/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6854%\n",
      "Epoch [37/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [32/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [33/225], Training Accuracy: 32.2443%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [34/225], Training Accuracy: 32.0772%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [35/225], Training Accuracy: 31.9643%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [36/225], Training Accuracy: 31.8576%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [37/225], Training Accuracy: 31.8412%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [39/225], Training Accuracy: 31.3702%, Training Loss: 0.6854%\n",
      "Epoch [37/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6855%\n",
      "Epoch [37/100], Step [41/225], Training Accuracy: 31.5168%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [42/225], Training Accuracy: 31.5104%, Training Loss: 0.6857%\n",
      "Epoch [37/100], Step [43/225], Training Accuracy: 31.7224%, Training Loss: 0.6857%\n",
      "Epoch [37/100], Step [44/225], Training Accuracy: 31.7827%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [45/225], Training Accuracy: 31.8750%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [46/225], Training Accuracy: 31.7935%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [47/225], Training Accuracy: 31.7154%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [48/225], Training Accuracy: 31.8034%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [49/225], Training Accuracy: 31.8559%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [50/225], Training Accuracy: 31.8750%, Training Loss: 0.6854%\n",
      "Epoch [37/100], Step [51/225], Training Accuracy: 32.0466%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [52/225], Training Accuracy: 32.0613%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [53/225], Training Accuracy: 31.9870%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [54/225], Training Accuracy: 31.7998%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [55/225], Training Accuracy: 31.8750%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [56/225], Training Accuracy: 32.0033%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [57/225], Training Accuracy: 31.9901%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [58/225], Training Accuracy: 31.8427%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [59/225], Training Accuracy: 32.2034%, Training Loss: 0.6848%\n",
      "Epoch [37/100], Step [60/225], Training Accuracy: 32.3177%, Training Loss: 0.6848%\n",
      "Epoch [37/100], Step [61/225], Training Accuracy: 32.2746%, Training Loss: 0.6848%\n",
      "Epoch [37/100], Step [62/225], Training Accuracy: 32.3085%, Training Loss: 0.6848%\n",
      "Epoch [37/100], Step [63/225], Training Accuracy: 32.3661%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [64/225], Training Accuracy: 32.3486%, Training Loss: 0.6848%\n",
      "Epoch [37/100], Step [65/225], Training Accuracy: 32.2356%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [66/225], Training Accuracy: 32.3153%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [67/225], Training Accuracy: 32.3228%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [68/225], Training Accuracy: 32.3989%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [69/225], Training Accuracy: 32.3596%, Training Loss: 0.6848%\n",
      "Epoch [37/100], Step [70/225], Training Accuracy: 32.3214%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [71/225], Training Accuracy: 32.3063%, Training Loss: 0.6849%\n",
      "Epoch [37/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Step [73/225], Training Accuracy: 31.9777%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [74/225], Training Accuracy: 32.0735%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [75/225], Training Accuracy: 31.9375%, Training Loss: 0.6850%\n",
      "Epoch [37/100], Step [76/225], Training Accuracy: 31.8873%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [77/225], Training Accuracy: 31.8182%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [78/225], Training Accuracy: 31.8510%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [79/225], Training Accuracy: 31.7840%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [80/225], Training Accuracy: 31.7578%, Training Loss: 0.6851%\n",
      "Epoch [37/100], Step [81/225], Training Accuracy: 31.6358%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [82/225], Training Accuracy: 31.6502%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [83/225], Training Accuracy: 31.6077%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [84/225], Training Accuracy: 31.6778%, Training Loss: 0.6852%\n",
      "Epoch [37/100], Step [85/225], Training Accuracy: 31.6544%, Training Loss: 0.6853%\n",
      "Epoch [37/100], Step [86/225], Training Accuracy: 31.6679%, Training Loss: 0.6854%\n",
      "Epoch [37/100], Step [87/225], Training Accuracy: 31.6631%, Training Loss: 0.6854%\n",
      "Epoch [37/100], Step [88/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [37/100], Step [89/225], Training Accuracy: 31.5836%, Training Loss: 0.6855%\n",
      "Epoch [37/100], Step [90/225], Training Accuracy: 31.4757%, Training Loss: 0.6855%\n",
      "Epoch [37/100], Step [91/225], Training Accuracy: 31.5591%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [92/225], Training Accuracy: 31.5048%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [93/225], Training Accuracy: 31.5020%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [94/225], Training Accuracy: 31.5991%, Training Loss: 0.6856%\n",
      "Epoch [37/100], Step [95/225], Training Accuracy: 31.4803%, Training Loss: 0.6857%\n",
      "Epoch [37/100], Step [96/225], Training Accuracy: 31.5430%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [97/225], Training Accuracy: 31.5561%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [98/225], Training Accuracy: 31.6008%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [99/225], Training Accuracy: 31.7235%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [100/225], Training Accuracy: 31.7031%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [101/225], Training Accuracy: 31.8379%, Training Loss: 0.6857%\n",
      "Epoch [37/100], Step [102/225], Training Accuracy: 31.7096%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [103/225], Training Accuracy: 31.7506%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [104/225], Training Accuracy: 31.7157%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [105/225], Training Accuracy: 31.6815%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [106/225], Training Accuracy: 31.6480%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [107/225], Training Accuracy: 31.5275%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [108/225], Training Accuracy: 31.6117%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [109/225], Training Accuracy: 31.5080%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [110/225], Training Accuracy: 31.5341%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [111/225], Training Accuracy: 31.4048%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [112/225], Training Accuracy: 31.4872%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [113/225], Training Accuracy: 31.4989%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [114/225], Training Accuracy: 31.5789%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [115/225], Training Accuracy: 31.5625%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [116/225], Training Accuracy: 31.5463%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [117/225], Training Accuracy: 31.5037%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [118/225], Training Accuracy: 31.4486%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [119/225], Training Accuracy: 31.4076%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [120/225], Training Accuracy: 31.4323%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [121/225], Training Accuracy: 31.3920%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [122/225], Training Accuracy: 31.4037%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [123/225], Training Accuracy: 31.4151%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [124/225], Training Accuracy: 31.3886%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [125/225], Training Accuracy: 31.3625%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [126/225], Training Accuracy: 31.3244%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [127/225], Training Accuracy: 31.2746%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [128/225], Training Accuracy: 31.2744%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [129/225], Training Accuracy: 31.2984%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [131/225], Training Accuracy: 31.2142%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [132/225], Training Accuracy: 31.2027%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [133/225], Training Accuracy: 31.2265%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [134/225], Training Accuracy: 31.2733%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [135/225], Training Accuracy: 31.2731%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [136/225], Training Accuracy: 31.3074%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [137/225], Training Accuracy: 31.3298%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [138/225], Training Accuracy: 31.3406%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [139/225], Training Accuracy: 31.3062%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [140/225], Training Accuracy: 31.2612%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [141/225], Training Accuracy: 31.2168%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [142/225], Training Accuracy: 31.2610%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [143/225], Training Accuracy: 31.2391%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [144/225], Training Accuracy: 31.2717%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [145/225], Training Accuracy: 31.3470%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [146/225], Training Accuracy: 31.3784%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [147/225], Training Accuracy: 31.3882%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [148/225], Training Accuracy: 31.3872%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [149/225], Training Accuracy: 31.4073%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [150/225], Training Accuracy: 31.4271%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [151/225], Training Accuracy: 31.4673%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [152/225], Training Accuracy: 31.4762%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [153/225], Training Accuracy: 31.4338%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [154/225], Training Accuracy: 31.4529%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [155/225], Training Accuracy: 31.4315%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [156/225], Training Accuracy: 31.4704%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [157/225], Training Accuracy: 31.4092%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [158/225], Training Accuracy: 31.3786%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [159/225], Training Accuracy: 31.4564%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [160/225], Training Accuracy: 31.4258%, Training Loss: 0.6863%\n",
      "Epoch [37/100], Step [161/225], Training Accuracy: 31.4732%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [162/225], Training Accuracy: 31.4525%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [163/225], Training Accuracy: 31.5376%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [164/225], Training Accuracy: 31.5358%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [165/225], Training Accuracy: 31.4583%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [166/225], Training Accuracy: 31.4759%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [167/225], Training Accuracy: 31.5213%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [168/225], Training Accuracy: 31.4825%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [169/225], Training Accuracy: 31.3887%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [170/225], Training Accuracy: 31.3419%, Training Loss: 0.6862%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Step [171/225], Training Accuracy: 31.3505%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [172/225], Training Accuracy: 31.3681%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [173/225], Training Accuracy: 31.3493%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [174/225], Training Accuracy: 31.3757%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [175/225], Training Accuracy: 31.4107%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [176/225], Training Accuracy: 31.4187%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [177/225], Training Accuracy: 31.4177%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [178/225], Training Accuracy: 31.4343%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [179/225], Training Accuracy: 31.4071%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [180/225], Training Accuracy: 31.4236%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [181/225], Training Accuracy: 31.3709%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [182/225], Training Accuracy: 31.3702%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [183/225], Training Accuracy: 31.3781%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [184/225], Training Accuracy: 31.3519%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [185/225], Training Accuracy: 31.3007%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [186/225], Training Accuracy: 31.3256%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [187/225], Training Accuracy: 31.3252%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [188/225], Training Accuracy: 31.3248%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [189/225], Training Accuracy: 31.3575%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [190/225], Training Accuracy: 31.3158%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [191/225], Training Accuracy: 31.2909%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [192/225], Training Accuracy: 31.2337%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [193/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [37/100], Step [194/225], Training Accuracy: 31.2581%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [195/225], Training Accuracy: 31.2420%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [196/225], Training Accuracy: 31.2181%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [197/225], Training Accuracy: 31.2738%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [198/225], Training Accuracy: 31.2895%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [199/225], Training Accuracy: 31.2657%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [200/225], Training Accuracy: 31.2422%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [201/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [202/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [203/225], Training Accuracy: 31.2577%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [204/225], Training Accuracy: 31.3343%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [205/225], Training Accuracy: 31.3262%, Training Loss: 0.6861%\n",
      "Epoch [37/100], Step [206/225], Training Accuracy: 31.3183%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [207/225], Training Accuracy: 31.3104%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [208/225], Training Accuracy: 31.3401%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [209/225], Training Accuracy: 31.3771%, Training Loss: 0.6860%\n",
      "Epoch [37/100], Step [210/225], Training Accuracy: 31.3839%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [211/225], Training Accuracy: 31.3833%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [212/225], Training Accuracy: 31.4416%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [213/225], Training Accuracy: 31.4114%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [214/225], Training Accuracy: 31.4471%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [215/225], Training Accuracy: 31.4099%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [216/225], Training Accuracy: 31.3513%, Training Loss: 0.6859%\n",
      "Epoch [37/100], Step [217/225], Training Accuracy: 31.3436%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [218/225], Training Accuracy: 31.3002%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [219/225], Training Accuracy: 31.3570%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [220/225], Training Accuracy: 31.3707%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [221/225], Training Accuracy: 31.3631%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [222/225], Training Accuracy: 31.3767%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [223/225], Training Accuracy: 31.3971%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [224/225], Training Accuracy: 31.3895%, Training Loss: 0.6858%\n",
      "Epoch [37/100], Step [225/225], Training Accuracy: 31.3716%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6863%\n",
      "Epoch [38/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6845%\n",
      "Epoch [38/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6874%\n",
      "Epoch [38/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6845%\n",
      "Epoch [38/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [7/225], Training Accuracy: 31.2500%, Training Loss: 0.6838%\n",
      "Epoch [38/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6837%\n",
      "Epoch [38/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6848%\n",
      "Epoch [38/100], Step [10/225], Training Accuracy: 30.3125%, Training Loss: 0.6845%\n",
      "Epoch [38/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6846%\n",
      "Epoch [38/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6844%\n",
      "Epoch [38/100], Step [13/225], Training Accuracy: 29.5673%, Training Loss: 0.6847%\n",
      "Epoch [38/100], Step [14/225], Training Accuracy: 29.6875%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [15/225], Training Accuracy: 30.0000%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [16/225], Training Accuracy: 29.9805%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [17/225], Training Accuracy: 29.5956%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [18/225], Training Accuracy: 29.5139%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [19/225], Training Accuracy: 30.0164%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [20/225], Training Accuracy: 30.4688%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [21/225], Training Accuracy: 30.5804%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [22/225], Training Accuracy: 30.7528%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [23/225], Training Accuracy: 30.7065%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [24/225], Training Accuracy: 30.7292%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [25/225], Training Accuracy: 31.0000%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [26/225], Training Accuracy: 31.3702%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [27/225], Training Accuracy: 31.0764%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [28/225], Training Accuracy: 30.8594%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [29/225], Training Accuracy: 31.1422%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [30/225], Training Accuracy: 31.1979%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [31/225], Training Accuracy: 31.0988%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [32/225], Training Accuracy: 31.2988%, Training Loss: 0.6848%\n",
      "Epoch [38/100], Step [33/225], Training Accuracy: 31.3920%, Training Loss: 0.6847%\n",
      "Epoch [38/100], Step [34/225], Training Accuracy: 31.1121%, Training Loss: 0.6846%\n",
      "Epoch [38/100], Step [35/225], Training Accuracy: 31.0268%, Training Loss: 0.6845%\n",
      "Epoch [38/100], Step [36/225], Training Accuracy: 30.9028%, Training Loss: 0.6846%\n",
      "Epoch [38/100], Step [37/225], Training Accuracy: 30.9122%, Training Loss: 0.6847%\n",
      "Epoch [38/100], Step [38/225], Training Accuracy: 30.7155%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [39/225], Training Accuracy: 30.4888%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [40/225], Training Accuracy: 30.5078%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [41/225], Training Accuracy: 30.6402%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [42/225], Training Accuracy: 30.4688%, Training Loss: 0.6854%\n",
      "Epoch [38/100], Step [43/225], Training Accuracy: 30.6323%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [44/225], Training Accuracy: 30.6108%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [45/225], Training Accuracy: 30.6944%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Step [46/225], Training Accuracy: 30.6046%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [47/225], Training Accuracy: 30.6516%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [48/225], Training Accuracy: 30.7943%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [49/225], Training Accuracy: 30.8355%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [50/225], Training Accuracy: 30.8125%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [51/225], Training Accuracy: 31.0355%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [52/225], Training Accuracy: 31.0697%, Training Loss: 0.6854%\n",
      "Epoch [38/100], Step [53/225], Training Accuracy: 31.0142%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [54/225], Training Accuracy: 30.9028%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [55/225], Training Accuracy: 31.0511%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [56/225], Training Accuracy: 31.1105%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [57/225], Training Accuracy: 31.1404%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [58/225], Training Accuracy: 31.0075%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [59/225], Training Accuracy: 31.3824%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [60/225], Training Accuracy: 31.5104%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [61/225], Training Accuracy: 31.4037%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [62/225], Training Accuracy: 31.4516%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [63/225], Training Accuracy: 31.5476%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [64/225], Training Accuracy: 31.5186%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [65/225], Training Accuracy: 31.4423%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [66/225], Training Accuracy: 31.6051%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [67/225], Training Accuracy: 31.5998%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [68/225], Training Accuracy: 31.6636%, Training Loss: 0.6848%\n",
      "Epoch [38/100], Step [69/225], Training Accuracy: 31.7029%, Training Loss: 0.6847%\n",
      "Epoch [38/100], Step [70/225], Training Accuracy: 31.6295%, Training Loss: 0.6847%\n",
      "Epoch [38/100], Step [71/225], Training Accuracy: 31.6901%, Training Loss: 0.6847%\n",
      "Epoch [38/100], Step [72/225], Training Accuracy: 31.4887%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [73/225], Training Accuracy: 31.3998%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [74/225], Training Accuracy: 31.4823%, Training Loss: 0.6848%\n",
      "Epoch [38/100], Step [75/225], Training Accuracy: 31.4375%, Training Loss: 0.6848%\n",
      "Epoch [38/100], Step [76/225], Training Accuracy: 31.3939%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [77/225], Training Accuracy: 31.3109%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [78/225], Training Accuracy: 31.3502%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [79/225], Training Accuracy: 31.2896%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [80/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [81/225], Training Accuracy: 31.1535%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [82/225], Training Accuracy: 31.1738%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [83/225], Training Accuracy: 31.0994%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [84/225], Training Accuracy: 31.1756%, Training Loss: 0.6849%\n",
      "Epoch [38/100], Step [85/225], Training Accuracy: 31.1765%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [86/225], Training Accuracy: 31.2318%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [87/225], Training Accuracy: 31.2680%, Training Loss: 0.6850%\n",
      "Epoch [38/100], Step [88/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [90/225], Training Accuracy: 31.0938%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [91/225], Training Accuracy: 31.1298%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [92/225], Training Accuracy: 31.0971%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [93/225], Training Accuracy: 31.0316%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [94/225], Training Accuracy: 31.1336%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [95/225], Training Accuracy: 31.0197%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [96/225], Training Accuracy: 31.1361%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [97/225], Training Accuracy: 31.1534%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [98/225], Training Accuracy: 31.1862%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [99/225], Training Accuracy: 31.2973%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [100/225], Training Accuracy: 31.2656%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [101/225], Training Accuracy: 31.4202%, Training Loss: 0.6851%\n",
      "Epoch [38/100], Step [102/225], Training Accuracy: 31.2960%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [103/225], Training Accuracy: 31.3258%, Training Loss: 0.6852%\n",
      "Epoch [38/100], Step [104/225], Training Accuracy: 31.3101%, Training Loss: 0.6853%\n",
      "Epoch [38/100], Step [105/225], Training Accuracy: 31.2798%, Training Loss: 0.6854%\n",
      "Epoch [38/100], Step [106/225], Training Accuracy: 31.2353%, Training Loss: 0.6854%\n",
      "Epoch [38/100], Step [107/225], Training Accuracy: 31.1186%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [108/225], Training Accuracy: 31.1632%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [109/225], Training Accuracy: 31.0350%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [110/225], Training Accuracy: 31.0369%, Training Loss: 0.6855%\n",
      "Epoch [38/100], Step [111/225], Training Accuracy: 30.9262%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [112/225], Training Accuracy: 30.9849%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [113/225], Training Accuracy: 30.9458%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [114/225], Training Accuracy: 31.0170%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [115/225], Training Accuracy: 30.9918%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [116/225], Training Accuracy: 30.9941%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [117/225], Training Accuracy: 30.9295%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [118/225], Training Accuracy: 30.9057%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [119/225], Training Accuracy: 30.8036%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [120/225], Training Accuracy: 30.8333%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [121/225], Training Accuracy: 30.8239%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [122/225], Training Accuracy: 30.8402%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [123/225], Training Accuracy: 30.8562%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [124/225], Training Accuracy: 30.8594%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [125/225], Training Accuracy: 30.8500%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [126/225], Training Accuracy: 30.8036%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [127/225], Training Accuracy: 30.7456%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [128/225], Training Accuracy: 30.7495%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [129/225], Training Accuracy: 30.8018%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [130/225], Training Accuracy: 30.7692%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [131/225], Training Accuracy: 30.7490%, Training Loss: 0.6860%\n",
      "Epoch [38/100], Step [132/225], Training Accuracy: 30.7292%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [133/225], Training Accuracy: 30.7331%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [134/225], Training Accuracy: 30.8069%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [135/225], Training Accuracy: 30.7986%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [136/225], Training Accuracy: 30.8019%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [137/225], Training Accuracy: 30.8394%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [138/225], Training Accuracy: 30.8424%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [139/225], Training Accuracy: 30.8004%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [140/225], Training Accuracy: 30.7701%, Training Loss: 0.6860%\n",
      "Epoch [38/100], Step [141/225], Training Accuracy: 30.7292%, Training Loss: 0.6860%\n",
      "Epoch [38/100], Step [142/225], Training Accuracy: 30.7768%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [143/225], Training Accuracy: 30.7692%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [144/225], Training Accuracy: 30.7943%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Step [145/225], Training Accuracy: 30.8621%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [146/225], Training Accuracy: 30.9075%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [147/225], Training Accuracy: 30.8992%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [148/225], Training Accuracy: 30.8805%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [149/225], Training Accuracy: 30.9144%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [150/225], Training Accuracy: 30.9479%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [151/225], Training Accuracy: 30.9913%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [152/225], Training Accuracy: 30.9930%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [153/225], Training Accuracy: 30.9641%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [154/225], Training Accuracy: 31.0065%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [155/225], Training Accuracy: 30.9980%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [156/225], Training Accuracy: 31.0196%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [157/225], Training Accuracy: 30.9514%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [158/225], Training Accuracy: 30.9039%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [159/225], Training Accuracy: 30.9847%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [160/225], Training Accuracy: 30.9473%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [161/225], Training Accuracy: 30.9880%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [162/225], Training Accuracy: 30.9510%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [163/225], Training Accuracy: 31.0008%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [164/225], Training Accuracy: 30.9832%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [165/225], Training Accuracy: 30.9091%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [166/225], Training Accuracy: 30.9111%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [167/225], Training Accuracy: 30.9412%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [168/225], Training Accuracy: 30.8873%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [169/225], Training Accuracy: 30.8247%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [170/225], Training Accuracy: 30.7721%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [171/225], Training Accuracy: 30.7840%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [172/225], Training Accuracy: 30.7958%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [173/225], Training Accuracy: 30.8074%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [174/225], Training Accuracy: 30.8010%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [175/225], Training Accuracy: 30.8661%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [176/225], Training Accuracy: 30.8860%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [177/225], Training Accuracy: 30.9234%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [178/225], Training Accuracy: 30.9077%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [179/225], Training Accuracy: 30.8921%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [180/225], Training Accuracy: 30.9288%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [181/225], Training Accuracy: 30.8788%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [182/225], Training Accuracy: 30.8723%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [183/225], Training Accuracy: 30.8829%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [184/225], Training Accuracy: 30.8594%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [185/225], Training Accuracy: 30.8277%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [186/225], Training Accuracy: 30.8552%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [187/225], Training Accuracy: 30.8656%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [188/225], Training Accuracy: 30.8677%, Training Loss: 0.6859%\n",
      "Epoch [38/100], Step [189/225], Training Accuracy: 30.9028%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [190/225], Training Accuracy: 30.8635%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [191/225], Training Accuracy: 30.8328%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [192/225], Training Accuracy: 30.7699%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [193/225], Training Accuracy: 30.7723%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [194/225], Training Accuracy: 30.7748%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [195/225], Training Accuracy: 30.7532%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [196/225], Training Accuracy: 30.7239%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [197/225], Training Accuracy: 30.7582%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [198/225], Training Accuracy: 30.7765%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [199/225], Training Accuracy: 30.7632%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [200/225], Training Accuracy: 30.7500%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [201/225], Training Accuracy: 30.7758%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [202/225], Training Accuracy: 30.7859%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [203/225], Training Accuracy: 30.7805%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [204/225], Training Accuracy: 30.8287%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [205/225], Training Accuracy: 30.8384%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [206/225], Training Accuracy: 30.8404%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [207/225], Training Accuracy: 30.8197%, Training Loss: 0.6858%\n",
      "Epoch [38/100], Step [208/225], Training Accuracy: 30.8519%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [209/225], Training Accuracy: 30.8911%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [210/225], Training Accuracy: 30.9152%, Training Loss: 0.6857%\n",
      "Epoch [38/100], Step [211/225], Training Accuracy: 30.9168%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [212/225], Training Accuracy: 30.9552%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [213/225], Training Accuracy: 30.9346%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [214/225], Training Accuracy: 30.9725%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [215/225], Training Accuracy: 30.9448%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [216/225], Training Accuracy: 30.8811%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [217/225], Training Accuracy: 30.8756%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [218/225], Training Accuracy: 30.8271%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [219/225], Training Accuracy: 30.8861%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [220/225], Training Accuracy: 30.9091%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [221/225], Training Accuracy: 30.8894%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [222/225], Training Accuracy: 30.9192%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [223/225], Training Accuracy: 30.9557%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [224/225], Training Accuracy: 30.9640%, Training Loss: 0.6856%\n",
      "Epoch [38/100], Step [225/225], Training Accuracy: 30.9477%, Training Loss: 0.6856%\n",
      "Epoch [39/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6909%\n",
      "Epoch [39/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6870%\n",
      "Epoch [39/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6857%\n",
      "Epoch [39/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6856%\n",
      "Epoch [39/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [11/225], Training Accuracy: 31.3920%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [12/225], Training Accuracy: 30.5990%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6857%\n",
      "Epoch [39/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6860%\n",
      "Epoch [39/100], Step [15/225], Training Accuracy: 31.3542%, Training Loss: 0.6859%\n",
      "Epoch [39/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6856%\n",
      "Epoch [39/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6858%\n",
      "Epoch [39/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6859%\n",
      "Epoch [39/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6861%\n",
      "Epoch [39/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6858%\n",
      "Epoch [39/100], Step [21/225], Training Accuracy: 31.6964%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [22/225], Training Accuracy: 31.8182%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [23/225], Training Accuracy: 31.5897%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [25/225], Training Accuracy: 31.9375%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [26/225], Training Accuracy: 32.1514%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [27/225], Training Accuracy: 31.8866%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Step [28/225], Training Accuracy: 31.6964%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [31/225], Training Accuracy: 31.9052%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [33/225], Training Accuracy: 32.3390%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [35/225], Training Accuracy: 32.1429%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [36/225], Training Accuracy: 31.9444%, Training Loss: 0.6845%\n",
      "Epoch [39/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [38/225], Training Accuracy: 31.8257%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [39/225], Training Accuracy: 31.5304%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [40/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [43/225], Training Accuracy: 31.6860%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [44/225], Training Accuracy: 31.6761%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [45/225], Training Accuracy: 31.6667%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [46/225], Training Accuracy: 31.5557%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [47/225], Training Accuracy: 31.5160%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [48/225], Training Accuracy: 31.6081%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [49/225], Training Accuracy: 31.6964%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [50/225], Training Accuracy: 31.6250%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [51/225], Training Accuracy: 31.8015%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [52/225], Training Accuracy: 31.7608%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [53/225], Training Accuracy: 31.7217%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [54/225], Training Accuracy: 31.6262%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [55/225], Training Accuracy: 31.7614%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [56/225], Training Accuracy: 31.8359%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [57/225], Training Accuracy: 31.8257%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [58/225], Training Accuracy: 31.6541%, Training Loss: 0.6846%\n",
      "Epoch [39/100], Step [59/225], Training Accuracy: 32.0180%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [60/225], Training Accuracy: 32.0833%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [61/225], Training Accuracy: 32.0184%, Training Loss: 0.6842%\n",
      "Epoch [39/100], Step [62/225], Training Accuracy: 31.9808%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [63/225], Training Accuracy: 31.9940%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [64/225], Training Accuracy: 31.9580%, Training Loss: 0.6842%\n",
      "Epoch [39/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [68/225], Training Accuracy: 31.9853%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [69/225], Training Accuracy: 31.9746%, Training Loss: 0.6841%\n",
      "Epoch [39/100], Step [70/225], Training Accuracy: 31.9420%, Training Loss: 0.6841%\n",
      "Epoch [39/100], Step [71/225], Training Accuracy: 31.9102%, Training Loss: 0.6841%\n",
      "Epoch [39/100], Step [72/225], Training Accuracy: 31.6840%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [73/225], Training Accuracy: 31.6353%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [74/225], Training Accuracy: 31.7145%, Training Loss: 0.6842%\n",
      "Epoch [39/100], Step [75/225], Training Accuracy: 31.6875%, Training Loss: 0.6841%\n",
      "Epoch [39/100], Step [76/225], Training Accuracy: 31.6201%, Training Loss: 0.6842%\n",
      "Epoch [39/100], Step [77/225], Training Accuracy: 31.5341%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [78/225], Training Accuracy: 31.5905%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [79/225], Training Accuracy: 31.4873%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [80/225], Training Accuracy: 31.4648%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [81/225], Training Accuracy: 31.3850%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [82/225], Training Accuracy: 31.4024%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [83/225], Training Accuracy: 31.3441%, Training Loss: 0.6843%\n",
      "Epoch [39/100], Step [84/225], Training Accuracy: 31.3802%, Training Loss: 0.6844%\n",
      "Epoch [39/100], Step [85/225], Training Accuracy: 31.3971%, Training Loss: 0.6845%\n",
      "Epoch [39/100], Step [86/225], Training Accuracy: 31.4135%, Training Loss: 0.6845%\n",
      "Epoch [39/100], Step [87/225], Training Accuracy: 31.4116%, Training Loss: 0.6845%\n",
      "Epoch [39/100], Step [88/225], Training Accuracy: 31.3743%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [89/225], Training Accuracy: 31.2851%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [90/225], Training Accuracy: 31.1979%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [92/225], Training Accuracy: 31.1651%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [93/225], Training Accuracy: 31.1492%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [94/225], Training Accuracy: 31.2334%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [95/225], Training Accuracy: 31.1020%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [96/225], Training Accuracy: 31.1686%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [97/225], Training Accuracy: 31.1856%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [98/225], Training Accuracy: 31.2181%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [99/225], Training Accuracy: 31.3447%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [100/225], Training Accuracy: 31.3125%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [101/225], Training Accuracy: 31.4666%, Training Loss: 0.6847%\n",
      "Epoch [39/100], Step [102/225], Training Accuracy: 31.3572%, Training Loss: 0.6848%\n",
      "Epoch [39/100], Step [103/225], Training Accuracy: 31.4169%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [104/225], Training Accuracy: 31.3852%, Training Loss: 0.6849%\n",
      "Epoch [39/100], Step [105/225], Training Accuracy: 31.3244%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [106/225], Training Accuracy: 31.2795%, Training Loss: 0.6850%\n",
      "Epoch [39/100], Step [107/225], Training Accuracy: 31.1916%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [108/225], Training Accuracy: 31.2789%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [109/225], Training Accuracy: 31.1783%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [110/225], Training Accuracy: 31.1790%, Training Loss: 0.6851%\n",
      "Epoch [39/100], Step [111/225], Training Accuracy: 31.0670%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [112/225], Training Accuracy: 31.1244%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [113/225], Training Accuracy: 31.0841%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [114/225], Training Accuracy: 31.1404%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [115/225], Training Accuracy: 31.1141%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [116/225], Training Accuracy: 31.1153%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [117/225], Training Accuracy: 31.0764%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [118/225], Training Accuracy: 31.0514%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [119/225], Training Accuracy: 30.9874%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [120/225], Training Accuracy: 31.0417%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [121/225], Training Accuracy: 31.0563%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [122/225], Training Accuracy: 31.0579%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [123/225], Training Accuracy: 31.0849%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [124/225], Training Accuracy: 31.0862%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [125/225], Training Accuracy: 31.0750%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [126/225], Training Accuracy: 31.0144%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [127/225], Training Accuracy: 30.9670%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [128/225], Training Accuracy: 30.9570%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [129/225], Training Accuracy: 30.9835%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [130/225], Training Accuracy: 30.9135%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Step [131/225], Training Accuracy: 30.8683%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [132/225], Training Accuracy: 30.8120%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [133/225], Training Accuracy: 30.8506%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [134/225], Training Accuracy: 30.9118%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [135/225], Training Accuracy: 30.9259%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [136/225], Training Accuracy: 30.9513%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [137/225], Training Accuracy: 30.9649%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [138/225], Training Accuracy: 31.0009%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [139/225], Training Accuracy: 30.9577%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [140/225], Training Accuracy: 30.9598%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [141/225], Training Accuracy: 30.9065%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [142/225], Training Accuracy: 30.9529%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [143/225], Training Accuracy: 30.9441%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [144/225], Training Accuracy: 30.9787%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [145/225], Training Accuracy: 31.0453%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [146/225], Training Accuracy: 31.0788%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [147/225], Training Accuracy: 31.1012%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [148/225], Training Accuracy: 31.0705%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [149/225], Training Accuracy: 31.1032%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [150/225], Training Accuracy: 31.1250%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [151/225], Training Accuracy: 31.1465%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [152/225], Training Accuracy: 31.1678%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [153/225], Training Accuracy: 31.1275%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [154/225], Training Accuracy: 31.1485%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [155/225], Training Accuracy: 31.1190%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [156/225], Training Accuracy: 31.1398%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [157/225], Training Accuracy: 31.0808%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [158/225], Training Accuracy: 31.0522%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [159/225], Training Accuracy: 31.1222%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [160/225], Training Accuracy: 31.1035%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [161/225], Training Accuracy: 31.1335%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [162/225], Training Accuracy: 31.1053%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [163/225], Training Accuracy: 31.1637%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [164/225], Training Accuracy: 31.1547%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [165/225], Training Accuracy: 31.0985%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [166/225], Training Accuracy: 31.0994%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [167/225], Training Accuracy: 31.1377%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [168/225], Training Accuracy: 31.0919%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [169/225], Training Accuracy: 31.0189%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [170/225], Training Accuracy: 30.9651%, Training Loss: 0.6855%\n",
      "Epoch [39/100], Step [171/225], Training Accuracy: 30.9667%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [172/225], Training Accuracy: 30.9866%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [173/225], Training Accuracy: 30.9700%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [174/225], Training Accuracy: 30.9716%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [175/225], Training Accuracy: 31.0089%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [176/225], Training Accuracy: 31.0281%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [177/225], Training Accuracy: 31.0381%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [178/225], Training Accuracy: 31.0657%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [179/225], Training Accuracy: 31.0230%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [180/225], Training Accuracy: 31.0677%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [181/225], Training Accuracy: 31.0169%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [182/225], Training Accuracy: 31.0096%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [183/225], Training Accuracy: 31.0195%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [184/225], Training Accuracy: 30.9783%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [185/225], Training Accuracy: 30.9375%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [186/225], Training Accuracy: 30.9644%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [187/225], Training Accuracy: 30.9408%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [188/225], Training Accuracy: 30.9259%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [189/225], Training Accuracy: 30.9606%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [190/225], Training Accuracy: 30.9211%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [191/225], Training Accuracy: 30.9064%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [192/225], Training Accuracy: 30.8594%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [193/225], Training Accuracy: 30.8533%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [194/225], Training Accuracy: 30.8634%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [195/225], Training Accuracy: 30.8413%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [196/225], Training Accuracy: 30.8115%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [197/225], Training Accuracy: 30.8534%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [198/225], Training Accuracy: 30.8712%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [199/225], Training Accuracy: 30.8496%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [200/225], Training Accuracy: 30.8359%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [201/225], Training Accuracy: 30.8458%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [202/225], Training Accuracy: 30.8478%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [203/225], Training Accuracy: 30.8344%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [204/225], Training Accuracy: 30.8900%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [205/225], Training Accuracy: 30.8994%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [206/225], Training Accuracy: 30.8859%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [207/225], Training Accuracy: 30.8575%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [208/225], Training Accuracy: 30.8744%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [209/225], Training Accuracy: 30.9136%, Training Loss: 0.6854%\n",
      "Epoch [39/100], Step [210/225], Training Accuracy: 30.9375%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [211/225], Training Accuracy: 30.9316%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [212/225], Training Accuracy: 30.9699%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [213/225], Training Accuracy: 30.9492%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [214/225], Training Accuracy: 30.9652%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [215/225], Training Accuracy: 30.9375%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [216/225], Training Accuracy: 30.8811%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [217/225], Training Accuracy: 30.8900%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [218/225], Training Accuracy: 30.8415%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [219/225], Training Accuracy: 30.9075%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [220/225], Training Accuracy: 30.9304%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [221/225], Training Accuracy: 30.9106%, Training Loss: 0.6853%\n",
      "Epoch [39/100], Step [222/225], Training Accuracy: 30.9262%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [223/225], Training Accuracy: 30.9697%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [224/225], Training Accuracy: 30.9710%, Training Loss: 0.6852%\n",
      "Epoch [39/100], Step [225/225], Training Accuracy: 30.9547%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6831%\n",
      "Epoch [40/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6813%\n",
      "Epoch [40/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6837%\n",
      "Epoch [40/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6861%\n",
      "Epoch [40/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6864%\n",
      "Epoch [40/100], Step [10/225], Training Accuracy: 30.3125%, Training Loss: 0.6862%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6863%\n",
      "Epoch [40/100], Step [12/225], Training Accuracy: 29.4271%, Training Loss: 0.6861%\n",
      "Epoch [40/100], Step [13/225], Training Accuracy: 29.0865%, Training Loss: 0.6861%\n",
      "Epoch [40/100], Step [14/225], Training Accuracy: 29.5759%, Training Loss: 0.6868%\n",
      "Epoch [40/100], Step [15/225], Training Accuracy: 30.0000%, Training Loss: 0.6867%\n",
      "Epoch [40/100], Step [16/225], Training Accuracy: 29.9805%, Training Loss: 0.6862%\n",
      "Epoch [40/100], Step [17/225], Training Accuracy: 29.8713%, Training Loss: 0.6864%\n",
      "Epoch [40/100], Step [18/225], Training Accuracy: 29.9479%, Training Loss: 0.6866%\n",
      "Epoch [40/100], Step [19/225], Training Accuracy: 30.3454%, Training Loss: 0.6865%\n",
      "Epoch [40/100], Step [20/225], Training Accuracy: 30.8594%, Training Loss: 0.6866%\n",
      "Epoch [40/100], Step [21/225], Training Accuracy: 30.8036%, Training Loss: 0.6864%\n",
      "Epoch [40/100], Step [22/225], Training Accuracy: 31.0369%, Training Loss: 0.6863%\n",
      "Epoch [40/100], Step [23/225], Training Accuracy: 31.0462%, Training Loss: 0.6862%\n",
      "Epoch [40/100], Step [24/225], Training Accuracy: 31.0547%, Training Loss: 0.6863%\n",
      "Epoch [40/100], Step [25/225], Training Accuracy: 31.3750%, Training Loss: 0.6861%\n",
      "Epoch [40/100], Step [26/225], Training Accuracy: 31.8510%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [27/225], Training Accuracy: 31.5972%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [28/225], Training Accuracy: 31.3616%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [29/225], Training Accuracy: 31.6810%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [30/225], Training Accuracy: 31.7188%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [32/225], Training Accuracy: 31.8848%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [33/225], Training Accuracy: 31.9602%, Training Loss: 0.6851%\n",
      "Epoch [40/100], Step [34/225], Training Accuracy: 31.7555%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [35/225], Training Accuracy: 31.6071%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [36/225], Training Accuracy: 31.3802%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [37/225], Training Accuracy: 31.3767%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [38/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [39/225], Training Accuracy: 31.0096%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [40/225], Training Accuracy: 31.1719%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [41/225], Training Accuracy: 31.1738%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [42/225], Training Accuracy: 31.0268%, Training Loss: 0.6861%\n",
      "Epoch [40/100], Step [43/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [45/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [46/225], Training Accuracy: 31.1821%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [47/225], Training Accuracy: 31.0838%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [48/225], Training Accuracy: 31.1849%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [49/225], Training Accuracy: 31.1862%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [50/225], Training Accuracy: 31.1562%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [51/225], Training Accuracy: 31.3419%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [52/225], Training Accuracy: 31.3702%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [53/225], Training Accuracy: 31.2795%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [54/225], Training Accuracy: 31.1921%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [55/225], Training Accuracy: 31.2784%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [56/225], Training Accuracy: 31.3616%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [57/225], Training Accuracy: 31.3871%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [58/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [59/225], Training Accuracy: 31.6208%, Training Loss: 0.6851%\n",
      "Epoch [40/100], Step [60/225], Training Accuracy: 31.7708%, Training Loss: 0.6851%\n",
      "Epoch [40/100], Step [61/225], Training Accuracy: 31.7367%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [62/225], Training Accuracy: 31.6784%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [63/225], Training Accuracy: 31.7460%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [64/225], Training Accuracy: 31.7383%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [65/225], Training Accuracy: 31.6346%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [66/225], Training Accuracy: 31.7472%, Training Loss: 0.6851%\n",
      "Epoch [40/100], Step [67/225], Training Accuracy: 31.6698%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [68/225], Training Accuracy: 31.7325%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [69/225], Training Accuracy: 31.7255%, Training Loss: 0.6849%\n",
      "Epoch [40/100], Step [70/225], Training Accuracy: 31.6741%, Training Loss: 0.6849%\n",
      "Epoch [40/100], Step [71/225], Training Accuracy: 31.7562%, Training Loss: 0.6849%\n",
      "Epoch [40/100], Step [72/225], Training Accuracy: 31.5321%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [73/225], Training Accuracy: 31.4854%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [74/225], Training Accuracy: 31.5456%, Training Loss: 0.6851%\n",
      "Epoch [40/100], Step [75/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [77/225], Training Accuracy: 31.4326%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [78/225], Training Accuracy: 31.4503%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [79/225], Training Accuracy: 31.3884%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [80/225], Training Accuracy: 31.3672%, Training Loss: 0.6849%\n",
      "Epoch [40/100], Step [81/225], Training Accuracy: 31.3079%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [82/225], Training Accuracy: 31.3262%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [83/225], Training Accuracy: 31.2688%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [84/225], Training Accuracy: 31.3802%, Training Loss: 0.6849%\n",
      "Epoch [40/100], Step [85/225], Training Accuracy: 31.3787%, Training Loss: 0.6850%\n",
      "Epoch [40/100], Step [86/225], Training Accuracy: 31.3953%, Training Loss: 0.6851%\n",
      "Epoch [40/100], Step [87/225], Training Accuracy: 31.3757%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [88/225], Training Accuracy: 31.3565%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [89/225], Training Accuracy: 31.2676%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [90/225], Training Accuracy: 31.1806%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [92/225], Training Accuracy: 31.2160%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [93/225], Training Accuracy: 31.2164%, Training Loss: 0.6853%\n",
      "Epoch [40/100], Step [94/225], Training Accuracy: 31.2999%, Training Loss: 0.6852%\n",
      "Epoch [40/100], Step [95/225], Training Accuracy: 31.1678%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [96/225], Training Accuracy: 31.2988%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [97/225], Training Accuracy: 31.3305%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [98/225], Training Accuracy: 31.3616%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [99/225], Training Accuracy: 31.4552%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [100/225], Training Accuracy: 31.4375%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [101/225], Training Accuracy: 31.5903%, Training Loss: 0.6854%\n",
      "Epoch [40/100], Step [102/225], Training Accuracy: 31.4491%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [103/225], Training Accuracy: 31.4775%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [104/225], Training Accuracy: 31.4603%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [105/225], Training Accuracy: 31.4137%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [106/225], Training Accuracy: 31.3974%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [107/225], Training Accuracy: 31.3084%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [108/225], Training Accuracy: 31.3657%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [109/225], Training Accuracy: 31.2357%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [110/225], Training Accuracy: 31.2642%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [111/225], Training Accuracy: 31.1655%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [112/225], Training Accuracy: 31.2221%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [113/225], Training Accuracy: 31.2085%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [114/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [115/225], Training Accuracy: 31.2092%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [116/225], Training Accuracy: 31.2231%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [117/225], Training Accuracy: 31.1699%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [118/225], Training Accuracy: 31.1573%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Step [119/225], Training Accuracy: 31.1056%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [120/225], Training Accuracy: 31.1328%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [121/225], Training Accuracy: 31.1080%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [123/225], Training Accuracy: 31.1611%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [124/225], Training Accuracy: 31.1492%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [125/225], Training Accuracy: 31.1375%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [126/225], Training Accuracy: 31.0640%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [127/225], Training Accuracy: 31.0408%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [128/225], Training Accuracy: 31.0303%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [129/225], Training Accuracy: 31.0804%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [130/225], Training Accuracy: 30.9976%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [131/225], Training Accuracy: 30.9757%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [132/225], Training Accuracy: 30.9541%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [133/225], Training Accuracy: 30.9915%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [134/225], Training Accuracy: 31.0168%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [135/225], Training Accuracy: 31.0185%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [136/225], Training Accuracy: 31.0202%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [137/225], Training Accuracy: 31.0447%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [138/225], Training Accuracy: 31.0802%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [139/225], Training Accuracy: 31.0477%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [140/225], Training Accuracy: 31.0268%, Training Loss: 0.6860%\n",
      "Epoch [40/100], Step [141/225], Training Accuracy: 30.9840%, Training Loss: 0.6860%\n",
      "Epoch [40/100], Step [142/225], Training Accuracy: 31.0519%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [143/225], Training Accuracy: 31.0315%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [144/225], Training Accuracy: 31.0438%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [145/225], Training Accuracy: 31.0991%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [146/225], Training Accuracy: 31.1216%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [147/225], Training Accuracy: 31.1224%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [148/225], Training Accuracy: 31.1022%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [149/225], Training Accuracy: 31.1451%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [150/225], Training Accuracy: 31.1562%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [151/225], Training Accuracy: 31.1879%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [152/225], Training Accuracy: 31.1883%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [153/225], Training Accuracy: 31.1479%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [154/225], Training Accuracy: 31.1891%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [155/225], Training Accuracy: 31.1794%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [156/225], Training Accuracy: 31.1999%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [157/225], Training Accuracy: 31.1405%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [158/225], Training Accuracy: 31.1017%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [159/225], Training Accuracy: 31.1616%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [160/225], Training Accuracy: 31.1426%, Training Loss: 0.6859%\n",
      "Epoch [40/100], Step [161/225], Training Accuracy: 31.2015%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [162/225], Training Accuracy: 31.1921%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [163/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [164/225], Training Accuracy: 31.2405%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [165/225], Training Accuracy: 31.1837%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [166/225], Training Accuracy: 31.1841%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [167/225], Training Accuracy: 31.2313%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [168/225], Training Accuracy: 31.1942%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [169/225], Training Accuracy: 31.1021%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [170/225], Training Accuracy: 31.0570%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [171/225], Training Accuracy: 31.0764%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [173/225], Training Accuracy: 31.1145%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [174/225], Training Accuracy: 31.1333%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [175/225], Training Accuracy: 31.1607%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [176/225], Training Accuracy: 31.1790%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [177/225], Training Accuracy: 31.1882%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [178/225], Training Accuracy: 31.1886%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [179/225], Training Accuracy: 31.1627%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [180/225], Training Accuracy: 31.2066%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [181/225], Training Accuracy: 31.1550%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [182/225], Training Accuracy: 31.1384%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [183/225], Training Accuracy: 31.1561%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [184/225], Training Accuracy: 31.1396%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [185/225], Training Accuracy: 31.1064%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [186/225], Training Accuracy: 31.1408%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [187/225], Training Accuracy: 31.1414%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [190/225], Training Accuracy: 31.1266%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [191/225], Training Accuracy: 31.0946%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [192/225], Training Accuracy: 31.0303%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [193/225], Training Accuracy: 31.0314%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [194/225], Training Accuracy: 31.0486%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [195/225], Training Accuracy: 31.0176%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [196/225], Training Accuracy: 30.9869%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [197/225], Training Accuracy: 31.0121%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [198/225], Training Accuracy: 31.0290%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [199/225], Training Accuracy: 30.9987%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [200/225], Training Accuracy: 30.9844%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [201/225], Training Accuracy: 31.0090%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [202/225], Training Accuracy: 31.0179%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [203/225], Training Accuracy: 31.0037%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [204/225], Training Accuracy: 31.0585%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [205/225], Training Accuracy: 31.0671%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [206/225], Training Accuracy: 31.0604%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [207/225], Training Accuracy: 31.0311%, Training Loss: 0.6858%\n",
      "Epoch [40/100], Step [208/225], Training Accuracy: 31.0547%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [209/225], Training Accuracy: 31.0930%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [210/225], Training Accuracy: 31.1161%, Training Loss: 0.6857%\n",
      "Epoch [40/100], Step [211/225], Training Accuracy: 31.1167%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [212/225], Training Accuracy: 31.1763%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [213/225], Training Accuracy: 31.1546%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [214/225], Training Accuracy: 31.1843%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [215/225], Training Accuracy: 31.1555%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [216/225], Training Accuracy: 31.0981%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [217/225], Training Accuracy: 31.0844%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [218/225], Training Accuracy: 31.0636%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [219/225], Training Accuracy: 31.1216%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [220/225], Training Accuracy: 31.1435%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [221/225], Training Accuracy: 31.1298%, Training Loss: 0.6856%\n",
      "Epoch [40/100], Step [222/225], Training Accuracy: 31.1444%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [223/225], Training Accuracy: 31.1869%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [224/225], Training Accuracy: 31.1942%, Training Loss: 0.6855%\n",
      "Epoch [40/100], Step [225/225], Training Accuracy: 31.1562%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6879%\n",
      "Epoch [41/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6876%\n",
      "Epoch [41/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6866%\n",
      "Epoch [41/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6863%\n",
      "Epoch [41/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6847%\n",
      "Epoch [41/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6848%\n",
      "Epoch [41/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6858%\n",
      "Epoch [41/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6860%\n",
      "Epoch [41/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6858%\n",
      "Epoch [41/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6865%\n",
      "Epoch [41/100], Step [15/225], Training Accuracy: 30.1042%, Training Loss: 0.6865%\n",
      "Epoch [41/100], Step [16/225], Training Accuracy: 30.1758%, Training Loss: 0.6862%\n",
      "Epoch [41/100], Step [17/225], Training Accuracy: 29.6875%, Training Loss: 0.6865%\n",
      "Epoch [41/100], Step [18/225], Training Accuracy: 29.6007%, Training Loss: 0.6866%\n",
      "Epoch [41/100], Step [19/225], Training Accuracy: 29.8520%, Training Loss: 0.6867%\n",
      "Epoch [41/100], Step [20/225], Training Accuracy: 30.3906%, Training Loss: 0.6864%\n",
      "Epoch [41/100], Step [21/225], Training Accuracy: 30.3571%, Training Loss: 0.6863%\n",
      "Epoch [41/100], Step [22/225], Training Accuracy: 30.3977%, Training Loss: 0.6861%\n",
      "Epoch [41/100], Step [23/225], Training Accuracy: 30.2310%, Training Loss: 0.6860%\n",
      "Epoch [41/100], Step [24/225], Training Accuracy: 30.4688%, Training Loss: 0.6862%\n",
      "Epoch [41/100], Step [25/225], Training Accuracy: 30.8750%, Training Loss: 0.6861%\n",
      "Epoch [41/100], Step [26/225], Training Accuracy: 31.1899%, Training Loss: 0.6859%\n",
      "Epoch [41/100], Step [27/225], Training Accuracy: 30.9028%, Training Loss: 0.6857%\n",
      "Epoch [41/100], Step [28/225], Training Accuracy: 30.9152%, Training Loss: 0.6857%\n",
      "Epoch [41/100], Step [29/225], Training Accuracy: 31.1961%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [30/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [31/225], Training Accuracy: 31.0988%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [32/225], Training Accuracy: 31.2988%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [33/225], Training Accuracy: 31.3447%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [34/225], Training Accuracy: 31.1581%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [35/225], Training Accuracy: 31.1161%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [36/225], Training Accuracy: 30.9462%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [37/225], Training Accuracy: 31.0389%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [38/225], Training Accuracy: 30.9211%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [39/225], Training Accuracy: 30.6090%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [40/225], Training Accuracy: 30.7422%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [41/225], Training Accuracy: 30.8308%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [42/225], Training Accuracy: 30.5804%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [43/225], Training Accuracy: 30.8140%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [44/225], Training Accuracy: 30.8239%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [45/225], Training Accuracy: 30.7986%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [46/225], Training Accuracy: 30.6726%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [47/225], Training Accuracy: 30.6516%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [48/225], Training Accuracy: 30.8268%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [49/225], Training Accuracy: 30.8992%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [50/225], Training Accuracy: 30.9062%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [51/225], Training Accuracy: 31.0355%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [52/225], Training Accuracy: 31.0697%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [53/225], Training Accuracy: 31.0142%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [54/225], Training Accuracy: 30.9028%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [55/225], Training Accuracy: 30.9943%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [56/225], Training Accuracy: 31.1105%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [57/225], Training Accuracy: 31.1678%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [58/225], Training Accuracy: 31.0884%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [59/225], Training Accuracy: 31.3824%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [60/225], Training Accuracy: 31.4844%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [61/225], Training Accuracy: 31.4293%, Training Loss: 0.6848%\n",
      "Epoch [41/100], Step [62/225], Training Accuracy: 31.4264%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [63/225], Training Accuracy: 31.4484%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [64/225], Training Accuracy: 31.4697%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [65/225], Training Accuracy: 31.4663%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [66/225], Training Accuracy: 31.5578%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [67/225], Training Accuracy: 31.4599%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [68/225], Training Accuracy: 31.5257%, Training Loss: 0.6848%\n",
      "Epoch [41/100], Step [69/225], Training Accuracy: 31.4764%, Training Loss: 0.6847%\n",
      "Epoch [41/100], Step [70/225], Training Accuracy: 31.4062%, Training Loss: 0.6847%\n",
      "Epoch [41/100], Step [71/225], Training Accuracy: 31.4481%, Training Loss: 0.6848%\n",
      "Epoch [41/100], Step [72/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [73/225], Training Accuracy: 31.1644%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [74/225], Training Accuracy: 31.2922%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [75/225], Training Accuracy: 31.3125%, Training Loss: 0.6848%\n",
      "Epoch [41/100], Step [76/225], Training Accuracy: 31.2911%, Training Loss: 0.6848%\n",
      "Epoch [41/100], Step [77/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [78/225], Training Accuracy: 31.2901%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [79/225], Training Accuracy: 31.2302%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [80/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [81/225], Training Accuracy: 31.1728%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [82/225], Training Accuracy: 31.2119%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [83/225], Training Accuracy: 31.1559%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [84/225], Training Accuracy: 31.2314%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [85/225], Training Accuracy: 31.2316%, Training Loss: 0.6849%\n",
      "Epoch [41/100], Step [86/225], Training Accuracy: 31.3045%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [87/225], Training Accuracy: 31.3039%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [88/225], Training Accuracy: 31.3033%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [89/225], Training Accuracy: 31.2324%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [90/225], Training Accuracy: 31.1458%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [91/225], Training Accuracy: 31.1985%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [92/225], Training Accuracy: 31.1651%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [93/225], Training Accuracy: 31.1492%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [94/225], Training Accuracy: 31.2334%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [95/225], Training Accuracy: 31.1184%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [96/225], Training Accuracy: 31.1849%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [97/225], Training Accuracy: 31.1695%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [98/225], Training Accuracy: 31.1703%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [99/225], Training Accuracy: 31.2973%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [100/225], Training Accuracy: 31.2812%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [101/225], Training Accuracy: 31.4202%, Training Loss: 0.6850%\n",
      "Epoch [41/100], Step [102/225], Training Accuracy: 31.2960%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [103/225], Training Accuracy: 31.3258%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [104/225], Training Accuracy: 31.2800%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [105/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [106/225], Training Accuracy: 31.2353%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [107/225], Training Accuracy: 31.1332%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [108/225], Training Accuracy: 31.2211%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [109/225], Training Accuracy: 31.0780%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [110/225], Training Accuracy: 31.0938%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [111/225], Training Accuracy: 30.9966%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [112/225], Training Accuracy: 31.0686%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Step [113/225], Training Accuracy: 31.0841%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [114/225], Training Accuracy: 31.1129%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [115/225], Training Accuracy: 31.0734%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [116/225], Training Accuracy: 31.0614%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [117/225], Training Accuracy: 31.0096%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [118/225], Training Accuracy: 30.9719%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [119/225], Training Accuracy: 30.9217%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [120/225], Training Accuracy: 30.9505%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [121/225], Training Accuracy: 30.9530%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [122/225], Training Accuracy: 30.9426%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [123/225], Training Accuracy: 30.9578%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [124/225], Training Accuracy: 30.9476%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [125/225], Training Accuracy: 30.9375%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [126/225], Training Accuracy: 30.8904%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [127/225], Training Accuracy: 30.8563%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [128/225], Training Accuracy: 30.8594%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [129/225], Training Accuracy: 30.8866%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [130/225], Training Accuracy: 30.8053%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [131/225], Training Accuracy: 30.7490%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [132/225], Training Accuracy: 30.7055%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [133/225], Training Accuracy: 30.7213%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [134/225], Training Accuracy: 30.8069%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [135/225], Training Accuracy: 30.7870%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [136/225], Training Accuracy: 30.8249%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [137/225], Training Accuracy: 30.8736%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [138/225], Training Accuracy: 30.8650%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [139/225], Training Accuracy: 30.8004%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [140/225], Training Accuracy: 30.7924%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [141/225], Training Accuracy: 30.7181%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [142/225], Training Accuracy: 30.7658%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [143/225], Training Accuracy: 30.7583%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [144/225], Training Accuracy: 30.7726%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [145/225], Training Accuracy: 30.8297%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [146/225], Training Accuracy: 30.8754%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [147/225], Training Accuracy: 30.9205%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [148/225], Training Accuracy: 30.9016%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [149/225], Training Accuracy: 30.9144%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [150/225], Training Accuracy: 30.9271%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [151/225], Training Accuracy: 30.9603%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [152/225], Training Accuracy: 30.9519%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [153/225], Training Accuracy: 30.9130%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [154/225], Training Accuracy: 30.9456%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [155/225], Training Accuracy: 30.9274%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [156/225], Training Accuracy: 30.9595%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [157/225], Training Accuracy: 30.9216%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [158/225], Training Accuracy: 30.9138%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [159/225], Training Accuracy: 30.9552%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [160/225], Training Accuracy: 30.9277%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [161/225], Training Accuracy: 30.9783%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [162/225], Training Accuracy: 30.9510%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [163/225], Training Accuracy: 31.0199%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [164/225], Training Accuracy: 30.9928%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [165/225], Training Accuracy: 30.9375%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [166/225], Training Accuracy: 30.9300%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [167/225], Training Accuracy: 30.9787%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [168/225], Training Accuracy: 30.9431%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [169/225], Training Accuracy: 30.8894%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [170/225], Training Accuracy: 30.8548%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [171/225], Training Accuracy: 30.8845%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [172/225], Training Accuracy: 30.9139%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [173/225], Training Accuracy: 30.9249%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [174/225], Training Accuracy: 30.9537%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [175/225], Training Accuracy: 30.9911%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [176/225], Training Accuracy: 31.0281%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [177/225], Training Accuracy: 31.0381%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [178/225], Training Accuracy: 31.0305%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [179/225], Training Accuracy: 30.9969%, Training Loss: 0.6856%\n",
      "Epoch [41/100], Step [180/225], Training Accuracy: 31.0156%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [181/225], Training Accuracy: 30.9565%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [182/225], Training Accuracy: 30.9323%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [183/225], Training Accuracy: 30.9426%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [184/225], Training Accuracy: 30.9188%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [185/225], Training Accuracy: 30.8615%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [186/225], Training Accuracy: 30.8888%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [187/225], Training Accuracy: 30.8907%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [188/225], Training Accuracy: 30.8926%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [189/225], Training Accuracy: 30.9028%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [190/225], Training Accuracy: 30.8717%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [191/225], Training Accuracy: 30.8491%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [192/225], Training Accuracy: 30.7943%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [193/225], Training Accuracy: 30.7966%, Training Loss: 0.6855%\n",
      "Epoch [41/100], Step [194/225], Training Accuracy: 30.8070%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [195/225], Training Accuracy: 30.7772%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [196/225], Training Accuracy: 30.7557%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [197/225], Training Accuracy: 30.8058%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [198/225], Training Accuracy: 30.8318%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [199/225], Training Accuracy: 30.8182%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [200/225], Training Accuracy: 30.7969%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [201/225], Training Accuracy: 30.8380%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [202/225], Training Accuracy: 30.8323%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [203/225], Training Accuracy: 30.8036%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [204/225], Training Accuracy: 30.8594%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [205/225], Training Accuracy: 30.8613%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [206/225], Training Accuracy: 30.8480%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [207/225], Training Accuracy: 30.8499%, Training Loss: 0.6854%\n",
      "Epoch [41/100], Step [208/225], Training Accuracy: 30.8669%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [209/225], Training Accuracy: 30.9136%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [210/225], Training Accuracy: 30.9226%, Training Loss: 0.6853%\n",
      "Epoch [41/100], Step [211/225], Training Accuracy: 30.9020%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [212/225], Training Accuracy: 30.9626%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [213/225], Training Accuracy: 30.9419%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [214/225], Training Accuracy: 30.9798%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [215/225], Training Accuracy: 30.9593%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [216/225], Training Accuracy: 30.9028%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [217/225], Training Accuracy: 30.9044%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [218/225], Training Accuracy: 30.8773%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [219/225], Training Accuracy: 30.9361%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [220/225], Training Accuracy: 30.9588%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [221/225], Training Accuracy: 30.9389%, Training Loss: 0.6852%\n",
      "Epoch [41/100], Step [222/225], Training Accuracy: 30.9544%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [223/225], Training Accuracy: 30.9908%, Training Loss: 0.6851%\n",
      "Epoch [41/100], Step [224/225], Training Accuracy: 30.9919%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Step [225/225], Training Accuracy: 30.9616%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6986%\n",
      "Epoch [42/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6907%\n",
      "Epoch [42/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6899%\n",
      "Epoch [42/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6870%\n",
      "Epoch [42/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6874%\n",
      "Epoch [42/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6873%\n",
      "Epoch [42/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [14/225], Training Accuracy: 30.1339%, Training Loss: 0.6866%\n",
      "Epoch [42/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6867%\n",
      "Epoch [42/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6863%\n",
      "Epoch [42/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6867%\n",
      "Epoch [42/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6868%\n",
      "Epoch [42/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6867%\n",
      "Epoch [42/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6868%\n",
      "Epoch [42/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6866%\n",
      "Epoch [42/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6866%\n",
      "Epoch [42/100], Step [23/225], Training Accuracy: 31.5217%, Training Loss: 0.6864%\n",
      "Epoch [42/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6862%\n",
      "Epoch [42/100], Step [25/225], Training Accuracy: 32.0000%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [26/225], Training Accuracy: 32.3918%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [27/225], Training Accuracy: 32.1181%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [28/225], Training Accuracy: 31.8638%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [29/225], Training Accuracy: 32.2198%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [30/225], Training Accuracy: 32.2396%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [31/225], Training Accuracy: 32.0565%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [34/225], Training Accuracy: 32.2151%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [35/225], Training Accuracy: 32.0536%, Training Loss: 0.6852%\n",
      "Epoch [42/100], Step [36/225], Training Accuracy: 31.9444%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [37/225], Training Accuracy: 31.9679%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [38/225], Training Accuracy: 31.8257%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [39/225], Training Accuracy: 31.5705%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [40/225], Training Accuracy: 31.6016%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [41/225], Training Accuracy: 31.5930%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [42/225], Training Accuracy: 31.4732%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [43/225], Training Accuracy: 31.6134%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [44/225], Training Accuracy: 31.5696%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [46/225], Training Accuracy: 31.3859%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [47/225], Training Accuracy: 31.3165%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [48/225], Training Accuracy: 31.4453%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [49/225], Training Accuracy: 31.5051%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [50/225], Training Accuracy: 31.4688%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [51/225], Training Accuracy: 31.6483%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [52/225], Training Accuracy: 31.6406%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [53/225], Training Accuracy: 31.5743%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [54/225], Training Accuracy: 31.4525%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [55/225], Training Accuracy: 31.5909%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [56/225], Training Accuracy: 31.6964%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [57/225], Training Accuracy: 31.6886%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [58/225], Training Accuracy: 31.5463%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [59/225], Training Accuracy: 31.8591%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [60/225], Training Accuracy: 31.9792%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [61/225], Training Accuracy: 31.9160%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [62/225], Training Accuracy: 31.9556%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [63/225], Training Accuracy: 32.0188%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [64/225], Training Accuracy: 32.0312%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [65/225], Training Accuracy: 32.0192%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [66/225], Training Accuracy: 32.1023%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [67/225], Training Accuracy: 32.0662%, Training Loss: 0.6849%\n",
      "Epoch [42/100], Step [68/225], Training Accuracy: 32.1232%, Training Loss: 0.6849%\n",
      "Epoch [42/100], Step [69/225], Training Accuracy: 32.0426%, Training Loss: 0.6848%\n",
      "Epoch [42/100], Step [70/225], Training Accuracy: 32.0089%, Training Loss: 0.6848%\n",
      "Epoch [42/100], Step [71/225], Training Accuracy: 32.0202%, Training Loss: 0.6848%\n",
      "Epoch [42/100], Step [72/225], Training Accuracy: 31.8142%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [73/225], Training Accuracy: 31.7423%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [74/225], Training Accuracy: 31.8623%, Training Loss: 0.6849%\n",
      "Epoch [42/100], Step [75/225], Training Accuracy: 31.7500%, Training Loss: 0.6849%\n",
      "Epoch [42/100], Step [76/225], Training Accuracy: 31.7023%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [77/225], Training Accuracy: 31.6558%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [78/225], Training Accuracy: 31.6506%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [79/225], Training Accuracy: 31.5862%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [80/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [81/225], Training Accuracy: 31.4429%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [82/225], Training Accuracy: 31.4787%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [83/225], Training Accuracy: 31.4006%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [84/225], Training Accuracy: 31.4918%, Training Loss: 0.6849%\n",
      "Epoch [42/100], Step [85/225], Training Accuracy: 31.4706%, Training Loss: 0.6850%\n",
      "Epoch [42/100], Step [86/225], Training Accuracy: 31.5044%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [87/225], Training Accuracy: 31.5014%, Training Loss: 0.6851%\n",
      "Epoch [42/100], Step [88/225], Training Accuracy: 31.4631%, Training Loss: 0.6852%\n",
      "Epoch [42/100], Step [89/225], Training Accuracy: 31.4080%, Training Loss: 0.6852%\n",
      "Epoch [42/100], Step [90/225], Training Accuracy: 31.3021%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [92/225], Training Accuracy: 31.3689%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [94/225], Training Accuracy: 31.4328%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [95/225], Training Accuracy: 31.3158%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [96/225], Training Accuracy: 31.3965%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [97/225], Training Accuracy: 31.4111%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [98/225], Training Accuracy: 31.3935%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Step [99/225], Training Accuracy: 31.5341%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [100/225], Training Accuracy: 31.5000%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [101/225], Training Accuracy: 31.5903%, Training Loss: 0.6853%\n",
      "Epoch [42/100], Step [102/225], Training Accuracy: 31.4798%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [103/225], Training Accuracy: 31.5382%, Training Loss: 0.6854%\n",
      "Epoch [42/100], Step [104/225], Training Accuracy: 31.5054%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [105/225], Training Accuracy: 31.4732%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [106/225], Training Accuracy: 31.4711%, Training Loss: 0.6855%\n",
      "Epoch [42/100], Step [107/225], Training Accuracy: 31.3522%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [108/225], Training Accuracy: 31.4091%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [109/225], Training Accuracy: 31.2357%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [110/225], Training Accuracy: 31.2642%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [111/225], Training Accuracy: 31.1374%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [112/225], Training Accuracy: 31.2221%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [113/225], Training Accuracy: 31.2223%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [114/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [115/225], Training Accuracy: 31.2636%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [116/225], Training Accuracy: 31.2904%, Training Loss: 0.6856%\n",
      "Epoch [42/100], Step [117/225], Training Accuracy: 31.2366%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [118/225], Training Accuracy: 31.1970%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [119/225], Training Accuracy: 31.1450%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [120/225], Training Accuracy: 31.1458%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [121/225], Training Accuracy: 31.1338%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [122/225], Training Accuracy: 31.1347%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [123/225], Training Accuracy: 31.1865%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [124/225], Training Accuracy: 31.1492%, Training Loss: 0.6857%\n",
      "Epoch [42/100], Step [125/225], Training Accuracy: 31.1375%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [126/225], Training Accuracy: 31.0764%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [127/225], Training Accuracy: 31.0285%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [128/225], Training Accuracy: 31.0181%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [129/225], Training Accuracy: 31.0441%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [130/225], Training Accuracy: 30.9736%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [131/225], Training Accuracy: 30.9637%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [132/225], Training Accuracy: 30.9186%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [133/225], Training Accuracy: 30.9563%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [134/225], Training Accuracy: 31.0168%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [135/225], Training Accuracy: 31.0185%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [136/225], Training Accuracy: 31.0202%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [137/225], Training Accuracy: 31.0561%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [138/225], Training Accuracy: 31.0462%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [139/225], Training Accuracy: 30.9802%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [140/225], Training Accuracy: 30.9375%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [141/225], Training Accuracy: 30.8732%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [142/225], Training Accuracy: 30.9309%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [143/225], Training Accuracy: 30.9441%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [144/225], Training Accuracy: 30.9787%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [145/225], Training Accuracy: 31.0345%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [146/225], Training Accuracy: 31.0574%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [147/225], Training Accuracy: 31.0799%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [148/225], Training Accuracy: 31.0705%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [149/225], Training Accuracy: 31.0927%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [150/225], Training Accuracy: 31.1146%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [151/225], Training Accuracy: 31.1362%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [152/225], Training Accuracy: 31.1472%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [153/225], Training Accuracy: 31.0968%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [154/225], Training Accuracy: 31.0775%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [155/225], Training Accuracy: 31.0585%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [156/225], Training Accuracy: 31.0597%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [157/225], Training Accuracy: 31.0111%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [158/225], Training Accuracy: 30.9929%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [159/225], Training Accuracy: 31.0633%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [160/225], Training Accuracy: 31.0254%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [161/225], Training Accuracy: 31.0850%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [162/225], Training Accuracy: 31.0378%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [163/225], Training Accuracy: 31.0966%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [164/225], Training Accuracy: 31.0785%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [165/225], Training Accuracy: 31.0133%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [166/225], Training Accuracy: 30.9864%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [167/225], Training Accuracy: 31.0348%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [168/225], Training Accuracy: 30.9710%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [169/225], Training Accuracy: 30.8802%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [170/225], Training Accuracy: 30.8548%, Training Loss: 0.6861%\n",
      "Epoch [42/100], Step [171/225], Training Accuracy: 30.8662%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [172/225], Training Accuracy: 30.8775%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [173/225], Training Accuracy: 30.8707%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [174/225], Training Accuracy: 30.8549%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [175/225], Training Accuracy: 30.8929%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [176/225], Training Accuracy: 30.9215%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [177/225], Training Accuracy: 30.9234%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [178/225], Training Accuracy: 30.8901%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [179/225], Training Accuracy: 30.8659%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [180/225], Training Accuracy: 30.9115%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [181/225], Training Accuracy: 30.8788%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [182/225], Training Accuracy: 30.8808%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [183/225], Training Accuracy: 30.8914%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [184/225], Training Accuracy: 30.8764%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [185/225], Training Accuracy: 30.8277%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [186/225], Training Accuracy: 30.8636%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [187/225], Training Accuracy: 30.8489%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [188/225], Training Accuracy: 30.8428%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [189/225], Training Accuracy: 30.8780%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [190/225], Training Accuracy: 30.8388%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [191/225], Training Accuracy: 30.8164%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [192/225], Training Accuracy: 30.7536%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [193/225], Training Accuracy: 30.7562%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [194/225], Training Accuracy: 30.7829%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [195/225], Training Accuracy: 30.7532%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [196/225], Training Accuracy: 30.7239%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Step [197/225], Training Accuracy: 30.7662%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [198/225], Training Accuracy: 30.7923%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [199/225], Training Accuracy: 30.7632%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [200/225], Training Accuracy: 30.7578%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [201/225], Training Accuracy: 30.7758%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [202/225], Training Accuracy: 30.7550%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [203/225], Training Accuracy: 30.7497%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [204/225], Training Accuracy: 30.8058%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [205/225], Training Accuracy: 30.8155%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [206/225], Training Accuracy: 30.8252%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [207/225], Training Accuracy: 30.8122%, Training Loss: 0.6860%\n",
      "Epoch [42/100], Step [208/225], Training Accuracy: 30.8444%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [209/225], Training Accuracy: 30.8911%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [210/225], Training Accuracy: 30.9375%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [211/225], Training Accuracy: 30.9168%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [212/225], Training Accuracy: 30.9847%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [213/225], Training Accuracy: 30.9639%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [214/225], Training Accuracy: 30.9945%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [215/225], Training Accuracy: 30.9738%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [216/225], Training Accuracy: 30.9245%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [217/225], Training Accuracy: 30.9188%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [218/225], Training Accuracy: 30.8916%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [219/225], Training Accuracy: 30.9503%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [220/225], Training Accuracy: 30.9588%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [221/225], Training Accuracy: 30.9460%, Training Loss: 0.6859%\n",
      "Epoch [42/100], Step [222/225], Training Accuracy: 30.9614%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [223/225], Training Accuracy: 30.9978%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [224/225], Training Accuracy: 30.9919%, Training Loss: 0.6858%\n",
      "Epoch [42/100], Step [225/225], Training Accuracy: 30.9755%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6921%\n",
      "Epoch [43/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6882%\n",
      "Epoch [43/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6890%\n",
      "Epoch [43/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6866%\n",
      "Epoch [43/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6870%\n",
      "Epoch [43/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6869%\n",
      "Epoch [43/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6866%\n",
      "Epoch [43/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6866%\n",
      "Epoch [43/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6865%\n",
      "Epoch [43/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6866%\n",
      "Epoch [43/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6872%\n",
      "Epoch [43/100], Step [15/225], Training Accuracy: 30.4167%, Training Loss: 0.6873%\n",
      "Epoch [43/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6870%\n",
      "Epoch [43/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6869%\n",
      "Epoch [43/100], Step [18/225], Training Accuracy: 30.4688%, Training Loss: 0.6869%\n",
      "Epoch [43/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6870%\n",
      "Epoch [43/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6868%\n",
      "Epoch [43/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6865%\n",
      "Epoch [43/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6865%\n",
      "Epoch [43/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6866%\n",
      "Epoch [43/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6868%\n",
      "Epoch [43/100], Step [25/225], Training Accuracy: 31.6875%, Training Loss: 0.6866%\n",
      "Epoch [43/100], Step [26/225], Training Accuracy: 32.0913%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [27/225], Training Accuracy: 31.8866%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [30/225], Training Accuracy: 32.1354%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [31/225], Training Accuracy: 32.0060%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [32/225], Training Accuracy: 32.2266%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [34/225], Training Accuracy: 32.3529%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [36/225], Training Accuracy: 31.9878%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [37/225], Training Accuracy: 32.0524%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [39/225], Training Accuracy: 31.5705%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [41/225], Training Accuracy: 31.7454%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [42/225], Training Accuracy: 31.5104%, Training Loss: 0.6864%\n",
      "Epoch [43/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [44/225], Training Accuracy: 31.6051%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [45/225], Training Accuracy: 31.5972%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [46/225], Training Accuracy: 31.4538%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [47/225], Training Accuracy: 31.3497%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [48/225], Training Accuracy: 31.4779%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [49/225], Training Accuracy: 31.5051%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [50/225], Training Accuracy: 31.4062%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [51/225], Training Accuracy: 31.5257%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [55/225], Training Accuracy: 31.4205%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [56/225], Training Accuracy: 31.4732%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [57/225], Training Accuracy: 31.5241%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [58/225], Training Accuracy: 31.4116%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [59/225], Training Accuracy: 31.7797%, Training Loss: 0.6855%\n",
      "Epoch [43/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [61/225], Training Accuracy: 31.7623%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [62/225], Training Accuracy: 31.8044%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [63/225], Training Accuracy: 31.8700%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [64/225], Training Accuracy: 31.8848%, Training Loss: 0.6853%\n",
      "Epoch [43/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6855%\n",
      "Epoch [43/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6853%\n",
      "Epoch [43/100], Step [69/225], Training Accuracy: 31.9293%, Training Loss: 0.6852%\n",
      "Epoch [43/100], Step [70/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [43/100], Step [71/225], Training Accuracy: 31.9322%, Training Loss: 0.6851%\n",
      "Epoch [43/100], Step [72/225], Training Accuracy: 31.7491%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [73/225], Training Accuracy: 31.6781%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [74/225], Training Accuracy: 31.7779%, Training Loss: 0.6853%\n",
      "Epoch [43/100], Step [75/225], Training Accuracy: 31.7292%, Training Loss: 0.6852%\n",
      "Epoch [43/100], Step [76/225], Training Accuracy: 31.7229%, Training Loss: 0.6852%\n",
      "Epoch [43/100], Step [77/225], Training Accuracy: 31.6558%, Training Loss: 0.6853%\n",
      "Epoch [43/100], Step [78/225], Training Accuracy: 31.6907%, Training Loss: 0.6853%\n",
      "Epoch [43/100], Step [79/225], Training Accuracy: 31.6258%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [80/225], Training Accuracy: 31.5820%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Step [81/225], Training Accuracy: 31.5201%, Training Loss: 0.6855%\n",
      "Epoch [43/100], Step [82/225], Training Accuracy: 31.5358%, Training Loss: 0.6855%\n",
      "Epoch [43/100], Step [83/225], Training Accuracy: 31.4571%, Training Loss: 0.6855%\n",
      "Epoch [43/100], Step [84/225], Training Accuracy: 31.4918%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [85/225], Training Accuracy: 31.4706%, Training Loss: 0.6854%\n",
      "Epoch [43/100], Step [86/225], Training Accuracy: 31.5044%, Training Loss: 0.6855%\n",
      "Epoch [43/100], Step [87/225], Training Accuracy: 31.5194%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [88/225], Training Accuracy: 31.4808%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [89/225], Training Accuracy: 31.3904%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [90/225], Training Accuracy: 31.2847%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [91/225], Training Accuracy: 31.3359%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [92/225], Training Accuracy: 31.3519%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [94/225], Training Accuracy: 31.4661%, Training Loss: 0.6856%\n",
      "Epoch [43/100], Step [95/225], Training Accuracy: 31.3487%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [96/225], Training Accuracy: 31.4616%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [97/225], Training Accuracy: 31.4594%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [98/225], Training Accuracy: 31.4573%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [99/225], Training Accuracy: 31.5972%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [100/225], Training Accuracy: 31.5781%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [101/225], Training Accuracy: 31.6832%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [102/225], Training Accuracy: 31.5717%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [103/225], Training Accuracy: 31.6292%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [104/225], Training Accuracy: 31.5956%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [105/225], Training Accuracy: 31.5476%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [106/225], Training Accuracy: 31.5448%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [107/225], Training Accuracy: 31.4690%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [108/225], Training Accuracy: 31.5683%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [109/225], Training Accuracy: 31.4507%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [110/225], Training Accuracy: 31.4489%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [111/225], Training Accuracy: 31.3485%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [112/225], Training Accuracy: 31.4035%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [113/225], Training Accuracy: 31.3744%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [114/225], Training Accuracy: 31.4145%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [115/225], Training Accuracy: 31.3859%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [116/225], Training Accuracy: 31.3712%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [117/225], Training Accuracy: 31.3168%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [118/225], Training Accuracy: 31.2632%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [119/225], Training Accuracy: 31.2237%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [120/225], Training Accuracy: 31.2630%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [121/225], Training Accuracy: 31.2371%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [122/225], Training Accuracy: 31.2628%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [123/225], Training Accuracy: 31.2881%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [124/225], Training Accuracy: 31.2878%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [125/225], Training Accuracy: 31.2750%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [126/225], Training Accuracy: 31.2252%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [127/225], Training Accuracy: 31.1762%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [128/225], Training Accuracy: 31.1646%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [129/225], Training Accuracy: 31.1773%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [130/225], Training Accuracy: 31.1298%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [131/225], Training Accuracy: 31.1069%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [132/225], Training Accuracy: 31.0488%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [133/225], Training Accuracy: 31.0385%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [134/225], Training Accuracy: 31.1217%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [135/225], Training Accuracy: 31.1343%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [136/225], Training Accuracy: 31.1581%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [137/225], Training Accuracy: 31.2158%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [138/225], Training Accuracy: 31.2387%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [139/225], Training Accuracy: 31.2163%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [140/225], Training Accuracy: 31.1719%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [141/225], Training Accuracy: 31.1281%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [142/225], Training Accuracy: 31.2060%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [143/225], Training Accuracy: 31.2063%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [144/225], Training Accuracy: 31.2066%, Training Loss: 0.6863%\n",
      "Epoch [43/100], Step [145/225], Training Accuracy: 31.2823%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [146/225], Training Accuracy: 31.3142%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [147/225], Training Accuracy: 31.3350%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [148/225], Training Accuracy: 31.3133%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [149/225], Training Accuracy: 31.3339%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [150/225], Training Accuracy: 31.3438%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [151/225], Training Accuracy: 31.4052%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [152/225], Training Accuracy: 31.3939%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [153/225], Training Accuracy: 31.3521%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [154/225], Training Accuracy: 31.3718%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [155/225], Training Accuracy: 31.3710%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [156/225], Training Accuracy: 31.3902%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [157/225], Training Accuracy: 31.3097%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [158/225], Training Accuracy: 31.3093%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [159/225], Training Accuracy: 31.3974%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [160/225], Training Accuracy: 31.3672%, Training Loss: 0.6862%\n",
      "Epoch [43/100], Step [161/225], Training Accuracy: 31.4053%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [162/225], Training Accuracy: 31.3754%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [163/225], Training Accuracy: 31.4513%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [164/225], Training Accuracy: 31.4310%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [165/225], Training Accuracy: 31.3636%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [166/225], Training Accuracy: 31.3441%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [167/225], Training Accuracy: 31.3810%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [168/225], Training Accuracy: 31.3523%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [169/225], Training Accuracy: 31.2870%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [170/225], Training Accuracy: 31.2592%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [171/225], Training Accuracy: 31.2957%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [172/225], Training Accuracy: 31.3045%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [173/225], Training Accuracy: 31.3132%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [174/225], Training Accuracy: 31.3398%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [175/225], Training Accuracy: 31.3839%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [176/225], Training Accuracy: 31.4009%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [177/225], Training Accuracy: 31.4089%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [178/225], Training Accuracy: 31.3904%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [179/225], Training Accuracy: 31.3635%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [180/225], Training Accuracy: 31.4062%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [181/225], Training Accuracy: 31.3709%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [182/225], Training Accuracy: 31.3530%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [183/225], Training Accuracy: 31.3610%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [184/225], Training Accuracy: 31.3434%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [185/225], Training Accuracy: 31.2922%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [186/225], Training Accuracy: 31.3088%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Step [187/225], Training Accuracy: 31.2918%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [188/225], Training Accuracy: 31.2916%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [189/225], Training Accuracy: 31.3327%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [190/225], Training Accuracy: 31.3158%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [191/225], Training Accuracy: 31.2827%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [192/225], Training Accuracy: 31.2012%, Training Loss: 0.6861%\n",
      "Epoch [43/100], Step [193/225], Training Accuracy: 31.2014%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [194/225], Training Accuracy: 31.2178%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [195/225], Training Accuracy: 31.2019%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [196/225], Training Accuracy: 31.1703%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [197/225], Training Accuracy: 31.1945%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [198/225], Training Accuracy: 31.2263%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [199/225], Training Accuracy: 31.1950%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [200/225], Training Accuracy: 31.1797%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [201/225], Training Accuracy: 31.2034%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [202/225], Training Accuracy: 31.2036%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [203/225], Training Accuracy: 31.1807%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [204/225], Training Accuracy: 31.2347%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [205/225], Training Accuracy: 31.2195%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [206/225], Training Accuracy: 31.2045%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [207/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [208/225], Training Accuracy: 31.2124%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [209/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [43/100], Step [210/225], Training Accuracy: 31.2872%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [211/225], Training Accuracy: 31.2796%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [212/225], Training Accuracy: 31.3458%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [213/225], Training Accuracy: 31.3234%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [214/225], Training Accuracy: 31.3376%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [215/225], Training Accuracy: 31.3081%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [216/225], Training Accuracy: 31.2645%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [217/225], Training Accuracy: 31.2572%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [218/225], Training Accuracy: 31.2285%, Training Loss: 0.6859%\n",
      "Epoch [43/100], Step [219/225], Training Accuracy: 31.2928%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [220/225], Training Accuracy: 31.3139%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [221/225], Training Accuracy: 31.3136%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [222/225], Training Accuracy: 31.3415%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [223/225], Training Accuracy: 31.3901%, Training Loss: 0.6858%\n",
      "Epoch [43/100], Step [224/225], Training Accuracy: 31.3825%, Training Loss: 0.6857%\n",
      "Epoch [43/100], Step [225/225], Training Accuracy: 31.3646%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6941%\n",
      "Epoch [44/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6898%\n",
      "Epoch [44/100], Step [3/225], Training Accuracy: 30.2083%, Training Loss: 0.6909%\n",
      "Epoch [44/100], Step [4/225], Training Accuracy: 29.6875%, Training Loss: 0.6876%\n",
      "Epoch [44/100], Step [5/225], Training Accuracy: 30.3125%, Training Loss: 0.6871%\n",
      "Epoch [44/100], Step [6/225], Training Accuracy: 29.9479%, Training Loss: 0.6869%\n",
      "Epoch [44/100], Step [7/225], Training Accuracy: 29.9107%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [8/225], Training Accuracy: 29.8828%, Training Loss: 0.6850%\n",
      "Epoch [44/100], Step [9/225], Training Accuracy: 30.2083%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [10/225], Training Accuracy: 29.8438%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [11/225], Training Accuracy: 29.9716%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6854%\n",
      "Epoch [44/100], Step [13/225], Training Accuracy: 29.4471%, Training Loss: 0.6854%\n",
      "Epoch [44/100], Step [14/225], Training Accuracy: 29.7991%, Training Loss: 0.6861%\n",
      "Epoch [44/100], Step [15/225], Training Accuracy: 30.2083%, Training Loss: 0.6861%\n",
      "Epoch [44/100], Step [16/225], Training Accuracy: 30.2734%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [17/225], Training Accuracy: 30.0551%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [18/225], Training Accuracy: 30.1215%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [19/225], Training Accuracy: 30.5921%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [20/225], Training Accuracy: 31.0938%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [21/225], Training Accuracy: 31.1012%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [22/225], Training Accuracy: 31.3920%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [23/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [44/100], Step [24/225], Training Accuracy: 31.4453%, Training Loss: 0.6854%\n",
      "Epoch [44/100], Step [25/225], Training Accuracy: 31.6875%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [29/225], Training Accuracy: 31.8427%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6844%\n",
      "Epoch [44/100], Step [31/225], Training Accuracy: 31.8548%, Training Loss: 0.6843%\n",
      "Epoch [44/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6840%\n",
      "Epoch [44/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6840%\n",
      "Epoch [44/100], Step [34/225], Training Accuracy: 32.2151%, Training Loss: 0.6840%\n",
      "Epoch [44/100], Step [35/225], Training Accuracy: 32.0089%, Training Loss: 0.6842%\n",
      "Epoch [44/100], Step [36/225], Training Accuracy: 31.8142%, Training Loss: 0.6844%\n",
      "Epoch [44/100], Step [37/225], Training Accuracy: 31.9257%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [38/225], Training Accuracy: 31.7434%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [40/225], Training Accuracy: 31.4844%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [41/225], Training Accuracy: 31.5168%, Training Loss: 0.6850%\n",
      "Epoch [44/100], Step [42/225], Training Accuracy: 31.2872%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [43/225], Training Accuracy: 31.4317%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [44/225], Training Accuracy: 31.3565%, Training Loss: 0.6850%\n",
      "Epoch [44/100], Step [45/225], Training Accuracy: 31.3194%, Training Loss: 0.6851%\n",
      "Epoch [44/100], Step [46/225], Training Accuracy: 31.2160%, Training Loss: 0.6851%\n",
      "Epoch [44/100], Step [47/225], Training Accuracy: 31.1503%, Training Loss: 0.6851%\n",
      "Epoch [44/100], Step [48/225], Training Accuracy: 31.2826%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [49/225], Training Accuracy: 31.3457%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [50/225], Training Accuracy: 31.4062%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [51/225], Training Accuracy: 31.5564%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [53/225], Training Accuracy: 31.4564%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [54/225], Training Accuracy: 31.3079%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [56/225], Training Accuracy: 31.5290%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [57/225], Training Accuracy: 31.5789%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [58/225], Training Accuracy: 31.4116%, Training Loss: 0.6845%\n",
      "Epoch [44/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6844%\n",
      "Epoch [44/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6845%\n",
      "Epoch [44/100], Step [61/225], Training Accuracy: 31.8135%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [62/225], Training Accuracy: 31.7792%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [63/225], Training Accuracy: 31.8204%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [64/225], Training Accuracy: 31.7871%, Training Loss: 0.6846%\n",
      "Epoch [44/100], Step [65/225], Training Accuracy: 31.7067%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [66/225], Training Accuracy: 31.8182%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [67/225], Training Accuracy: 31.8097%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [68/225], Training Accuracy: 31.8474%, Training Loss: 0.6846%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Step [69/225], Training Accuracy: 31.8841%, Training Loss: 0.6845%\n",
      "Epoch [44/100], Step [70/225], Training Accuracy: 31.8527%, Training Loss: 0.6845%\n",
      "Epoch [44/100], Step [71/225], Training Accuracy: 31.8882%, Training Loss: 0.6845%\n",
      "Epoch [44/100], Step [72/225], Training Accuracy: 31.6840%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [73/225], Training Accuracy: 31.5925%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [74/225], Training Accuracy: 31.6723%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [75/225], Training Accuracy: 31.5833%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6847%\n",
      "Epoch [44/100], Step [77/225], Training Accuracy: 31.4529%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [78/225], Training Accuracy: 31.4904%, Training Loss: 0.6848%\n",
      "Epoch [44/100], Step [79/225], Training Accuracy: 31.4082%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [80/225], Training Accuracy: 31.3867%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [81/225], Training Accuracy: 31.3465%, Training Loss: 0.6849%\n",
      "Epoch [44/100], Step [82/225], Training Accuracy: 31.3262%, Training Loss: 0.6850%\n",
      "Epoch [44/100], Step [83/225], Training Accuracy: 31.2688%, Training Loss: 0.6850%\n",
      "Epoch [44/100], Step [84/225], Training Accuracy: 31.3058%, Training Loss: 0.6850%\n",
      "Epoch [44/100], Step [85/225], Training Accuracy: 31.3051%, Training Loss: 0.6851%\n",
      "Epoch [44/100], Step [86/225], Training Accuracy: 31.3227%, Training Loss: 0.6851%\n",
      "Epoch [44/100], Step [87/225], Training Accuracy: 31.3398%, Training Loss: 0.6851%\n",
      "Epoch [44/100], Step [88/225], Training Accuracy: 31.3033%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [89/225], Training Accuracy: 31.2324%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [90/225], Training Accuracy: 31.1285%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [91/225], Training Accuracy: 31.1813%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [92/225], Training Accuracy: 31.1821%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [93/225], Training Accuracy: 31.1660%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [94/225], Training Accuracy: 31.2666%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [95/225], Training Accuracy: 31.1349%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [96/225], Training Accuracy: 31.2337%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [97/225], Training Accuracy: 31.2822%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [98/225], Training Accuracy: 31.2978%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [99/225], Training Accuracy: 31.4236%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [100/225], Training Accuracy: 31.3750%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [101/225], Training Accuracy: 31.5130%, Training Loss: 0.6852%\n",
      "Epoch [44/100], Step [102/225], Training Accuracy: 31.4185%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [103/225], Training Accuracy: 31.4927%, Training Loss: 0.6853%\n",
      "Epoch [44/100], Step [104/225], Training Accuracy: 31.4754%, Training Loss: 0.6855%\n",
      "Epoch [44/100], Step [105/225], Training Accuracy: 31.4286%, Training Loss: 0.6854%\n",
      "Epoch [44/100], Step [106/225], Training Accuracy: 31.4564%, Training Loss: 0.6855%\n",
      "Epoch [44/100], Step [107/225], Training Accuracy: 31.3376%, Training Loss: 0.6855%\n",
      "Epoch [44/100], Step [108/225], Training Accuracy: 31.4091%, Training Loss: 0.6855%\n",
      "Epoch [44/100], Step [109/225], Training Accuracy: 31.2787%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [110/225], Training Accuracy: 31.3068%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [111/225], Training Accuracy: 31.2078%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [112/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [113/225], Training Accuracy: 31.2362%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [114/225], Training Accuracy: 31.2637%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [115/225], Training Accuracy: 31.2364%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [116/225], Training Accuracy: 31.2635%, Training Loss: 0.6855%\n",
      "Epoch [44/100], Step [117/225], Training Accuracy: 31.2233%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [118/225], Training Accuracy: 31.2103%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [119/225], Training Accuracy: 31.1581%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [120/225], Training Accuracy: 31.2109%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [121/225], Training Accuracy: 31.1596%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [122/225], Training Accuracy: 31.1732%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [123/225], Training Accuracy: 31.1865%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [124/225], Training Accuracy: 31.1618%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [125/225], Training Accuracy: 31.1250%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [126/225], Training Accuracy: 31.0764%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [127/225], Training Accuracy: 31.0408%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [128/225], Training Accuracy: 31.0059%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [129/225], Training Accuracy: 31.0320%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [130/225], Training Accuracy: 30.9615%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [131/225], Training Accuracy: 30.9160%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [132/225], Training Accuracy: 30.8712%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [133/225], Training Accuracy: 30.8858%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [134/225], Training Accuracy: 30.9352%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [135/225], Training Accuracy: 30.9491%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [137/225], Training Accuracy: 31.0105%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [138/225], Training Accuracy: 31.0009%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [139/225], Training Accuracy: 30.9802%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [140/225], Training Accuracy: 30.9375%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [141/225], Training Accuracy: 30.9065%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [142/225], Training Accuracy: 30.9529%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [143/225], Training Accuracy: 30.9441%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [144/225], Training Accuracy: 30.9570%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [145/225], Training Accuracy: 31.0237%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [146/225], Training Accuracy: 31.0467%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [147/225], Training Accuracy: 31.0374%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [148/225], Training Accuracy: 31.0072%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [149/225], Training Accuracy: 31.0298%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [150/225], Training Accuracy: 31.0625%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [151/225], Training Accuracy: 31.0844%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [152/225], Training Accuracy: 31.0855%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [153/225], Training Accuracy: 31.0560%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [154/225], Training Accuracy: 31.0877%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [155/225], Training Accuracy: 31.0685%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [156/225], Training Accuracy: 31.1098%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [157/225], Training Accuracy: 31.0510%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [158/225], Training Accuracy: 31.0127%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [159/225], Training Accuracy: 31.0535%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [160/225], Training Accuracy: 31.0156%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [161/225], Training Accuracy: 31.0656%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [162/225], Training Accuracy: 31.0571%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [163/225], Training Accuracy: 31.0966%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [164/225], Training Accuracy: 31.0976%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [165/225], Training Accuracy: 31.0227%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [166/225], Training Accuracy: 31.0241%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [167/225], Training Accuracy: 31.0629%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [168/225], Training Accuracy: 30.9989%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [169/225], Training Accuracy: 30.9264%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [170/225], Training Accuracy: 30.8824%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [171/225], Training Accuracy: 30.9119%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [172/225], Training Accuracy: 30.9320%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [173/225], Training Accuracy: 30.9249%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [174/225], Training Accuracy: 30.9626%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Step [175/225], Training Accuracy: 31.0000%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [176/225], Training Accuracy: 31.0192%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [177/225], Training Accuracy: 31.0293%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [178/225], Training Accuracy: 31.0218%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [179/225], Training Accuracy: 31.0056%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [180/225], Training Accuracy: 31.0503%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [181/225], Training Accuracy: 31.0083%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [182/225], Training Accuracy: 31.0010%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [183/225], Training Accuracy: 31.0109%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [184/225], Training Accuracy: 30.9952%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [185/225], Training Accuracy: 30.9628%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [186/225], Training Accuracy: 30.9980%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [187/225], Training Accuracy: 31.0077%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [188/225], Training Accuracy: 31.0173%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [189/225], Training Accuracy: 31.0351%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [190/225], Training Accuracy: 30.9951%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [191/225], Training Accuracy: 30.9719%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [192/225], Training Accuracy: 30.9001%, Training Loss: 0.6860%\n",
      "Epoch [44/100], Step [193/225], Training Accuracy: 30.9019%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [194/225], Training Accuracy: 30.9117%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [195/225], Training Accuracy: 30.8974%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [196/225], Training Accuracy: 30.8833%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [197/225], Training Accuracy: 30.9248%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [198/225], Training Accuracy: 30.9501%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [199/225], Training Accuracy: 30.9359%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [200/225], Training Accuracy: 30.9297%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [201/225], Training Accuracy: 30.9468%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [202/225], Training Accuracy: 30.9483%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [203/225], Training Accuracy: 30.9421%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [204/225], Training Accuracy: 31.0049%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [205/225], Training Accuracy: 31.0213%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [206/225], Training Accuracy: 31.0149%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [207/225], Training Accuracy: 30.9783%, Training Loss: 0.6859%\n",
      "Epoch [44/100], Step [208/225], Training Accuracy: 31.0096%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [209/225], Training Accuracy: 31.0332%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [210/225], Training Accuracy: 31.0640%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [211/225], Training Accuracy: 31.0352%, Training Loss: 0.6858%\n",
      "Epoch [44/100], Step [212/225], Training Accuracy: 31.1026%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [213/225], Training Accuracy: 31.0813%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [214/225], Training Accuracy: 31.0967%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [215/225], Training Accuracy: 31.0683%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [216/225], Training Accuracy: 31.0041%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [217/225], Training Accuracy: 30.9980%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [218/225], Training Accuracy: 30.9705%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [219/225], Training Accuracy: 31.0360%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [220/225], Training Accuracy: 31.0511%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [221/225], Training Accuracy: 31.0379%, Training Loss: 0.6857%\n",
      "Epoch [44/100], Step [222/225], Training Accuracy: 31.0529%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [223/225], Training Accuracy: 31.0959%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [224/225], Training Accuracy: 31.0965%, Training Loss: 0.6856%\n",
      "Epoch [44/100], Step [225/225], Training Accuracy: 31.0867%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6925%\n",
      "Epoch [45/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6882%\n",
      "Epoch [45/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6889%\n",
      "Epoch [45/100], Step [4/225], Training Accuracy: 30.8594%, Training Loss: 0.6867%\n",
      "Epoch [45/100], Step [5/225], Training Accuracy: 31.8750%, Training Loss: 0.6867%\n",
      "Epoch [45/100], Step [6/225], Training Accuracy: 30.9896%, Training Loss: 0.6872%\n",
      "Epoch [45/100], Step [7/225], Training Accuracy: 31.0268%, Training Loss: 0.6865%\n",
      "Epoch [45/100], Step [8/225], Training Accuracy: 30.2734%, Training Loss: 0.6861%\n",
      "Epoch [45/100], Step [9/225], Training Accuracy: 30.3819%, Training Loss: 0.6864%\n",
      "Epoch [45/100], Step [10/225], Training Accuracy: 30.1562%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [12/225], Training Accuracy: 29.4271%, Training Loss: 0.6862%\n",
      "Epoch [45/100], Step [13/225], Training Accuracy: 29.0865%, Training Loss: 0.6865%\n",
      "Epoch [45/100], Step [14/225], Training Accuracy: 29.2411%, Training Loss: 0.6871%\n",
      "Epoch [45/100], Step [15/225], Training Accuracy: 29.8958%, Training Loss: 0.6869%\n",
      "Epoch [45/100], Step [16/225], Training Accuracy: 29.9805%, Training Loss: 0.6865%\n",
      "Epoch [45/100], Step [17/225], Training Accuracy: 29.5037%, Training Loss: 0.6866%\n",
      "Epoch [45/100], Step [18/225], Training Accuracy: 29.6875%, Training Loss: 0.6866%\n",
      "Epoch [45/100], Step [19/225], Training Accuracy: 30.1809%, Training Loss: 0.6867%\n",
      "Epoch [45/100], Step [20/225], Training Accuracy: 30.6250%, Training Loss: 0.6866%\n",
      "Epoch [45/100], Step [21/225], Training Accuracy: 30.6548%, Training Loss: 0.6864%\n",
      "Epoch [45/100], Step [22/225], Training Accuracy: 30.7528%, Training Loss: 0.6862%\n",
      "Epoch [45/100], Step [23/225], Training Accuracy: 30.5707%, Training Loss: 0.6861%\n",
      "Epoch [45/100], Step [24/225], Training Accuracy: 30.7943%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [25/225], Training Accuracy: 31.0000%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [26/225], Training Accuracy: 31.4904%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [27/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [28/225], Training Accuracy: 31.0268%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [29/225], Training Accuracy: 31.4116%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [30/225], Training Accuracy: 31.5104%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [31/225], Training Accuracy: 31.4012%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [32/225], Training Accuracy: 31.7383%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [33/225], Training Accuracy: 31.8655%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [34/225], Training Accuracy: 31.7555%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [35/225], Training Accuracy: 31.6071%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [36/225], Training Accuracy: 31.4670%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [37/225], Training Accuracy: 31.5034%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [38/225], Training Accuracy: 31.3322%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [39/225], Training Accuracy: 31.0096%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [40/225], Training Accuracy: 31.0156%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [41/225], Training Accuracy: 31.1357%, Training Loss: 0.6852%\n",
      "Epoch [45/100], Step [42/225], Training Accuracy: 30.9524%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [43/225], Training Accuracy: 31.1047%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [44/225], Training Accuracy: 31.0369%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [45/225], Training Accuracy: 31.0069%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [46/225], Training Accuracy: 30.8424%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [47/225], Training Accuracy: 30.7846%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [48/225], Training Accuracy: 30.9245%, Training Loss: 0.6852%\n",
      "Epoch [45/100], Step [49/225], Training Accuracy: 30.9949%, Training Loss: 0.6852%\n",
      "Epoch [45/100], Step [50/225], Training Accuracy: 31.0312%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [51/225], Training Accuracy: 31.1581%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [52/225], Training Accuracy: 31.1899%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [53/225], Training Accuracy: 31.1026%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [54/225], Training Accuracy: 31.0185%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [55/225], Training Accuracy: 31.1648%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [56/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [57/225], Training Accuracy: 31.3048%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [58/225], Training Accuracy: 31.2231%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [59/225], Training Accuracy: 31.5148%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Step [60/225], Training Accuracy: 31.6667%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [61/225], Training Accuracy: 31.6086%, Training Loss: 0.6846%\n",
      "Epoch [45/100], Step [62/225], Training Accuracy: 31.6532%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [63/225], Training Accuracy: 31.6964%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [64/225], Training Accuracy: 31.6650%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [65/225], Training Accuracy: 31.6106%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [66/225], Training Accuracy: 31.6998%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [67/225], Training Accuracy: 31.7397%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [68/225], Training Accuracy: 31.7785%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [69/225], Training Accuracy: 31.7708%, Training Loss: 0.6846%\n",
      "Epoch [45/100], Step [70/225], Training Accuracy: 31.7411%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [71/225], Training Accuracy: 31.8442%, Training Loss: 0.6846%\n",
      "Epoch [45/100], Step [72/225], Training Accuracy: 31.6406%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [73/225], Training Accuracy: 31.5711%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [74/225], Training Accuracy: 31.6301%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [75/225], Training Accuracy: 31.5625%, Training Loss: 0.6847%\n",
      "Epoch [45/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [77/225], Training Accuracy: 31.4529%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [78/225], Training Accuracy: 31.5104%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [79/225], Training Accuracy: 31.4280%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [80/225], Training Accuracy: 31.4062%, Training Loss: 0.6848%\n",
      "Epoch [45/100], Step [81/225], Training Accuracy: 31.3079%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [82/225], Training Accuracy: 31.3262%, Training Loss: 0.6849%\n",
      "Epoch [45/100], Step [83/225], Training Accuracy: 31.2877%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [84/225], Training Accuracy: 31.3616%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [85/225], Training Accuracy: 31.2868%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [86/225], Training Accuracy: 31.3590%, Training Loss: 0.6850%\n",
      "Epoch [45/100], Step [87/225], Training Accuracy: 31.3398%, Training Loss: 0.6851%\n",
      "Epoch [45/100], Step [88/225], Training Accuracy: 31.3388%, Training Loss: 0.6852%\n",
      "Epoch [45/100], Step [89/225], Training Accuracy: 31.2851%, Training Loss: 0.6852%\n",
      "Epoch [45/100], Step [90/225], Training Accuracy: 31.1806%, Training Loss: 0.6853%\n",
      "Epoch [45/100], Step [91/225], Training Accuracy: 31.2157%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [92/225], Training Accuracy: 31.1990%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [93/225], Training Accuracy: 31.2332%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [94/225], Training Accuracy: 31.2999%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [95/225], Training Accuracy: 31.1842%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [96/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [97/225], Training Accuracy: 31.2661%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [98/225], Training Accuracy: 31.2659%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [99/225], Training Accuracy: 31.3763%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [100/225], Training Accuracy: 31.3906%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [101/225], Training Accuracy: 31.5130%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [102/225], Training Accuracy: 31.4032%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [103/225], Training Accuracy: 31.4624%, Training Loss: 0.6854%\n",
      "Epoch [45/100], Step [104/225], Training Accuracy: 31.4453%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [105/225], Training Accuracy: 31.3988%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [106/225], Training Accuracy: 31.3827%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [107/225], Training Accuracy: 31.2938%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [108/225], Training Accuracy: 31.3657%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [109/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [110/225], Training Accuracy: 31.2784%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [111/225], Training Accuracy: 31.1796%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [112/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [113/225], Training Accuracy: 31.2362%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [114/225], Training Accuracy: 31.3048%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [115/225], Training Accuracy: 31.2772%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [116/225], Training Accuracy: 31.2769%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [117/225], Training Accuracy: 31.2366%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [118/225], Training Accuracy: 31.2235%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [119/225], Training Accuracy: 31.1712%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [120/225], Training Accuracy: 31.2240%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [121/225], Training Accuracy: 31.2113%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [122/225], Training Accuracy: 31.2116%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [123/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [124/225], Training Accuracy: 31.2626%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [125/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [126/225], Training Accuracy: 31.2004%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [127/225], Training Accuracy: 31.1516%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [128/225], Training Accuracy: 31.1523%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [129/225], Training Accuracy: 31.2137%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [130/225], Training Accuracy: 31.1659%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [131/225], Training Accuracy: 31.1188%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [132/225], Training Accuracy: 31.0843%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [133/225], Training Accuracy: 31.0973%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [134/225], Training Accuracy: 31.1334%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [135/225], Training Accuracy: 31.1343%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [136/225], Training Accuracy: 31.1466%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [137/225], Training Accuracy: 31.1816%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [138/225], Training Accuracy: 31.2047%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [139/225], Training Accuracy: 31.1713%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [140/225], Training Accuracy: 31.1607%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [141/225], Training Accuracy: 31.1059%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [142/225], Training Accuracy: 31.1400%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [143/225], Training Accuracy: 31.1298%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [144/225], Training Accuracy: 31.1523%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [145/225], Training Accuracy: 31.1853%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [146/225], Training Accuracy: 31.2286%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [147/225], Training Accuracy: 31.2287%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [148/225], Training Accuracy: 31.1761%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [149/225], Training Accuracy: 31.2185%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [151/225], Training Accuracy: 31.2914%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [152/225], Training Accuracy: 31.2911%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [153/225], Training Accuracy: 31.2398%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [154/225], Training Accuracy: 31.2399%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [155/225], Training Accuracy: 31.2198%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [156/225], Training Accuracy: 31.2400%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [157/225], Training Accuracy: 31.1803%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [158/225], Training Accuracy: 31.1709%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [159/225], Training Accuracy: 31.2402%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [161/225], Training Accuracy: 31.2597%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [162/225], Training Accuracy: 31.2211%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [163/225], Training Accuracy: 31.2596%, Training Loss: 0.6860%\n",
      "Epoch [45/100], Step [164/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [165/225], Training Accuracy: 31.2121%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [166/225], Training Accuracy: 31.1935%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [167/225], Training Accuracy: 31.2313%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100], Step [168/225], Training Accuracy: 31.1756%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [169/225], Training Accuracy: 31.1021%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [170/225], Training Accuracy: 31.0662%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [171/225], Training Accuracy: 31.0947%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [173/225], Training Accuracy: 31.1326%, Training Loss: 0.6859%\n",
      "Epoch [45/100], Step [174/225], Training Accuracy: 31.1692%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [175/225], Training Accuracy: 31.1964%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [176/225], Training Accuracy: 31.2145%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [177/225], Training Accuracy: 31.2323%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [178/225], Training Accuracy: 31.2061%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [179/225], Training Accuracy: 31.1802%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [181/225], Training Accuracy: 31.1637%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [182/225], Training Accuracy: 31.1384%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [183/225], Training Accuracy: 31.1475%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [184/225], Training Accuracy: 31.1311%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [185/225], Training Accuracy: 31.1064%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [186/225], Training Accuracy: 31.1492%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [187/225], Training Accuracy: 31.1330%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [190/225], Training Accuracy: 31.1349%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [191/225], Training Accuracy: 31.1109%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [192/225], Training Accuracy: 31.0465%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [193/225], Training Accuracy: 31.0476%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [194/225], Training Accuracy: 31.0648%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [195/225], Training Accuracy: 31.0577%, Training Loss: 0.6858%\n",
      "Epoch [45/100], Step [196/225], Training Accuracy: 31.0348%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [197/225], Training Accuracy: 31.0755%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [198/225], Training Accuracy: 31.0922%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [199/225], Training Accuracy: 31.0537%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [200/225], Training Accuracy: 31.0391%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [201/225], Training Accuracy: 31.0634%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [202/225], Training Accuracy: 31.0721%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [203/225], Training Accuracy: 31.0653%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [204/225], Training Accuracy: 31.1198%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [205/225], Training Accuracy: 31.1052%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [207/225], Training Accuracy: 31.0688%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [208/225], Training Accuracy: 31.0998%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [209/225], Training Accuracy: 31.1005%, Training Loss: 0.6857%\n",
      "Epoch [45/100], Step [210/225], Training Accuracy: 31.1458%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [211/225], Training Accuracy: 31.1315%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [212/225], Training Accuracy: 31.1910%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [213/225], Training Accuracy: 31.1693%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [214/225], Training Accuracy: 31.1770%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [215/225], Training Accuracy: 31.1483%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [216/225], Training Accuracy: 31.0836%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [217/225], Training Accuracy: 31.0700%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [218/225], Training Accuracy: 31.0421%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [219/225], Training Accuracy: 31.1073%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [220/225], Training Accuracy: 31.1222%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [221/225], Training Accuracy: 31.1015%, Training Loss: 0.6856%\n",
      "Epoch [45/100], Step [222/225], Training Accuracy: 31.1163%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [223/225], Training Accuracy: 31.1449%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [224/225], Training Accuracy: 31.1454%, Training Loss: 0.6855%\n",
      "Epoch [45/100], Step [225/225], Training Accuracy: 31.1145%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6823%\n",
      "Epoch [46/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6880%\n",
      "Epoch [46/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6841%\n",
      "Epoch [46/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6842%\n",
      "Epoch [46/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6859%\n",
      "Epoch [46/100], Step [15/225], Training Accuracy: 30.4167%, Training Loss: 0.6859%\n",
      "Epoch [46/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6857%\n",
      "Epoch [46/100], Step [18/225], Training Accuracy: 30.4688%, Training Loss: 0.6857%\n",
      "Epoch [46/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6858%\n",
      "Epoch [46/100], Step [20/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [46/100], Step [21/225], Training Accuracy: 31.1756%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [22/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [23/225], Training Accuracy: 31.1821%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [24/225], Training Accuracy: 31.3802%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [25/225], Training Accuracy: 31.5000%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [27/225], Training Accuracy: 31.5394%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [28/225], Training Accuracy: 31.4174%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [29/225], Training Accuracy: 31.7349%, Training Loss: 0.6848%\n",
      "Epoch [46/100], Step [30/225], Training Accuracy: 31.7708%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [32/225], Training Accuracy: 31.8359%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [33/225], Training Accuracy: 31.9129%, Training Loss: 0.6841%\n",
      "Epoch [46/100], Step [34/225], Training Accuracy: 31.8015%, Training Loss: 0.6841%\n",
      "Epoch [46/100], Step [35/225], Training Accuracy: 31.6964%, Training Loss: 0.6842%\n",
      "Epoch [46/100], Step [36/225], Training Accuracy: 31.5104%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [37/225], Training Accuracy: 31.5878%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [38/225], Training Accuracy: 31.4556%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [39/225], Training Accuracy: 31.1699%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [40/225], Training Accuracy: 31.2891%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [41/225], Training Accuracy: 31.3262%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [42/225], Training Accuracy: 31.1384%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [43/225], Training Accuracy: 31.3227%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [44/225], Training Accuracy: 31.3210%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [45/225], Training Accuracy: 31.3542%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [46/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [47/225], Training Accuracy: 31.2168%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [48/225], Training Accuracy: 31.3802%, Training Loss: 0.6848%\n",
      "Epoch [46/100], Step [49/225], Training Accuracy: 31.4413%, Training Loss: 0.6848%\n",
      "Epoch [46/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [51/225], Training Accuracy: 31.4645%, Training Loss: 0.6848%\n",
      "Epoch [46/100], Step [52/225], Training Accuracy: 31.4603%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [55/225], Training Accuracy: 31.4773%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [56/225], Training Accuracy: 31.5848%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [57/225], Training Accuracy: 31.6064%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [58/225], Training Accuracy: 31.5463%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [59/225], Training Accuracy: 31.9121%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [60/225], Training Accuracy: 31.9792%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [61/225], Training Accuracy: 31.9160%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [62/225], Training Accuracy: 31.9304%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [63/225], Training Accuracy: 32.0437%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [64/225], Training Accuracy: 32.0557%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [65/225], Training Accuracy: 31.9471%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [66/225], Training Accuracy: 32.0549%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [67/225], Training Accuracy: 31.9963%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [68/225], Training Accuracy: 32.0542%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [69/225], Training Accuracy: 32.0199%, Training Loss: 0.6842%\n",
      "Epoch [46/100], Step [70/225], Training Accuracy: 31.9420%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [71/225], Training Accuracy: 31.9542%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [72/225], Training Accuracy: 31.7274%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [73/225], Training Accuracy: 31.6995%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [74/225], Training Accuracy: 31.7779%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [75/225], Training Accuracy: 31.7083%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6842%\n",
      "Epoch [46/100], Step [77/225], Training Accuracy: 31.5747%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [78/225], Training Accuracy: 31.6306%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [79/225], Training Accuracy: 31.5665%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [80/225], Training Accuracy: 31.5625%, Training Loss: 0.6843%\n",
      "Epoch [46/100], Step [81/225], Training Accuracy: 31.5008%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [82/225], Training Accuracy: 31.4977%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [83/225], Training Accuracy: 31.4383%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [84/225], Training Accuracy: 31.5104%, Training Loss: 0.6844%\n",
      "Epoch [46/100], Step [85/225], Training Accuracy: 31.4706%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [86/225], Training Accuracy: 31.5044%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [87/225], Training Accuracy: 31.5374%, Training Loss: 0.6845%\n",
      "Epoch [46/100], Step [88/225], Training Accuracy: 31.5341%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [89/225], Training Accuracy: 31.4607%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [90/225], Training Accuracy: 31.3715%, Training Loss: 0.6846%\n",
      "Epoch [46/100], Step [91/225], Training Accuracy: 31.4045%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [92/225], Training Accuracy: 31.4029%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [93/225], Training Accuracy: 31.4180%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [94/225], Training Accuracy: 31.4827%, Training Loss: 0.6847%\n",
      "Epoch [46/100], Step [95/225], Training Accuracy: 31.3651%, Training Loss: 0.6848%\n",
      "Epoch [46/100], Step [96/225], Training Accuracy: 31.4290%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [97/225], Training Accuracy: 31.4755%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [98/225], Training Accuracy: 31.4732%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [100/225], Training Accuracy: 31.6094%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [101/225], Training Accuracy: 31.7296%, Training Loss: 0.6848%\n",
      "Epoch [46/100], Step [102/225], Training Accuracy: 31.6176%, Training Loss: 0.6849%\n",
      "Epoch [46/100], Step [103/225], Training Accuracy: 31.6444%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [104/225], Training Accuracy: 31.6256%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [105/225], Training Accuracy: 31.5923%, Training Loss: 0.6850%\n",
      "Epoch [46/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [107/225], Training Accuracy: 31.4690%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [109/225], Training Accuracy: 31.4220%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [110/225], Training Accuracy: 31.4062%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [111/225], Training Accuracy: 31.2922%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [112/225], Training Accuracy: 31.3895%, Training Loss: 0.6851%\n",
      "Epoch [46/100], Step [113/225], Training Accuracy: 31.4021%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [114/225], Training Accuracy: 31.4556%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [115/225], Training Accuracy: 31.4266%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [116/225], Training Accuracy: 31.4116%, Training Loss: 0.6852%\n",
      "Epoch [46/100], Step [117/225], Training Accuracy: 31.3568%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [118/225], Training Accuracy: 31.3294%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [119/225], Training Accuracy: 31.2369%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [120/225], Training Accuracy: 31.2240%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [121/225], Training Accuracy: 31.2113%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [122/225], Training Accuracy: 31.1988%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [123/225], Training Accuracy: 31.2119%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [124/225], Training Accuracy: 31.1870%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [125/225], Training Accuracy: 31.1750%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [126/225], Training Accuracy: 31.1136%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [127/225], Training Accuracy: 31.0901%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [128/225], Training Accuracy: 31.0913%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [129/225], Training Accuracy: 31.1410%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [130/225], Training Accuracy: 31.0817%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [131/225], Training Accuracy: 31.0353%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [132/225], Training Accuracy: 30.9896%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [133/225], Training Accuracy: 31.0150%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [134/225], Training Accuracy: 31.0634%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [135/225], Training Accuracy: 31.0648%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [136/225], Training Accuracy: 31.0777%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [137/225], Training Accuracy: 31.1245%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [138/225], Training Accuracy: 31.1481%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [139/225], Training Accuracy: 31.1039%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [140/225], Training Accuracy: 31.0714%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [141/225], Training Accuracy: 31.0284%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [142/225], Training Accuracy: 31.0629%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [143/225], Training Accuracy: 31.0424%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [144/225], Training Accuracy: 31.0764%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [145/225], Training Accuracy: 31.1207%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [146/225], Training Accuracy: 31.1644%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [147/225], Training Accuracy: 31.1969%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [149/225], Training Accuracy: 31.2185%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [150/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [151/225], Training Accuracy: 31.2914%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [152/225], Training Accuracy: 31.2706%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [153/225], Training Accuracy: 31.2398%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [154/225], Training Accuracy: 31.2601%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [155/225], Training Accuracy: 31.2298%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [156/225], Training Accuracy: 31.2600%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [157/225], Training Accuracy: 31.1903%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [158/225], Training Accuracy: 31.1808%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Step [159/225], Training Accuracy: 31.2697%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [160/225], Training Accuracy: 31.2402%, Training Loss: 0.6856%\n",
      "Epoch [46/100], Step [161/225], Training Accuracy: 31.2888%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [162/225], Training Accuracy: 31.2789%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [163/225], Training Accuracy: 31.3267%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [164/225], Training Accuracy: 31.3167%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [165/225], Training Accuracy: 31.2311%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [166/225], Training Accuracy: 31.2312%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [167/225], Training Accuracy: 31.2594%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [168/225], Training Accuracy: 31.2221%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [169/225], Training Accuracy: 31.1483%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [170/225], Training Accuracy: 31.1029%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [171/225], Training Accuracy: 31.1221%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [172/225], Training Accuracy: 31.1228%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [173/225], Training Accuracy: 31.1055%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [174/225], Training Accuracy: 31.1153%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [175/225], Training Accuracy: 31.1607%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [176/225], Training Accuracy: 31.1790%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [177/225], Training Accuracy: 31.1882%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [178/225], Training Accuracy: 31.1710%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [179/225], Training Accuracy: 31.1453%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [180/225], Training Accuracy: 31.1979%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [181/225], Training Accuracy: 31.1550%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [182/225], Training Accuracy: 31.1470%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [183/225], Training Accuracy: 31.1561%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [184/225], Training Accuracy: 31.1311%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [185/225], Training Accuracy: 31.1064%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [186/225], Training Accuracy: 31.1240%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [187/225], Training Accuracy: 31.1247%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [188/225], Training Accuracy: 31.1087%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [189/225], Training Accuracy: 31.1177%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [190/225], Training Accuracy: 31.0855%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [191/225], Training Accuracy: 31.0618%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [192/225], Training Accuracy: 30.9896%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [193/225], Training Accuracy: 30.9990%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [194/225], Training Accuracy: 30.9923%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [195/225], Training Accuracy: 30.9696%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [196/225], Training Accuracy: 30.9391%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [197/225], Training Accuracy: 30.9803%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [198/225], Training Accuracy: 31.0054%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [199/225], Training Accuracy: 30.9752%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [200/225], Training Accuracy: 30.9688%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [201/225], Training Accuracy: 30.9779%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [202/225], Training Accuracy: 30.9870%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [203/225], Training Accuracy: 30.9883%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [204/225], Training Accuracy: 31.0432%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [205/225], Training Accuracy: 31.0290%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [206/225], Training Accuracy: 31.0300%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [207/225], Training Accuracy: 30.9934%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [208/225], Training Accuracy: 31.0171%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [209/225], Training Accuracy: 31.0481%, Training Loss: 0.6855%\n",
      "Epoch [46/100], Step [210/225], Training Accuracy: 31.0938%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [211/225], Training Accuracy: 31.0871%, Training Loss: 0.6854%\n",
      "Epoch [46/100], Step [212/225], Training Accuracy: 31.1321%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [213/225], Training Accuracy: 31.1033%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [214/225], Training Accuracy: 31.1332%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [215/225], Training Accuracy: 31.1119%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [216/225], Training Accuracy: 31.0547%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [217/225], Training Accuracy: 31.0556%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [218/225], Training Accuracy: 31.0135%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [219/225], Training Accuracy: 31.0716%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [220/225], Training Accuracy: 31.0795%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [221/225], Training Accuracy: 31.0732%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [222/225], Training Accuracy: 31.0881%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [223/225], Training Accuracy: 31.1099%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [224/225], Training Accuracy: 31.1035%, Training Loss: 0.6853%\n",
      "Epoch [46/100], Step [225/225], Training Accuracy: 31.0867%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6940%\n",
      "Epoch [47/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6885%\n",
      "Epoch [47/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6896%\n",
      "Epoch [47/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6865%\n",
      "Epoch [47/100], Step [5/225], Training Accuracy: 34.0625%, Training Loss: 0.6858%\n",
      "Epoch [47/100], Step [6/225], Training Accuracy: 33.3333%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [7/225], Training Accuracy: 32.8125%, Training Loss: 0.6840%\n",
      "Epoch [47/100], Step [8/225], Training Accuracy: 32.4219%, Training Loss: 0.6837%\n",
      "Epoch [47/100], Step [9/225], Training Accuracy: 32.4653%, Training Loss: 0.6846%\n",
      "Epoch [47/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6845%\n",
      "Epoch [47/100], Step [11/225], Training Accuracy: 31.6761%, Training Loss: 0.6844%\n",
      "Epoch [47/100], Step [12/225], Training Accuracy: 30.9896%, Training Loss: 0.6842%\n",
      "Epoch [47/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6845%\n",
      "Epoch [47/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [17/225], Training Accuracy: 30.7904%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [24/225], Training Accuracy: 31.4453%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [25/225], Training Accuracy: 31.6875%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [27/225], Training Accuracy: 31.5972%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [28/225], Training Accuracy: 31.4174%, Training Loss: 0.6849%\n",
      "Epoch [47/100], Step [29/225], Training Accuracy: 31.8427%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [30/225], Training Accuracy: 31.8750%, Training Loss: 0.6846%\n",
      "Epoch [47/100], Step [31/225], Training Accuracy: 31.7540%, Training Loss: 0.6845%\n",
      "Epoch [47/100], Step [32/225], Training Accuracy: 31.8848%, Training Loss: 0.6843%\n",
      "Epoch [47/100], Step [33/225], Training Accuracy: 32.0076%, Training Loss: 0.6843%\n",
      "Epoch [47/100], Step [34/225], Training Accuracy: 31.8015%, Training Loss: 0.6843%\n",
      "Epoch [47/100], Step [35/225], Training Accuracy: 31.6518%, Training Loss: 0.6845%\n",
      "Epoch [47/100], Step [36/225], Training Accuracy: 31.5538%, Training Loss: 0.6845%\n",
      "Epoch [47/100], Step [37/225], Training Accuracy: 31.5456%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [38/225], Training Accuracy: 31.3734%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [39/225], Training Accuracy: 31.1298%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [40/225], Training Accuracy: 31.1719%, Training Loss: 0.6850%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Step [41/225], Training Accuracy: 31.2119%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [42/225], Training Accuracy: 30.9896%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [43/225], Training Accuracy: 31.1773%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [45/225], Training Accuracy: 31.1458%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [46/225], Training Accuracy: 31.0802%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [47/225], Training Accuracy: 31.0505%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [48/225], Training Accuracy: 31.2174%, Training Loss: 0.6849%\n",
      "Epoch [47/100], Step [49/225], Training Accuracy: 31.2819%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [50/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [51/225], Training Accuracy: 31.4338%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [52/225], Training Accuracy: 31.4603%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [54/225], Training Accuracy: 31.3079%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [55/225], Training Accuracy: 31.4205%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [56/225], Training Accuracy: 31.4453%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [57/225], Training Accuracy: 31.4419%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [58/225], Training Accuracy: 31.3308%, Training Loss: 0.6849%\n",
      "Epoch [47/100], Step [59/225], Training Accuracy: 31.7002%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [60/225], Training Accuracy: 31.8490%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [62/225], Training Accuracy: 31.8548%, Training Loss: 0.6849%\n",
      "Epoch [47/100], Step [63/225], Training Accuracy: 31.9196%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [64/225], Training Accuracy: 31.9336%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [69/225], Training Accuracy: 31.9973%, Training Loss: 0.6846%\n",
      "Epoch [47/100], Step [70/225], Training Accuracy: 31.9866%, Training Loss: 0.6846%\n",
      "Epoch [47/100], Step [71/225], Training Accuracy: 32.0863%, Training Loss: 0.6845%\n",
      "Epoch [47/100], Step [72/225], Training Accuracy: 31.8359%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [73/225], Training Accuracy: 31.7851%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [74/225], Training Accuracy: 31.8834%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [76/225], Training Accuracy: 31.7434%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [77/225], Training Accuracy: 31.6761%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [78/225], Training Accuracy: 31.7107%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [79/225], Training Accuracy: 31.6456%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [80/225], Training Accuracy: 31.6406%, Training Loss: 0.6846%\n",
      "Epoch [47/100], Step [81/225], Training Accuracy: 31.5201%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [82/225], Training Accuracy: 31.5358%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [83/225], Training Accuracy: 31.4571%, Training Loss: 0.6846%\n",
      "Epoch [47/100], Step [84/225], Training Accuracy: 31.5290%, Training Loss: 0.6847%\n",
      "Epoch [47/100], Step [85/225], Training Accuracy: 31.5441%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [86/225], Training Accuracy: 31.6134%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [87/225], Training Accuracy: 31.6451%, Training Loss: 0.6848%\n",
      "Epoch [47/100], Step [88/225], Training Accuracy: 31.6229%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [89/225], Training Accuracy: 31.5133%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [90/225], Training Accuracy: 31.4062%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [91/225], Training Accuracy: 31.4389%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [92/225], Training Accuracy: 31.3859%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [94/225], Training Accuracy: 31.4328%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [95/225], Training Accuracy: 31.2993%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [96/225], Training Accuracy: 31.3639%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [97/225], Training Accuracy: 31.3950%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [98/225], Training Accuracy: 31.4094%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [99/225], Training Accuracy: 31.5183%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [100/225], Training Accuracy: 31.5000%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [101/225], Training Accuracy: 31.6368%, Training Loss: 0.6850%\n",
      "Epoch [47/100], Step [102/225], Training Accuracy: 31.4951%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [103/225], Training Accuracy: 31.5382%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [104/225], Training Accuracy: 31.5355%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [105/225], Training Accuracy: 31.4881%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [106/225], Training Accuracy: 31.4711%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [107/225], Training Accuracy: 31.3814%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [108/225], Training Accuracy: 31.4236%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [109/225], Training Accuracy: 31.3073%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [110/225], Training Accuracy: 31.2926%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [111/225], Training Accuracy: 31.1796%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [112/225], Training Accuracy: 31.2640%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [113/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [114/225], Training Accuracy: 31.2911%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [115/225], Training Accuracy: 31.2772%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [116/225], Training Accuracy: 31.2904%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [117/225], Training Accuracy: 31.2233%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [118/225], Training Accuracy: 31.1970%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [119/225], Training Accuracy: 31.1843%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [120/225], Training Accuracy: 31.1979%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [121/225], Training Accuracy: 31.1725%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [122/225], Training Accuracy: 31.1860%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [123/225], Training Accuracy: 31.2119%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [124/225], Training Accuracy: 31.1996%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [125/225], Training Accuracy: 31.1750%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [126/225], Training Accuracy: 31.1136%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [127/225], Training Accuracy: 31.1024%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [128/225], Training Accuracy: 31.0913%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [129/225], Training Accuracy: 31.1289%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [130/225], Training Accuracy: 31.0817%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [131/225], Training Accuracy: 31.0353%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [132/225], Training Accuracy: 31.0014%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [133/225], Training Accuracy: 31.0268%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [134/225], Training Accuracy: 31.1101%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [135/225], Training Accuracy: 31.1227%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [136/225], Training Accuracy: 31.1351%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [137/225], Training Accuracy: 31.1816%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [138/225], Training Accuracy: 31.1821%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [139/225], Training Accuracy: 31.1263%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [140/225], Training Accuracy: 31.1049%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [141/225], Training Accuracy: 31.0616%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [142/225], Training Accuracy: 31.0960%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [143/225], Training Accuracy: 31.0861%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [144/225], Training Accuracy: 31.0872%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [145/225], Training Accuracy: 31.1530%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [146/225], Training Accuracy: 31.1965%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [147/225], Training Accuracy: 31.2287%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [148/225], Training Accuracy: 31.2078%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [150/225], Training Accuracy: 31.2708%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [151/225], Training Accuracy: 31.2914%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [152/225], Training Accuracy: 31.2911%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [153/225], Training Accuracy: 31.2704%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [154/225], Training Accuracy: 31.3210%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [155/225], Training Accuracy: 31.3105%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [156/225], Training Accuracy: 31.3301%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [157/225], Training Accuracy: 31.2699%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [158/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [159/225], Training Accuracy: 31.2893%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [160/225], Training Accuracy: 31.2598%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [161/225], Training Accuracy: 31.3082%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [162/225], Training Accuracy: 31.2886%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [163/225], Training Accuracy: 31.3554%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [164/225], Training Accuracy: 31.3357%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [165/225], Training Accuracy: 31.2784%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [166/225], Training Accuracy: 31.2594%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [167/225], Training Accuracy: 31.2874%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [168/225], Training Accuracy: 31.2407%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [169/225], Training Accuracy: 31.1575%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [170/225], Training Accuracy: 31.1213%, Training Loss: 0.6857%\n",
      "Epoch [47/100], Step [171/225], Training Accuracy: 31.1312%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [172/225], Training Accuracy: 31.1319%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [173/225], Training Accuracy: 31.1326%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [174/225], Training Accuracy: 31.1512%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [175/225], Training Accuracy: 31.1786%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [176/225], Training Accuracy: 31.2056%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [177/225], Training Accuracy: 31.2059%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [178/225], Training Accuracy: 31.2061%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [179/225], Training Accuracy: 31.1802%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [180/225], Training Accuracy: 31.2326%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [181/225], Training Accuracy: 31.1896%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [182/225], Training Accuracy: 31.1899%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [183/225], Training Accuracy: 31.1902%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [185/225], Training Accuracy: 31.1318%, Training Loss: 0.6856%\n",
      "Epoch [47/100], Step [186/225], Training Accuracy: 31.1492%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [187/225], Training Accuracy: 31.1414%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [188/225], Training Accuracy: 31.1336%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [190/225], Training Accuracy: 31.1266%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [191/225], Training Accuracy: 31.1027%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [192/225], Training Accuracy: 31.0384%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [193/225], Training Accuracy: 31.0233%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [194/225], Training Accuracy: 31.0406%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [195/225], Training Accuracy: 31.0256%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [196/225], Training Accuracy: 30.9949%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [197/225], Training Accuracy: 31.0041%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [198/225], Training Accuracy: 31.0211%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [199/225], Training Accuracy: 30.9909%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [200/225], Training Accuracy: 30.9766%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [201/225], Training Accuracy: 30.9857%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [202/225], Training Accuracy: 30.9870%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [203/225], Training Accuracy: 30.9652%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [204/225], Training Accuracy: 31.0432%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [205/225], Training Accuracy: 31.0442%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [206/225], Training Accuracy: 31.0452%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [207/225], Training Accuracy: 31.0386%, Training Loss: 0.6855%\n",
      "Epoch [47/100], Step [208/225], Training Accuracy: 31.0472%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [209/225], Training Accuracy: 31.0855%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [210/225], Training Accuracy: 31.1086%, Training Loss: 0.6854%\n",
      "Epoch [47/100], Step [211/225], Training Accuracy: 31.0945%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [212/225], Training Accuracy: 31.1394%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [213/225], Training Accuracy: 31.1180%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [214/225], Training Accuracy: 31.1405%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [215/225], Training Accuracy: 31.1192%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [216/225], Training Accuracy: 31.0547%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [217/225], Training Accuracy: 31.0700%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [218/225], Training Accuracy: 31.0421%, Training Loss: 0.6853%\n",
      "Epoch [47/100], Step [219/225], Training Accuracy: 31.1002%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [220/225], Training Accuracy: 31.1080%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [221/225], Training Accuracy: 31.1015%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [222/225], Training Accuracy: 31.1163%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [223/225], Training Accuracy: 31.1449%, Training Loss: 0.6852%\n",
      "Epoch [47/100], Step [224/225], Training Accuracy: 31.1523%, Training Loss: 0.6851%\n",
      "Epoch [47/100], Step [225/225], Training Accuracy: 31.1284%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [1/225], Training Accuracy: 37.5000%, Training Loss: 0.6918%\n",
      "Epoch [48/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6872%\n",
      "Epoch [48/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6878%\n",
      "Epoch [48/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6856%\n",
      "Epoch [48/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6843%\n",
      "Epoch [48/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6843%\n",
      "Epoch [48/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6856%\n",
      "Epoch [48/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [20/225], Training Accuracy: 31.4062%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [23/225], Training Accuracy: 31.5897%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [24/225], Training Accuracy: 31.6406%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [26/225], Training Accuracy: 32.0913%, Training Loss: 0.6843%\n",
      "Epoch [48/100], Step [27/225], Training Accuracy: 31.7708%, Training Loss: 0.6842%\n",
      "Epoch [48/100], Step [28/225], Training Accuracy: 31.5290%, Training Loss: 0.6843%\n",
      "Epoch [48/100], Step [29/225], Training Accuracy: 31.8966%, Training Loss: 0.6841%\n",
      "Epoch [48/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6842%\n",
      "Epoch [48/100], Step [31/225], Training Accuracy: 31.7036%, Training Loss: 0.6842%\n",
      "Epoch [48/100], Step [32/225], Training Accuracy: 31.9336%, Training Loss: 0.6840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Step [33/225], Training Accuracy: 32.0076%, Training Loss: 0.6839%\n",
      "Epoch [48/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6839%\n",
      "Epoch [48/100], Step [35/225], Training Accuracy: 31.7857%, Training Loss: 0.6840%\n",
      "Epoch [48/100], Step [36/225], Training Accuracy: 31.6406%, Training Loss: 0.6843%\n",
      "Epoch [48/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6845%\n",
      "Epoch [48/100], Step [38/225], Training Accuracy: 31.5789%, Training Loss: 0.6846%\n",
      "Epoch [48/100], Step [39/225], Training Accuracy: 31.2500%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [40/225], Training Accuracy: 31.3672%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [41/225], Training Accuracy: 31.3643%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [42/225], Training Accuracy: 31.1384%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [43/225], Training Accuracy: 31.3227%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [44/225], Training Accuracy: 31.3565%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [45/225], Training Accuracy: 31.4236%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [46/225], Training Accuracy: 31.2840%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [47/225], Training Accuracy: 31.2168%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [48/225], Training Accuracy: 31.2826%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [49/225], Training Accuracy: 31.3138%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [51/225], Training Accuracy: 31.4951%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [52/225], Training Accuracy: 31.4904%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [56/225], Training Accuracy: 31.5011%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [57/225], Training Accuracy: 31.4967%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [58/225], Training Accuracy: 31.4116%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [59/225], Training Accuracy: 31.7267%, Training Loss: 0.6845%\n",
      "Epoch [48/100], Step [60/225], Training Accuracy: 31.8229%, Training Loss: 0.6845%\n",
      "Epoch [48/100], Step [61/225], Training Accuracy: 31.7367%, Training Loss: 0.6845%\n",
      "Epoch [48/100], Step [62/225], Training Accuracy: 31.8044%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [63/225], Training Accuracy: 31.8948%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [64/225], Training Accuracy: 31.9092%, Training Loss: 0.6846%\n",
      "Epoch [48/100], Step [65/225], Training Accuracy: 31.8750%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [66/225], Training Accuracy: 31.9366%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6846%\n",
      "Epoch [48/100], Step [69/225], Training Accuracy: 31.9746%, Training Loss: 0.6844%\n",
      "Epoch [48/100], Step [70/225], Training Accuracy: 31.8750%, Training Loss: 0.6845%\n",
      "Epoch [48/100], Step [71/225], Training Accuracy: 31.9102%, Training Loss: 0.6845%\n",
      "Epoch [48/100], Step [72/225], Training Accuracy: 31.7274%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [73/225], Training Accuracy: 31.6567%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [74/225], Training Accuracy: 31.7145%, Training Loss: 0.6846%\n",
      "Epoch [48/100], Step [75/225], Training Accuracy: 31.6458%, Training Loss: 0.6846%\n",
      "Epoch [48/100], Step [76/225], Training Accuracy: 31.6201%, Training Loss: 0.6847%\n",
      "Epoch [48/100], Step [77/225], Training Accuracy: 31.5138%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [78/225], Training Accuracy: 31.5905%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [79/225], Training Accuracy: 31.5269%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [80/225], Training Accuracy: 31.5039%, Training Loss: 0.6848%\n",
      "Epoch [48/100], Step [81/225], Training Accuracy: 31.4236%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [82/225], Training Accuracy: 31.4405%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [83/225], Training Accuracy: 31.3818%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [84/225], Training Accuracy: 31.4732%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [85/225], Training Accuracy: 31.4154%, Training Loss: 0.6849%\n",
      "Epoch [48/100], Step [86/225], Training Accuracy: 31.4499%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [87/225], Training Accuracy: 31.4655%, Training Loss: 0.6850%\n",
      "Epoch [48/100], Step [88/225], Training Accuracy: 31.4631%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [89/225], Training Accuracy: 31.4080%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [90/225], Training Accuracy: 31.3368%, Training Loss: 0.6851%\n",
      "Epoch [48/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [92/225], Training Accuracy: 31.3349%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [94/225], Training Accuracy: 31.4162%, Training Loss: 0.6852%\n",
      "Epoch [48/100], Step [95/225], Training Accuracy: 31.2993%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [96/225], Training Accuracy: 31.3639%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [97/225], Training Accuracy: 31.3950%, Training Loss: 0.6855%\n",
      "Epoch [48/100], Step [98/225], Training Accuracy: 31.3776%, Training Loss: 0.6855%\n",
      "Epoch [48/100], Step [99/225], Training Accuracy: 31.5183%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [100/225], Training Accuracy: 31.5156%, Training Loss: 0.6855%\n",
      "Epoch [48/100], Step [101/225], Training Accuracy: 31.6213%, Training Loss: 0.6854%\n",
      "Epoch [48/100], Step [102/225], Training Accuracy: 31.5257%, Training Loss: 0.6855%\n",
      "Epoch [48/100], Step [103/225], Training Accuracy: 31.6141%, Training Loss: 0.6855%\n",
      "Epoch [48/100], Step [104/225], Training Accuracy: 31.6106%, Training Loss: 0.6856%\n",
      "Epoch [48/100], Step [105/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [48/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6856%\n",
      "Epoch [48/100], Step [107/225], Training Accuracy: 31.4982%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [108/225], Training Accuracy: 31.5972%, Training Loss: 0.6856%\n",
      "Epoch [48/100], Step [109/225], Training Accuracy: 31.4650%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [110/225], Training Accuracy: 31.4773%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [111/225], Training Accuracy: 31.3626%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [112/225], Training Accuracy: 31.4314%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [113/225], Training Accuracy: 31.4159%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [114/225], Training Accuracy: 31.4282%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [115/225], Training Accuracy: 31.4130%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [116/225], Training Accuracy: 31.3982%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [117/225], Training Accuracy: 31.3168%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [118/225], Training Accuracy: 31.3030%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [119/225], Training Accuracy: 31.2631%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [120/225], Training Accuracy: 31.3021%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [121/225], Training Accuracy: 31.2887%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [122/225], Training Accuracy: 31.3140%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [123/225], Training Accuracy: 31.3389%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [124/225], Training Accuracy: 31.3130%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [125/225], Training Accuracy: 31.2750%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [126/225], Training Accuracy: 31.2128%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [127/225], Training Accuracy: 31.1762%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [128/225], Training Accuracy: 31.1401%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [129/225], Training Accuracy: 31.1773%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [130/225], Training Accuracy: 31.1298%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [131/225], Training Accuracy: 31.0830%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [132/225], Training Accuracy: 31.0724%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [133/225], Training Accuracy: 31.0973%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Step [134/225], Training Accuracy: 31.1451%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [135/225], Training Accuracy: 31.1690%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [136/225], Training Accuracy: 31.1696%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [137/225], Training Accuracy: 31.1816%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [138/225], Training Accuracy: 31.1934%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [139/225], Training Accuracy: 31.1826%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [140/225], Training Accuracy: 31.1719%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [141/225], Training Accuracy: 31.1170%, Training Loss: 0.6861%\n",
      "Epoch [48/100], Step [142/225], Training Accuracy: 31.1620%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [143/225], Training Accuracy: 31.1626%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [144/225], Training Accuracy: 31.1740%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [145/225], Training Accuracy: 31.2177%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [146/225], Training Accuracy: 31.2714%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [147/225], Training Accuracy: 31.2925%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [148/225], Training Accuracy: 31.2711%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [149/225], Training Accuracy: 31.3129%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [150/225], Training Accuracy: 31.3542%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [151/225], Training Accuracy: 31.4156%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [152/225], Training Accuracy: 31.3939%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [153/225], Training Accuracy: 31.3521%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [154/225], Training Accuracy: 31.3515%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [155/225], Training Accuracy: 31.3206%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [156/225], Training Accuracy: 31.3602%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [157/225], Training Accuracy: 31.2799%, Training Loss: 0.6861%\n",
      "Epoch [48/100], Step [158/225], Training Accuracy: 31.2599%, Training Loss: 0.6861%\n",
      "Epoch [48/100], Step [159/225], Training Accuracy: 31.3188%, Training Loss: 0.6861%\n",
      "Epoch [48/100], Step [160/225], Training Accuracy: 31.2891%, Training Loss: 0.6861%\n",
      "Epoch [48/100], Step [161/225], Training Accuracy: 31.3373%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [162/225], Training Accuracy: 31.3175%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [163/225], Training Accuracy: 31.3650%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [164/225], Training Accuracy: 31.3262%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [165/225], Training Accuracy: 31.2595%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [166/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [167/225], Training Accuracy: 31.2968%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [168/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [169/225], Training Accuracy: 31.1760%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [170/225], Training Accuracy: 31.1397%, Training Loss: 0.6861%\n",
      "Epoch [48/100], Step [171/225], Training Accuracy: 31.1769%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [172/225], Training Accuracy: 31.1864%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [175/225], Training Accuracy: 31.2232%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [176/225], Training Accuracy: 31.2411%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [177/225], Training Accuracy: 31.2412%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [178/225], Training Accuracy: 31.2412%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [179/225], Training Accuracy: 31.2064%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [180/225], Training Accuracy: 31.2326%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [181/225], Training Accuracy: 31.1809%, Training Loss: 0.6860%\n",
      "Epoch [48/100], Step [182/225], Training Accuracy: 31.1813%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [183/225], Training Accuracy: 31.1817%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [184/225], Training Accuracy: 31.1651%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [185/225], Training Accuracy: 31.1402%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [186/225], Training Accuracy: 31.1576%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [187/225], Training Accuracy: 31.1581%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [188/225], Training Accuracy: 31.1253%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [189/225], Training Accuracy: 31.1591%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [190/225], Training Accuracy: 31.1266%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [191/225], Training Accuracy: 31.1109%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [192/225], Training Accuracy: 31.0384%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [193/225], Training Accuracy: 31.0395%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [194/225], Training Accuracy: 31.0648%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [195/225], Training Accuracy: 31.0417%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [196/225], Training Accuracy: 31.0108%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [197/225], Training Accuracy: 31.0596%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [198/225], Training Accuracy: 31.0764%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [199/225], Training Accuracy: 31.0694%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [200/225], Training Accuracy: 31.0469%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [201/225], Training Accuracy: 31.0557%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [202/225], Training Accuracy: 31.0566%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [203/225], Training Accuracy: 31.0499%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [204/225], Training Accuracy: 31.1045%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [205/225], Training Accuracy: 31.0899%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [207/225], Training Accuracy: 31.0613%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [208/225], Training Accuracy: 31.0847%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [209/225], Training Accuracy: 31.1229%, Training Loss: 0.6859%\n",
      "Epoch [48/100], Step [210/225], Training Accuracy: 31.1458%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [211/225], Training Accuracy: 31.1389%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [212/225], Training Accuracy: 31.1616%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [213/225], Training Accuracy: 31.1400%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [214/225], Training Accuracy: 31.1478%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [215/225], Training Accuracy: 31.1119%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [216/225], Training Accuracy: 31.0547%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [217/225], Training Accuracy: 31.0628%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [218/225], Training Accuracy: 31.0350%, Training Loss: 0.6858%\n",
      "Epoch [48/100], Step [219/225], Training Accuracy: 31.1002%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [220/225], Training Accuracy: 31.1222%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [221/225], Training Accuracy: 31.1298%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [222/225], Training Accuracy: 31.1444%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [223/225], Training Accuracy: 31.1799%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [224/225], Training Accuracy: 31.1802%, Training Loss: 0.6857%\n",
      "Epoch [48/100], Step [225/225], Training Accuracy: 31.1492%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6875%\n",
      "Epoch [49/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6863%\n",
      "Epoch [49/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6878%\n",
      "Epoch [49/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [8/225], Training Accuracy: 32.2266%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [10/225], Training Accuracy: 31.7188%, Training Loss: 0.6852%\n",
      "Epoch [49/100], Step [11/225], Training Accuracy: 31.8182%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [12/225], Training Accuracy: 30.9896%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [13/225], Training Accuracy: 30.8894%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6860%\n",
      "Epoch [49/100], Step [15/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Step [16/225], Training Accuracy: 31.1523%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6860%\n",
      "Epoch [49/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [20/225], Training Accuracy: 31.3281%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [21/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [23/225], Training Accuracy: 31.5217%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6853%\n",
      "Epoch [49/100], Step [25/225], Training Accuracy: 31.8750%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [27/225], Training Accuracy: 31.7708%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [28/225], Training Accuracy: 31.6964%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [29/225], Training Accuracy: 31.9504%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6846%\n",
      "Epoch [49/100], Step [32/225], Training Accuracy: 32.2266%, Training Loss: 0.6844%\n",
      "Epoch [49/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6843%\n",
      "Epoch [49/100], Step [34/225], Training Accuracy: 32.1691%, Training Loss: 0.6843%\n",
      "Epoch [49/100], Step [35/225], Training Accuracy: 32.0536%, Training Loss: 0.6845%\n",
      "Epoch [49/100], Step [36/225], Training Accuracy: 31.9444%, Training Loss: 0.6846%\n",
      "Epoch [49/100], Step [37/225], Training Accuracy: 31.9257%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [38/225], Training Accuracy: 31.8257%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [39/225], Training Accuracy: 31.4904%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [40/225], Training Accuracy: 31.4844%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [41/225], Training Accuracy: 31.5549%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [42/225], Training Accuracy: 31.3616%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [43/225], Training Accuracy: 31.5407%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [44/225], Training Accuracy: 31.5341%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [46/225], Training Accuracy: 31.4198%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [47/225], Training Accuracy: 31.3830%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [48/225], Training Accuracy: 31.4453%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [49/225], Training Accuracy: 31.4413%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [50/225], Training Accuracy: 31.4688%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [51/225], Training Accuracy: 31.5870%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [52/225], Training Accuracy: 31.6106%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [53/225], Training Accuracy: 31.5743%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [54/225], Training Accuracy: 31.4525%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [55/225], Training Accuracy: 31.5057%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [56/225], Training Accuracy: 31.5569%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [57/225], Training Accuracy: 31.5789%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [58/225], Training Accuracy: 31.4655%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [60/225], Training Accuracy: 31.8490%, Training Loss: 0.6846%\n",
      "Epoch [49/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6846%\n",
      "Epoch [49/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [63/225], Training Accuracy: 31.9444%, Training Loss: 0.6846%\n",
      "Epoch [49/100], Step [64/225], Training Accuracy: 31.9580%, Training Loss: 0.6845%\n",
      "Epoch [49/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6846%\n",
      "Epoch [49/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6848%\n",
      "Epoch [49/100], Step [67/225], Training Accuracy: 31.8797%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [69/225], Training Accuracy: 31.9520%, Training Loss: 0.6845%\n",
      "Epoch [49/100], Step [70/225], Training Accuracy: 31.8750%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [71/225], Training Accuracy: 31.9322%, Training Loss: 0.6847%\n",
      "Epoch [49/100], Step [72/225], Training Accuracy: 31.7274%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [73/225], Training Accuracy: 31.6567%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [74/225], Training Accuracy: 31.7568%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [75/225], Training Accuracy: 31.7292%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [77/225], Training Accuracy: 31.5950%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [78/225], Training Accuracy: 31.6106%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [79/225], Training Accuracy: 31.5269%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [80/225], Training Accuracy: 31.4844%, Training Loss: 0.6849%\n",
      "Epoch [49/100], Step [81/225], Training Accuracy: 31.3850%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [82/225], Training Accuracy: 31.4024%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [83/225], Training Accuracy: 31.3253%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [84/225], Training Accuracy: 31.3988%, Training Loss: 0.6850%\n",
      "Epoch [49/100], Step [85/225], Training Accuracy: 31.3603%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [86/225], Training Accuracy: 31.3953%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [87/225], Training Accuracy: 31.3937%, Training Loss: 0.6851%\n",
      "Epoch [49/100], Step [88/225], Training Accuracy: 31.3920%, Training Loss: 0.6852%\n",
      "Epoch [49/100], Step [89/225], Training Accuracy: 31.3202%, Training Loss: 0.6852%\n",
      "Epoch [49/100], Step [90/225], Training Accuracy: 31.2326%, Training Loss: 0.6852%\n",
      "Epoch [49/100], Step [91/225], Training Accuracy: 31.2672%, Training Loss: 0.6853%\n",
      "Epoch [49/100], Step [92/225], Training Accuracy: 31.3010%, Training Loss: 0.6853%\n",
      "Epoch [49/100], Step [93/225], Training Accuracy: 31.2668%, Training Loss: 0.6853%\n",
      "Epoch [49/100], Step [94/225], Training Accuracy: 31.3331%, Training Loss: 0.6853%\n",
      "Epoch [49/100], Step [95/225], Training Accuracy: 31.2171%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [96/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [97/225], Training Accuracy: 31.2822%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [98/225], Training Accuracy: 31.3138%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [99/225], Training Accuracy: 31.4710%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [100/225], Training Accuracy: 31.4531%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [101/225], Training Accuracy: 31.5903%, Training Loss: 0.6854%\n",
      "Epoch [49/100], Step [102/225], Training Accuracy: 31.4798%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [103/225], Training Accuracy: 31.5231%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [104/225], Training Accuracy: 31.4904%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [105/225], Training Accuracy: 31.4435%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [106/225], Training Accuracy: 31.4711%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [107/225], Training Accuracy: 31.3814%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [108/225], Training Accuracy: 31.4670%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [109/225], Training Accuracy: 31.3503%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [110/225], Training Accuracy: 31.3636%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [111/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [112/225], Training Accuracy: 31.3058%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [113/225], Training Accuracy: 31.3053%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [114/225], Training Accuracy: 31.4008%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [115/225], Training Accuracy: 31.3451%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [116/225], Training Accuracy: 31.3443%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [117/225], Training Accuracy: 31.3034%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [118/225], Training Accuracy: 31.2632%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [119/225], Training Accuracy: 31.2106%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [120/225], Training Accuracy: 31.2630%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [121/225], Training Accuracy: 31.2758%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [122/225], Training Accuracy: 31.2884%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100], Step [123/225], Training Accuracy: 31.3135%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [124/225], Training Accuracy: 31.3004%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [125/225], Training Accuracy: 31.2625%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [126/225], Training Accuracy: 31.2252%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [127/225], Training Accuracy: 31.2008%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [128/225], Training Accuracy: 31.1768%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [129/225], Training Accuracy: 31.2137%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [130/225], Training Accuracy: 31.1298%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [131/225], Training Accuracy: 31.0830%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [132/225], Training Accuracy: 31.0251%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [133/225], Training Accuracy: 31.0503%, Training Loss: 0.6860%\n",
      "Epoch [49/100], Step [134/225], Training Accuracy: 31.0868%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [135/225], Training Accuracy: 31.0880%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [136/225], Training Accuracy: 31.1581%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [137/225], Training Accuracy: 31.1816%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [138/225], Training Accuracy: 31.1934%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [139/225], Training Accuracy: 31.1601%, Training Loss: 0.6860%\n",
      "Epoch [49/100], Step [140/225], Training Accuracy: 31.1384%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [141/225], Training Accuracy: 31.0949%, Training Loss: 0.6860%\n",
      "Epoch [49/100], Step [142/225], Training Accuracy: 31.1510%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [143/225], Training Accuracy: 31.1189%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [144/225], Training Accuracy: 31.1415%, Training Loss: 0.6859%\n",
      "Epoch [49/100], Step [145/225], Training Accuracy: 31.1961%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [146/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [147/225], Training Accuracy: 31.2394%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [148/225], Training Accuracy: 31.1972%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [149/225], Training Accuracy: 31.2290%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [150/225], Training Accuracy: 31.2708%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [151/225], Training Accuracy: 31.2914%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [152/225], Training Accuracy: 31.2808%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [153/225], Training Accuracy: 31.2398%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [154/225], Training Accuracy: 31.2703%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [155/225], Training Accuracy: 31.2399%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [156/225], Training Accuracy: 31.2600%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [157/225], Training Accuracy: 31.1803%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [158/225], Training Accuracy: 31.1709%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [159/225], Training Accuracy: 31.2402%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [160/225], Training Accuracy: 31.2109%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [161/225], Training Accuracy: 31.2597%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [162/225], Training Accuracy: 31.2404%, Training Loss: 0.6858%\n",
      "Epoch [49/100], Step [163/225], Training Accuracy: 31.3075%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [164/225], Training Accuracy: 31.2881%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [165/225], Training Accuracy: 31.2121%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [166/225], Training Accuracy: 31.2123%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [167/225], Training Accuracy: 31.2687%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [168/225], Training Accuracy: 31.2221%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [169/225], Training Accuracy: 31.1575%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [170/225], Training Accuracy: 31.1029%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [171/225], Training Accuracy: 31.1221%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [172/225], Training Accuracy: 31.1410%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [173/225], Training Accuracy: 31.1326%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [174/225], Training Accuracy: 31.1692%, Training Loss: 0.6857%\n",
      "Epoch [49/100], Step [175/225], Training Accuracy: 31.1875%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [176/225], Training Accuracy: 31.2056%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [177/225], Training Accuracy: 31.2147%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [178/225], Training Accuracy: 31.2237%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [179/225], Training Accuracy: 31.2151%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [180/225], Training Accuracy: 31.2587%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [181/225], Training Accuracy: 31.2068%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [182/225], Training Accuracy: 31.1985%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [183/225], Training Accuracy: 31.2158%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [184/225], Training Accuracy: 31.1990%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [185/225], Training Accuracy: 31.1655%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [186/225], Training Accuracy: 31.1828%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [187/225], Training Accuracy: 31.2082%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [188/225], Training Accuracy: 31.1918%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [189/225], Training Accuracy: 31.2087%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [190/225], Training Accuracy: 31.1678%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [191/225], Training Accuracy: 31.1437%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [192/225], Training Accuracy: 31.0791%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [193/225], Training Accuracy: 31.0881%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [194/225], Training Accuracy: 31.0970%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [195/225], Training Accuracy: 31.0897%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [196/225], Training Accuracy: 31.0587%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [197/225], Training Accuracy: 31.1152%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [198/225], Training Accuracy: 31.1316%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [199/225], Training Accuracy: 31.1087%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [200/225], Training Accuracy: 31.0859%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [201/225], Training Accuracy: 31.1023%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [202/225], Training Accuracy: 31.0876%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [203/225], Training Accuracy: 31.0653%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [204/225], Training Accuracy: 31.1198%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [205/225], Training Accuracy: 31.1052%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [207/225], Training Accuracy: 31.0537%, Training Loss: 0.6856%\n",
      "Epoch [49/100], Step [208/225], Training Accuracy: 31.0772%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [209/225], Training Accuracy: 31.1154%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [210/225], Training Accuracy: 31.1458%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [211/225], Training Accuracy: 31.1315%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [212/225], Training Accuracy: 31.1616%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [213/225], Training Accuracy: 31.1400%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [214/225], Training Accuracy: 31.1478%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [215/225], Training Accuracy: 31.1192%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [216/225], Training Accuracy: 31.0692%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [217/225], Training Accuracy: 31.0628%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [218/225], Training Accuracy: 31.0278%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [219/225], Training Accuracy: 31.0859%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [220/225], Training Accuracy: 31.0938%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [221/225], Training Accuracy: 31.0662%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [222/225], Training Accuracy: 31.0881%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [223/225], Training Accuracy: 31.1379%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [224/225], Training Accuracy: 31.1314%, Training Loss: 0.6855%\n",
      "Epoch [49/100], Step [225/225], Training Accuracy: 31.1006%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6910%\n",
      "Epoch [50/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6896%\n",
      "Epoch [50/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6915%\n",
      "Epoch [50/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6875%\n",
      "Epoch [50/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6869%\n",
      "Epoch [50/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6868%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6857%\n",
      "Epoch [50/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6857%\n",
      "Epoch [50/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [11/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [12/225], Training Accuracy: 30.5990%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [14/225], Training Accuracy: 30.9152%, Training Loss: 0.6859%\n",
      "Epoch [50/100], Step [15/225], Training Accuracy: 31.3542%, Training Loss: 0.6858%\n",
      "Epoch [50/100], Step [16/225], Training Accuracy: 31.1523%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [18/225], Training Accuracy: 30.9028%, Training Loss: 0.6858%\n",
      "Epoch [50/100], Step [19/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [50/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6860%\n",
      "Epoch [50/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [23/225], Training Accuracy: 31.5217%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [24/225], Training Accuracy: 31.7708%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [25/225], Training Accuracy: 32.0000%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [27/225], Training Accuracy: 32.0602%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [29/225], Training Accuracy: 32.2737%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [30/225], Training Accuracy: 32.2917%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [31/225], Training Accuracy: 32.1573%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [32/225], Training Accuracy: 32.4707%, Training Loss: 0.6840%\n",
      "Epoch [50/100], Step [33/225], Training Accuracy: 32.6231%, Training Loss: 0.6839%\n",
      "Epoch [50/100], Step [34/225], Training Accuracy: 32.4449%, Training Loss: 0.6839%\n",
      "Epoch [50/100], Step [35/225], Training Accuracy: 32.3214%, Training Loss: 0.6841%\n",
      "Epoch [50/100], Step [36/225], Training Accuracy: 32.1181%, Training Loss: 0.6842%\n",
      "Epoch [50/100], Step [37/225], Training Accuracy: 32.0524%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [39/225], Training Accuracy: 31.6907%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [40/225], Training Accuracy: 31.7188%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [41/225], Training Accuracy: 31.7073%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [42/225], Training Accuracy: 31.6220%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [43/225], Training Accuracy: 31.7951%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [44/225], Training Accuracy: 31.6761%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [45/225], Training Accuracy: 31.6319%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [46/225], Training Accuracy: 31.5557%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [47/225], Training Accuracy: 31.5160%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [48/225], Training Accuracy: 31.6406%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [49/225], Training Accuracy: 31.6645%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [50/225], Training Accuracy: 31.6562%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [51/225], Training Accuracy: 31.8321%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [52/225], Training Accuracy: 31.8209%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [53/225], Training Accuracy: 31.7807%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [55/225], Training Accuracy: 31.8466%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [56/225], Training Accuracy: 31.9196%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [57/225], Training Accuracy: 31.9079%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [58/225], Training Accuracy: 31.8427%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [59/225], Training Accuracy: 32.1769%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [60/225], Training Accuracy: 32.2656%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [61/225], Training Accuracy: 32.1465%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [62/225], Training Accuracy: 32.1573%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [63/225], Training Accuracy: 32.2173%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [64/225], Training Accuracy: 32.2266%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [65/225], Training Accuracy: 32.0673%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [66/225], Training Accuracy: 32.1496%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [67/225], Training Accuracy: 32.1362%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [68/225], Training Accuracy: 32.1921%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [69/225], Training Accuracy: 32.1784%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [70/225], Training Accuracy: 32.1429%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [71/225], Training Accuracy: 32.2403%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [72/225], Training Accuracy: 32.0095%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [73/225], Training Accuracy: 31.9349%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [74/225], Training Accuracy: 32.0312%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [75/225], Training Accuracy: 31.9583%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [76/225], Training Accuracy: 31.9285%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [77/225], Training Accuracy: 31.8588%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [78/225], Training Accuracy: 31.8910%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [79/225], Training Accuracy: 31.8038%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [80/225], Training Accuracy: 31.7773%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [81/225], Training Accuracy: 31.7130%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [82/225], Training Accuracy: 31.7264%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [83/225], Training Accuracy: 31.6830%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [84/225], Training Accuracy: 31.6592%, Training Loss: 0.6843%\n",
      "Epoch [50/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6844%\n",
      "Epoch [50/100], Step [86/225], Training Accuracy: 31.6315%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [87/225], Training Accuracy: 31.6451%, Training Loss: 0.6845%\n",
      "Epoch [50/100], Step [88/225], Training Accuracy: 31.5874%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [89/225], Training Accuracy: 31.4782%, Training Loss: 0.6846%\n",
      "Epoch [50/100], Step [90/225], Training Accuracy: 31.3889%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [91/225], Training Accuracy: 31.4389%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [92/225], Training Accuracy: 31.4368%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [93/225], Training Accuracy: 31.4012%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [94/225], Training Accuracy: 31.4661%, Training Loss: 0.6847%\n",
      "Epoch [50/100], Step [95/225], Training Accuracy: 31.3487%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [96/225], Training Accuracy: 31.4616%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [97/225], Training Accuracy: 31.4755%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [98/225], Training Accuracy: 31.5051%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [100/225], Training Accuracy: 31.5938%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [101/225], Training Accuracy: 31.7141%, Training Loss: 0.6848%\n",
      "Epoch [50/100], Step [102/225], Training Accuracy: 31.5717%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [103/225], Training Accuracy: 31.6444%, Training Loss: 0.6849%\n",
      "Epoch [50/100], Step [104/225], Training Accuracy: 31.6106%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [105/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [107/225], Training Accuracy: 31.4690%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [108/225], Training Accuracy: 31.5249%, Training Loss: 0.6850%\n",
      "Epoch [50/100], Step [109/225], Training Accuracy: 31.3790%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [110/225], Training Accuracy: 31.3920%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [111/225], Training Accuracy: 31.2782%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [112/225], Training Accuracy: 31.3477%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Step [113/225], Training Accuracy: 31.3330%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [114/225], Training Accuracy: 31.4282%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [115/225], Training Accuracy: 31.4130%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [116/225], Training Accuracy: 31.4251%, Training Loss: 0.6851%\n",
      "Epoch [50/100], Step [117/225], Training Accuracy: 31.3702%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [118/225], Training Accuracy: 31.3294%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [119/225], Training Accuracy: 31.2631%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [120/225], Training Accuracy: 31.3151%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [121/225], Training Accuracy: 31.3017%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [122/225], Training Accuracy: 31.3268%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [123/225], Training Accuracy: 31.3516%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [124/225], Training Accuracy: 31.3256%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [125/225], Training Accuracy: 31.2875%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [126/225], Training Accuracy: 31.2252%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [127/225], Training Accuracy: 31.2131%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [128/225], Training Accuracy: 31.2012%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [129/225], Training Accuracy: 31.2379%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [130/225], Training Accuracy: 31.1418%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [131/225], Training Accuracy: 31.0949%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [132/225], Training Accuracy: 31.0606%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [133/225], Training Accuracy: 31.0973%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [134/225], Training Accuracy: 31.1684%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [135/225], Training Accuracy: 31.1921%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [136/225], Training Accuracy: 31.2155%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [137/225], Training Accuracy: 31.2728%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [138/225], Training Accuracy: 31.3179%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [139/225], Training Accuracy: 31.2950%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [140/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [141/225], Training Accuracy: 31.1946%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [142/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [143/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [144/225], Training Accuracy: 31.2717%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [145/225], Training Accuracy: 31.3147%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [146/225], Training Accuracy: 31.3463%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [147/225], Training Accuracy: 31.3563%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [148/225], Training Accuracy: 31.3133%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [149/225], Training Accuracy: 31.3234%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [150/225], Training Accuracy: 31.3646%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [151/225], Training Accuracy: 31.4052%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [152/225], Training Accuracy: 31.4042%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [153/225], Training Accuracy: 31.3930%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [154/225], Training Accuracy: 31.4123%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [155/225], Training Accuracy: 31.4012%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [156/225], Training Accuracy: 31.4203%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [157/225], Training Accuracy: 31.3495%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [158/225], Training Accuracy: 31.3192%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [159/225], Training Accuracy: 31.3778%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [160/225], Training Accuracy: 31.3477%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [161/225], Training Accuracy: 31.4150%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [162/225], Training Accuracy: 31.3850%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [163/225], Training Accuracy: 31.4321%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [164/225], Training Accuracy: 31.4120%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [165/225], Training Accuracy: 31.3447%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [166/225], Training Accuracy: 31.3347%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [167/225], Training Accuracy: 31.3716%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [168/225], Training Accuracy: 31.3337%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [169/225], Training Accuracy: 31.2592%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [170/225], Training Accuracy: 31.2132%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [171/225], Training Accuracy: 31.2317%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [172/225], Training Accuracy: 31.2409%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [173/225], Training Accuracy: 31.2590%, Training Loss: 0.6856%\n",
      "Epoch [50/100], Step [174/225], Training Accuracy: 31.2769%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [175/225], Training Accuracy: 31.3304%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [176/225], Training Accuracy: 31.3477%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [177/225], Training Accuracy: 31.3559%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [178/225], Training Accuracy: 31.3729%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [179/225], Training Accuracy: 31.3460%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [180/225], Training Accuracy: 31.3889%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [181/225], Training Accuracy: 31.3450%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [182/225], Training Accuracy: 31.3359%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [183/225], Training Accuracy: 31.3439%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [184/225], Training Accuracy: 31.3264%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [185/225], Training Accuracy: 31.2922%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [186/225], Training Accuracy: 31.3172%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [187/225], Training Accuracy: 31.3252%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [188/225], Training Accuracy: 31.3331%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [189/225], Training Accuracy: 31.3492%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [190/225], Training Accuracy: 31.3158%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [191/225], Training Accuracy: 31.3073%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [192/225], Training Accuracy: 31.2419%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [193/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [194/225], Training Accuracy: 31.2581%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [195/225], Training Accuracy: 31.2340%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [196/225], Training Accuracy: 31.2101%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [197/225], Training Accuracy: 31.2341%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [198/225], Training Accuracy: 31.2579%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [199/225], Training Accuracy: 31.2343%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [200/225], Training Accuracy: 31.2109%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [201/225], Training Accuracy: 31.2345%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [202/225], Training Accuracy: 31.2345%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [203/225], Training Accuracy: 31.2269%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [204/225], Training Accuracy: 31.2960%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [205/225], Training Accuracy: 31.2881%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [206/225], Training Accuracy: 31.2879%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [207/225], Training Accuracy: 31.2651%, Training Loss: 0.6855%\n",
      "Epoch [50/100], Step [208/225], Training Accuracy: 31.2876%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [209/225], Training Accuracy: 31.3023%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [210/225], Training Accuracy: 31.3318%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [211/225], Training Accuracy: 31.3092%, Training Loss: 0.6854%\n",
      "Epoch [50/100], Step [212/225], Training Accuracy: 31.3532%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [213/225], Training Accuracy: 31.3307%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [214/225], Training Accuracy: 31.3303%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [215/225], Training Accuracy: 31.2863%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [216/225], Training Accuracy: 31.2211%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Step [217/225], Training Accuracy: 31.2068%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [218/225], Training Accuracy: 31.1712%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [219/225], Training Accuracy: 31.2286%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [220/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [221/225], Training Accuracy: 31.2288%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [222/225], Training Accuracy: 31.2359%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [223/225], Training Accuracy: 31.2780%, Training Loss: 0.6852%\n",
      "Epoch [50/100], Step [224/225], Training Accuracy: 31.2779%, Training Loss: 0.6853%\n",
      "Epoch [50/100], Step [225/225], Training Accuracy: 31.2535%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6868%\n",
      "Epoch [51/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6842%\n",
      "Epoch [51/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6866%\n",
      "Epoch [51/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6841%\n",
      "Epoch [51/100], Step [8/225], Training Accuracy: 32.2266%, Training Loss: 0.6839%\n",
      "Epoch [51/100], Step [9/225], Training Accuracy: 32.2917%, Training Loss: 0.6847%\n",
      "Epoch [51/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6844%\n",
      "Epoch [51/100], Step [11/225], Training Accuracy: 31.3920%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [12/225], Training Accuracy: 30.5990%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [14/225], Training Accuracy: 30.9152%, Training Loss: 0.6854%\n",
      "Epoch [51/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6854%\n",
      "Epoch [51/100], Step [16/225], Training Accuracy: 31.1523%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [17/225], Training Accuracy: 30.9743%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [18/225], Training Accuracy: 30.9028%, Training Loss: 0.6857%\n",
      "Epoch [51/100], Step [19/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [51/100], Step [20/225], Training Accuracy: 31.7188%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [21/225], Training Accuracy: 31.6964%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [22/225], Training Accuracy: 31.8892%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [24/225], Training Accuracy: 31.8359%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [25/225], Training Accuracy: 32.0625%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [26/225], Training Accuracy: 32.3918%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [27/225], Training Accuracy: 32.0602%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [28/225], Training Accuracy: 31.8080%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [30/225], Training Accuracy: 32.0312%, Training Loss: 0.6847%\n",
      "Epoch [51/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6846%\n",
      "Epoch [51/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6844%\n",
      "Epoch [51/100], Step [33/225], Training Accuracy: 32.1970%, Training Loss: 0.6843%\n",
      "Epoch [51/100], Step [34/225], Training Accuracy: 32.0772%, Training Loss: 0.6843%\n",
      "Epoch [51/100], Step [35/225], Training Accuracy: 31.7857%, Training Loss: 0.6845%\n",
      "Epoch [51/100], Step [36/225], Training Accuracy: 31.6840%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [37/225], Training Accuracy: 31.7568%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [38/225], Training Accuracy: 31.5789%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [39/225], Training Accuracy: 31.2901%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [40/225], Training Accuracy: 31.3281%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [41/225], Training Accuracy: 31.3643%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [42/225], Training Accuracy: 31.1756%, Training Loss: 0.6854%\n",
      "Epoch [51/100], Step [43/225], Training Accuracy: 31.3227%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [44/225], Training Accuracy: 31.2855%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [45/225], Training Accuracy: 31.2847%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [46/225], Training Accuracy: 31.1141%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [47/225], Training Accuracy: 31.0173%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [48/225], Training Accuracy: 31.1523%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [49/225], Training Accuracy: 31.1862%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [50/225], Training Accuracy: 31.1875%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [51/225], Training Accuracy: 31.3419%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [52/225], Training Accuracy: 31.3401%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [53/225], Training Accuracy: 31.2795%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [54/225], Training Accuracy: 31.1632%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [55/225], Training Accuracy: 31.2216%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [56/225], Training Accuracy: 31.2779%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [57/225], Training Accuracy: 31.3322%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [58/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [59/225], Training Accuracy: 31.5413%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [60/225], Training Accuracy: 31.6406%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [61/225], Training Accuracy: 31.5830%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [62/225], Training Accuracy: 31.6532%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [63/225], Training Accuracy: 31.6964%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [64/225], Training Accuracy: 31.7139%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [65/225], Training Accuracy: 31.6346%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [66/225], Training Accuracy: 31.6998%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [67/225], Training Accuracy: 31.6465%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [68/225], Training Accuracy: 31.6866%, Training Loss: 0.6849%\n",
      "Epoch [51/100], Step [69/225], Training Accuracy: 31.7029%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [70/225], Training Accuracy: 31.6518%, Training Loss: 0.6848%\n",
      "Epoch [51/100], Step [71/225], Training Accuracy: 31.6901%, Training Loss: 0.6847%\n",
      "Epoch [51/100], Step [72/225], Training Accuracy: 31.4670%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [73/225], Training Accuracy: 31.4426%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [74/225], Training Accuracy: 31.5667%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [75/225], Training Accuracy: 31.5833%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [76/225], Training Accuracy: 31.5584%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [77/225], Training Accuracy: 31.4732%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [78/225], Training Accuracy: 31.5104%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [79/225], Training Accuracy: 31.4478%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [80/225], Training Accuracy: 31.4062%, Training Loss: 0.6850%\n",
      "Epoch [51/100], Step [81/225], Training Accuracy: 31.3465%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [82/225], Training Accuracy: 31.3643%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [83/225], Training Accuracy: 31.2877%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [84/225], Training Accuracy: 31.3616%, Training Loss: 0.6851%\n",
      "Epoch [51/100], Step [85/225], Training Accuracy: 31.3419%, Training Loss: 0.6852%\n",
      "Epoch [51/100], Step [86/225], Training Accuracy: 31.4135%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [87/225], Training Accuracy: 31.4655%, Training Loss: 0.6853%\n",
      "Epoch [51/100], Step [88/225], Training Accuracy: 31.4276%, Training Loss: 0.6854%\n",
      "Epoch [51/100], Step [89/225], Training Accuracy: 31.3553%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [90/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [91/225], Training Accuracy: 31.2672%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [92/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [93/225], Training Accuracy: 31.2332%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [94/225], Training Accuracy: 31.3165%, Training Loss: 0.6854%\n",
      "Epoch [51/100], Step [95/225], Training Accuracy: 31.2007%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [96/225], Training Accuracy: 31.3151%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [97/225], Training Accuracy: 31.3305%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [98/225], Training Accuracy: 31.3776%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Step [99/225], Training Accuracy: 31.5183%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [100/225], Training Accuracy: 31.4844%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [101/225], Training Accuracy: 31.6058%, Training Loss: 0.6855%\n",
      "Epoch [51/100], Step [102/225], Training Accuracy: 31.4951%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [103/225], Training Accuracy: 31.5382%, Training Loss: 0.6856%\n",
      "Epoch [51/100], Step [104/225], Training Accuracy: 31.5204%, Training Loss: 0.6857%\n",
      "Epoch [51/100], Step [105/225], Training Accuracy: 31.4732%, Training Loss: 0.6857%\n",
      "Epoch [51/100], Step [106/225], Training Accuracy: 31.5006%, Training Loss: 0.6857%\n",
      "Epoch [51/100], Step [107/225], Training Accuracy: 31.3960%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [108/225], Training Accuracy: 31.4959%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [109/225], Training Accuracy: 31.3790%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [110/225], Training Accuracy: 31.4062%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [111/225], Training Accuracy: 31.2922%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [112/225], Training Accuracy: 31.3616%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [113/225], Training Accuracy: 31.3606%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [114/225], Training Accuracy: 31.4145%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [115/225], Training Accuracy: 31.3995%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [116/225], Training Accuracy: 31.3712%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [117/225], Training Accuracy: 31.3034%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [118/225], Training Accuracy: 31.2368%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [119/225], Training Accuracy: 31.2106%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [120/225], Training Accuracy: 31.2370%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [121/225], Training Accuracy: 31.2113%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [122/225], Training Accuracy: 31.1860%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [123/225], Training Accuracy: 31.1992%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [124/225], Training Accuracy: 31.1870%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [125/225], Training Accuracy: 31.1500%, Training Loss: 0.6860%\n",
      "Epoch [51/100], Step [126/225], Training Accuracy: 31.0764%, Training Loss: 0.6860%\n",
      "Epoch [51/100], Step [127/225], Training Accuracy: 31.0162%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [128/225], Training Accuracy: 30.9937%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [129/225], Training Accuracy: 31.0199%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [130/225], Training Accuracy: 30.9375%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [131/225], Training Accuracy: 30.9160%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [132/225], Training Accuracy: 30.9067%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [133/225], Training Accuracy: 30.9211%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [134/225], Training Accuracy: 31.0051%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [135/225], Training Accuracy: 31.0069%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [136/225], Training Accuracy: 31.0202%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [137/225], Training Accuracy: 31.0561%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [138/225], Training Accuracy: 31.0575%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [139/225], Training Accuracy: 31.0027%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [140/225], Training Accuracy: 30.9821%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [141/225], Training Accuracy: 30.9176%, Training Loss: 0.6863%\n",
      "Epoch [51/100], Step [142/225], Training Accuracy: 30.9749%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [143/225], Training Accuracy: 30.9768%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [144/225], Training Accuracy: 30.9787%, Training Loss: 0.6863%\n",
      "Epoch [51/100], Step [145/225], Training Accuracy: 31.0453%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [146/225], Training Accuracy: 31.0788%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [147/225], Training Accuracy: 31.1012%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [148/225], Training Accuracy: 31.0811%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [149/225], Training Accuracy: 31.1346%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [150/225], Training Accuracy: 31.1771%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [151/225], Training Accuracy: 31.2086%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [152/225], Training Accuracy: 31.1883%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [153/225], Training Accuracy: 31.1479%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [154/225], Training Accuracy: 31.1790%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [155/225], Training Accuracy: 31.1694%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [156/225], Training Accuracy: 31.1799%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [157/225], Training Accuracy: 31.1007%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [158/225], Training Accuracy: 31.0918%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [159/225], Training Accuracy: 31.1616%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [160/225], Training Accuracy: 31.1328%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [161/225], Training Accuracy: 31.1918%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [162/225], Training Accuracy: 31.1632%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [163/225], Training Accuracy: 31.2117%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [164/225], Training Accuracy: 31.1833%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [165/225], Training Accuracy: 31.1174%, Training Loss: 0.6862%\n",
      "Epoch [51/100], Step [166/225], Training Accuracy: 31.0900%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [167/225], Training Accuracy: 31.1377%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [168/225], Training Accuracy: 31.1012%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [169/225], Training Accuracy: 31.0374%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [170/225], Training Accuracy: 30.9835%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [171/225], Training Accuracy: 31.0124%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [172/225], Training Accuracy: 31.0229%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [173/225], Training Accuracy: 31.0152%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [174/225], Training Accuracy: 31.0165%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [175/225], Training Accuracy: 31.0268%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [176/225], Training Accuracy: 31.0636%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [177/225], Training Accuracy: 31.0734%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [178/225], Training Accuracy: 31.0657%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [179/225], Training Accuracy: 31.0405%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [180/225], Training Accuracy: 31.0677%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [181/225], Training Accuracy: 31.0342%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [182/225], Training Accuracy: 31.0354%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [183/225], Training Accuracy: 31.0451%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [184/225], Training Accuracy: 31.0122%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [185/225], Training Accuracy: 30.9797%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [186/225], Training Accuracy: 31.0064%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [187/225], Training Accuracy: 31.0077%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [188/225], Training Accuracy: 30.9924%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [189/225], Training Accuracy: 31.0351%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [190/225], Training Accuracy: 30.9868%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [191/225], Training Accuracy: 30.9555%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [192/225], Training Accuracy: 30.8919%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [193/225], Training Accuracy: 30.8938%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [194/225], Training Accuracy: 30.9117%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [195/225], Training Accuracy: 30.8974%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [196/225], Training Accuracy: 30.8753%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [197/225], Training Accuracy: 30.9169%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [198/225], Training Accuracy: 30.9422%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [199/225], Training Accuracy: 30.9202%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [200/225], Training Accuracy: 30.9141%, Training Loss: 0.6861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100], Step [201/225], Training Accuracy: 30.9235%, Training Loss: 0.6860%\n",
      "Epoch [51/100], Step [202/225], Training Accuracy: 30.9097%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [203/225], Training Accuracy: 30.9113%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [204/225], Training Accuracy: 30.9589%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [205/225], Training Accuracy: 30.9604%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [206/225], Training Accuracy: 30.9390%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [207/225], Training Accuracy: 30.9254%, Training Loss: 0.6861%\n",
      "Epoch [51/100], Step [208/225], Training Accuracy: 30.9570%, Training Loss: 0.6860%\n",
      "Epoch [51/100], Step [209/225], Training Accuracy: 30.9958%, Training Loss: 0.6860%\n",
      "Epoch [51/100], Step [210/225], Training Accuracy: 31.0417%, Training Loss: 0.6860%\n",
      "Epoch [51/100], Step [211/225], Training Accuracy: 31.0427%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [212/225], Training Accuracy: 31.0952%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [213/225], Training Accuracy: 31.0739%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [214/225], Training Accuracy: 31.0894%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [215/225], Training Accuracy: 31.0683%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [216/225], Training Accuracy: 31.0185%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [217/225], Training Accuracy: 31.0196%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [218/225], Training Accuracy: 30.9848%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [219/225], Training Accuracy: 31.0502%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [220/225], Training Accuracy: 31.0653%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [221/225], Training Accuracy: 31.0520%, Training Loss: 0.6859%\n",
      "Epoch [51/100], Step [222/225], Training Accuracy: 31.0670%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [223/225], Training Accuracy: 31.0888%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [224/225], Training Accuracy: 31.0826%, Training Loss: 0.6858%\n",
      "Epoch [51/100], Step [225/225], Training Accuracy: 31.0728%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6879%\n",
      "Epoch [52/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6833%\n",
      "Epoch [52/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6865%\n",
      "Epoch [52/100], Step [4/225], Training Accuracy: 30.8594%, Training Loss: 0.6850%\n",
      "Epoch [52/100], Step [5/225], Training Accuracy: 31.8750%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6856%\n",
      "Epoch [52/100], Step [14/225], Training Accuracy: 30.9152%, Training Loss: 0.6863%\n",
      "Epoch [52/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6863%\n",
      "Epoch [52/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6864%\n",
      "Epoch [52/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6864%\n",
      "Epoch [52/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6865%\n",
      "Epoch [52/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6867%\n",
      "Epoch [52/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6865%\n",
      "Epoch [52/100], Step [22/225], Training Accuracy: 31.7472%, Training Loss: 0.6863%\n",
      "Epoch [52/100], Step [23/225], Training Accuracy: 31.5897%, Training Loss: 0.6863%\n",
      "Epoch [52/100], Step [24/225], Training Accuracy: 31.6406%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [25/225], Training Accuracy: 31.8750%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [27/225], Training Accuracy: 32.0023%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [28/225], Training Accuracy: 31.8080%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [30/225], Training Accuracy: 32.1354%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [31/225], Training Accuracy: 32.0060%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [32/225], Training Accuracy: 32.1777%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [33/225], Training Accuracy: 32.3390%, Training Loss: 0.6851%\n",
      "Epoch [52/100], Step [34/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [52/100], Step [35/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [37/225], Training Accuracy: 31.6723%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [38/225], Training Accuracy: 31.4556%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [39/225], Training Accuracy: 31.1699%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [40/225], Training Accuracy: 31.2109%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [41/225], Training Accuracy: 31.2881%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [42/225], Training Accuracy: 31.1012%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [43/225], Training Accuracy: 31.2863%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [44/225], Training Accuracy: 31.2855%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [45/225], Training Accuracy: 31.3194%, Training Loss: 0.6856%\n",
      "Epoch [52/100], Step [46/225], Training Accuracy: 31.2160%, Training Loss: 0.6856%\n",
      "Epoch [52/100], Step [47/225], Training Accuracy: 31.1503%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [48/225], Training Accuracy: 31.3151%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [49/225], Training Accuracy: 31.4094%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [51/225], Training Accuracy: 31.4338%, Training Loss: 0.6856%\n",
      "Epoch [52/100], Step [52/225], Training Accuracy: 31.4002%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [53/225], Training Accuracy: 31.3384%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [54/225], Training Accuracy: 31.2211%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [55/225], Training Accuracy: 31.3352%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [56/225], Training Accuracy: 31.3895%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [57/225], Training Accuracy: 31.4419%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [58/225], Training Accuracy: 31.3039%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [59/225], Training Accuracy: 31.6472%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [60/225], Training Accuracy: 31.7969%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [61/225], Training Accuracy: 31.6855%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [62/225], Training Accuracy: 31.7288%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [63/225], Training Accuracy: 31.7956%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [64/225], Training Accuracy: 31.7871%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [65/225], Training Accuracy: 31.6827%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [66/225], Training Accuracy: 31.7945%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [67/225], Training Accuracy: 31.7864%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [68/225], Training Accuracy: 31.8474%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [69/225], Training Accuracy: 31.8388%, Training Loss: 0.6851%\n",
      "Epoch [52/100], Step [70/225], Training Accuracy: 31.8080%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [71/225], Training Accuracy: 31.8442%, Training Loss: 0.6852%\n",
      "Epoch [52/100], Step [72/225], Training Accuracy: 31.6189%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [73/225], Training Accuracy: 31.5283%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [74/225], Training Accuracy: 31.6301%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [75/225], Training Accuracy: 31.5625%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [77/225], Training Accuracy: 31.4123%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [78/225], Training Accuracy: 31.3902%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [79/225], Training Accuracy: 31.2896%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [80/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [81/225], Training Accuracy: 31.1921%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [82/225], Training Accuracy: 31.1928%, Training Loss: 0.6853%\n",
      "Epoch [52/100], Step [83/225], Training Accuracy: 31.1370%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [84/225], Training Accuracy: 31.2314%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Step [85/225], Training Accuracy: 31.2132%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [86/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [87/225], Training Accuracy: 31.2680%, Training Loss: 0.6854%\n",
      "Epoch [52/100], Step [88/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [89/225], Training Accuracy: 31.1622%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [90/225], Training Accuracy: 31.0764%, Training Loss: 0.6855%\n",
      "Epoch [52/100], Step [91/225], Training Accuracy: 31.0955%, Training Loss: 0.6856%\n",
      "Epoch [52/100], Step [92/225], Training Accuracy: 31.0971%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [93/225], Training Accuracy: 31.1156%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [94/225], Training Accuracy: 31.1669%, Training Loss: 0.6856%\n",
      "Epoch [52/100], Step [95/225], Training Accuracy: 31.0526%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [96/225], Training Accuracy: 31.1686%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [97/225], Training Accuracy: 31.1856%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [98/225], Training Accuracy: 31.2022%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [99/225], Training Accuracy: 31.3289%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [100/225], Training Accuracy: 31.3125%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [101/225], Training Accuracy: 31.4511%, Training Loss: 0.6857%\n",
      "Epoch [52/100], Step [102/225], Training Accuracy: 31.3572%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [103/225], Training Accuracy: 31.4320%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [104/225], Training Accuracy: 31.4153%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [105/225], Training Accuracy: 31.3839%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [106/225], Training Accuracy: 31.3679%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [107/225], Training Accuracy: 31.2792%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [108/225], Training Accuracy: 31.3513%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [109/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [110/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [111/225], Training Accuracy: 31.1092%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [112/225], Training Accuracy: 31.1802%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [113/225], Training Accuracy: 31.1532%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [114/225], Training Accuracy: 31.2363%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [115/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [116/225], Training Accuracy: 31.2365%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [117/225], Training Accuracy: 31.1832%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [118/225], Training Accuracy: 31.1573%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [119/225], Training Accuracy: 31.1056%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [120/225], Training Accuracy: 31.1458%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [121/225], Training Accuracy: 31.1209%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [123/225], Training Accuracy: 31.1611%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [124/225], Training Accuracy: 31.1618%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [125/225], Training Accuracy: 31.1375%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [126/225], Training Accuracy: 31.0764%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [127/225], Training Accuracy: 31.0531%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [128/225], Training Accuracy: 31.0425%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [129/225], Training Accuracy: 31.0925%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [130/225], Training Accuracy: 31.0577%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [131/225], Training Accuracy: 31.0234%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [132/225], Training Accuracy: 31.0014%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [133/225], Training Accuracy: 31.0385%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [134/225], Training Accuracy: 31.0984%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [135/225], Training Accuracy: 31.1227%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [136/225], Training Accuracy: 31.1351%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [137/225], Training Accuracy: 31.1702%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [138/225], Training Accuracy: 31.1821%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [139/225], Training Accuracy: 31.1376%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [140/225], Training Accuracy: 31.1049%, Training Loss: 0.6863%\n",
      "Epoch [52/100], Step [141/225], Training Accuracy: 31.0616%, Training Loss: 0.6863%\n",
      "Epoch [52/100], Step [142/225], Training Accuracy: 31.1180%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [143/225], Training Accuracy: 31.1189%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [144/225], Training Accuracy: 31.1415%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [145/225], Training Accuracy: 31.2177%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [146/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [147/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [148/225], Training Accuracy: 31.2289%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [150/225], Training Accuracy: 31.2708%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [151/225], Training Accuracy: 31.3017%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [152/225], Training Accuracy: 31.3014%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [153/225], Training Accuracy: 31.2398%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [154/225], Training Accuracy: 31.2399%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [155/225], Training Accuracy: 31.2298%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [156/225], Training Accuracy: 31.2700%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [157/225], Training Accuracy: 31.2102%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [158/225], Training Accuracy: 31.1907%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [159/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [161/225], Training Accuracy: 31.2694%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [162/225], Training Accuracy: 31.2307%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [163/225], Training Accuracy: 31.2979%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [164/225], Training Accuracy: 31.2691%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [165/225], Training Accuracy: 31.2121%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [166/225], Training Accuracy: 31.2029%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [167/225], Training Accuracy: 31.2406%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [168/225], Training Accuracy: 31.1849%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [169/225], Training Accuracy: 31.1021%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [170/225], Training Accuracy: 31.0570%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [171/225], Training Accuracy: 31.0581%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [172/225], Training Accuracy: 31.0774%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [173/225], Training Accuracy: 31.0784%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [174/225], Training Accuracy: 31.0884%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [175/225], Training Accuracy: 31.1250%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [176/225], Training Accuracy: 31.1523%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [177/225], Training Accuracy: 31.1617%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [178/225], Training Accuracy: 31.1622%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [179/225], Training Accuracy: 31.1278%, Training Loss: 0.6862%\n",
      "Epoch [52/100], Step [180/225], Training Accuracy: 31.1719%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [181/225], Training Accuracy: 31.1291%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [182/225], Training Accuracy: 31.0955%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [183/225], Training Accuracy: 31.1048%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [184/225], Training Accuracy: 31.0887%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [185/225], Training Accuracy: 31.0473%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [186/225], Training Accuracy: 31.0904%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [187/225], Training Accuracy: 31.1163%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [188/225], Training Accuracy: 31.0921%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [189/225], Training Accuracy: 31.1095%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [190/225], Training Accuracy: 31.0691%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [191/225], Training Accuracy: 31.0455%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [192/225], Training Accuracy: 30.9733%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [193/225], Training Accuracy: 30.9747%, Training Loss: 0.6861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Step [194/225], Training Accuracy: 31.0003%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [195/225], Training Accuracy: 30.9776%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [196/225], Training Accuracy: 30.9550%, Training Loss: 0.6861%\n",
      "Epoch [52/100], Step [197/225], Training Accuracy: 30.9883%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [198/225], Training Accuracy: 31.0054%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [199/225], Training Accuracy: 30.9752%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [200/225], Training Accuracy: 30.9688%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [201/225], Training Accuracy: 30.9935%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [202/225], Training Accuracy: 31.0025%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [203/225], Training Accuracy: 31.0037%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [204/225], Training Accuracy: 31.0432%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [205/225], Training Accuracy: 31.0442%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [206/225], Training Accuracy: 31.0452%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [207/225], Training Accuracy: 31.0311%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [208/225], Training Accuracy: 31.0622%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [209/225], Training Accuracy: 31.1080%, Training Loss: 0.6860%\n",
      "Epoch [52/100], Step [210/225], Training Accuracy: 31.1384%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [211/225], Training Accuracy: 31.1241%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [212/225], Training Accuracy: 31.1837%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [213/225], Training Accuracy: 31.1620%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [214/225], Training Accuracy: 31.1770%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [215/225], Training Accuracy: 31.1483%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [216/225], Training Accuracy: 31.0836%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [217/225], Training Accuracy: 31.0772%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [218/225], Training Accuracy: 31.0565%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [219/225], Training Accuracy: 31.1216%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [220/225], Training Accuracy: 31.1364%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [221/225], Training Accuracy: 31.1157%, Training Loss: 0.6859%\n",
      "Epoch [52/100], Step [222/225], Training Accuracy: 31.1233%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [223/225], Training Accuracy: 31.1659%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [224/225], Training Accuracy: 31.1663%, Training Loss: 0.6858%\n",
      "Epoch [52/100], Step [225/225], Training Accuracy: 31.1492%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6936%\n",
      "Epoch [53/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6897%\n",
      "Epoch [53/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6898%\n",
      "Epoch [53/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6867%\n",
      "Epoch [53/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6868%\n",
      "Epoch [53/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6862%\n",
      "Epoch [53/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6845%\n",
      "Epoch [53/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6849%\n",
      "Epoch [53/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [13/225], Training Accuracy: 30.4087%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [14/225], Training Accuracy: 30.6920%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [16/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6862%\n",
      "Epoch [53/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6863%\n",
      "Epoch [53/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6864%\n",
      "Epoch [53/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6864%\n",
      "Epoch [53/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6862%\n",
      "Epoch [53/100], Step [22/225], Training Accuracy: 31.8182%, Training Loss: 0.6861%\n",
      "Epoch [53/100], Step [23/225], Training Accuracy: 31.6576%, Training Loss: 0.6862%\n",
      "Epoch [53/100], Step [24/225], Training Accuracy: 31.7708%, Training Loss: 0.6862%\n",
      "Epoch [53/100], Step [25/225], Training Accuracy: 32.0000%, Training Loss: 0.6862%\n",
      "Epoch [53/100], Step [26/225], Training Accuracy: 32.3918%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [27/225], Training Accuracy: 32.0602%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [29/225], Training Accuracy: 32.2198%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [30/225], Training Accuracy: 32.2396%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [31/225], Training Accuracy: 32.1573%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [32/225], Training Accuracy: 32.4219%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [33/225], Training Accuracy: 32.4811%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [35/225], Training Accuracy: 32.0982%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [36/225], Training Accuracy: 31.8576%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [37/225], Training Accuracy: 31.8412%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [38/225], Training Accuracy: 31.7023%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [41/225], Training Accuracy: 31.5930%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [42/225], Training Accuracy: 31.3616%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [43/225], Training Accuracy: 31.5770%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [44/225], Training Accuracy: 31.5696%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [46/225], Training Accuracy: 31.4198%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [47/225], Training Accuracy: 31.3830%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [48/225], Training Accuracy: 31.5430%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [49/225], Training Accuracy: 31.5689%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [50/225], Training Accuracy: 31.5625%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [51/225], Training Accuracy: 31.7096%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [52/225], Training Accuracy: 31.7007%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [53/225], Training Accuracy: 31.6333%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [54/225], Training Accuracy: 31.5104%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [55/225], Training Accuracy: 31.6477%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [56/225], Training Accuracy: 31.6964%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [57/225], Training Accuracy: 31.7160%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [58/225], Training Accuracy: 31.6810%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [59/225], Training Accuracy: 32.0975%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [60/225], Training Accuracy: 32.1615%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [61/225], Training Accuracy: 32.0953%, Training Loss: 0.6849%\n",
      "Epoch [53/100], Step [62/225], Training Accuracy: 32.1321%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [63/225], Training Accuracy: 32.2173%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [64/225], Training Accuracy: 32.2266%, Training Loss: 0.6849%\n",
      "Epoch [53/100], Step [65/225], Training Accuracy: 32.1154%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [66/225], Training Accuracy: 32.2680%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [67/225], Training Accuracy: 32.2528%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [68/225], Training Accuracy: 32.2610%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [69/225], Training Accuracy: 32.1784%, Training Loss: 0.6849%\n",
      "Epoch [53/100], Step [70/225], Training Accuracy: 32.1205%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [71/225], Training Accuracy: 32.1523%, Training Loss: 0.6850%\n",
      "Epoch [53/100], Step [72/225], Training Accuracy: 31.9444%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [73/225], Training Accuracy: 31.8707%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [74/225], Training Accuracy: 31.9679%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [75/225], Training Accuracy: 31.9167%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [76/225], Training Accuracy: 31.8873%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Step [77/225], Training Accuracy: 31.8385%, Training Loss: 0.6851%\n",
      "Epoch [53/100], Step [78/225], Training Accuracy: 31.8710%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [79/225], Training Accuracy: 31.7840%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [80/225], Training Accuracy: 31.7383%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [81/225], Training Accuracy: 31.6358%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [82/225], Training Accuracy: 31.6311%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [83/225], Training Accuracy: 31.5512%, Training Loss: 0.6852%\n",
      "Epoch [53/100], Step [84/225], Training Accuracy: 31.6220%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [86/225], Training Accuracy: 31.6679%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [87/225], Training Accuracy: 31.6631%, Training Loss: 0.6853%\n",
      "Epoch [53/100], Step [88/225], Training Accuracy: 31.6229%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [89/225], Training Accuracy: 31.4958%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [90/225], Training Accuracy: 31.3889%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [91/225], Training Accuracy: 31.4217%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [92/225], Training Accuracy: 31.4029%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [94/225], Training Accuracy: 31.4661%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [95/225], Training Accuracy: 31.3651%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [96/225], Training Accuracy: 31.4616%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [97/225], Training Accuracy: 31.4916%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [98/225], Training Accuracy: 31.5370%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [100/225], Training Accuracy: 31.6250%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [101/225], Training Accuracy: 31.7605%, Training Loss: 0.6854%\n",
      "Epoch [53/100], Step [102/225], Training Accuracy: 31.6330%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [103/225], Training Accuracy: 31.6748%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [104/225], Training Accuracy: 31.6406%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [105/225], Training Accuracy: 31.6071%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [106/225], Training Accuracy: 31.6038%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [107/225], Training Accuracy: 31.4982%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [108/225], Training Accuracy: 31.5828%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [109/225], Training Accuracy: 31.4507%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [110/225], Training Accuracy: 31.4773%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [111/225], Training Accuracy: 31.3626%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [112/225], Training Accuracy: 31.4314%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [113/225], Training Accuracy: 31.4298%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [114/225], Training Accuracy: 31.4830%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [115/225], Training Accuracy: 31.4266%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [116/225], Training Accuracy: 31.4251%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [117/225], Training Accuracy: 31.3568%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [118/225], Training Accuracy: 31.3294%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [119/225], Training Accuracy: 31.2631%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [120/225], Training Accuracy: 31.2891%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [121/225], Training Accuracy: 31.2887%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [122/225], Training Accuracy: 31.2884%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [123/225], Training Accuracy: 31.3135%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [124/225], Training Accuracy: 31.3004%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [125/225], Training Accuracy: 31.2875%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [126/225], Training Accuracy: 31.2128%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [127/225], Training Accuracy: 31.1885%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [128/225], Training Accuracy: 31.1646%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [129/225], Training Accuracy: 31.2137%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [130/225], Training Accuracy: 31.1538%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [131/225], Training Accuracy: 31.1069%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [132/225], Training Accuracy: 31.0724%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [133/225], Training Accuracy: 31.0973%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [134/225], Training Accuracy: 31.1334%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [135/225], Training Accuracy: 31.1227%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [136/225], Training Accuracy: 31.1236%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [137/225], Training Accuracy: 31.1702%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [138/225], Training Accuracy: 31.1594%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [139/225], Training Accuracy: 31.1601%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [140/225], Training Accuracy: 31.1496%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [141/225], Training Accuracy: 31.0838%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [142/225], Training Accuracy: 31.1400%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [143/225], Training Accuracy: 31.1626%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [144/225], Training Accuracy: 31.1523%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [145/225], Training Accuracy: 31.2284%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [146/225], Training Accuracy: 31.2393%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [147/225], Training Accuracy: 31.2713%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [148/225], Training Accuracy: 31.2289%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [149/225], Training Accuracy: 31.2710%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [150/225], Training Accuracy: 31.3125%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [151/225], Training Accuracy: 31.3638%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [152/225], Training Accuracy: 31.3528%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [153/225], Training Accuracy: 31.3215%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [154/225], Training Accuracy: 31.3616%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [155/225], Training Accuracy: 31.3407%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [156/225], Training Accuracy: 31.3602%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [157/225], Training Accuracy: 31.2998%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [158/225], Training Accuracy: 31.2896%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [159/225], Training Accuracy: 31.3483%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [160/225], Training Accuracy: 31.3281%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [161/225], Training Accuracy: 31.3762%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [162/225], Training Accuracy: 31.3465%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [163/225], Training Accuracy: 31.4225%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [164/225], Training Accuracy: 31.3929%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [165/225], Training Accuracy: 31.3258%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [166/225], Training Accuracy: 31.3159%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [167/225], Training Accuracy: 31.3623%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [168/225], Training Accuracy: 31.3058%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [169/225], Training Accuracy: 31.2408%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [170/225], Training Accuracy: 31.1949%, Training Loss: 0.6860%\n",
      "Epoch [53/100], Step [171/225], Training Accuracy: 31.2409%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [172/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [173/225], Training Accuracy: 31.2319%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [174/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [175/225], Training Accuracy: 31.2679%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100], Step [176/225], Training Accuracy: 31.2766%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [177/225], Training Accuracy: 31.2677%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [178/225], Training Accuracy: 31.2676%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [179/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [180/225], Training Accuracy: 31.2934%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [181/225], Training Accuracy: 31.2414%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [182/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [183/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [184/225], Training Accuracy: 31.2075%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [185/225], Training Accuracy: 31.1740%, Training Loss: 0.6859%\n",
      "Epoch [53/100], Step [186/225], Training Accuracy: 31.1996%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [187/225], Training Accuracy: 31.2166%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [188/225], Training Accuracy: 31.2001%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [189/225], Training Accuracy: 31.2169%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [190/225], Training Accuracy: 31.1842%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [191/225], Training Accuracy: 31.1600%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [192/225], Training Accuracy: 31.0872%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [193/225], Training Accuracy: 31.0881%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [194/225], Training Accuracy: 31.1050%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [195/225], Training Accuracy: 31.0978%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [196/225], Training Accuracy: 31.0826%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [197/225], Training Accuracy: 31.1152%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [198/225], Training Accuracy: 31.1395%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [199/225], Training Accuracy: 31.1244%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [200/225], Training Accuracy: 31.1016%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [201/225], Training Accuracy: 31.1101%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [202/225], Training Accuracy: 31.0953%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [203/225], Training Accuracy: 31.0807%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [204/225], Training Accuracy: 31.1428%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [205/225], Training Accuracy: 31.1357%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [206/225], Training Accuracy: 31.1135%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [207/225], Training Accuracy: 31.0839%, Training Loss: 0.6858%\n",
      "Epoch [53/100], Step [208/225], Training Accuracy: 31.0998%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [209/225], Training Accuracy: 31.1304%, Training Loss: 0.6857%\n",
      "Epoch [53/100], Step [210/225], Training Accuracy: 31.1607%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [211/225], Training Accuracy: 31.1463%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [212/225], Training Accuracy: 31.1910%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [213/225], Training Accuracy: 31.1620%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [214/225], Training Accuracy: 31.1916%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [215/225], Training Accuracy: 31.1628%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [216/225], Training Accuracy: 31.1053%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [217/225], Training Accuracy: 31.1060%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [218/225], Training Accuracy: 31.0851%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [219/225], Training Accuracy: 31.1358%, Training Loss: 0.6856%\n",
      "Epoch [53/100], Step [220/225], Training Accuracy: 31.1577%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [221/225], Training Accuracy: 31.1581%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [222/225], Training Accuracy: 31.1726%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [223/225], Training Accuracy: 31.2150%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [224/225], Training Accuracy: 31.2221%, Training Loss: 0.6855%\n",
      "Epoch [53/100], Step [225/225], Training Accuracy: 31.2118%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6881%\n",
      "Epoch [54/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6878%\n",
      "Epoch [54/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6891%\n",
      "Epoch [54/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6871%\n",
      "Epoch [54/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6869%\n",
      "Epoch [54/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6872%\n",
      "Epoch [54/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6861%\n",
      "Epoch [54/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6861%\n",
      "Epoch [54/100], Step [12/225], Training Accuracy: 29.8177%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [13/225], Training Accuracy: 29.8077%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [14/225], Training Accuracy: 30.1339%, Training Loss: 0.6865%\n",
      "Epoch [54/100], Step [15/225], Training Accuracy: 30.4167%, Training Loss: 0.6868%\n",
      "Epoch [54/100], Step [16/225], Training Accuracy: 30.3711%, Training Loss: 0.6865%\n",
      "Epoch [54/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6869%\n",
      "Epoch [54/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6868%\n",
      "Epoch [54/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6867%\n",
      "Epoch [54/100], Step [20/225], Training Accuracy: 31.3281%, Training Loss: 0.6867%\n",
      "Epoch [54/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6865%\n",
      "Epoch [54/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6863%\n",
      "Epoch [54/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6864%\n",
      "Epoch [54/100], Step [24/225], Training Accuracy: 31.6406%, Training Loss: 0.6864%\n",
      "Epoch [54/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6862%\n",
      "Epoch [54/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6860%\n",
      "Epoch [54/100], Step [27/225], Training Accuracy: 31.7708%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [28/225], Training Accuracy: 31.6964%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [29/225], Training Accuracy: 32.0043%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [30/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [32/225], Training Accuracy: 32.1777%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [33/225], Training Accuracy: 32.2443%, Training Loss: 0.6848%\n",
      "Epoch [54/100], Step [34/225], Training Accuracy: 32.1232%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [35/225], Training Accuracy: 31.9643%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [36/225], Training Accuracy: 31.8576%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [37/225], Training Accuracy: 31.8834%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [38/225], Training Accuracy: 31.7434%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [40/225], Training Accuracy: 31.4062%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [41/225], Training Accuracy: 31.4024%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [42/225], Training Accuracy: 31.1384%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [43/225], Training Accuracy: 31.3227%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [44/225], Training Accuracy: 31.2855%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [45/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [46/225], Training Accuracy: 31.1481%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [47/225], Training Accuracy: 31.1170%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [48/225], Training Accuracy: 31.2174%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [49/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [50/225], Training Accuracy: 31.2188%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [51/225], Training Accuracy: 31.3725%, Training Loss: 0.6852%\n",
      "Epoch [54/100], Step [52/225], Training Accuracy: 31.4002%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [53/225], Training Accuracy: 31.3679%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [54/225], Training Accuracy: 31.2211%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [55/225], Training Accuracy: 31.3636%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [56/225], Training Accuracy: 31.4453%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [57/225], Training Accuracy: 31.4693%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [58/225], Training Accuracy: 31.3847%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [61/225], Training Accuracy: 31.8135%, Training Loss: 0.6848%\n",
      "Epoch [54/100], Step [62/225], Training Accuracy: 31.7540%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [63/225], Training Accuracy: 31.7956%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Step [64/225], Training Accuracy: 31.7627%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [65/225], Training Accuracy: 31.7067%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [66/225], Training Accuracy: 31.8182%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [67/225], Training Accuracy: 31.7397%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [68/225], Training Accuracy: 31.8244%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [69/225], Training Accuracy: 31.8161%, Training Loss: 0.6848%\n",
      "Epoch [54/100], Step [70/225], Training Accuracy: 31.7857%, Training Loss: 0.6848%\n",
      "Epoch [54/100], Step [71/225], Training Accuracy: 31.8882%, Training Loss: 0.6848%\n",
      "Epoch [54/100], Step [72/225], Training Accuracy: 31.7057%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [73/225], Training Accuracy: 31.6139%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [74/225], Training Accuracy: 31.6723%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [75/225], Training Accuracy: 31.6250%, Training Loss: 0.6848%\n",
      "Epoch [54/100], Step [76/225], Training Accuracy: 31.6201%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [77/225], Training Accuracy: 31.5138%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [78/225], Training Accuracy: 31.5104%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [79/225], Training Accuracy: 31.4082%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [80/225], Training Accuracy: 31.3867%, Training Loss: 0.6849%\n",
      "Epoch [54/100], Step [81/225], Training Accuracy: 31.2886%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [82/225], Training Accuracy: 31.3072%, Training Loss: 0.6850%\n",
      "Epoch [54/100], Step [83/225], Training Accuracy: 31.2312%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [84/225], Training Accuracy: 31.2872%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [85/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [86/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [87/225], Training Accuracy: 31.2680%, Training Loss: 0.6851%\n",
      "Epoch [54/100], Step [88/225], Training Accuracy: 31.2678%, Training Loss: 0.6852%\n",
      "Epoch [54/100], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [90/225], Training Accuracy: 31.0938%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [91/225], Training Accuracy: 31.1470%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [92/225], Training Accuracy: 31.1311%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [93/225], Training Accuracy: 31.1156%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [94/225], Training Accuracy: 31.2334%, Training Loss: 0.6852%\n",
      "Epoch [54/100], Step [95/225], Training Accuracy: 31.1349%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [96/225], Training Accuracy: 31.2174%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [97/225], Training Accuracy: 31.2339%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [98/225], Training Accuracy: 31.2819%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [99/225], Training Accuracy: 31.4078%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [100/225], Training Accuracy: 31.3906%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [101/225], Training Accuracy: 31.5130%, Training Loss: 0.6853%\n",
      "Epoch [54/100], Step [102/225], Training Accuracy: 31.4032%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [103/225], Training Accuracy: 31.4472%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [104/225], Training Accuracy: 31.4303%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [105/225], Training Accuracy: 31.3839%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [106/225], Training Accuracy: 31.3679%, Training Loss: 0.6854%\n",
      "Epoch [54/100], Step [107/225], Training Accuracy: 31.2646%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [108/225], Training Accuracy: 31.3368%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [109/225], Training Accuracy: 31.2070%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [110/225], Training Accuracy: 31.2358%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [111/225], Training Accuracy: 31.1374%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [112/225], Training Accuracy: 31.2081%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [113/225], Training Accuracy: 31.1947%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [114/225], Training Accuracy: 31.2089%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [115/225], Training Accuracy: 31.1685%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [116/225], Training Accuracy: 31.1827%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [117/225], Training Accuracy: 31.1298%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [118/225], Training Accuracy: 31.1308%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [119/225], Training Accuracy: 31.0793%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [120/225], Training Accuracy: 31.1198%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [121/225], Training Accuracy: 31.0950%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [122/225], Training Accuracy: 31.0835%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [123/225], Training Accuracy: 31.1103%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [124/225], Training Accuracy: 31.0988%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [125/225], Training Accuracy: 31.0750%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [126/225], Training Accuracy: 31.0268%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [127/225], Training Accuracy: 31.0285%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [128/225], Training Accuracy: 31.0181%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [129/225], Training Accuracy: 31.0562%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [130/225], Training Accuracy: 30.9736%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [131/225], Training Accuracy: 30.9280%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [132/225], Training Accuracy: 30.8712%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [133/225], Training Accuracy: 30.8976%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [134/225], Training Accuracy: 30.9701%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [135/225], Training Accuracy: 30.9722%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [137/225], Training Accuracy: 31.0447%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [138/225], Training Accuracy: 31.0575%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [139/225], Training Accuracy: 31.0252%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [140/225], Training Accuracy: 30.9933%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [141/225], Training Accuracy: 30.9397%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [142/225], Training Accuracy: 30.9969%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [143/225], Training Accuracy: 30.9987%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [144/225], Training Accuracy: 31.0330%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [145/225], Training Accuracy: 31.0991%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [146/225], Training Accuracy: 31.1323%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [147/225], Training Accuracy: 31.1650%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [148/225], Training Accuracy: 31.1550%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [149/225], Training Accuracy: 31.1766%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [150/225], Training Accuracy: 31.2188%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [151/225], Training Accuracy: 31.2810%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [152/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [153/225], Training Accuracy: 31.2194%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [154/225], Training Accuracy: 31.2703%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [155/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [156/225], Training Accuracy: 31.2600%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [157/225], Training Accuracy: 31.2002%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [158/225], Training Accuracy: 31.1907%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [159/225], Training Accuracy: 31.2795%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [160/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [54/100], Step [161/225], Training Accuracy: 31.3082%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [162/225], Training Accuracy: 31.2693%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Step [163/225], Training Accuracy: 31.3267%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [164/225], Training Accuracy: 31.3167%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [165/225], Training Accuracy: 31.2784%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [166/225], Training Accuracy: 31.2877%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [167/225], Training Accuracy: 31.3249%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [168/225], Training Accuracy: 31.2965%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [169/225], Training Accuracy: 31.2130%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [170/225], Training Accuracy: 31.1673%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [171/225], Training Accuracy: 31.1678%, Training Loss: 0.6858%\n",
      "Epoch [54/100], Step [172/225], Training Accuracy: 31.1864%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [173/225], Training Accuracy: 31.1958%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [175/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [176/225], Training Accuracy: 31.2855%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [177/225], Training Accuracy: 31.3030%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [178/225], Training Accuracy: 31.3027%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [179/225], Training Accuracy: 31.2762%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [180/225], Training Accuracy: 31.3108%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [181/225], Training Accuracy: 31.2586%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [182/225], Training Accuracy: 31.2414%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [183/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [184/225], Training Accuracy: 31.2160%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [185/225], Training Accuracy: 31.1655%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [186/225], Training Accuracy: 31.1912%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [187/225], Training Accuracy: 31.2082%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [188/225], Training Accuracy: 31.2084%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [189/225], Training Accuracy: 31.2417%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [190/225], Training Accuracy: 31.2253%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [191/225], Training Accuracy: 31.1927%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [192/225], Training Accuracy: 31.1361%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [193/225], Training Accuracy: 31.1286%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [195/225], Training Accuracy: 31.1218%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [196/225], Training Accuracy: 31.0985%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [197/225], Training Accuracy: 31.1390%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [198/225], Training Accuracy: 31.1632%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [199/225], Training Accuracy: 31.1322%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [200/225], Training Accuracy: 31.1094%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [201/225], Training Accuracy: 31.1256%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [202/225], Training Accuracy: 31.1340%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [203/225], Training Accuracy: 31.1192%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [204/225], Training Accuracy: 31.1811%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [205/225], Training Accuracy: 31.1738%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [206/225], Training Accuracy: 31.1590%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [207/225], Training Accuracy: 31.1368%, Training Loss: 0.6857%\n",
      "Epoch [54/100], Step [208/225], Training Accuracy: 31.1448%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [209/225], Training Accuracy: 31.1752%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [210/225], Training Accuracy: 31.1905%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [211/225], Training Accuracy: 31.1685%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [212/225], Training Accuracy: 31.2131%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [213/225], Training Accuracy: 31.1913%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [214/225], Training Accuracy: 31.2062%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [215/225], Training Accuracy: 31.1773%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [216/225], Training Accuracy: 31.1053%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [217/225], Training Accuracy: 31.1132%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [218/225], Training Accuracy: 31.0923%, Training Loss: 0.6856%\n",
      "Epoch [54/100], Step [219/225], Training Accuracy: 31.1572%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [220/225], Training Accuracy: 31.1790%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [221/225], Training Accuracy: 31.1652%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [222/225], Training Accuracy: 31.1867%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [223/225], Training Accuracy: 31.2290%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [224/225], Training Accuracy: 31.2221%, Training Loss: 0.6855%\n",
      "Epoch [54/100], Step [225/225], Training Accuracy: 31.1979%, Training Loss: 0.6855%\n",
      "Epoch [55/100], Step [1/225], Training Accuracy: 39.0625%, Training Loss: 0.6872%\n",
      "Epoch [55/100], Step [2/225], Training Accuracy: 35.1562%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [3/225], Training Accuracy: 33.8542%, Training Loss: 0.6875%\n",
      "Epoch [55/100], Step [4/225], Training Accuracy: 33.5938%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [5/225], Training Accuracy: 34.6875%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [6/225], Training Accuracy: 33.8542%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [7/225], Training Accuracy: 33.7054%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [8/225], Training Accuracy: 33.0078%, Training Loss: 0.6839%\n",
      "Epoch [55/100], Step [9/225], Training Accuracy: 32.8125%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [10/225], Training Accuracy: 32.5000%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [11/225], Training Accuracy: 32.1023%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [12/225], Training Accuracy: 31.6406%, Training Loss: 0.6845%\n",
      "Epoch [55/100], Step [13/225], Training Accuracy: 31.8510%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [14/225], Training Accuracy: 31.9196%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [15/225], Training Accuracy: 32.2917%, Training Loss: 0.6855%\n",
      "Epoch [55/100], Step [16/225], Training Accuracy: 32.2266%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [17/225], Training Accuracy: 31.8934%, Training Loss: 0.6858%\n",
      "Epoch [55/100], Step [18/225], Training Accuracy: 31.7708%, Training Loss: 0.6858%\n",
      "Epoch [55/100], Step [19/225], Training Accuracy: 32.0724%, Training Loss: 0.6858%\n",
      "Epoch [55/100], Step [20/225], Training Accuracy: 32.4219%, Training Loss: 0.6856%\n",
      "Epoch [55/100], Step [21/225], Training Accuracy: 32.3661%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [22/225], Training Accuracy: 32.5284%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [23/225], Training Accuracy: 32.2690%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [24/225], Training Accuracy: 32.4870%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [25/225], Training Accuracy: 32.8125%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [26/225], Training Accuracy: 33.1130%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [27/225], Training Accuracy: 32.8125%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [28/225], Training Accuracy: 32.5893%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [29/225], Training Accuracy: 32.9741%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [30/225], Training Accuracy: 32.9167%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [31/225], Training Accuracy: 32.7621%, Training Loss: 0.6845%\n",
      "Epoch [55/100], Step [32/225], Training Accuracy: 33.0078%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [33/225], Training Accuracy: 33.0492%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [34/225], Training Accuracy: 32.9504%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [35/225], Training Accuracy: 32.8125%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [36/225], Training Accuracy: 32.6389%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [37/225], Training Accuracy: 32.6014%, Training Loss: 0.6845%\n",
      "Epoch [55/100], Step [38/225], Training Accuracy: 32.3602%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [39/225], Training Accuracy: 32.0913%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [40/225], Training Accuracy: 32.1484%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [41/225], Training Accuracy: 32.1646%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [42/225], Training Accuracy: 31.8824%, Training Loss: 0.6849%\n",
      "Epoch [55/100], Step [43/225], Training Accuracy: 32.0858%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [44/225], Training Accuracy: 32.1023%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [45/225], Training Accuracy: 32.1528%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [46/225], Training Accuracy: 32.0312%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [47/225], Training Accuracy: 31.9481%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Step [48/225], Training Accuracy: 32.0638%, Training Loss: 0.6845%\n",
      "Epoch [55/100], Step [49/225], Training Accuracy: 32.1110%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [50/225], Training Accuracy: 32.1250%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [51/225], Training Accuracy: 32.2610%, Training Loss: 0.6845%\n",
      "Epoch [55/100], Step [52/225], Training Accuracy: 32.2716%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [53/225], Training Accuracy: 32.1934%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [54/225], Training Accuracy: 32.0312%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [55/225], Training Accuracy: 32.1023%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [56/225], Training Accuracy: 32.1429%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [57/225], Training Accuracy: 32.1272%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [58/225], Training Accuracy: 32.0582%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [59/225], Training Accuracy: 32.4682%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [60/225], Training Accuracy: 32.5781%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [61/225], Training Accuracy: 32.4795%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [62/225], Training Accuracy: 32.5353%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [63/225], Training Accuracy: 32.5645%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [64/225], Training Accuracy: 32.5684%, Training Loss: 0.6840%\n",
      "Epoch [55/100], Step [65/225], Training Accuracy: 32.4519%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [66/225], Training Accuracy: 32.5284%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [67/225], Training Accuracy: 32.4860%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [68/225], Training Accuracy: 32.5597%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [69/225], Training Accuracy: 32.5408%, Training Loss: 0.6840%\n",
      "Epoch [55/100], Step [70/225], Training Accuracy: 32.4777%, Training Loss: 0.6840%\n",
      "Epoch [55/100], Step [71/225], Training Accuracy: 32.4824%, Training Loss: 0.6840%\n",
      "Epoch [55/100], Step [72/225], Training Accuracy: 32.2483%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [73/225], Training Accuracy: 32.1918%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [74/225], Training Accuracy: 32.2635%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [75/225], Training Accuracy: 32.2292%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [76/225], Training Accuracy: 32.1546%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [77/225], Training Accuracy: 32.0820%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [78/225], Training Accuracy: 32.1314%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [79/225], Training Accuracy: 32.0411%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [80/225], Training Accuracy: 31.9922%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [81/225], Training Accuracy: 31.9059%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [82/225], Training Accuracy: 31.8979%, Training Loss: 0.6841%\n",
      "Epoch [55/100], Step [83/225], Training Accuracy: 31.8524%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [84/225], Training Accuracy: 31.8638%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [85/225], Training Accuracy: 31.8566%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [86/225], Training Accuracy: 31.8677%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [87/225], Training Accuracy: 31.8966%, Training Loss: 0.6842%\n",
      "Epoch [55/100], Step [88/225], Training Accuracy: 31.8537%, Training Loss: 0.6843%\n",
      "Epoch [55/100], Step [89/225], Training Accuracy: 31.7416%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [90/225], Training Accuracy: 31.5972%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [91/225], Training Accuracy: 31.6277%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [92/225], Training Accuracy: 31.6236%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [93/225], Training Accuracy: 31.6364%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [94/225], Training Accuracy: 31.6822%, Training Loss: 0.6844%\n",
      "Epoch [55/100], Step [95/225], Training Accuracy: 31.5625%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [96/225], Training Accuracy: 31.6243%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [97/225], Training Accuracy: 31.6527%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [98/225], Training Accuracy: 31.6645%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [99/225], Training Accuracy: 31.8024%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [100/225], Training Accuracy: 31.7969%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [101/225], Training Accuracy: 31.8843%, Training Loss: 0.6846%\n",
      "Epoch [55/100], Step [102/225], Training Accuracy: 31.7862%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [103/225], Training Accuracy: 31.8416%, Training Loss: 0.6847%\n",
      "Epoch [55/100], Step [104/225], Training Accuracy: 31.8059%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [105/225], Training Accuracy: 31.7411%, Training Loss: 0.6848%\n",
      "Epoch [55/100], Step [106/225], Training Accuracy: 31.7364%, Training Loss: 0.6849%\n",
      "Epoch [55/100], Step [107/225], Training Accuracy: 31.6735%, Training Loss: 0.6849%\n",
      "Epoch [55/100], Step [108/225], Training Accuracy: 31.7274%, Training Loss: 0.6849%\n",
      "Epoch [55/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [110/225], Training Accuracy: 31.5909%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [111/225], Training Accuracy: 31.4752%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [112/225], Training Accuracy: 31.5430%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [113/225], Training Accuracy: 31.5265%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [114/225], Training Accuracy: 31.6064%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [115/225], Training Accuracy: 31.5897%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [116/225], Training Accuracy: 31.5867%, Training Loss: 0.6850%\n",
      "Epoch [55/100], Step [117/225], Training Accuracy: 31.5304%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [118/225], Training Accuracy: 31.4883%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [119/225], Training Accuracy: 31.4076%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [120/225], Training Accuracy: 31.4323%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [121/225], Training Accuracy: 31.4308%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [122/225], Training Accuracy: 31.4421%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [123/225], Training Accuracy: 31.5041%, Training Loss: 0.6851%\n",
      "Epoch [55/100], Step [124/225], Training Accuracy: 31.5146%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [125/225], Training Accuracy: 31.4875%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [126/225], Training Accuracy: 31.4112%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [127/225], Training Accuracy: 31.3853%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [128/225], Training Accuracy: 31.3843%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [129/225], Training Accuracy: 31.4317%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [130/225], Training Accuracy: 31.3702%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [131/225], Training Accuracy: 31.3335%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [132/225], Training Accuracy: 31.3092%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [133/225], Training Accuracy: 31.3205%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [134/225], Training Accuracy: 31.3783%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [135/225], Training Accuracy: 31.4005%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [136/225], Training Accuracy: 31.4108%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [137/225], Training Accuracy: 31.4325%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [138/225], Training Accuracy: 31.4538%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [139/225], Training Accuracy: 31.4186%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [140/225], Training Accuracy: 31.3951%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [141/225], Training Accuracy: 31.3276%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [142/225], Training Accuracy: 31.3820%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [143/225], Training Accuracy: 31.3811%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [144/225], Training Accuracy: 31.4128%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [145/225], Training Accuracy: 31.4655%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [146/225], Training Accuracy: 31.4854%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [147/225], Training Accuracy: 31.4945%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [148/225], Training Accuracy: 31.4611%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [149/225], Training Accuracy: 31.4912%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [150/225], Training Accuracy: 31.5312%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [151/225], Training Accuracy: 31.5811%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [152/225], Training Accuracy: 31.5481%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [153/225], Training Accuracy: 31.5359%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [154/225], Training Accuracy: 31.5848%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [155/225], Training Accuracy: 31.5726%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [156/225], Training Accuracy: 31.6006%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [157/225], Training Accuracy: 31.5386%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100], Step [158/225], Training Accuracy: 31.5368%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [159/225], Training Accuracy: 31.6038%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [160/225], Training Accuracy: 31.5625%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [161/225], Training Accuracy: 31.6091%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [162/225], Training Accuracy: 31.5779%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [163/225], Training Accuracy: 31.6238%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [164/225], Training Accuracy: 31.5930%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [165/225], Training Accuracy: 31.5341%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [166/225], Training Accuracy: 31.5324%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [167/225], Training Accuracy: 31.5962%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [168/225], Training Accuracy: 31.5569%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [169/225], Training Accuracy: 31.4904%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [170/225], Training Accuracy: 31.4430%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [171/225], Training Accuracy: 31.4784%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [172/225], Training Accuracy: 31.4953%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [173/225], Training Accuracy: 31.4848%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [174/225], Training Accuracy: 31.5014%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [175/225], Training Accuracy: 31.5268%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [176/225], Training Accuracy: 31.5430%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [177/225], Training Accuracy: 31.5590%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [178/225], Training Accuracy: 31.5485%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [179/225], Training Accuracy: 31.5031%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [180/225], Training Accuracy: 31.5451%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [181/225], Training Accuracy: 31.4917%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [182/225], Training Accuracy: 31.4818%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [183/225], Training Accuracy: 31.4891%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [184/225], Training Accuracy: 31.4623%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [185/225], Training Accuracy: 31.4274%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [186/225], Training Accuracy: 31.4348%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [187/225], Training Accuracy: 31.4589%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [188/225], Training Accuracy: 31.4495%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [189/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [190/225], Training Accuracy: 31.4391%, Training Loss: 0.6854%\n",
      "Epoch [55/100], Step [191/225], Training Accuracy: 31.4136%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [192/225], Training Accuracy: 31.3477%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [193/225], Training Accuracy: 31.3472%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [194/225], Training Accuracy: 31.3547%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [195/225], Training Accuracy: 31.3462%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [196/225], Training Accuracy: 31.3138%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [197/225], Training Accuracy: 31.3214%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [198/225], Training Accuracy: 31.3526%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [199/225], Training Accuracy: 31.3207%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [200/225], Training Accuracy: 31.3047%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [201/225], Training Accuracy: 31.3277%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [202/225], Training Accuracy: 31.3196%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [203/225], Training Accuracy: 31.3116%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [204/225], Training Accuracy: 31.3649%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [205/225], Training Accuracy: 31.3567%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [206/225], Training Accuracy: 31.3486%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [207/225], Training Accuracy: 31.3255%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [208/225], Training Accuracy: 31.3627%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [209/225], Training Accuracy: 31.4070%, Training Loss: 0.6853%\n",
      "Epoch [55/100], Step [210/225], Training Accuracy: 31.4360%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [211/225], Training Accuracy: 31.4129%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [212/225], Training Accuracy: 31.4637%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [213/225], Training Accuracy: 31.4407%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [214/225], Training Accuracy: 31.4471%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [215/225], Training Accuracy: 31.4244%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [216/225], Training Accuracy: 31.3657%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [217/225], Training Accuracy: 31.3508%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [218/225], Training Accuracy: 31.3073%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [219/225], Training Accuracy: 31.3713%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [220/225], Training Accuracy: 31.3920%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [221/225], Training Accuracy: 31.3773%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [222/225], Training Accuracy: 31.3978%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [223/225], Training Accuracy: 31.4322%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [224/225], Training Accuracy: 31.4383%, Training Loss: 0.6852%\n",
      "Epoch [55/100], Step [225/225], Training Accuracy: 31.4133%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [1/225], Training Accuracy: 37.5000%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [2/225], Training Accuracy: 35.1562%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [3/225], Training Accuracy: 33.8542%, Training Loss: 0.6866%\n",
      "Epoch [56/100], Step [4/225], Training Accuracy: 33.9844%, Training Loss: 0.6848%\n",
      "Epoch [56/100], Step [5/225], Training Accuracy: 34.6875%, Training Loss: 0.6859%\n",
      "Epoch [56/100], Step [6/225], Training Accuracy: 33.8542%, Training Loss: 0.6860%\n",
      "Epoch [56/100], Step [7/225], Training Accuracy: 32.8125%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [8/225], Training Accuracy: 32.2266%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6859%\n",
      "Epoch [56/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6855%\n",
      "Epoch [56/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6858%\n",
      "Epoch [56/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6856%\n",
      "Epoch [56/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6858%\n",
      "Epoch [56/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6862%\n",
      "Epoch [56/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6861%\n",
      "Epoch [56/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6859%\n",
      "Epoch [56/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6859%\n",
      "Epoch [56/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6859%\n",
      "Epoch [56/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6860%\n",
      "Epoch [56/100], Step [20/225], Training Accuracy: 31.8750%, Training Loss: 0.6859%\n",
      "Epoch [56/100], Step [21/225], Training Accuracy: 31.8452%, Training Loss: 0.6855%\n",
      "Epoch [56/100], Step [22/225], Training Accuracy: 31.9602%, Training Loss: 0.6855%\n",
      "Epoch [56/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [24/225], Training Accuracy: 32.0312%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [26/225], Training Accuracy: 32.5120%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [27/225], Training Accuracy: 32.2338%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [28/225], Training Accuracy: 32.0871%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [29/225], Training Accuracy: 32.4892%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [30/225], Training Accuracy: 32.5521%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [31/225], Training Accuracy: 32.4597%, Training Loss: 0.6841%\n",
      "Epoch [56/100], Step [32/225], Training Accuracy: 32.7148%, Training Loss: 0.6838%\n",
      "Epoch [56/100], Step [33/225], Training Accuracy: 32.7652%, Training Loss: 0.6838%\n",
      "Epoch [56/100], Step [34/225], Training Accuracy: 32.5827%, Training Loss: 0.6839%\n",
      "Epoch [56/100], Step [35/225], Training Accuracy: 32.5000%, Training Loss: 0.6839%\n",
      "Epoch [56/100], Step [36/225], Training Accuracy: 32.2483%, Training Loss: 0.6840%\n",
      "Epoch [56/100], Step [37/225], Training Accuracy: 32.3057%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [38/225], Training Accuracy: 32.0724%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [39/225], Training Accuracy: 31.8109%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [40/225], Training Accuracy: 31.7969%, Training Loss: 0.6845%\n",
      "Epoch [56/100], Step [41/225], Training Accuracy: 31.9360%, Training Loss: 0.6845%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Step [42/225], Training Accuracy: 31.7336%, Training Loss: 0.6847%\n",
      "Epoch [56/100], Step [43/225], Training Accuracy: 31.9404%, Training Loss: 0.6845%\n",
      "Epoch [56/100], Step [44/225], Training Accuracy: 31.9247%, Training Loss: 0.6845%\n",
      "Epoch [56/100], Step [45/225], Training Accuracy: 31.9097%, Training Loss: 0.6845%\n",
      "Epoch [56/100], Step [46/225], Training Accuracy: 31.7255%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [47/225], Training Accuracy: 31.6822%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [48/225], Training Accuracy: 31.8034%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6845%\n",
      "Epoch [56/100], Step [50/225], Training Accuracy: 31.7812%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [51/225], Training Accuracy: 31.9547%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [52/225], Training Accuracy: 32.0012%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [53/225], Training Accuracy: 31.9281%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [54/225], Training Accuracy: 31.7419%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [55/225], Training Accuracy: 31.8182%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [57/225], Training Accuracy: 31.8805%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [58/225], Training Accuracy: 31.8157%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [59/225], Training Accuracy: 32.1769%, Training Loss: 0.6841%\n",
      "Epoch [56/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6841%\n",
      "Epoch [56/100], Step [61/225], Training Accuracy: 32.2490%, Training Loss: 0.6841%\n",
      "Epoch [56/100], Step [62/225], Training Accuracy: 32.2833%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [63/225], Training Accuracy: 32.3661%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [64/225], Training Accuracy: 32.3730%, Training Loss: 0.6841%\n",
      "Epoch [56/100], Step [65/225], Training Accuracy: 32.2356%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [66/225], Training Accuracy: 32.3390%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [67/225], Training Accuracy: 32.2761%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [68/225], Training Accuracy: 32.3529%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [69/225], Training Accuracy: 32.3596%, Training Loss: 0.6840%\n",
      "Epoch [56/100], Step [70/225], Training Accuracy: 32.2991%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [71/225], Training Accuracy: 32.3063%, Training Loss: 0.6841%\n",
      "Epoch [56/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [73/225], Training Accuracy: 31.9777%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [74/225], Training Accuracy: 32.0524%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [75/225], Training Accuracy: 32.0208%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [76/225], Training Accuracy: 31.9490%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [77/225], Training Accuracy: 31.8791%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [78/225], Training Accuracy: 31.8910%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [79/225], Training Accuracy: 31.7840%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [80/225], Training Accuracy: 31.7578%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [81/225], Training Accuracy: 31.6937%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [82/225], Training Accuracy: 31.7073%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [83/225], Training Accuracy: 31.6453%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [84/225], Training Accuracy: 31.7336%, Training Loss: 0.6842%\n",
      "Epoch [56/100], Step [85/225], Training Accuracy: 31.6912%, Training Loss: 0.6843%\n",
      "Epoch [56/100], Step [86/225], Training Accuracy: 31.7406%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [87/225], Training Accuracy: 31.7170%, Training Loss: 0.6844%\n",
      "Epoch [56/100], Step [88/225], Training Accuracy: 31.7116%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [89/225], Training Accuracy: 31.6538%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [90/225], Training Accuracy: 31.5625%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [91/225], Training Accuracy: 31.6277%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [92/225], Training Accuracy: 31.6067%, Training Loss: 0.6845%\n",
      "Epoch [56/100], Step [93/225], Training Accuracy: 31.6196%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [94/225], Training Accuracy: 31.6988%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [95/225], Training Accuracy: 31.5789%, Training Loss: 0.6848%\n",
      "Epoch [56/100], Step [96/225], Training Accuracy: 31.6732%, Training Loss: 0.6848%\n",
      "Epoch [56/100], Step [97/225], Training Accuracy: 31.7010%, Training Loss: 0.6848%\n",
      "Epoch [56/100], Step [98/225], Training Accuracy: 31.7124%, Training Loss: 0.6847%\n",
      "Epoch [56/100], Step [99/225], Training Accuracy: 31.8024%, Training Loss: 0.6847%\n",
      "Epoch [56/100], Step [100/225], Training Accuracy: 31.7812%, Training Loss: 0.6847%\n",
      "Epoch [56/100], Step [101/225], Training Accuracy: 31.8843%, Training Loss: 0.6846%\n",
      "Epoch [56/100], Step [102/225], Training Accuracy: 31.7555%, Training Loss: 0.6847%\n",
      "Epoch [56/100], Step [103/225], Training Accuracy: 31.8265%, Training Loss: 0.6848%\n",
      "Epoch [56/100], Step [104/225], Training Accuracy: 31.8209%, Training Loss: 0.6848%\n",
      "Epoch [56/100], Step [105/225], Training Accuracy: 31.7708%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [106/225], Training Accuracy: 31.7512%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [107/225], Training Accuracy: 31.6589%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [108/225], Training Accuracy: 31.7274%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [110/225], Training Accuracy: 31.5625%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [111/225], Training Accuracy: 31.4471%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [112/225], Training Accuracy: 31.5290%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [113/225], Training Accuracy: 31.5404%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [114/225], Training Accuracy: 31.5789%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [115/225], Training Accuracy: 31.5489%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [116/225], Training Accuracy: 31.5194%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [117/225], Training Accuracy: 31.4503%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [118/225], Training Accuracy: 31.4354%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [119/225], Training Accuracy: 31.3550%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [120/225], Training Accuracy: 31.3802%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [121/225], Training Accuracy: 31.3275%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [122/225], Training Accuracy: 31.3268%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [123/225], Training Accuracy: 31.3643%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [124/225], Training Accuracy: 31.3634%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [125/225], Training Accuracy: 31.3375%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [126/225], Training Accuracy: 31.2748%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [127/225], Training Accuracy: 31.2623%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [128/225], Training Accuracy: 31.2622%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [129/225], Training Accuracy: 31.3227%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [131/225], Training Accuracy: 31.2023%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [132/225], Training Accuracy: 31.1908%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [133/225], Training Accuracy: 31.2148%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [134/225], Training Accuracy: 31.2733%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [135/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [136/225], Training Accuracy: 31.2615%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [137/225], Training Accuracy: 31.3070%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [138/225], Training Accuracy: 31.3179%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [139/225], Training Accuracy: 31.2837%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [140/225], Training Accuracy: 31.2723%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [141/225], Training Accuracy: 31.2168%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [142/225], Training Accuracy: 31.2720%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [143/225], Training Accuracy: 31.2609%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [144/225], Training Accuracy: 31.2717%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [145/225], Training Accuracy: 31.3362%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [146/225], Training Accuracy: 31.3677%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [147/225], Training Accuracy: 31.3669%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [148/225], Training Accuracy: 31.3345%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [149/225], Training Accuracy: 31.3654%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [150/225], Training Accuracy: 31.3958%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Step [151/225], Training Accuracy: 31.4363%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [152/225], Training Accuracy: 31.4350%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [153/225], Training Accuracy: 31.3930%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [154/225], Training Accuracy: 31.4225%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [155/225], Training Accuracy: 31.4113%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [156/225], Training Accuracy: 31.4503%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [157/225], Training Accuracy: 31.3893%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [158/225], Training Accuracy: 31.3588%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [159/225], Training Accuracy: 31.4072%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [160/225], Training Accuracy: 31.3770%, Training Loss: 0.6854%\n",
      "Epoch [56/100], Step [161/225], Training Accuracy: 31.4344%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [162/225], Training Accuracy: 31.4236%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [163/225], Training Accuracy: 31.4896%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [164/225], Training Accuracy: 31.4787%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [165/225], Training Accuracy: 31.4015%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [166/225], Training Accuracy: 31.4100%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [167/225], Training Accuracy: 31.4558%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [168/225], Training Accuracy: 31.4174%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [169/225], Training Accuracy: 31.3332%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [170/225], Training Accuracy: 31.2868%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [171/225], Training Accuracy: 31.2865%, Training Loss: 0.6853%\n",
      "Epoch [56/100], Step [172/225], Training Accuracy: 31.3045%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [173/225], Training Accuracy: 31.2952%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [174/225], Training Accuracy: 31.3308%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [175/225], Training Accuracy: 31.3661%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [176/225], Training Accuracy: 31.3565%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [177/225], Training Accuracy: 31.3648%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [178/225], Training Accuracy: 31.3553%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [179/225], Training Accuracy: 31.3111%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [180/225], Training Accuracy: 31.3455%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [181/225], Training Accuracy: 31.2845%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [182/225], Training Accuracy: 31.2586%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [183/225], Training Accuracy: 31.2671%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [184/225], Training Accuracy: 31.2245%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [185/225], Training Accuracy: 31.1993%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [186/225], Training Accuracy: 31.2080%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [187/225], Training Accuracy: 31.2249%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [188/225], Training Accuracy: 31.2251%, Training Loss: 0.6852%\n",
      "Epoch [56/100], Step [189/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [190/225], Training Accuracy: 31.2253%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [191/225], Training Accuracy: 31.2173%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [192/225], Training Accuracy: 31.1605%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [193/225], Training Accuracy: 31.1690%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [194/225], Training Accuracy: 31.1695%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [195/225], Training Accuracy: 31.1538%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [196/225], Training Accuracy: 31.1065%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [197/225], Training Accuracy: 31.1469%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [198/225], Training Accuracy: 31.1790%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [199/225], Training Accuracy: 31.1558%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [200/225], Training Accuracy: 31.1328%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [201/225], Training Accuracy: 31.1334%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [202/225], Training Accuracy: 31.1185%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [203/225], Training Accuracy: 31.1115%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [204/225], Training Accuracy: 31.1811%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [205/225], Training Accuracy: 31.1966%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [206/225], Training Accuracy: 31.1893%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [207/225], Training Accuracy: 31.1670%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [208/225], Training Accuracy: 31.1974%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [209/225], Training Accuracy: 31.2276%, Training Loss: 0.6851%\n",
      "Epoch [56/100], Step [210/225], Training Accuracy: 31.2574%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [211/225], Training Accuracy: 31.2426%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [212/225], Training Accuracy: 31.2942%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [213/225], Training Accuracy: 31.2720%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [214/225], Training Accuracy: 31.3084%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [215/225], Training Accuracy: 31.2718%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [216/225], Training Accuracy: 31.2066%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [217/225], Training Accuracy: 31.2068%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [218/225], Training Accuracy: 31.1855%, Training Loss: 0.6850%\n",
      "Epoch [56/100], Step [219/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [220/225], Training Accuracy: 31.2642%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [221/225], Training Accuracy: 31.2641%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [222/225], Training Accuracy: 31.2852%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [223/225], Training Accuracy: 31.3201%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [224/225], Training Accuracy: 31.3267%, Training Loss: 0.6849%\n",
      "Epoch [56/100], Step [225/225], Training Accuracy: 31.3091%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6898%\n",
      "Epoch [57/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6863%\n",
      "Epoch [57/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6877%\n",
      "Epoch [57/100], Step [4/225], Training Accuracy: 30.8594%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [8/225], Training Accuracy: 32.0312%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [9/225], Training Accuracy: 32.2917%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [10/225], Training Accuracy: 31.7188%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [11/225], Training Accuracy: 31.3920%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [12/225], Training Accuracy: 30.9896%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6864%\n",
      "Epoch [57/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6868%\n",
      "Epoch [57/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6866%\n",
      "Epoch [57/100], Step [17/225], Training Accuracy: 30.9743%, Training Loss: 0.6870%\n",
      "Epoch [57/100], Step [18/225], Training Accuracy: 31.0764%, Training Loss: 0.6871%\n",
      "Epoch [57/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6870%\n",
      "Epoch [57/100], Step [20/225], Training Accuracy: 31.9531%, Training Loss: 0.6869%\n",
      "Epoch [57/100], Step [21/225], Training Accuracy: 31.8452%, Training Loss: 0.6863%\n",
      "Epoch [57/100], Step [22/225], Training Accuracy: 32.0312%, Training Loss: 0.6861%\n",
      "Epoch [57/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6860%\n",
      "Epoch [57/100], Step [24/225], Training Accuracy: 31.9010%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Step [26/225], Training Accuracy: 32.3317%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [27/225], Training Accuracy: 32.1181%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [28/225], Training Accuracy: 31.9754%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [29/225], Training Accuracy: 32.3815%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [30/225], Training Accuracy: 32.3958%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [31/225], Training Accuracy: 32.3085%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [32/225], Training Accuracy: 32.4707%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [33/225], Training Accuracy: 32.6231%, Training Loss: 0.6851%\n",
      "Epoch [57/100], Step [34/225], Training Accuracy: 32.5368%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [35/225], Training Accuracy: 32.3661%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [36/225], Training Accuracy: 32.2483%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [37/225], Training Accuracy: 32.2213%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [38/225], Training Accuracy: 32.0312%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [39/225], Training Accuracy: 31.8109%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [40/225], Training Accuracy: 31.7578%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [41/225], Training Accuracy: 31.7073%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6860%\n",
      "Epoch [57/100], Step [43/225], Training Accuracy: 31.6860%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [44/225], Training Accuracy: 31.6051%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [45/225], Training Accuracy: 31.6319%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [46/225], Training Accuracy: 31.6236%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [48/225], Training Accuracy: 31.7057%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [49/225], Training Accuracy: 31.6645%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [50/225], Training Accuracy: 31.7500%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [51/225], Training Accuracy: 31.9240%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [52/225], Training Accuracy: 31.9111%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [53/225], Training Accuracy: 31.8101%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [54/225], Training Accuracy: 31.6551%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [55/225], Training Accuracy: 31.7045%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [56/225], Training Accuracy: 31.8080%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [57/225], Training Accuracy: 31.7708%, Training Loss: 0.6848%\n",
      "Epoch [57/100], Step [58/225], Training Accuracy: 31.6810%, Training Loss: 0.6848%\n",
      "Epoch [57/100], Step [59/225], Training Accuracy: 31.9915%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [60/225], Training Accuracy: 32.1094%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [61/225], Training Accuracy: 32.0441%, Training Loss: 0.6845%\n",
      "Epoch [57/100], Step [62/225], Training Accuracy: 32.0817%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [63/225], Training Accuracy: 32.1429%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [64/225], Training Accuracy: 32.1533%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [65/225], Training Accuracy: 32.0673%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [66/225], Training Accuracy: 32.1970%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [67/225], Training Accuracy: 32.1828%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [68/225], Training Accuracy: 32.2610%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [69/225], Training Accuracy: 32.2464%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [70/225], Training Accuracy: 32.1652%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [71/225], Training Accuracy: 32.1743%, Training Loss: 0.6846%\n",
      "Epoch [57/100], Step [72/225], Training Accuracy: 31.9661%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [73/225], Training Accuracy: 31.8921%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [74/225], Training Accuracy: 31.9890%, Training Loss: 0.6848%\n",
      "Epoch [57/100], Step [75/225], Training Accuracy: 31.9167%, Training Loss: 0.6848%\n",
      "Epoch [57/100], Step [76/225], Training Accuracy: 31.8668%, Training Loss: 0.6847%\n",
      "Epoch [57/100], Step [77/225], Training Accuracy: 31.7979%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [78/225], Training Accuracy: 31.8109%, Training Loss: 0.6848%\n",
      "Epoch [57/100], Step [79/225], Training Accuracy: 31.7445%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [80/225], Training Accuracy: 31.7188%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [81/225], Training Accuracy: 31.6165%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [82/225], Training Accuracy: 31.6120%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [83/225], Training Accuracy: 31.5512%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [84/225], Training Accuracy: 31.6034%, Training Loss: 0.6849%\n",
      "Epoch [57/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [86/225], Training Accuracy: 31.6679%, Training Loss: 0.6850%\n",
      "Epoch [57/100], Step [87/225], Training Accuracy: 31.6810%, Training Loss: 0.6851%\n",
      "Epoch [57/100], Step [88/225], Training Accuracy: 31.6406%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [89/225], Training Accuracy: 31.5660%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [90/225], Training Accuracy: 31.4757%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [91/225], Training Accuracy: 31.5076%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [92/225], Training Accuracy: 31.5048%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [93/225], Training Accuracy: 31.5020%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [94/225], Training Accuracy: 31.5991%, Training Loss: 0.6852%\n",
      "Epoch [57/100], Step [95/225], Training Accuracy: 31.4803%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [96/225], Training Accuracy: 31.5430%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [97/225], Training Accuracy: 31.5883%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [98/225], Training Accuracy: 31.6008%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [99/225], Training Accuracy: 31.7077%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [100/225], Training Accuracy: 31.7188%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [101/225], Training Accuracy: 31.8224%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [102/225], Training Accuracy: 31.6942%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [103/225], Training Accuracy: 31.7506%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [104/225], Training Accuracy: 31.7308%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [105/225], Training Accuracy: 31.6964%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [106/225], Training Accuracy: 31.6480%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [107/225], Training Accuracy: 31.5713%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [108/225], Training Accuracy: 31.6117%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [109/225], Training Accuracy: 31.4794%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [110/225], Training Accuracy: 31.4915%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [111/225], Training Accuracy: 31.3767%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [112/225], Training Accuracy: 31.4732%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [113/225], Training Accuracy: 31.4712%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [114/225], Training Accuracy: 31.5652%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [115/225], Training Accuracy: 31.5082%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [116/225], Training Accuracy: 31.4790%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [117/225], Training Accuracy: 31.4370%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [118/225], Training Accuracy: 31.3957%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [119/225], Training Accuracy: 31.3288%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [120/225], Training Accuracy: 31.3802%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [121/225], Training Accuracy: 31.3662%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [122/225], Training Accuracy: 31.3909%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [123/225], Training Accuracy: 31.4151%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [124/225], Training Accuracy: 31.4138%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [125/225], Training Accuracy: 31.3750%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Step [126/225], Training Accuracy: 31.3244%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [127/225], Training Accuracy: 31.2746%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [128/225], Training Accuracy: 31.2622%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [129/225], Training Accuracy: 31.3106%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [130/225], Training Accuracy: 31.2380%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [131/225], Training Accuracy: 31.2023%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [132/225], Training Accuracy: 31.1435%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [133/225], Training Accuracy: 31.1795%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [134/225], Training Accuracy: 31.2150%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [135/225], Training Accuracy: 31.2269%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [136/225], Training Accuracy: 31.2385%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [137/225], Training Accuracy: 31.2728%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [138/225], Training Accuracy: 31.2840%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [139/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [140/225], Training Accuracy: 31.1942%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [141/225], Training Accuracy: 31.1503%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [142/225], Training Accuracy: 31.2170%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [143/225], Training Accuracy: 31.2172%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [144/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [145/225], Training Accuracy: 31.3254%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [146/225], Training Accuracy: 31.3463%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [147/225], Training Accuracy: 31.3669%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [148/225], Training Accuracy: 31.3239%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [149/225], Training Accuracy: 31.3339%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [150/225], Training Accuracy: 31.3438%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [151/225], Training Accuracy: 31.3949%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [152/225], Training Accuracy: 31.4042%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [153/225], Training Accuracy: 31.3828%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [154/225], Training Accuracy: 31.3819%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [155/225], Training Accuracy: 31.3710%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [156/225], Training Accuracy: 31.4002%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [157/225], Training Accuracy: 31.3495%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [158/225], Training Accuracy: 31.3093%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [159/225], Training Accuracy: 31.3679%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [160/225], Training Accuracy: 31.3477%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [161/225], Training Accuracy: 31.4053%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [162/225], Training Accuracy: 31.3754%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [163/225], Training Accuracy: 31.4130%, Training Loss: 0.6859%\n",
      "Epoch [57/100], Step [164/225], Training Accuracy: 31.3929%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [165/225], Training Accuracy: 31.3163%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [166/225], Training Accuracy: 31.3253%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [167/225], Training Accuracy: 31.3623%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [168/225], Training Accuracy: 31.3151%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [169/225], Training Accuracy: 31.2315%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [170/225], Training Accuracy: 31.1765%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [171/225], Training Accuracy: 31.2043%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [172/225], Training Accuracy: 31.2137%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [173/225], Training Accuracy: 31.2139%, Training Loss: 0.6858%\n",
      "Epoch [57/100], Step [174/225], Training Accuracy: 31.2231%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [175/225], Training Accuracy: 31.2679%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [176/225], Training Accuracy: 31.3121%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [177/225], Training Accuracy: 31.3206%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [178/225], Training Accuracy: 31.3290%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [179/225], Training Accuracy: 31.2849%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [180/225], Training Accuracy: 31.3194%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [181/225], Training Accuracy: 31.2759%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [182/225], Training Accuracy: 31.2672%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [183/225], Training Accuracy: 31.2756%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [184/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [185/225], Training Accuracy: 31.2162%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [186/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [187/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [188/225], Training Accuracy: 31.2417%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [189/225], Training Accuracy: 31.2748%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [190/225], Training Accuracy: 31.2418%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [191/225], Training Accuracy: 31.2255%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [192/225], Training Accuracy: 31.1605%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [193/225], Training Accuracy: 31.1690%, Training Loss: 0.6857%\n",
      "Epoch [57/100], Step [194/225], Training Accuracy: 31.1856%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [195/225], Training Accuracy: 31.1699%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [196/225], Training Accuracy: 31.1384%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [197/225], Training Accuracy: 31.1786%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [198/225], Training Accuracy: 31.1948%, Training Loss: 0.6856%\n",
      "Epoch [57/100], Step [199/225], Training Accuracy: 31.1793%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [200/225], Training Accuracy: 31.1562%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [201/225], Training Accuracy: 31.1645%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [202/225], Training Accuracy: 31.1494%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [203/225], Training Accuracy: 31.1499%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [204/225], Training Accuracy: 31.1887%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [205/225], Training Accuracy: 31.1890%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [206/225], Training Accuracy: 31.1666%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [207/225], Training Accuracy: 31.1519%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [208/225], Training Accuracy: 31.1824%, Training Loss: 0.6855%\n",
      "Epoch [57/100], Step [209/225], Training Accuracy: 31.2350%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [210/225], Training Accuracy: 31.2723%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [211/225], Training Accuracy: 31.2648%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [212/225], Training Accuracy: 31.3163%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [213/225], Training Accuracy: 31.2867%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [214/225], Training Accuracy: 31.3157%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [215/225], Training Accuracy: 31.2791%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [216/225], Training Accuracy: 31.2283%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [217/225], Training Accuracy: 31.2212%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [218/225], Training Accuracy: 31.1998%, Training Loss: 0.6854%\n",
      "Epoch [57/100], Step [219/225], Training Accuracy: 31.2643%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [220/225], Training Accuracy: 31.2642%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [221/225], Training Accuracy: 31.2429%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [222/225], Training Accuracy: 31.2641%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [223/225], Training Accuracy: 31.2990%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100], Step [224/225], Training Accuracy: 31.3058%, Training Loss: 0.6853%\n",
      "Epoch [57/100], Step [225/225], Training Accuracy: 31.2813%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6926%\n",
      "Epoch [58/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6874%\n",
      "Epoch [58/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6886%\n",
      "Epoch [58/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6862%\n",
      "Epoch [58/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6866%\n",
      "Epoch [58/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [9/225], Training Accuracy: 31.9444%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6865%\n",
      "Epoch [58/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6864%\n",
      "Epoch [58/100], Step [16/225], Training Accuracy: 30.5664%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6867%\n",
      "Epoch [58/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6865%\n",
      "Epoch [58/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6867%\n",
      "Epoch [58/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6865%\n",
      "Epoch [58/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6864%\n",
      "Epoch [58/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6863%\n",
      "Epoch [58/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6864%\n",
      "Epoch [58/100], Step [24/225], Training Accuracy: 31.7708%, Training Loss: 0.6863%\n",
      "Epoch [58/100], Step [25/225], Training Accuracy: 31.7500%, Training Loss: 0.6863%\n",
      "Epoch [58/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6862%\n",
      "Epoch [58/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [28/225], Training Accuracy: 31.3058%, Training Loss: 0.6863%\n",
      "Epoch [58/100], Step [29/225], Training Accuracy: 31.6810%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [30/225], Training Accuracy: 31.7188%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [31/225], Training Accuracy: 31.6028%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [32/225], Training Accuracy: 31.8848%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [33/225], Training Accuracy: 32.0076%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [34/225], Training Accuracy: 31.9393%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [35/225], Training Accuracy: 31.7857%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [38/225], Training Accuracy: 31.6201%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [39/225], Training Accuracy: 31.3702%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6862%\n",
      "Epoch [58/100], Step [41/225], Training Accuracy: 31.5168%, Training Loss: 0.6862%\n",
      "Epoch [58/100], Step [42/225], Training Accuracy: 31.3244%, Training Loss: 0.6864%\n",
      "Epoch [58/100], Step [43/225], Training Accuracy: 31.5044%, Training Loss: 0.6864%\n",
      "Epoch [58/100], Step [44/225], Training Accuracy: 31.4631%, Training Loss: 0.6864%\n",
      "Epoch [58/100], Step [45/225], Training Accuracy: 31.4931%, Training Loss: 0.6862%\n",
      "Epoch [58/100], Step [46/225], Training Accuracy: 31.4538%, Training Loss: 0.6862%\n",
      "Epoch [58/100], Step [47/225], Training Accuracy: 31.4162%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [48/225], Training Accuracy: 31.5430%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [49/225], Training Accuracy: 31.5689%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [50/225], Training Accuracy: 31.5312%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [52/225], Training Accuracy: 31.7007%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [53/225], Training Accuracy: 31.6333%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [54/225], Training Accuracy: 31.4236%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [56/225], Training Accuracy: 31.5848%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [57/225], Training Accuracy: 31.5789%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [58/225], Training Accuracy: 31.4655%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [59/225], Training Accuracy: 31.8326%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [60/225], Training Accuracy: 31.9271%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [61/225], Training Accuracy: 31.8648%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [62/225], Training Accuracy: 31.8548%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [63/225], Training Accuracy: 31.8948%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [64/225], Training Accuracy: 31.9092%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [69/225], Training Accuracy: 31.9973%, Training Loss: 0.6852%\n",
      "Epoch [58/100], Step [70/225], Training Accuracy: 31.9643%, Training Loss: 0.6852%\n",
      "Epoch [58/100], Step [71/225], Training Accuracy: 31.9542%, Training Loss: 0.6852%\n",
      "Epoch [58/100], Step [72/225], Training Accuracy: 31.7274%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [73/225], Training Accuracy: 31.6995%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [74/225], Training Accuracy: 31.7779%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [75/225], Training Accuracy: 31.7083%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [77/225], Training Accuracy: 31.5747%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [78/225], Training Accuracy: 31.6106%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [79/225], Training Accuracy: 31.5467%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [80/225], Training Accuracy: 31.5234%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [81/225], Training Accuracy: 31.4622%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [82/225], Training Accuracy: 31.4596%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [83/225], Training Accuracy: 31.3630%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [84/225], Training Accuracy: 31.4360%, Training Loss: 0.6853%\n",
      "Epoch [58/100], Step [85/225], Training Accuracy: 31.3971%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [86/225], Training Accuracy: 31.4680%, Training Loss: 0.6854%\n",
      "Epoch [58/100], Step [87/225], Training Accuracy: 31.4655%, Training Loss: 0.6855%\n",
      "Epoch [58/100], Step [88/225], Training Accuracy: 31.4453%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [89/225], Training Accuracy: 31.3729%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [90/225], Training Accuracy: 31.2847%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [92/225], Training Accuracy: 31.3519%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [93/225], Training Accuracy: 31.3172%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [94/225], Training Accuracy: 31.4162%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [95/225], Training Accuracy: 31.3158%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [96/225], Training Accuracy: 31.4128%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [97/225], Training Accuracy: 31.4433%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Step [98/225], Training Accuracy: 31.4413%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [99/225], Training Accuracy: 31.5814%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [100/225], Training Accuracy: 31.5625%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [101/225], Training Accuracy: 31.6986%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [102/225], Training Accuracy: 31.6023%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [103/225], Training Accuracy: 31.6596%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [104/225], Training Accuracy: 31.6256%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [105/225], Training Accuracy: 31.5774%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [107/225], Training Accuracy: 31.4836%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [109/225], Training Accuracy: 31.3933%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [110/225], Training Accuracy: 31.3778%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [111/225], Training Accuracy: 31.2782%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [112/225], Training Accuracy: 31.3198%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [113/225], Training Accuracy: 31.2915%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [114/225], Training Accuracy: 31.3048%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [115/225], Training Accuracy: 31.3043%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [116/225], Training Accuracy: 31.2769%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [117/225], Training Accuracy: 31.2233%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [118/225], Training Accuracy: 31.1838%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [119/225], Training Accuracy: 31.1187%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [120/225], Training Accuracy: 31.1589%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [121/225], Training Accuracy: 31.1467%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [122/225], Training Accuracy: 31.1732%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [123/225], Training Accuracy: 31.2119%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [124/225], Training Accuracy: 31.1618%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [125/225], Training Accuracy: 31.1375%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [126/225], Training Accuracy: 31.0640%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [127/225], Training Accuracy: 31.0162%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [128/225], Training Accuracy: 31.0181%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [129/225], Training Accuracy: 31.0683%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [130/225], Training Accuracy: 30.9976%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [131/225], Training Accuracy: 30.9399%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [132/225], Training Accuracy: 30.9186%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [133/225], Training Accuracy: 30.9211%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [134/225], Training Accuracy: 30.9935%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [135/225], Training Accuracy: 30.9838%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [137/225], Training Accuracy: 31.0561%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [138/225], Training Accuracy: 31.0575%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [139/225], Training Accuracy: 31.0027%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [140/225], Training Accuracy: 31.0156%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [141/225], Training Accuracy: 30.9730%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [142/225], Training Accuracy: 31.0299%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [143/225], Training Accuracy: 31.0096%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [144/225], Training Accuracy: 31.0330%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [145/225], Training Accuracy: 31.0991%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [146/225], Training Accuracy: 31.1323%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [147/225], Training Accuracy: 31.1543%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [148/225], Training Accuracy: 31.1339%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [149/225], Training Accuracy: 31.1556%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [150/225], Training Accuracy: 31.1979%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [151/225], Training Accuracy: 31.2397%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [152/225], Training Accuracy: 31.2397%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [153/225], Training Accuracy: 31.2194%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [154/225], Training Accuracy: 31.2601%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [155/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [156/225], Training Accuracy: 31.2600%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [157/225], Training Accuracy: 31.1903%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [158/225], Training Accuracy: 31.1511%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [159/225], Training Accuracy: 31.2402%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [160/225], Training Accuracy: 31.2109%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [161/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [162/225], Training Accuracy: 31.2114%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [163/225], Training Accuracy: 31.2596%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [164/225], Training Accuracy: 31.2405%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [165/225], Training Accuracy: 31.1742%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [166/225], Training Accuracy: 31.1747%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [167/225], Training Accuracy: 31.2032%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [168/225], Training Accuracy: 31.1570%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [169/225], Training Accuracy: 31.0836%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [170/225], Training Accuracy: 31.0294%, Training Loss: 0.6861%\n",
      "Epoch [58/100], Step [171/225], Training Accuracy: 31.0398%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [172/225], Training Accuracy: 31.0501%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [173/225], Training Accuracy: 31.0603%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [174/225], Training Accuracy: 31.0704%, Training Loss: 0.6860%\n",
      "Epoch [58/100], Step [175/225], Training Accuracy: 31.1161%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [176/225], Training Accuracy: 31.1168%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [177/225], Training Accuracy: 31.1264%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [178/225], Training Accuracy: 31.1008%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [179/225], Training Accuracy: 31.0667%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [180/225], Training Accuracy: 31.1024%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [181/225], Training Accuracy: 31.0515%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [182/225], Training Accuracy: 31.0525%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [183/225], Training Accuracy: 31.0707%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [184/225], Training Accuracy: 31.0462%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [185/225], Training Accuracy: 31.0051%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [186/225], Training Accuracy: 31.0400%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [187/225], Training Accuracy: 31.0411%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [188/225], Training Accuracy: 31.0505%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [189/225], Training Accuracy: 31.0516%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [190/225], Training Accuracy: 31.0115%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [191/225], Training Accuracy: 30.9882%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [192/225], Training Accuracy: 30.9326%, Training Loss: 0.6859%\n",
      "Epoch [58/100], Step [193/225], Training Accuracy: 30.9424%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [194/225], Training Accuracy: 30.9520%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [195/225], Training Accuracy: 30.9215%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [196/225], Training Accuracy: 30.8833%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [197/225], Training Accuracy: 30.9169%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Step [198/225], Training Accuracy: 30.9343%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [199/225], Training Accuracy: 30.9045%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [200/225], Training Accuracy: 30.8906%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [201/225], Training Accuracy: 30.9157%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [202/225], Training Accuracy: 30.9174%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [203/225], Training Accuracy: 30.8959%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [204/225], Training Accuracy: 30.9513%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [205/225], Training Accuracy: 30.9375%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [206/225], Training Accuracy: 30.9390%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [207/225], Training Accuracy: 30.9254%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [208/225], Training Accuracy: 30.9645%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [209/225], Training Accuracy: 31.0108%, Training Loss: 0.6858%\n",
      "Epoch [58/100], Step [210/225], Training Accuracy: 31.0417%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [211/225], Training Accuracy: 31.0427%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [212/225], Training Accuracy: 31.1026%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [213/225], Training Accuracy: 31.0813%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [214/225], Training Accuracy: 31.1113%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [215/225], Training Accuracy: 31.0828%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [216/225], Training Accuracy: 31.0185%, Training Loss: 0.6857%\n",
      "Epoch [58/100], Step [217/225], Training Accuracy: 31.0124%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [218/225], Training Accuracy: 30.9848%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [219/225], Training Accuracy: 31.0502%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [220/225], Training Accuracy: 31.0653%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [221/225], Training Accuracy: 31.0379%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [222/225], Training Accuracy: 31.0529%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [223/225], Training Accuracy: 31.0959%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [224/225], Training Accuracy: 31.1035%, Training Loss: 0.6856%\n",
      "Epoch [58/100], Step [225/225], Training Accuracy: 31.0937%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6891%\n",
      "Epoch [59/100], Step [2/225], Training Accuracy: 35.1562%, Training Loss: 0.6858%\n",
      "Epoch [59/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6884%\n",
      "Epoch [59/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6865%\n",
      "Epoch [59/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6868%\n",
      "Epoch [59/100], Step [6/225], Training Accuracy: 33.0729%, Training Loss: 0.6870%\n",
      "Epoch [59/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6860%\n",
      "Epoch [59/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6862%\n",
      "Epoch [59/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6859%\n",
      "Epoch [59/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6858%\n",
      "Epoch [59/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6859%\n",
      "Epoch [59/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6861%\n",
      "Epoch [59/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6860%\n",
      "Epoch [59/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6861%\n",
      "Epoch [59/100], Step [18/225], Training Accuracy: 30.8160%, Training Loss: 0.6861%\n",
      "Epoch [59/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6863%\n",
      "Epoch [59/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6864%\n",
      "Epoch [59/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6862%\n",
      "Epoch [59/100], Step [22/225], Training Accuracy: 31.7472%, Training Loss: 0.6861%\n",
      "Epoch [59/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6860%\n",
      "Epoch [59/100], Step [24/225], Training Accuracy: 31.7708%, Training Loss: 0.6859%\n",
      "Epoch [59/100], Step [25/225], Training Accuracy: 31.7500%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [27/225], Training Accuracy: 31.8287%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [28/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [29/225], Training Accuracy: 31.8966%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [33/225], Training Accuracy: 32.2443%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [34/225], Training Accuracy: 32.1691%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [35/225], Training Accuracy: 32.0089%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [36/225], Training Accuracy: 31.9878%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [37/225], Training Accuracy: 31.9679%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [38/225], Training Accuracy: 31.7434%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [39/225], Training Accuracy: 31.4904%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [40/225], Training Accuracy: 31.5234%, Training Loss: 0.6852%\n",
      "Epoch [59/100], Step [41/225], Training Accuracy: 31.6311%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [42/225], Training Accuracy: 31.3988%, Training Loss: 0.6853%\n",
      "Epoch [59/100], Step [43/225], Training Accuracy: 31.5770%, Training Loss: 0.6852%\n",
      "Epoch [59/100], Step [44/225], Training Accuracy: 31.5341%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [46/225], Training Accuracy: 31.3859%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [47/225], Training Accuracy: 31.3165%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [48/225], Training Accuracy: 31.4453%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [49/225], Training Accuracy: 31.4732%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [50/225], Training Accuracy: 31.3750%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [51/225], Training Accuracy: 31.4951%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6848%\n",
      "Epoch [59/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6848%\n",
      "Epoch [59/100], Step [54/225], Training Accuracy: 31.3079%, Training Loss: 0.6848%\n",
      "Epoch [59/100], Step [55/225], Training Accuracy: 31.3920%, Training Loss: 0.6848%\n",
      "Epoch [59/100], Step [56/225], Training Accuracy: 31.4453%, Training Loss: 0.6848%\n",
      "Epoch [59/100], Step [57/225], Training Accuracy: 31.4693%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [58/225], Training Accuracy: 31.4116%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [59/225], Training Accuracy: 31.7002%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [60/225], Training Accuracy: 31.7969%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [61/225], Training Accuracy: 31.7367%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [62/225], Training Accuracy: 31.8548%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [63/225], Training Accuracy: 31.8948%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [64/225], Training Accuracy: 31.8604%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [66/225], Training Accuracy: 31.9366%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [67/225], Training Accuracy: 31.9263%, Training Loss: 0.6844%\n",
      "Epoch [59/100], Step [68/225], Training Accuracy: 32.0083%, Training Loss: 0.6844%\n",
      "Epoch [59/100], Step [69/225], Training Accuracy: 32.0199%, Training Loss: 0.6843%\n",
      "Epoch [59/100], Step [70/225], Training Accuracy: 32.0089%, Training Loss: 0.6844%\n",
      "Epoch [59/100], Step [71/225], Training Accuracy: 32.0643%, Training Loss: 0.6844%\n",
      "Epoch [59/100], Step [72/225], Training Accuracy: 31.8576%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [73/225], Training Accuracy: 31.8065%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [74/225], Training Accuracy: 31.9046%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [76/225], Training Accuracy: 31.7640%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [77/225], Training Accuracy: 31.6558%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [78/225], Training Accuracy: 31.6907%, Training Loss: 0.6845%\n",
      "Epoch [59/100], Step [79/225], Training Accuracy: 31.6258%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [80/225], Training Accuracy: 31.5820%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [81/225], Training Accuracy: 31.4815%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [82/225], Training Accuracy: 31.4977%, Training Loss: 0.6846%\n",
      "Epoch [59/100], Step [83/225], Training Accuracy: 31.4194%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [84/225], Training Accuracy: 31.4918%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Step [85/225], Training Accuracy: 31.4890%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [86/225], Training Accuracy: 31.5589%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [87/225], Training Accuracy: 31.5553%, Training Loss: 0.6847%\n",
      "Epoch [59/100], Step [88/225], Training Accuracy: 31.5518%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [89/225], Training Accuracy: 31.4958%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [90/225], Training Accuracy: 31.3889%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [91/225], Training Accuracy: 31.4904%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [92/225], Training Accuracy: 31.4878%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [93/225], Training Accuracy: 31.4516%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [94/225], Training Accuracy: 31.5658%, Training Loss: 0.6849%\n",
      "Epoch [59/100], Step [95/225], Training Accuracy: 31.4309%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [96/225], Training Accuracy: 31.5267%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [97/225], Training Accuracy: 31.5722%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [98/225], Training Accuracy: 31.5689%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [99/225], Training Accuracy: 31.6604%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [100/225], Training Accuracy: 31.6094%, Training Loss: 0.6851%\n",
      "Epoch [59/100], Step [101/225], Training Accuracy: 31.7450%, Training Loss: 0.6850%\n",
      "Epoch [59/100], Step [102/225], Training Accuracy: 31.6176%, Training Loss: 0.6852%\n",
      "Epoch [59/100], Step [103/225], Training Accuracy: 31.6444%, Training Loss: 0.6852%\n",
      "Epoch [59/100], Step [104/225], Training Accuracy: 31.6256%, Training Loss: 0.6853%\n",
      "Epoch [59/100], Step [105/225], Training Accuracy: 31.5476%, Training Loss: 0.6853%\n",
      "Epoch [59/100], Step [106/225], Training Accuracy: 31.5448%, Training Loss: 0.6853%\n",
      "Epoch [59/100], Step [107/225], Training Accuracy: 31.4398%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [108/225], Training Accuracy: 31.5249%, Training Loss: 0.6853%\n",
      "Epoch [59/100], Step [109/225], Training Accuracy: 31.4364%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [110/225], Training Accuracy: 31.4347%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [111/225], Training Accuracy: 31.3204%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [112/225], Training Accuracy: 31.4035%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [113/225], Training Accuracy: 31.4159%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [114/225], Training Accuracy: 31.4830%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [115/225], Training Accuracy: 31.4130%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [116/225], Training Accuracy: 31.3847%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [117/225], Training Accuracy: 31.3168%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [118/225], Training Accuracy: 31.2632%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [119/225], Training Accuracy: 31.1975%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [120/225], Training Accuracy: 31.1979%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [121/225], Training Accuracy: 31.2242%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [122/225], Training Accuracy: 31.2372%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [123/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [124/225], Training Accuracy: 31.2248%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [125/225], Training Accuracy: 31.2000%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [126/225], Training Accuracy: 31.1384%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [127/225], Training Accuracy: 31.1024%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [128/225], Training Accuracy: 31.1157%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [129/225], Training Accuracy: 31.1410%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [130/225], Training Accuracy: 31.0938%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [131/225], Training Accuracy: 31.0234%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [132/225], Training Accuracy: 30.9896%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [133/225], Training Accuracy: 31.0150%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [134/225], Training Accuracy: 31.0634%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [135/225], Training Accuracy: 31.0648%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [136/225], Training Accuracy: 31.0547%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [137/225], Training Accuracy: 31.0903%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [138/225], Training Accuracy: 31.1028%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [139/225], Training Accuracy: 31.0814%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [140/225], Training Accuracy: 31.0826%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [141/225], Training Accuracy: 31.0173%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [142/225], Training Accuracy: 31.0739%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [143/225], Training Accuracy: 31.0533%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [144/225], Training Accuracy: 31.0547%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [145/225], Training Accuracy: 31.0991%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [146/225], Training Accuracy: 31.1002%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [147/225], Training Accuracy: 31.1012%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [148/225], Training Accuracy: 31.0600%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [149/225], Training Accuracy: 31.0927%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [150/225], Training Accuracy: 31.1146%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [151/225], Training Accuracy: 31.1672%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [152/225], Training Accuracy: 31.1678%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [153/225], Training Accuracy: 31.1275%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [154/225], Training Accuracy: 31.1485%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [155/225], Training Accuracy: 31.1391%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [156/225], Training Accuracy: 31.1799%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [157/225], Training Accuracy: 31.1007%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [158/225], Training Accuracy: 31.0720%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [159/225], Training Accuracy: 31.1419%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [160/225], Training Accuracy: 31.1133%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [161/225], Training Accuracy: 31.1432%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [162/225], Training Accuracy: 31.1150%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [163/225], Training Accuracy: 31.1541%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [164/225], Training Accuracy: 31.1452%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [165/225], Training Accuracy: 31.0985%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [166/225], Training Accuracy: 31.0712%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [167/225], Training Accuracy: 31.1190%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [168/225], Training Accuracy: 31.0547%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [169/225], Training Accuracy: 30.9911%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [170/225], Training Accuracy: 30.9467%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [171/225], Training Accuracy: 30.9667%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [172/225], Training Accuracy: 30.9775%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [173/225], Training Accuracy: 30.9610%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [174/225], Training Accuracy: 30.9896%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [175/225], Training Accuracy: 31.0179%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [176/225], Training Accuracy: 31.0281%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [177/225], Training Accuracy: 31.0293%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [178/225], Training Accuracy: 31.0218%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [179/225], Training Accuracy: 30.9969%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [180/225], Training Accuracy: 31.0243%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [181/225], Training Accuracy: 30.9910%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [182/225], Training Accuracy: 30.9753%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Step [183/225], Training Accuracy: 30.9939%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [184/225], Training Accuracy: 30.9952%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [185/225], Training Accuracy: 30.9628%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [186/225], Training Accuracy: 31.0064%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [187/225], Training Accuracy: 30.9910%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [188/225], Training Accuracy: 31.0090%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [189/225], Training Accuracy: 31.0433%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [190/225], Training Accuracy: 31.0115%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [191/225], Training Accuracy: 30.9882%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [192/225], Training Accuracy: 30.9163%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [193/225], Training Accuracy: 30.9181%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [194/225], Training Accuracy: 30.9278%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [195/225], Training Accuracy: 30.9135%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [196/225], Training Accuracy: 30.8913%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [197/225], Training Accuracy: 30.9169%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [198/225], Training Accuracy: 30.9265%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [199/225], Training Accuracy: 30.8967%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [200/225], Training Accuracy: 30.8750%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [201/225], Training Accuracy: 30.9002%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [202/225], Training Accuracy: 30.9019%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [203/225], Training Accuracy: 30.8959%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [204/225], Training Accuracy: 30.9743%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [205/225], Training Accuracy: 30.9527%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [206/225], Training Accuracy: 30.9390%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [207/225], Training Accuracy: 30.9179%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [208/225], Training Accuracy: 30.9420%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [209/225], Training Accuracy: 30.9809%, Training Loss: 0.6857%\n",
      "Epoch [59/100], Step [210/225], Training Accuracy: 30.9970%, Training Loss: 0.6856%\n",
      "Epoch [59/100], Step [211/225], Training Accuracy: 30.9834%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [212/225], Training Accuracy: 31.0215%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [213/225], Training Accuracy: 30.9859%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [214/225], Training Accuracy: 31.0091%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [215/225], Training Accuracy: 30.9738%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [216/225], Training Accuracy: 30.9028%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [217/225], Training Accuracy: 30.9188%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [218/225], Training Accuracy: 30.8988%, Training Loss: 0.6855%\n",
      "Epoch [59/100], Step [219/225], Training Accuracy: 30.9575%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [220/225], Training Accuracy: 30.9872%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [221/225], Training Accuracy: 30.9672%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [222/225], Training Accuracy: 30.9896%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [223/225], Training Accuracy: 31.0328%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [224/225], Training Accuracy: 31.0338%, Training Loss: 0.6854%\n",
      "Epoch [59/100], Step [225/225], Training Accuracy: 31.0311%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [1/225], Training Accuracy: 37.5000%, Training Loss: 0.6875%\n",
      "Epoch [60/100], Step [2/225], Training Accuracy: 35.1562%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [3/225], Training Accuracy: 33.8542%, Training Loss: 0.6876%\n",
      "Epoch [60/100], Step [4/225], Training Accuracy: 33.9844%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [5/225], Training Accuracy: 34.3750%, Training Loss: 0.6864%\n",
      "Epoch [60/100], Step [6/225], Training Accuracy: 33.5938%, Training Loss: 0.6872%\n",
      "Epoch [60/100], Step [7/225], Training Accuracy: 33.2589%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [8/225], Training Accuracy: 33.0078%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [9/225], Training Accuracy: 32.4653%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [11/225], Training Accuracy: 31.6761%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [12/225], Training Accuracy: 31.1198%, Training Loss: 0.6859%\n",
      "Epoch [60/100], Step [13/225], Training Accuracy: 30.7692%, Training Loss: 0.6861%\n",
      "Epoch [60/100], Step [14/225], Training Accuracy: 30.9152%, Training Loss: 0.6864%\n",
      "Epoch [60/100], Step [15/225], Training Accuracy: 31.4583%, Training Loss: 0.6862%\n",
      "Epoch [60/100], Step [16/225], Training Accuracy: 31.5430%, Training Loss: 0.6861%\n",
      "Epoch [60/100], Step [17/225], Training Accuracy: 31.4338%, Training Loss: 0.6863%\n",
      "Epoch [60/100], Step [18/225], Training Accuracy: 31.3368%, Training Loss: 0.6865%\n",
      "Epoch [60/100], Step [19/225], Training Accuracy: 31.5789%, Training Loss: 0.6865%\n",
      "Epoch [60/100], Step [20/225], Training Accuracy: 32.1094%, Training Loss: 0.6863%\n",
      "Epoch [60/100], Step [21/225], Training Accuracy: 32.0685%, Training Loss: 0.6860%\n",
      "Epoch [60/100], Step [22/225], Training Accuracy: 32.3153%, Training Loss: 0.6859%\n",
      "Epoch [60/100], Step [23/225], Training Accuracy: 32.2011%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [24/225], Training Accuracy: 32.0964%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [25/225], Training Accuracy: 32.2500%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [26/225], Training Accuracy: 32.3918%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [27/225], Training Accuracy: 32.1181%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [28/225], Training Accuracy: 31.8080%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [31/225], Training Accuracy: 32.0060%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [32/225], Training Accuracy: 32.2754%, Training Loss: 0.6848%\n",
      "Epoch [60/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6846%\n",
      "Epoch [60/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6846%\n",
      "Epoch [60/100], Step [35/225], Training Accuracy: 32.1429%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [36/225], Training Accuracy: 31.9444%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [37/225], Training Accuracy: 31.8834%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [41/225], Training Accuracy: 31.5549%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [42/225], Training Accuracy: 31.3616%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [43/225], Training Accuracy: 31.5407%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [44/225], Training Accuracy: 31.5696%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [45/225], Training Accuracy: 31.5278%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [46/225], Training Accuracy: 31.3859%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [47/225], Training Accuracy: 31.3497%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [48/225], Training Accuracy: 31.4453%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [49/225], Training Accuracy: 31.5051%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [50/225], Training Accuracy: 31.5312%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [51/225], Training Accuracy: 31.6176%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [52/225], Training Accuracy: 31.6106%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [53/225], Training Accuracy: 31.5448%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [54/225], Training Accuracy: 31.4236%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [55/225], Training Accuracy: 31.5625%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [56/225], Training Accuracy: 31.6127%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Step [57/225], Training Accuracy: 31.6612%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [58/225], Training Accuracy: 31.5463%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [59/225], Training Accuracy: 31.9121%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [60/225], Training Accuracy: 32.0312%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [61/225], Training Accuracy: 31.9928%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [62/225], Training Accuracy: 31.9808%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [63/225], Training Accuracy: 32.0933%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [64/225], Training Accuracy: 32.0801%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [65/225], Training Accuracy: 31.9471%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [66/225], Training Accuracy: 32.0549%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [67/225], Training Accuracy: 31.9963%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [68/225], Training Accuracy: 32.0083%, Training Loss: 0.6848%\n",
      "Epoch [60/100], Step [69/225], Training Accuracy: 31.9746%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [70/225], Training Accuracy: 31.9420%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [71/225], Training Accuracy: 32.0202%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [72/225], Training Accuracy: 31.7925%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [73/225], Training Accuracy: 31.7423%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [74/225], Training Accuracy: 31.8412%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [75/225], Training Accuracy: 31.8125%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [76/225], Training Accuracy: 31.8257%, Training Loss: 0.6847%\n",
      "Epoch [60/100], Step [77/225], Training Accuracy: 31.7573%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [78/225], Training Accuracy: 31.7909%, Training Loss: 0.6848%\n",
      "Epoch [60/100], Step [79/225], Training Accuracy: 31.7049%, Training Loss: 0.6848%\n",
      "Epoch [60/100], Step [80/225], Training Accuracy: 31.6406%, Training Loss: 0.6848%\n",
      "Epoch [60/100], Step [81/225], Training Accuracy: 31.5972%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [82/225], Training Accuracy: 31.6120%, Training Loss: 0.6848%\n",
      "Epoch [60/100], Step [83/225], Training Accuracy: 31.5512%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [84/225], Training Accuracy: 31.6220%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [85/225], Training Accuracy: 31.6176%, Training Loss: 0.6849%\n",
      "Epoch [60/100], Step [86/225], Training Accuracy: 31.6315%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [87/225], Training Accuracy: 31.6451%, Training Loss: 0.6850%\n",
      "Epoch [60/100], Step [88/225], Training Accuracy: 31.6406%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [89/225], Training Accuracy: 31.5309%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [90/225], Training Accuracy: 31.4410%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [91/225], Training Accuracy: 31.4904%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [92/225], Training Accuracy: 31.4878%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [93/225], Training Accuracy: 31.4516%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [94/225], Training Accuracy: 31.5658%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [95/225], Training Accuracy: 31.4474%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [97/225], Training Accuracy: 31.5883%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [98/225], Training Accuracy: 31.6167%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [99/225], Training Accuracy: 31.7235%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [100/225], Training Accuracy: 31.7188%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [101/225], Training Accuracy: 31.8533%, Training Loss: 0.6851%\n",
      "Epoch [60/100], Step [102/225], Training Accuracy: 31.7402%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [103/225], Training Accuracy: 31.7506%, Training Loss: 0.6852%\n",
      "Epoch [60/100], Step [104/225], Training Accuracy: 31.7308%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [105/225], Training Accuracy: 31.6667%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [106/225], Training Accuracy: 31.6922%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [107/225], Training Accuracy: 31.6297%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [108/225], Training Accuracy: 31.6985%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [110/225], Training Accuracy: 31.6193%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [111/225], Training Accuracy: 31.4893%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [112/225], Training Accuracy: 31.5709%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [113/225], Training Accuracy: 31.5265%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [114/225], Training Accuracy: 31.5927%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [115/225], Training Accuracy: 31.5353%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [116/225], Training Accuracy: 31.5329%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [117/225], Training Accuracy: 31.4770%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [118/225], Training Accuracy: 31.4486%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [119/225], Training Accuracy: 31.3682%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [120/225], Training Accuracy: 31.4193%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [121/225], Training Accuracy: 31.3920%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [122/225], Training Accuracy: 31.4037%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [123/225], Training Accuracy: 31.4405%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [124/225], Training Accuracy: 31.4264%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [126/225], Training Accuracy: 31.3616%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [127/225], Training Accuracy: 31.2992%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [128/225], Training Accuracy: 31.2866%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [129/225], Training Accuracy: 31.3348%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [130/225], Training Accuracy: 31.2861%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [131/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [132/225], Training Accuracy: 31.2145%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [133/225], Training Accuracy: 31.2617%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [134/225], Training Accuracy: 31.2966%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [135/225], Training Accuracy: 31.3079%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [136/225], Training Accuracy: 31.3304%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [137/225], Training Accuracy: 31.3526%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [138/225], Training Accuracy: 31.3745%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [139/225], Training Accuracy: 31.3399%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [140/225], Training Accuracy: 31.3058%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [141/225], Training Accuracy: 31.2168%, Training Loss: 0.6858%\n",
      "Epoch [60/100], Step [142/225], Training Accuracy: 31.2830%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [143/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [144/225], Training Accuracy: 31.2717%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [145/225], Training Accuracy: 31.3362%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [146/225], Training Accuracy: 31.3570%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [147/225], Training Accuracy: 31.3776%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [148/225], Training Accuracy: 31.3556%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [149/225], Training Accuracy: 31.3968%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [150/225], Training Accuracy: 31.4062%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [151/225], Training Accuracy: 31.4156%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [152/225], Training Accuracy: 31.4248%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [153/225], Training Accuracy: 31.3725%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [154/225], Training Accuracy: 31.3616%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [155/225], Training Accuracy: 31.3407%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Step [156/225], Training Accuracy: 31.3602%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [157/225], Training Accuracy: 31.3097%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [158/225], Training Accuracy: 31.2698%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [159/225], Training Accuracy: 31.2991%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [160/225], Training Accuracy: 31.2695%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [161/225], Training Accuracy: 31.3179%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [162/225], Training Accuracy: 31.2886%, Training Loss: 0.6857%\n",
      "Epoch [60/100], Step [163/225], Training Accuracy: 31.3459%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [164/225], Training Accuracy: 31.3357%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [165/225], Training Accuracy: 31.2784%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [166/225], Training Accuracy: 31.2782%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [167/225], Training Accuracy: 31.3155%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [168/225], Training Accuracy: 31.2686%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [169/225], Training Accuracy: 31.1945%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [170/225], Training Accuracy: 31.1489%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [171/225], Training Accuracy: 31.1678%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [172/225], Training Accuracy: 31.1864%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [173/225], Training Accuracy: 31.1777%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [175/225], Training Accuracy: 31.2411%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [176/225], Training Accuracy: 31.2678%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [177/225], Training Accuracy: 31.2677%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [178/225], Training Accuracy: 31.2851%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [179/225], Training Accuracy: 31.2675%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [180/225], Training Accuracy: 31.3108%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [181/225], Training Accuracy: 31.2586%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [182/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [183/225], Training Accuracy: 31.2756%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [184/225], Training Accuracy: 31.2330%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [185/225], Training Accuracy: 31.1993%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [186/225], Training Accuracy: 31.2248%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [187/225], Training Accuracy: 31.2333%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [188/225], Training Accuracy: 31.2251%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [189/225], Training Accuracy: 31.2417%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [190/225], Training Accuracy: 31.2089%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [191/225], Training Accuracy: 31.1846%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [192/225], Training Accuracy: 31.1279%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [193/225], Training Accuracy: 31.1367%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [195/225], Training Accuracy: 31.1218%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [196/225], Training Accuracy: 31.0826%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [197/225], Training Accuracy: 31.1310%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [198/225], Training Accuracy: 31.1553%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [199/225], Training Accuracy: 31.1401%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [200/225], Training Accuracy: 31.1172%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [201/225], Training Accuracy: 31.1256%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [202/225], Training Accuracy: 31.1340%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [203/225], Training Accuracy: 31.1192%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [204/225], Training Accuracy: 31.1657%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [205/225], Training Accuracy: 31.1585%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [206/225], Training Accuracy: 31.1590%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [207/225], Training Accuracy: 31.1594%, Training Loss: 0.6856%\n",
      "Epoch [60/100], Step [208/225], Training Accuracy: 31.1974%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [209/225], Training Accuracy: 31.2425%, Training Loss: 0.6855%\n",
      "Epoch [60/100], Step [210/225], Training Accuracy: 31.2798%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [211/225], Training Accuracy: 31.2648%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [212/225], Training Accuracy: 31.3090%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [213/225], Training Accuracy: 31.2867%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [214/225], Training Accuracy: 31.3157%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [215/225], Training Accuracy: 31.2645%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [216/225], Training Accuracy: 31.2066%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [217/225], Training Accuracy: 31.2068%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [218/225], Training Accuracy: 31.1712%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [219/225], Training Accuracy: 31.2357%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [220/225], Training Accuracy: 31.2429%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [221/225], Training Accuracy: 31.2217%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [222/225], Training Accuracy: 31.2359%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [223/225], Training Accuracy: 31.2780%, Training Loss: 0.6854%\n",
      "Epoch [60/100], Step [224/225], Training Accuracy: 31.2640%, Training Loss: 0.6853%\n",
      "Epoch [60/100], Step [225/225], Training Accuracy: 31.2465%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6890%\n",
      "Epoch [61/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6862%\n",
      "Epoch [61/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6891%\n",
      "Epoch [61/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6865%\n",
      "Epoch [61/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6864%\n",
      "Epoch [61/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6864%\n",
      "Epoch [61/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6849%\n",
      "Epoch [61/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [10/225], Training Accuracy: 30.4688%, Training Loss: 0.6849%\n",
      "Epoch [61/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [12/225], Training Accuracy: 29.6875%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [13/225], Training Accuracy: 29.4471%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [14/225], Training Accuracy: 29.7991%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [19/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [20/225], Training Accuracy: 31.7969%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [21/225], Training Accuracy: 31.6964%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [22/225], Training Accuracy: 31.8182%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [24/225], Training Accuracy: 31.9661%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [26/225], Training Accuracy: 32.4519%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [27/225], Training Accuracy: 32.1759%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [28/225], Training Accuracy: 31.9754%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [29/225], Training Accuracy: 32.3815%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [30/225], Training Accuracy: 32.4479%, Training Loss: 0.6843%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Step [31/225], Training Accuracy: 32.2581%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [32/225], Training Accuracy: 32.5195%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [33/225], Training Accuracy: 32.6231%, Training Loss: 0.6838%\n",
      "Epoch [61/100], Step [34/225], Training Accuracy: 32.4449%, Training Loss: 0.6838%\n",
      "Epoch [61/100], Step [35/225], Training Accuracy: 32.3214%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [36/225], Training Accuracy: 32.2049%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [37/225], Training Accuracy: 32.1791%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [40/225], Training Accuracy: 31.7188%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [41/225], Training Accuracy: 31.7835%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [42/225], Training Accuracy: 31.6220%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [43/225], Training Accuracy: 31.7951%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [44/225], Training Accuracy: 31.7116%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [45/225], Training Accuracy: 31.7014%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [46/225], Training Accuracy: 31.6236%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [47/225], Training Accuracy: 31.5492%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [49/225], Training Accuracy: 31.7921%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [50/225], Training Accuracy: 31.7500%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [51/225], Training Accuracy: 31.9240%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [52/225], Training Accuracy: 31.9712%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [53/225], Training Accuracy: 31.8986%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [54/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [55/225], Training Accuracy: 31.8182%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [57/225], Training Accuracy: 31.9353%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [58/225], Training Accuracy: 31.8427%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [59/225], Training Accuracy: 32.2034%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [60/225], Training Accuracy: 32.2396%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [61/225], Training Accuracy: 32.1721%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [62/225], Training Accuracy: 32.1825%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [64/225], Training Accuracy: 32.2510%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [65/225], Training Accuracy: 32.0913%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [66/225], Training Accuracy: 32.1733%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [67/225], Training Accuracy: 32.1828%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [68/225], Training Accuracy: 32.2610%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [69/225], Training Accuracy: 32.2237%, Training Loss: 0.6839%\n",
      "Epoch [61/100], Step [70/225], Training Accuracy: 32.1429%, Training Loss: 0.6839%\n",
      "Epoch [61/100], Step [71/225], Training Accuracy: 32.1743%, Training Loss: 0.6840%\n",
      "Epoch [61/100], Step [72/225], Training Accuracy: 31.9878%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [73/225], Training Accuracy: 31.8921%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [74/225], Training Accuracy: 31.9679%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [75/225], Training Accuracy: 31.9167%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [76/225], Training Accuracy: 31.8668%, Training Loss: 0.6841%\n",
      "Epoch [61/100], Step [77/225], Training Accuracy: 31.7979%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [78/225], Training Accuracy: 31.8510%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [79/225], Training Accuracy: 31.7840%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [80/225], Training Accuracy: 31.7578%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [81/225], Training Accuracy: 31.6551%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [82/225], Training Accuracy: 31.6502%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [83/225], Training Accuracy: 31.5889%, Training Loss: 0.6843%\n",
      "Epoch [61/100], Step [84/225], Training Accuracy: 31.6406%, Training Loss: 0.6842%\n",
      "Epoch [61/100], Step [85/225], Training Accuracy: 31.5625%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [86/225], Training Accuracy: 31.5770%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [87/225], Training Accuracy: 31.6092%, Training Loss: 0.6844%\n",
      "Epoch [61/100], Step [88/225], Training Accuracy: 31.6051%, Training Loss: 0.6845%\n",
      "Epoch [61/100], Step [89/225], Training Accuracy: 31.4958%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [90/225], Training Accuracy: 31.4062%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [91/225], Training Accuracy: 31.4389%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [92/225], Training Accuracy: 31.4198%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [93/225], Training Accuracy: 31.4516%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [94/225], Training Accuracy: 31.5658%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [95/225], Training Accuracy: 31.4638%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [97/225], Training Accuracy: 31.6044%, Training Loss: 0.6848%\n",
      "Epoch [61/100], Step [98/225], Training Accuracy: 31.5848%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [99/225], Training Accuracy: 31.6919%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [100/225], Training Accuracy: 31.6562%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [101/225], Training Accuracy: 31.7760%, Training Loss: 0.6846%\n",
      "Epoch [61/100], Step [102/225], Training Accuracy: 31.6636%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [103/225], Training Accuracy: 31.7354%, Training Loss: 0.6847%\n",
      "Epoch [61/100], Step [104/225], Training Accuracy: 31.7157%, Training Loss: 0.6848%\n",
      "Epoch [61/100], Step [105/225], Training Accuracy: 31.6667%, Training Loss: 0.6849%\n",
      "Epoch [61/100], Step [106/225], Training Accuracy: 31.6627%, Training Loss: 0.6849%\n",
      "Epoch [61/100], Step [107/225], Training Accuracy: 31.5713%, Training Loss: 0.6849%\n",
      "Epoch [61/100], Step [108/225], Training Accuracy: 31.6696%, Training Loss: 0.6849%\n",
      "Epoch [61/100], Step [109/225], Training Accuracy: 31.5080%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [110/225], Training Accuracy: 31.5057%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [111/225], Training Accuracy: 31.4330%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [112/225], Training Accuracy: 31.5011%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [113/225], Training Accuracy: 31.4712%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [114/225], Training Accuracy: 31.5515%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [115/225], Training Accuracy: 31.5217%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [116/225], Training Accuracy: 31.5194%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [117/225], Training Accuracy: 31.4637%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [118/225], Training Accuracy: 31.4354%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [119/225], Training Accuracy: 31.3944%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [120/225], Training Accuracy: 31.4062%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [121/225], Training Accuracy: 31.3791%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [122/225], Training Accuracy: 31.4037%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [123/225], Training Accuracy: 31.4405%, Training Loss: 0.6850%\n",
      "Epoch [61/100], Step [124/225], Training Accuracy: 31.4264%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [126/225], Training Accuracy: 31.3492%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [127/225], Training Accuracy: 31.2992%, Training Loss: 0.6851%\n",
      "Epoch [61/100], Step [128/225], Training Accuracy: 31.2866%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [129/225], Training Accuracy: 31.3227%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [131/225], Training Accuracy: 31.2142%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [132/225], Training Accuracy: 31.1908%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [133/225], Training Accuracy: 31.2148%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [134/225], Training Accuracy: 31.2617%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [135/225], Training Accuracy: 31.2616%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [136/225], Training Accuracy: 31.2845%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [137/225], Training Accuracy: 31.3412%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [138/225], Training Accuracy: 31.3406%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [139/225], Training Accuracy: 31.3399%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [140/225], Training Accuracy: 31.2835%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [141/225], Training Accuracy: 31.2168%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Step [142/225], Training Accuracy: 31.2720%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [143/225], Training Accuracy: 31.2609%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [144/225], Training Accuracy: 31.2934%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [145/225], Training Accuracy: 31.3362%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [146/225], Training Accuracy: 31.3463%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [147/225], Training Accuracy: 31.3457%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [148/225], Training Accuracy: 31.3556%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [149/225], Training Accuracy: 31.3654%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [150/225], Training Accuracy: 31.3646%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [151/225], Training Accuracy: 31.4052%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [152/225], Training Accuracy: 31.3939%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [153/225], Training Accuracy: 31.3623%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [154/225], Training Accuracy: 31.3718%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [155/225], Training Accuracy: 31.3508%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [156/225], Training Accuracy: 31.3401%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [157/225], Training Accuracy: 31.2699%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [158/225], Training Accuracy: 31.2401%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [159/225], Training Accuracy: 31.2991%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [160/225], Training Accuracy: 31.2891%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [161/225], Training Accuracy: 31.3373%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [162/225], Training Accuracy: 31.3175%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [163/225], Training Accuracy: 31.3746%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [164/225], Training Accuracy: 31.3643%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [165/225], Training Accuracy: 31.2973%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [166/225], Training Accuracy: 31.2877%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [167/225], Training Accuracy: 31.3342%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [168/225], Training Accuracy: 31.2872%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [169/225], Training Accuracy: 31.2130%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [170/225], Training Accuracy: 31.1765%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [171/225], Training Accuracy: 31.1860%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [172/225], Training Accuracy: 31.1955%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [175/225], Training Accuracy: 31.2411%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [176/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [177/225], Training Accuracy: 31.2412%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [178/225], Training Accuracy: 31.2149%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [179/225], Training Accuracy: 31.1714%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [181/225], Training Accuracy: 31.1637%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [182/225], Training Accuracy: 31.1727%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [183/225], Training Accuracy: 31.1732%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [185/225], Training Accuracy: 31.1402%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [186/225], Training Accuracy: 31.1660%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [187/225], Training Accuracy: 31.1664%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [188/225], Training Accuracy: 31.1503%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [189/225], Training Accuracy: 31.1591%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [190/225], Training Accuracy: 31.1020%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [191/225], Training Accuracy: 31.0782%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [192/225], Training Accuracy: 31.0221%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [193/225], Training Accuracy: 31.0314%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [194/225], Training Accuracy: 31.0486%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [195/225], Training Accuracy: 31.0417%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [196/225], Training Accuracy: 31.0188%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [197/225], Training Accuracy: 31.0596%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [198/225], Training Accuracy: 31.0843%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [199/225], Training Accuracy: 31.0694%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [200/225], Training Accuracy: 31.0547%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [201/225], Training Accuracy: 31.0712%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [202/225], Training Accuracy: 31.0644%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [203/225], Training Accuracy: 31.0499%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [204/225], Training Accuracy: 31.1121%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [205/225], Training Accuracy: 31.1357%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [206/225], Training Accuracy: 31.1362%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [207/225], Training Accuracy: 31.1141%, Training Loss: 0.6854%\n",
      "Epoch [61/100], Step [208/225], Training Accuracy: 31.1373%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [209/225], Training Accuracy: 31.1678%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [210/225], Training Accuracy: 31.1979%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [211/225], Training Accuracy: 31.1908%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [213/225], Training Accuracy: 31.2060%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [214/225], Training Accuracy: 31.2281%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [215/225], Training Accuracy: 31.2064%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [216/225], Training Accuracy: 31.1415%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [217/225], Training Accuracy: 31.1420%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [218/225], Training Accuracy: 31.1067%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [219/225], Training Accuracy: 31.1787%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [220/225], Training Accuracy: 31.1932%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [221/225], Training Accuracy: 31.1934%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [222/225], Training Accuracy: 31.2078%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [223/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [61/100], Step [224/225], Training Accuracy: 31.2430%, Training Loss: 0.6852%\n",
      "Epoch [61/100], Step [225/225], Training Accuracy: 31.2187%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6914%\n",
      "Epoch [62/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6881%\n",
      "Epoch [62/100], Step [4/225], Training Accuracy: 33.2031%, Training Loss: 0.6865%\n",
      "Epoch [62/100], Step [5/225], Training Accuracy: 34.0625%, Training Loss: 0.6871%\n",
      "Epoch [62/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6867%\n",
      "Epoch [62/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6861%\n",
      "Epoch [62/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [9/225], Training Accuracy: 31.9444%, Training Loss: 0.6863%\n",
      "Epoch [62/100], Step [10/225], Training Accuracy: 31.7188%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6861%\n",
      "Epoch [62/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6864%\n",
      "Epoch [62/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6868%\n",
      "Epoch [62/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6874%\n",
      "Epoch [62/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6875%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6870%\n",
      "Epoch [62/100], Step [17/225], Training Accuracy: 30.2390%, Training Loss: 0.6873%\n",
      "Epoch [62/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6873%\n",
      "Epoch [62/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6874%\n",
      "Epoch [62/100], Step [20/225], Training Accuracy: 31.4062%, Training Loss: 0.6872%\n",
      "Epoch [62/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6869%\n",
      "Epoch [62/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6868%\n",
      "Epoch [62/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6867%\n",
      "Epoch [62/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6866%\n",
      "Epoch [62/100], Step [25/225], Training Accuracy: 31.6875%, Training Loss: 0.6865%\n",
      "Epoch [62/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6862%\n",
      "Epoch [62/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6861%\n",
      "Epoch [62/100], Step [28/225], Training Accuracy: 31.5290%, Training Loss: 0.6862%\n",
      "Epoch [62/100], Step [29/225], Training Accuracy: 31.7888%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [30/225], Training Accuracy: 31.7708%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [31/225], Training Accuracy: 31.7036%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [32/225], Training Accuracy: 32.0312%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [33/225], Training Accuracy: 32.1496%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [34/225], Training Accuracy: 31.8934%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [35/225], Training Accuracy: 31.7857%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [36/225], Training Accuracy: 31.6406%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [38/225], Training Accuracy: 31.5378%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [39/225], Training Accuracy: 31.2901%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [40/225], Training Accuracy: 31.3672%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [41/225], Training Accuracy: 31.4787%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [42/225], Training Accuracy: 31.2872%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [43/225], Training Accuracy: 31.4680%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [44/225], Training Accuracy: 31.4276%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [45/225], Training Accuracy: 31.4931%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [46/225], Training Accuracy: 31.3519%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [47/225], Training Accuracy: 31.3165%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [48/225], Training Accuracy: 31.4453%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [49/225], Training Accuracy: 31.5051%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [50/225], Training Accuracy: 31.5000%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [52/225], Training Accuracy: 31.7007%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [53/225], Training Accuracy: 31.6333%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [54/225], Training Accuracy: 31.5104%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [55/225], Training Accuracy: 31.5909%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [56/225], Training Accuracy: 31.6964%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [57/225], Training Accuracy: 31.7160%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [58/225], Training Accuracy: 31.6541%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [59/225], Training Accuracy: 31.9915%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [60/225], Training Accuracy: 32.1094%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [61/225], Training Accuracy: 32.0441%, Training Loss: 0.6849%\n",
      "Epoch [62/100], Step [62/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [63/225], Training Accuracy: 32.0933%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [64/225], Training Accuracy: 32.0801%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [65/225], Training Accuracy: 31.9952%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [66/225], Training Accuracy: 32.1023%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [67/225], Training Accuracy: 32.1129%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [68/225], Training Accuracy: 32.2151%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [69/225], Training Accuracy: 32.2011%, Training Loss: 0.6849%\n",
      "Epoch [62/100], Step [70/225], Training Accuracy: 32.2098%, Training Loss: 0.6849%\n",
      "Epoch [62/100], Step [71/225], Training Accuracy: 32.2623%, Training Loss: 0.6849%\n",
      "Epoch [62/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [73/225], Training Accuracy: 31.9991%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [74/225], Training Accuracy: 32.0946%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [75/225], Training Accuracy: 32.0208%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [76/225], Training Accuracy: 32.0107%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [77/225], Training Accuracy: 31.9196%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [78/225], Training Accuracy: 31.9712%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [79/225], Training Accuracy: 31.9027%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [80/225], Training Accuracy: 31.9141%, Training Loss: 0.6850%\n",
      "Epoch [62/100], Step [81/225], Training Accuracy: 31.7901%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [82/225], Training Accuracy: 31.8026%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [83/225], Training Accuracy: 31.7395%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [84/225], Training Accuracy: 31.8266%, Training Loss: 0.6851%\n",
      "Epoch [62/100], Step [85/225], Training Accuracy: 31.8382%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [86/225], Training Accuracy: 31.8677%, Training Loss: 0.6852%\n",
      "Epoch [62/100], Step [87/225], Training Accuracy: 31.8427%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [88/225], Training Accuracy: 31.8182%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [89/225], Training Accuracy: 31.7591%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [90/225], Training Accuracy: 31.6667%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [91/225], Training Accuracy: 31.6621%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [92/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [93/225], Training Accuracy: 31.6364%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [94/225], Training Accuracy: 31.6988%, Training Loss: 0.6853%\n",
      "Epoch [62/100], Step [95/225], Training Accuracy: 31.5789%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [96/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [97/225], Training Accuracy: 31.6205%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [98/225], Training Accuracy: 31.6327%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [99/225], Training Accuracy: 31.7551%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [100/225], Training Accuracy: 31.7500%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [101/225], Training Accuracy: 31.8843%, Training Loss: 0.6854%\n",
      "Epoch [62/100], Step [102/225], Training Accuracy: 31.7708%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [103/225], Training Accuracy: 31.8265%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [104/225], Training Accuracy: 31.7909%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [105/225], Training Accuracy: 31.7560%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [106/225], Training Accuracy: 31.7364%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [107/225], Training Accuracy: 31.6735%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [108/225], Training Accuracy: 31.7419%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [110/225], Training Accuracy: 31.6193%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [111/225], Training Accuracy: 31.5315%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [112/225], Training Accuracy: 31.6127%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [113/225], Training Accuracy: 31.5957%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [114/225], Training Accuracy: 31.6201%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [115/225], Training Accuracy: 31.6033%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [116/225], Training Accuracy: 31.6002%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [117/225], Training Accuracy: 31.5572%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [118/225], Training Accuracy: 31.5413%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [119/225], Training Accuracy: 31.4863%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [120/225], Training Accuracy: 31.5234%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [121/225], Training Accuracy: 31.4954%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [122/225], Training Accuracy: 31.5190%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [123/225], Training Accuracy: 31.5549%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [124/225], Training Accuracy: 31.5398%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [125/225], Training Accuracy: 31.5250%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [126/225], Training Accuracy: 31.4856%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [127/225], Training Accuracy: 31.4222%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Step [128/225], Training Accuracy: 31.4209%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [129/225], Training Accuracy: 31.4438%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [130/225], Training Accuracy: 31.3942%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [131/225], Training Accuracy: 31.3693%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [132/225], Training Accuracy: 31.3329%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [133/225], Training Accuracy: 31.3322%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [134/225], Training Accuracy: 31.3549%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [135/225], Training Accuracy: 31.3889%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [136/225], Training Accuracy: 31.3879%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [137/225], Training Accuracy: 31.3983%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [138/225], Training Accuracy: 31.4312%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [139/225], Training Accuracy: 31.4186%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [140/225], Training Accuracy: 31.3728%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [141/225], Training Accuracy: 31.3054%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [142/225], Training Accuracy: 31.3490%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [143/225], Training Accuracy: 31.3483%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [144/225], Training Accuracy: 31.3694%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [145/225], Training Accuracy: 31.4332%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [146/225], Training Accuracy: 31.4747%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [147/225], Training Accuracy: 31.4838%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [148/225], Training Accuracy: 31.4400%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [149/225], Training Accuracy: 31.4597%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [150/225], Training Accuracy: 31.4896%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [151/225], Training Accuracy: 31.5294%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [152/225], Training Accuracy: 31.5070%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [153/225], Training Accuracy: 31.4645%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [154/225], Training Accuracy: 31.4732%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [155/225], Training Accuracy: 31.4718%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [156/225], Training Accuracy: 31.4904%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [157/225], Training Accuracy: 31.4192%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [158/225], Training Accuracy: 31.3983%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [159/225], Training Accuracy: 31.4564%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [160/225], Training Accuracy: 31.4160%, Training Loss: 0.6860%\n",
      "Epoch [62/100], Step [161/225], Training Accuracy: 31.4732%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [162/225], Training Accuracy: 31.4333%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [163/225], Training Accuracy: 31.4896%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [164/225], Training Accuracy: 31.4691%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [165/225], Training Accuracy: 31.3920%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [166/225], Training Accuracy: 31.4100%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [167/225], Training Accuracy: 31.4558%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [168/225], Training Accuracy: 31.4081%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [169/225], Training Accuracy: 31.3332%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [170/225], Training Accuracy: 31.2868%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [171/225], Training Accuracy: 31.2865%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [172/225], Training Accuracy: 31.2954%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [173/225], Training Accuracy: 31.3042%, Training Loss: 0.6859%\n",
      "Epoch [62/100], Step [174/225], Training Accuracy: 31.3039%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [175/225], Training Accuracy: 31.3393%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [176/225], Training Accuracy: 31.3477%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [177/225], Training Accuracy: 31.3648%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [178/225], Training Accuracy: 31.3466%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [179/225], Training Accuracy: 31.3111%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [180/225], Training Accuracy: 31.3542%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [181/225], Training Accuracy: 31.3104%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [182/225], Training Accuracy: 31.3101%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [183/225], Training Accuracy: 31.3183%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [184/225], Training Accuracy: 31.2755%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [185/225], Training Accuracy: 31.2416%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [186/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [187/225], Training Accuracy: 31.2667%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [188/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [189/225], Training Accuracy: 31.2831%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [190/225], Training Accuracy: 31.2336%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [191/225], Training Accuracy: 31.2173%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [192/225], Training Accuracy: 31.1361%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [193/225], Training Accuracy: 31.1528%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [194/225], Training Accuracy: 31.1614%, Training Loss: 0.6858%\n",
      "Epoch [62/100], Step [195/225], Training Accuracy: 31.1378%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [196/225], Training Accuracy: 31.1065%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [197/225], Training Accuracy: 31.1469%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [198/225], Training Accuracy: 31.1711%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [199/225], Training Accuracy: 31.1558%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [200/225], Training Accuracy: 31.1406%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [201/225], Training Accuracy: 31.1489%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [202/225], Training Accuracy: 31.1494%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [203/225], Training Accuracy: 31.1422%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [204/225], Training Accuracy: 31.1964%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [205/225], Training Accuracy: 31.1814%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [206/225], Training Accuracy: 31.1742%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [207/225], Training Accuracy: 31.1519%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [208/225], Training Accuracy: 31.1674%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [209/225], Training Accuracy: 31.2126%, Training Loss: 0.6857%\n",
      "Epoch [62/100], Step [210/225], Training Accuracy: 31.2426%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [211/225], Training Accuracy: 31.2278%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [212/225], Training Accuracy: 31.2647%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [213/225], Training Accuracy: 31.2427%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [214/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [215/225], Training Accuracy: 31.2209%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [216/225], Training Accuracy: 31.1487%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [217/225], Training Accuracy: 31.1492%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [218/225], Training Accuracy: 31.1210%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [219/225], Training Accuracy: 31.1787%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [220/225], Training Accuracy: 31.1932%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [221/225], Training Accuracy: 31.1934%, Training Loss: 0.6856%\n",
      "Epoch [62/100], Step [222/225], Training Accuracy: 31.2078%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [223/225], Training Accuracy: 31.2360%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [224/225], Training Accuracy: 31.2360%, Training Loss: 0.6855%\n",
      "Epoch [62/100], Step [225/225], Training Accuracy: 31.2257%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6904%\n",
      "Epoch [63/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6894%\n",
      "Epoch [63/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6891%\n",
      "Epoch [63/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6861%\n",
      "Epoch [63/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6869%\n",
      "Epoch [63/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6872%\n",
      "Epoch [63/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6861%\n",
      "Epoch [63/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6864%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6863%\n",
      "Epoch [63/100], Step [16/225], Training Accuracy: 31.3477%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [17/225], Training Accuracy: 30.9743%, Training Loss: 0.6861%\n",
      "Epoch [63/100], Step [18/225], Training Accuracy: 30.9896%, Training Loss: 0.6862%\n",
      "Epoch [63/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6863%\n",
      "Epoch [63/100], Step [20/225], Training Accuracy: 31.7969%, Training Loss: 0.6864%\n",
      "Epoch [63/100], Step [21/225], Training Accuracy: 31.6964%, Training Loss: 0.6861%\n",
      "Epoch [63/100], Step [22/225], Training Accuracy: 31.8892%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [24/225], Training Accuracy: 31.9010%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [25/225], Training Accuracy: 32.1875%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [26/225], Training Accuracy: 32.5120%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [27/225], Training Accuracy: 32.1759%, Training Loss: 0.6849%\n",
      "Epoch [63/100], Step [28/225], Training Accuracy: 32.0871%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [29/225], Training Accuracy: 32.4353%, Training Loss: 0.6847%\n",
      "Epoch [63/100], Step [30/225], Training Accuracy: 32.3958%, Training Loss: 0.6846%\n",
      "Epoch [63/100], Step [31/225], Training Accuracy: 32.2077%, Training Loss: 0.6846%\n",
      "Epoch [63/100], Step [32/225], Training Accuracy: 32.4219%, Training Loss: 0.6843%\n",
      "Epoch [63/100], Step [33/225], Training Accuracy: 32.5284%, Training Loss: 0.6841%\n",
      "Epoch [63/100], Step [34/225], Training Accuracy: 32.4908%, Training Loss: 0.6843%\n",
      "Epoch [63/100], Step [35/225], Training Accuracy: 32.3661%, Training Loss: 0.6843%\n",
      "Epoch [63/100], Step [36/225], Training Accuracy: 32.2049%, Training Loss: 0.6844%\n",
      "Epoch [63/100], Step [37/225], Training Accuracy: 32.2213%, Training Loss: 0.6845%\n",
      "Epoch [63/100], Step [38/225], Training Accuracy: 32.0724%, Training Loss: 0.6847%\n",
      "Epoch [63/100], Step [39/225], Training Accuracy: 31.7708%, Training Loss: 0.6849%\n",
      "Epoch [63/100], Step [40/225], Training Accuracy: 31.7969%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [41/225], Training Accuracy: 31.7835%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6854%\n",
      "Epoch [63/100], Step [43/225], Training Accuracy: 31.7587%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [44/225], Training Accuracy: 31.7116%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [45/225], Training Accuracy: 31.6319%, Training Loss: 0.6854%\n",
      "Epoch [63/100], Step [46/225], Training Accuracy: 31.4878%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [47/225], Training Accuracy: 31.4495%, Training Loss: 0.6854%\n",
      "Epoch [63/100], Step [48/225], Training Accuracy: 31.6081%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [49/225], Training Accuracy: 31.6327%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [50/225], Training Accuracy: 31.5625%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [52/225], Training Accuracy: 31.7007%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [53/225], Training Accuracy: 31.6333%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [54/225], Training Accuracy: 31.5104%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [55/225], Training Accuracy: 31.5341%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [56/225], Training Accuracy: 31.6127%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [57/225], Training Accuracy: 31.6338%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [58/225], Training Accuracy: 31.5194%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [59/225], Training Accuracy: 31.8061%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [60/225], Training Accuracy: 31.8490%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [61/225], Training Accuracy: 31.8135%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [63/225], Training Accuracy: 31.9196%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [64/225], Training Accuracy: 31.8848%, Training Loss: 0.6849%\n",
      "Epoch [63/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [67/225], Training Accuracy: 31.9263%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [68/225], Training Accuracy: 32.0083%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [69/225], Training Accuracy: 32.0199%, Training Loss: 0.6849%\n",
      "Epoch [63/100], Step [70/225], Training Accuracy: 32.0089%, Training Loss: 0.6848%\n",
      "Epoch [63/100], Step [71/225], Training Accuracy: 32.0423%, Training Loss: 0.6848%\n",
      "Epoch [63/100], Step [72/225], Training Accuracy: 31.8359%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [73/225], Training Accuracy: 31.7423%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [74/225], Training Accuracy: 31.8412%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [75/225], Training Accuracy: 31.7708%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [77/225], Training Accuracy: 31.5950%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [78/225], Training Accuracy: 31.6106%, Training Loss: 0.6850%\n",
      "Epoch [63/100], Step [79/225], Training Accuracy: 31.5665%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [80/225], Training Accuracy: 31.5430%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [81/225], Training Accuracy: 31.5008%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [82/225], Training Accuracy: 31.5168%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [83/225], Training Accuracy: 31.4383%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [84/225], Training Accuracy: 31.5104%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [85/225], Training Accuracy: 31.4338%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [86/225], Training Accuracy: 31.4862%, Training Loss: 0.6851%\n",
      "Epoch [63/100], Step [87/225], Training Accuracy: 31.5194%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [88/225], Training Accuracy: 31.4986%, Training Loss: 0.6852%\n",
      "Epoch [63/100], Step [89/225], Training Accuracy: 31.4256%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [90/225], Training Accuracy: 31.3368%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [92/225], Training Accuracy: 31.3689%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [93/225], Training Accuracy: 31.3844%, Training Loss: 0.6854%\n",
      "Epoch [63/100], Step [94/225], Training Accuracy: 31.4495%, Training Loss: 0.6853%\n",
      "Epoch [63/100], Step [95/225], Training Accuracy: 31.3487%, Training Loss: 0.6854%\n",
      "Epoch [63/100], Step [96/225], Training Accuracy: 31.4453%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [97/225], Training Accuracy: 31.4916%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [98/225], Training Accuracy: 31.4732%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [100/225], Training Accuracy: 31.6094%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [101/225], Training Accuracy: 31.7296%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [102/225], Training Accuracy: 31.6176%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [103/225], Training Accuracy: 31.6596%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [104/225], Training Accuracy: 31.6406%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [105/225], Training Accuracy: 31.5774%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [106/225], Training Accuracy: 31.6038%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [107/225], Training Accuracy: 31.5421%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [108/225], Training Accuracy: 31.6117%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [109/225], Training Accuracy: 31.4794%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [110/225], Training Accuracy: 31.4205%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [111/225], Training Accuracy: 31.3063%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [112/225], Training Accuracy: 31.3895%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [113/225], Training Accuracy: 31.3744%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [114/225], Training Accuracy: 31.4282%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Step [115/225], Training Accuracy: 31.3723%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [116/225], Training Accuracy: 31.3578%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [117/225], Training Accuracy: 31.3034%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [118/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [119/225], Training Accuracy: 31.1975%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [120/225], Training Accuracy: 31.2240%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [121/225], Training Accuracy: 31.2371%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [122/225], Training Accuracy: 31.2628%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [123/225], Training Accuracy: 31.2881%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [124/225], Training Accuracy: 31.2878%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [125/225], Training Accuracy: 31.2750%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [126/225], Training Accuracy: 31.2004%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [127/225], Training Accuracy: 31.1393%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [128/225], Training Accuracy: 31.1279%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [129/225], Training Accuracy: 31.1773%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [130/225], Training Accuracy: 31.1178%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [131/225], Training Accuracy: 31.0830%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [132/225], Training Accuracy: 31.0251%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [133/225], Training Accuracy: 31.0385%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [134/225], Training Accuracy: 31.0868%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [135/225], Training Accuracy: 31.0880%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [136/225], Training Accuracy: 31.1236%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [137/225], Training Accuracy: 31.1702%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [138/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [139/225], Training Accuracy: 31.1601%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [140/225], Training Accuracy: 31.1496%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [141/225], Training Accuracy: 31.0727%, Training Loss: 0.6861%\n",
      "Epoch [63/100], Step [142/225], Training Accuracy: 31.1180%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [143/225], Training Accuracy: 31.0970%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [144/225], Training Accuracy: 31.1198%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [145/225], Training Accuracy: 31.1853%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [146/225], Training Accuracy: 31.2072%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [147/225], Training Accuracy: 31.2287%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [148/225], Training Accuracy: 31.1972%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [150/225], Training Accuracy: 31.2917%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [151/225], Training Accuracy: 31.3535%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [152/225], Training Accuracy: 31.3425%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [153/225], Training Accuracy: 31.3317%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [154/225], Training Accuracy: 31.3413%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [155/225], Training Accuracy: 31.3407%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [156/225], Training Accuracy: 31.3602%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [157/225], Training Accuracy: 31.2699%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [158/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [159/225], Training Accuracy: 31.2991%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [160/225], Training Accuracy: 31.2793%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [161/225], Training Accuracy: 31.3082%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [162/225], Training Accuracy: 31.2693%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [163/225], Training Accuracy: 31.3554%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [164/225], Training Accuracy: 31.3357%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [165/225], Training Accuracy: 31.2784%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [166/225], Training Accuracy: 31.2688%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [167/225], Training Accuracy: 31.3155%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [168/225], Training Accuracy: 31.2593%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [169/225], Training Accuracy: 31.2038%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [170/225], Training Accuracy: 31.1489%, Training Loss: 0.6860%\n",
      "Epoch [63/100], Step [171/225], Training Accuracy: 31.1678%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [172/225], Training Accuracy: 31.1955%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [174/225], Training Accuracy: 31.2231%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [175/225], Training Accuracy: 31.2589%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [176/225], Training Accuracy: 31.3121%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [177/225], Training Accuracy: 31.3206%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [178/225], Training Accuracy: 31.3290%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [179/225], Training Accuracy: 31.3024%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [180/225], Training Accuracy: 31.3455%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [181/225], Training Accuracy: 31.3018%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [182/225], Training Accuracy: 31.3015%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [183/225], Training Accuracy: 31.3098%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [184/225], Training Accuracy: 31.2840%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [185/225], Training Accuracy: 31.2584%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [186/225], Training Accuracy: 31.2920%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [187/225], Training Accuracy: 31.2918%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [188/225], Training Accuracy: 31.2666%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [189/225], Training Accuracy: 31.2913%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [190/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [191/225], Training Accuracy: 31.2255%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [192/225], Training Accuracy: 31.1442%, Training Loss: 0.6859%\n",
      "Epoch [63/100], Step [193/225], Training Accuracy: 31.1528%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [194/225], Training Accuracy: 31.1695%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [195/225], Training Accuracy: 31.1378%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [196/225], Training Accuracy: 31.1065%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [197/225], Training Accuracy: 31.1390%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [198/225], Training Accuracy: 31.1553%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [199/225], Training Accuracy: 31.1479%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [200/225], Training Accuracy: 31.1172%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [201/225], Training Accuracy: 31.1412%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [202/225], Training Accuracy: 31.1494%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [203/225], Training Accuracy: 31.1422%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [204/225], Training Accuracy: 31.1887%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [205/225], Training Accuracy: 31.2043%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [206/225], Training Accuracy: 31.2045%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [207/225], Training Accuracy: 31.1821%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [208/225], Training Accuracy: 31.2049%, Training Loss: 0.6858%\n",
      "Epoch [63/100], Step [209/225], Training Accuracy: 31.2425%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [210/225], Training Accuracy: 31.2798%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [211/225], Training Accuracy: 31.2796%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [212/225], Training Accuracy: 31.3311%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [213/225], Training Accuracy: 31.3087%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100], Step [214/225], Training Accuracy: 31.3230%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [215/225], Training Accuracy: 31.2863%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [216/225], Training Accuracy: 31.2355%, Training Loss: 0.6857%\n",
      "Epoch [63/100], Step [217/225], Training Accuracy: 31.2428%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [218/225], Training Accuracy: 31.1998%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [219/225], Training Accuracy: 31.2643%, Training Loss: 0.6856%\n",
      "Epoch [63/100], Step [220/225], Training Accuracy: 31.2713%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [221/225], Training Accuracy: 31.2571%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [222/225], Training Accuracy: 31.2711%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [223/225], Training Accuracy: 31.3201%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [224/225], Training Accuracy: 31.3267%, Training Loss: 0.6855%\n",
      "Epoch [63/100], Step [225/225], Training Accuracy: 31.3021%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6917%\n",
      "Epoch [64/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6876%\n",
      "Epoch [64/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6877%\n",
      "Epoch [64/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6857%\n",
      "Epoch [64/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6857%\n",
      "Epoch [64/100], Step [10/225], Training Accuracy: 30.4688%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [11/225], Training Accuracy: 30.3977%, Training Loss: 0.6859%\n",
      "Epoch [64/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6858%\n",
      "Epoch [64/100], Step [13/225], Training Accuracy: 29.8077%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6860%\n",
      "Epoch [64/100], Step [15/225], Training Accuracy: 30.8333%, Training Loss: 0.6858%\n",
      "Epoch [64/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6857%\n",
      "Epoch [64/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6857%\n",
      "Epoch [64/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6857%\n",
      "Epoch [64/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [22/225], Training Accuracy: 31.7472%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [23/225], Training Accuracy: 31.6576%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [24/225], Training Accuracy: 31.7708%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [25/225], Training Accuracy: 32.0625%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [27/225], Training Accuracy: 32.0602%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [30/225], Training Accuracy: 32.0312%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [31/225], Training Accuracy: 31.9052%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6846%\n",
      "Epoch [64/100], Step [33/225], Training Accuracy: 32.1970%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [34/225], Training Accuracy: 32.1691%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [35/225], Training Accuracy: 32.0089%, Training Loss: 0.6846%\n",
      "Epoch [64/100], Step [36/225], Training Accuracy: 31.9010%, Training Loss: 0.6847%\n",
      "Epoch [64/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [38/225], Training Accuracy: 31.8257%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [39/225], Training Accuracy: 31.5304%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [40/225], Training Accuracy: 31.5625%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [41/225], Training Accuracy: 31.5930%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [42/225], Training Accuracy: 31.3616%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [43/225], Training Accuracy: 31.5407%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [44/225], Training Accuracy: 31.5341%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [45/225], Training Accuracy: 31.5972%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [46/225], Training Accuracy: 31.4878%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [47/225], Training Accuracy: 31.4162%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [48/225], Training Accuracy: 31.5104%, Training Loss: 0.6847%\n",
      "Epoch [64/100], Step [49/225], Training Accuracy: 31.6327%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [50/225], Training Accuracy: 31.6250%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [51/225], Training Accuracy: 31.8015%, Training Loss: 0.6846%\n",
      "Epoch [64/100], Step [52/225], Training Accuracy: 31.8810%, Training Loss: 0.6844%\n",
      "Epoch [64/100], Step [53/225], Training Accuracy: 31.8101%, Training Loss: 0.6844%\n",
      "Epoch [64/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [55/225], Training Accuracy: 31.7614%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [57/225], Training Accuracy: 31.8805%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [58/225], Training Accuracy: 31.7619%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [59/225], Training Accuracy: 32.1504%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [61/225], Training Accuracy: 32.1721%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [62/225], Training Accuracy: 32.1825%, Training Loss: 0.6844%\n",
      "Epoch [64/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [64/225], Training Accuracy: 32.2021%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [65/225], Training Accuracy: 32.0913%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [66/225], Training Accuracy: 32.1970%, Training Loss: 0.6842%\n",
      "Epoch [64/100], Step [67/225], Training Accuracy: 32.1595%, Training Loss: 0.6842%\n",
      "Epoch [64/100], Step [68/225], Training Accuracy: 32.1921%, Training Loss: 0.6843%\n",
      "Epoch [64/100], Step [69/225], Training Accuracy: 32.1558%, Training Loss: 0.6842%\n",
      "Epoch [64/100], Step [70/225], Training Accuracy: 32.1429%, Training Loss: 0.6842%\n",
      "Epoch [64/100], Step [71/225], Training Accuracy: 32.2183%, Training Loss: 0.6842%\n",
      "Epoch [64/100], Step [72/225], Training Accuracy: 31.9444%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [73/225], Training Accuracy: 31.8921%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [74/225], Training Accuracy: 31.9890%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [75/225], Training Accuracy: 31.9583%, Training Loss: 0.6844%\n",
      "Epoch [64/100], Step [76/225], Training Accuracy: 32.0107%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [77/225], Training Accuracy: 31.9805%, Training Loss: 0.6846%\n",
      "Epoch [64/100], Step [78/225], Training Accuracy: 31.9912%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [79/225], Training Accuracy: 31.9225%, Training Loss: 0.6846%\n",
      "Epoch [64/100], Step [80/225], Training Accuracy: 31.8945%, Training Loss: 0.6845%\n",
      "Epoch [64/100], Step [81/225], Training Accuracy: 31.8094%, Training Loss: 0.6846%\n",
      "Epoch [64/100], Step [82/225], Training Accuracy: 31.8216%, Training Loss: 0.6847%\n",
      "Epoch [64/100], Step [83/225], Training Accuracy: 31.7583%, Training Loss: 0.6847%\n",
      "Epoch [64/100], Step [84/225], Training Accuracy: 31.7894%, Training Loss: 0.6847%\n",
      "Epoch [64/100], Step [85/225], Training Accuracy: 31.7647%, Training Loss: 0.6847%\n",
      "Epoch [64/100], Step [86/225], Training Accuracy: 31.7951%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [87/225], Training Accuracy: 31.7888%, Training Loss: 0.6848%\n",
      "Epoch [64/100], Step [88/225], Training Accuracy: 31.7294%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Step [89/225], Training Accuracy: 31.6362%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [90/225], Training Accuracy: 31.5278%, Training Loss: 0.6849%\n",
      "Epoch [64/100], Step [91/225], Training Accuracy: 31.5419%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [92/225], Training Accuracy: 31.5387%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [93/225], Training Accuracy: 31.5356%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [94/225], Training Accuracy: 31.6157%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [95/225], Training Accuracy: 31.5132%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [97/225], Training Accuracy: 31.6044%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [98/225], Training Accuracy: 31.6008%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [99/225], Training Accuracy: 31.7077%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [100/225], Training Accuracy: 31.6562%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [101/225], Training Accuracy: 31.7760%, Training Loss: 0.6850%\n",
      "Epoch [64/100], Step [102/225], Training Accuracy: 31.6789%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [103/225], Training Accuracy: 31.7051%, Training Loss: 0.6851%\n",
      "Epoch [64/100], Step [104/225], Training Accuracy: 31.6556%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [105/225], Training Accuracy: 31.5923%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [107/225], Training Accuracy: 31.4690%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [109/225], Training Accuracy: 31.4077%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [110/225], Training Accuracy: 31.3920%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [111/225], Training Accuracy: 31.3063%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [112/225], Training Accuracy: 31.3895%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [113/225], Training Accuracy: 31.3606%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [114/225], Training Accuracy: 31.4282%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [115/225], Training Accuracy: 31.3995%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [116/225], Training Accuracy: 31.4116%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [117/225], Training Accuracy: 31.3702%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [118/225], Training Accuracy: 31.3692%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [119/225], Training Accuracy: 31.3288%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [120/225], Training Accuracy: 31.3802%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [121/225], Training Accuracy: 31.3404%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [122/225], Training Accuracy: 31.3397%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [123/225], Training Accuracy: 31.3643%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [124/225], Training Accuracy: 31.3382%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [125/225], Training Accuracy: 31.3375%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [126/225], Training Accuracy: 31.2872%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [127/225], Training Accuracy: 31.2377%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [128/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [129/225], Training Accuracy: 31.2863%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [130/225], Training Accuracy: 31.2019%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [131/225], Training Accuracy: 31.1665%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [132/225], Training Accuracy: 31.1198%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [133/225], Training Accuracy: 31.1325%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [134/225], Training Accuracy: 31.1917%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [135/225], Training Accuracy: 31.2037%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [136/225], Training Accuracy: 31.2040%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [137/225], Training Accuracy: 31.2272%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [138/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [139/225], Training Accuracy: 31.2050%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [140/225], Training Accuracy: 31.1942%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [141/225], Training Accuracy: 31.1281%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [142/225], Training Accuracy: 31.1840%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [143/225], Training Accuracy: 31.1626%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [144/225], Training Accuracy: 31.1957%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [145/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [146/225], Training Accuracy: 31.2714%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [147/225], Training Accuracy: 31.2713%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [148/225], Training Accuracy: 31.2606%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [149/225], Training Accuracy: 31.2815%, Training Loss: 0.6856%\n",
      "Epoch [64/100], Step [150/225], Training Accuracy: 31.3229%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [151/225], Training Accuracy: 31.3845%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [152/225], Training Accuracy: 31.3631%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [153/225], Training Accuracy: 31.3215%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [154/225], Training Accuracy: 31.3819%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [155/225], Training Accuracy: 31.3609%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [156/225], Training Accuracy: 31.3702%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [157/225], Training Accuracy: 31.2898%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [158/225], Training Accuracy: 31.2797%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [159/225], Training Accuracy: 31.3581%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [160/225], Training Accuracy: 31.3184%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [161/225], Training Accuracy: 31.3568%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [162/225], Training Accuracy: 31.3368%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [163/225], Training Accuracy: 31.3746%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [164/225], Training Accuracy: 31.3643%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [165/225], Training Accuracy: 31.3068%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [166/225], Training Accuracy: 31.3159%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [167/225], Training Accuracy: 31.3529%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [168/225], Training Accuracy: 31.3151%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [169/225], Training Accuracy: 31.2408%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [170/225], Training Accuracy: 31.1949%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [171/225], Training Accuracy: 31.2043%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [172/225], Training Accuracy: 31.2318%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [173/225], Training Accuracy: 31.2319%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [174/225], Training Accuracy: 31.2590%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [175/225], Training Accuracy: 31.3036%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [176/225], Training Accuracy: 31.3210%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [177/225], Training Accuracy: 31.3294%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [178/225], Training Accuracy: 31.3114%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [179/225], Training Accuracy: 31.2675%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [180/225], Training Accuracy: 31.3194%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [181/225], Training Accuracy: 31.2932%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [182/225], Training Accuracy: 31.2843%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [183/225], Training Accuracy: 31.2927%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [184/225], Training Accuracy: 31.2840%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [185/225], Training Accuracy: 31.2584%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [186/225], Training Accuracy: 31.2836%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [187/225], Training Accuracy: 31.3001%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [188/225], Training Accuracy: 31.2749%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [189/225], Training Accuracy: 31.3161%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [190/225], Training Accuracy: 31.2829%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [191/225], Training Accuracy: 31.2582%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [192/225], Training Accuracy: 31.1930%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [193/225], Training Accuracy: 31.2176%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [194/225], Training Accuracy: 31.2258%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [195/225], Training Accuracy: 31.2019%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [196/225], Training Accuracy: 31.1783%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [197/225], Training Accuracy: 31.2024%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [198/225], Training Accuracy: 31.2263%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Step [199/225], Training Accuracy: 31.2107%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [200/225], Training Accuracy: 31.1953%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [201/225], Training Accuracy: 31.2111%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [202/225], Training Accuracy: 31.2036%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [203/225], Training Accuracy: 31.1961%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [204/225], Training Accuracy: 31.2730%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [205/225], Training Accuracy: 31.2576%, Training Loss: 0.6855%\n",
      "Epoch [64/100], Step [206/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [207/225], Training Accuracy: 31.2274%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [208/225], Training Accuracy: 31.2575%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [209/225], Training Accuracy: 31.2949%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [210/225], Training Accuracy: 31.3170%, Training Loss: 0.6854%\n",
      "Epoch [64/100], Step [211/225], Training Accuracy: 31.2944%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [212/225], Training Accuracy: 31.3384%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [213/225], Training Accuracy: 31.3160%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [214/225], Training Accuracy: 31.3449%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [215/225], Training Accuracy: 31.3081%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [216/225], Training Accuracy: 31.2428%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [217/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [218/225], Training Accuracy: 31.2142%, Training Loss: 0.6853%\n",
      "Epoch [64/100], Step [219/225], Training Accuracy: 31.2785%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [220/225], Training Accuracy: 31.2855%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [221/225], Training Accuracy: 31.2783%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [222/225], Training Accuracy: 31.2993%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [223/225], Training Accuracy: 31.3341%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [224/225], Training Accuracy: 31.3267%, Training Loss: 0.6852%\n",
      "Epoch [64/100], Step [225/225], Training Accuracy: 31.3021%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6884%\n",
      "Epoch [65/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6867%\n",
      "Epoch [65/100], Step [3/225], Training Accuracy: 33.8542%, Training Loss: 0.6881%\n",
      "Epoch [65/100], Step [4/225], Training Accuracy: 33.9844%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [5/225], Training Accuracy: 34.3750%, Training Loss: 0.6863%\n",
      "Epoch [65/100], Step [6/225], Training Accuracy: 33.3333%, Training Loss: 0.6866%\n",
      "Epoch [65/100], Step [7/225], Training Accuracy: 33.2589%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [8/225], Training Accuracy: 32.6172%, Training Loss: 0.6850%\n",
      "Epoch [65/100], Step [9/225], Training Accuracy: 32.4653%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [11/225], Training Accuracy: 31.8182%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [12/225], Training Accuracy: 30.9896%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [13/225], Training Accuracy: 30.8894%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [14/225], Training Accuracy: 31.1384%, Training Loss: 0.6863%\n",
      "Epoch [65/100], Step [15/225], Training Accuracy: 31.5625%, Training Loss: 0.6862%\n",
      "Epoch [65/100], Step [16/225], Training Accuracy: 31.6406%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [17/225], Training Accuracy: 31.5257%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [18/225], Training Accuracy: 31.5972%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [19/225], Training Accuracy: 32.0724%, Training Loss: 0.6862%\n",
      "Epoch [65/100], Step [20/225], Training Accuracy: 32.5000%, Training Loss: 0.6862%\n",
      "Epoch [65/100], Step [21/225], Training Accuracy: 32.3661%, Training Loss: 0.6861%\n",
      "Epoch [65/100], Step [22/225], Training Accuracy: 32.5284%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [23/225], Training Accuracy: 32.2690%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [24/225], Training Accuracy: 32.4870%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [25/225], Training Accuracy: 32.6875%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [26/225], Training Accuracy: 32.9928%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [27/225], Training Accuracy: 32.6968%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [28/225], Training Accuracy: 32.3661%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [29/225], Training Accuracy: 32.5970%, Training Loss: 0.6854%\n",
      "Epoch [65/100], Step [30/225], Training Accuracy: 32.5521%, Training Loss: 0.6854%\n",
      "Epoch [65/100], Step [31/225], Training Accuracy: 32.3589%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [32/225], Training Accuracy: 32.4707%, Training Loss: 0.6850%\n",
      "Epoch [65/100], Step [33/225], Training Accuracy: 32.5284%, Training Loss: 0.6849%\n",
      "Epoch [65/100], Step [34/225], Training Accuracy: 32.4908%, Training Loss: 0.6849%\n",
      "Epoch [65/100], Step [35/225], Training Accuracy: 32.3661%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [36/225], Training Accuracy: 32.2049%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [37/225], Training Accuracy: 32.2213%, Training Loss: 0.6854%\n",
      "Epoch [65/100], Step [38/225], Training Accuracy: 32.0724%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [39/225], Training Accuracy: 31.8109%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [40/225], Training Accuracy: 31.8359%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [41/225], Training Accuracy: 31.8598%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [42/225], Training Accuracy: 31.6964%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [43/225], Training Accuracy: 31.8314%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [44/225], Training Accuracy: 31.7472%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [45/225], Training Accuracy: 31.7014%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [46/225], Training Accuracy: 31.6236%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [48/225], Training Accuracy: 31.7057%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [49/225], Training Accuracy: 31.7602%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [50/225], Training Accuracy: 31.7500%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [51/225], Training Accuracy: 31.9240%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [52/225], Training Accuracy: 31.8810%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [53/225], Training Accuracy: 31.8396%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [54/225], Training Accuracy: 31.7130%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [55/225], Training Accuracy: 31.8466%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [56/225], Training Accuracy: 31.8917%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [57/225], Training Accuracy: 31.9079%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [58/225], Training Accuracy: 31.8427%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [59/225], Training Accuracy: 32.2299%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6850%\n",
      "Epoch [65/100], Step [61/225], Training Accuracy: 32.1977%, Training Loss: 0.6850%\n",
      "Epoch [65/100], Step [62/225], Training Accuracy: 32.2329%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [63/225], Training Accuracy: 32.2669%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [64/225], Training Accuracy: 32.2754%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [65/225], Training Accuracy: 32.2115%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [66/225], Training Accuracy: 32.3153%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [67/225], Training Accuracy: 32.3228%, Training Loss: 0.6850%\n",
      "Epoch [65/100], Step [68/225], Training Accuracy: 32.3989%, Training Loss: 0.6850%\n",
      "Epoch [65/100], Step [69/225], Training Accuracy: 32.3370%, Training Loss: 0.6849%\n",
      "Epoch [65/100], Step [70/225], Training Accuracy: 32.2991%, Training Loss: 0.6849%\n",
      "Epoch [65/100], Step [71/225], Training Accuracy: 32.3724%, Training Loss: 0.6849%\n",
      "Epoch [65/100], Step [72/225], Training Accuracy: 32.1615%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [73/225], Training Accuracy: 32.1062%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [74/225], Training Accuracy: 32.2213%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [75/225], Training Accuracy: 32.1250%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [76/225], Training Accuracy: 32.1135%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [77/225], Training Accuracy: 31.9805%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [78/225], Training Accuracy: 32.0112%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [79/225], Training Accuracy: 31.9620%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [80/225], Training Accuracy: 31.9336%, Training Loss: 0.6851%\n",
      "Epoch [65/100], Step [81/225], Training Accuracy: 31.8287%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [82/225], Training Accuracy: 31.8407%, Training Loss: 0.6852%\n",
      "Epoch [65/100], Step [83/225], Training Accuracy: 31.7395%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [84/225], Training Accuracy: 31.7708%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Step [85/225], Training Accuracy: 31.7463%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [86/225], Training Accuracy: 31.7951%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [87/225], Training Accuracy: 31.8068%, Training Loss: 0.6853%\n",
      "Epoch [65/100], Step [88/225], Training Accuracy: 31.7827%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [89/225], Training Accuracy: 31.6889%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [90/225], Training Accuracy: 31.5972%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [91/225], Training Accuracy: 31.6621%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [92/225], Training Accuracy: 31.6576%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [93/225], Training Accuracy: 31.6196%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [94/225], Training Accuracy: 31.6988%, Training Loss: 0.6854%\n",
      "Epoch [65/100], Step [95/225], Training Accuracy: 31.5954%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [96/225], Training Accuracy: 31.7220%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [97/225], Training Accuracy: 31.7171%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [98/225], Training Accuracy: 31.6805%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [99/225], Training Accuracy: 31.8024%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [100/225], Training Accuracy: 31.7812%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [101/225], Training Accuracy: 31.9152%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [102/225], Training Accuracy: 31.8321%, Training Loss: 0.6855%\n",
      "Epoch [65/100], Step [103/225], Training Accuracy: 31.9023%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [104/225], Training Accuracy: 31.8810%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [105/225], Training Accuracy: 31.8601%, Training Loss: 0.6856%\n",
      "Epoch [65/100], Step [106/225], Training Accuracy: 31.8396%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [107/225], Training Accuracy: 31.7465%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [108/225], Training Accuracy: 31.8142%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [109/225], Training Accuracy: 31.6800%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [110/225], Training Accuracy: 31.6619%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [111/225], Training Accuracy: 31.5456%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [112/225], Training Accuracy: 31.6406%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [113/225], Training Accuracy: 31.6372%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [114/225], Training Accuracy: 31.6886%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [115/225], Training Accuracy: 31.6712%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [116/225], Training Accuracy: 31.6541%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [117/225], Training Accuracy: 31.6106%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [118/225], Training Accuracy: 31.5678%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [119/225], Training Accuracy: 31.5257%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [120/225], Training Accuracy: 31.5495%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [121/225], Training Accuracy: 31.5341%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [122/225], Training Accuracy: 31.5318%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [123/225], Training Accuracy: 31.5549%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [124/225], Training Accuracy: 31.5146%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [125/225], Training Accuracy: 31.4750%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [126/225], Training Accuracy: 31.4236%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [127/225], Training Accuracy: 31.3607%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [128/225], Training Accuracy: 31.3599%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [129/225], Training Accuracy: 31.4075%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [130/225], Training Accuracy: 31.3341%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [131/225], Training Accuracy: 31.3216%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [132/225], Training Accuracy: 31.2855%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [133/225], Training Accuracy: 31.2970%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [134/225], Training Accuracy: 31.3666%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [135/225], Training Accuracy: 31.3773%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [136/225], Training Accuracy: 31.3994%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [137/225], Training Accuracy: 31.4325%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [138/225], Training Accuracy: 31.4425%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [139/225], Training Accuracy: 31.4074%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [140/225], Training Accuracy: 31.4062%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [141/225], Training Accuracy: 31.3276%, Training Loss: 0.6861%\n",
      "Epoch [65/100], Step [142/225], Training Accuracy: 31.3930%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [143/225], Training Accuracy: 31.3811%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [144/225], Training Accuracy: 31.3911%, Training Loss: 0.6861%\n",
      "Epoch [65/100], Step [145/225], Training Accuracy: 31.4655%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [146/225], Training Accuracy: 31.4640%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [147/225], Training Accuracy: 31.4945%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [148/225], Training Accuracy: 31.4611%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [149/225], Training Accuracy: 31.4807%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [150/225], Training Accuracy: 31.5000%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [151/225], Training Accuracy: 31.5294%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [152/225], Training Accuracy: 31.5275%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [153/225], Training Accuracy: 31.5155%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [154/225], Training Accuracy: 31.4834%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [155/225], Training Accuracy: 31.4617%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [156/225], Training Accuracy: 31.4804%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [157/225], Training Accuracy: 31.4092%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [158/225], Training Accuracy: 31.3786%, Training Loss: 0.6861%\n",
      "Epoch [65/100], Step [159/225], Training Accuracy: 31.4465%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [160/225], Training Accuracy: 31.4062%, Training Loss: 0.6861%\n",
      "Epoch [65/100], Step [161/225], Training Accuracy: 31.4732%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [162/225], Training Accuracy: 31.4622%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [163/225], Training Accuracy: 31.5280%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [164/225], Training Accuracy: 31.4977%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [165/225], Training Accuracy: 31.4299%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [166/225], Training Accuracy: 31.4194%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [167/225], Training Accuracy: 31.4652%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [168/225], Training Accuracy: 31.4267%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [169/225], Training Accuracy: 31.3609%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [170/225], Training Accuracy: 31.3051%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [171/225], Training Accuracy: 31.3505%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [172/225], Training Accuracy: 31.3681%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [173/225], Training Accuracy: 31.3584%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [174/225], Training Accuracy: 31.3847%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [175/225], Training Accuracy: 31.4196%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [176/225], Training Accuracy: 31.4364%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [177/225], Training Accuracy: 31.4354%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [178/225], Training Accuracy: 31.4256%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [179/225], Training Accuracy: 31.3984%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [180/225], Training Accuracy: 31.4583%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [181/225], Training Accuracy: 31.4140%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [182/225], Training Accuracy: 31.4131%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [183/225], Training Accuracy: 31.4208%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [184/225], Training Accuracy: 31.3944%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [185/225], Training Accuracy: 31.3598%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [186/225], Training Accuracy: 31.3928%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [187/225], Training Accuracy: 31.4171%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [188/225], Training Accuracy: 31.4079%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [189/225], Training Accuracy: 31.4484%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [190/225], Training Accuracy: 31.4227%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [191/225], Training Accuracy: 31.3891%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [192/225], Training Accuracy: 31.3070%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [193/225], Training Accuracy: 31.3067%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [194/225], Training Accuracy: 31.3225%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100], Step [195/225], Training Accuracy: 31.2981%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [196/225], Training Accuracy: 31.2580%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [197/225], Training Accuracy: 31.2976%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [198/225], Training Accuracy: 31.3210%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [199/225], Training Accuracy: 31.3050%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [200/225], Training Accuracy: 31.2969%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [201/225], Training Accuracy: 31.3200%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [202/225], Training Accuracy: 31.3119%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [203/225], Training Accuracy: 31.3039%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [204/225], Training Accuracy: 31.3725%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [205/225], Training Accuracy: 31.3567%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [206/225], Training Accuracy: 31.3258%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [207/225], Training Accuracy: 31.3104%, Training Loss: 0.6860%\n",
      "Epoch [65/100], Step [208/225], Training Accuracy: 31.3251%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [209/225], Training Accuracy: 31.3547%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [210/225], Training Accuracy: 31.3839%, Training Loss: 0.6859%\n",
      "Epoch [65/100], Step [211/225], Training Accuracy: 31.3759%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [212/225], Training Accuracy: 31.4195%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [213/225], Training Accuracy: 31.3894%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [214/225], Training Accuracy: 31.3960%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [215/225], Training Accuracy: 31.3445%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [216/225], Training Accuracy: 31.2862%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [217/225], Training Accuracy: 31.2716%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [218/225], Training Accuracy: 31.2428%, Training Loss: 0.6858%\n",
      "Epoch [65/100], Step [219/225], Training Accuracy: 31.3071%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [220/225], Training Accuracy: 31.3210%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [221/225], Training Accuracy: 31.3136%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [222/225], Training Accuracy: 31.3133%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [223/225], Training Accuracy: 31.3551%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [224/225], Training Accuracy: 31.3616%, Training Loss: 0.6857%\n",
      "Epoch [65/100], Step [225/225], Training Accuracy: 31.3438%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6873%\n",
      "Epoch [66/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6844%\n",
      "Epoch [66/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6870%\n",
      "Epoch [66/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6865%\n",
      "Epoch [66/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6863%\n",
      "Epoch [66/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [10/225], Training Accuracy: 30.4688%, Training Loss: 0.6859%\n",
      "Epoch [66/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [12/225], Training Accuracy: 29.5573%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [13/225], Training Accuracy: 29.4471%, Training Loss: 0.6859%\n",
      "Epoch [66/100], Step [14/225], Training Accuracy: 29.7991%, Training Loss: 0.6865%\n",
      "Epoch [66/100], Step [15/225], Training Accuracy: 30.1042%, Training Loss: 0.6866%\n",
      "Epoch [66/100], Step [16/225], Training Accuracy: 29.9805%, Training Loss: 0.6865%\n",
      "Epoch [66/100], Step [17/225], Training Accuracy: 29.6875%, Training Loss: 0.6865%\n",
      "Epoch [66/100], Step [18/225], Training Accuracy: 29.7743%, Training Loss: 0.6867%\n",
      "Epoch [66/100], Step [19/225], Training Accuracy: 30.2632%, Training Loss: 0.6865%\n",
      "Epoch [66/100], Step [20/225], Training Accuracy: 30.7031%, Training Loss: 0.6865%\n",
      "Epoch [66/100], Step [21/225], Training Accuracy: 30.6548%, Training Loss: 0.6863%\n",
      "Epoch [66/100], Step [22/225], Training Accuracy: 30.8239%, Training Loss: 0.6863%\n",
      "Epoch [66/100], Step [23/225], Training Accuracy: 30.7745%, Training Loss: 0.6862%\n",
      "Epoch [66/100], Step [24/225], Training Accuracy: 30.8594%, Training Loss: 0.6861%\n",
      "Epoch [66/100], Step [25/225], Training Accuracy: 31.0000%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [26/225], Training Accuracy: 31.3702%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [27/225], Training Accuracy: 31.1343%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [28/225], Training Accuracy: 30.9710%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [29/225], Training Accuracy: 31.3578%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [30/225], Training Accuracy: 31.3542%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [31/225], Training Accuracy: 31.3004%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [32/225], Training Accuracy: 31.6406%, Training Loss: 0.6850%\n",
      "Epoch [66/100], Step [33/225], Training Accuracy: 31.7235%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [34/225], Training Accuracy: 31.6176%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [35/225], Training Accuracy: 31.5179%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [36/225], Training Accuracy: 31.3802%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [37/225], Training Accuracy: 31.3345%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [38/225], Training Accuracy: 31.1678%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [39/225], Training Accuracy: 31.0096%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [40/225], Training Accuracy: 31.0156%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [41/225], Training Accuracy: 31.1738%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [42/225], Training Accuracy: 30.9524%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [43/225], Training Accuracy: 31.1410%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [44/225], Training Accuracy: 31.1435%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [45/225], Training Accuracy: 31.2153%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [46/225], Training Accuracy: 31.1141%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [47/225], Training Accuracy: 31.0838%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [48/225], Training Accuracy: 31.2174%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [49/225], Training Accuracy: 31.1543%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [50/225], Training Accuracy: 31.0938%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [51/225], Training Accuracy: 31.1887%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [52/225], Training Accuracy: 31.2200%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [53/225], Training Accuracy: 31.1910%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [54/225], Training Accuracy: 31.0764%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [55/225], Training Accuracy: 31.1932%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [56/225], Training Accuracy: 31.2779%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [57/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [58/225], Training Accuracy: 31.1961%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [59/225], Training Accuracy: 31.5943%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [60/225], Training Accuracy: 31.6667%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [61/225], Training Accuracy: 31.5574%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [62/225], Training Accuracy: 31.6028%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [63/225], Training Accuracy: 31.6964%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [64/225], Training Accuracy: 31.6650%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [65/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [66/100], Step [66/225], Training Accuracy: 31.6525%, Training Loss: 0.6850%\n",
      "Epoch [66/100], Step [67/225], Training Accuracy: 31.6465%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [68/225], Training Accuracy: 31.7096%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [69/225], Training Accuracy: 31.6576%, Training Loss: 0.6846%\n",
      "Epoch [66/100], Step [70/225], Training Accuracy: 31.6295%, Training Loss: 0.6846%\n",
      "Epoch [66/100], Step [71/225], Training Accuracy: 31.6681%, Training Loss: 0.6846%\n",
      "Epoch [66/100], Step [72/225], Training Accuracy: 31.4670%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [73/225], Training Accuracy: 31.3998%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [74/225], Training Accuracy: 31.5034%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [75/225], Training Accuracy: 31.4792%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Step [76/225], Training Accuracy: 31.4556%, Training Loss: 0.6846%\n",
      "Epoch [66/100], Step [77/225], Training Accuracy: 31.3920%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [78/225], Training Accuracy: 31.4103%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [79/225], Training Accuracy: 31.3291%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [80/225], Training Accuracy: 31.3281%, Training Loss: 0.6846%\n",
      "Epoch [66/100], Step [81/225], Training Accuracy: 31.2693%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [82/225], Training Accuracy: 31.2881%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [83/225], Training Accuracy: 31.2312%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [84/225], Training Accuracy: 31.2872%, Training Loss: 0.6847%\n",
      "Epoch [66/100], Step [85/225], Training Accuracy: 31.2132%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [86/225], Training Accuracy: 31.2863%, Training Loss: 0.6848%\n",
      "Epoch [66/100], Step [87/225], Training Accuracy: 31.3039%, Training Loss: 0.6849%\n",
      "Epoch [66/100], Step [88/225], Training Accuracy: 31.2678%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [90/225], Training Accuracy: 31.0938%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [91/225], Training Accuracy: 31.0955%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [92/225], Training Accuracy: 31.0971%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [93/225], Training Accuracy: 31.0652%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [94/225], Training Accuracy: 31.1004%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [95/225], Training Accuracy: 31.0033%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [96/225], Training Accuracy: 31.1361%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [97/225], Training Accuracy: 31.1856%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [98/225], Training Accuracy: 31.1862%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [99/225], Training Accuracy: 31.3131%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [100/225], Training Accuracy: 31.2656%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [101/225], Training Accuracy: 31.4047%, Training Loss: 0.6851%\n",
      "Epoch [66/100], Step [102/225], Training Accuracy: 31.2806%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [103/225], Training Accuracy: 31.3107%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [104/225], Training Accuracy: 31.2800%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [105/225], Training Accuracy: 31.2202%, Training Loss: 0.6852%\n",
      "Epoch [66/100], Step [106/225], Training Accuracy: 31.2353%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [107/225], Training Accuracy: 31.1478%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [108/225], Training Accuracy: 31.2645%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [109/225], Training Accuracy: 31.1353%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [110/225], Training Accuracy: 31.1222%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [111/225], Training Accuracy: 31.0107%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [112/225], Training Accuracy: 31.0826%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [113/225], Training Accuracy: 31.0841%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [114/225], Training Accuracy: 31.1678%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [115/225], Training Accuracy: 31.1413%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [116/225], Training Accuracy: 31.1422%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [117/225], Training Accuracy: 31.0897%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [118/225], Training Accuracy: 31.0514%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [119/225], Training Accuracy: 31.0137%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [120/225], Training Accuracy: 31.0156%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [121/225], Training Accuracy: 30.9530%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [122/225], Training Accuracy: 30.9298%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [123/225], Training Accuracy: 30.9705%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [124/225], Training Accuracy: 30.9350%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [125/225], Training Accuracy: 30.9125%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [126/225], Training Accuracy: 30.8532%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [127/225], Training Accuracy: 30.8194%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [128/225], Training Accuracy: 30.8105%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [129/225], Training Accuracy: 30.8503%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [130/225], Training Accuracy: 30.7572%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [131/225], Training Accuracy: 30.7252%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [132/225], Training Accuracy: 30.7055%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [133/225], Training Accuracy: 30.7213%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [134/225], Training Accuracy: 30.7952%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [135/225], Training Accuracy: 30.8102%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [136/225], Training Accuracy: 30.7790%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [137/225], Training Accuracy: 30.8280%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [138/225], Training Accuracy: 30.8424%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [139/225], Training Accuracy: 30.8228%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [140/225], Training Accuracy: 30.8371%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [141/225], Training Accuracy: 30.7735%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [142/225], Training Accuracy: 30.8429%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [143/225], Training Accuracy: 30.8239%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [144/225], Training Accuracy: 30.8377%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [145/225], Training Accuracy: 30.8944%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [146/225], Training Accuracy: 30.9503%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [147/225], Training Accuracy: 30.9630%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [148/225], Training Accuracy: 30.9333%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [149/225], Training Accuracy: 30.9564%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [150/225], Training Accuracy: 31.0000%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [151/225], Training Accuracy: 31.0534%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [152/225], Training Accuracy: 31.0547%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [153/225], Training Accuracy: 31.0560%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [154/225], Training Accuracy: 31.0775%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [155/225], Training Accuracy: 31.0685%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [156/225], Training Accuracy: 31.1198%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [157/225], Training Accuracy: 31.0410%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [158/225], Training Accuracy: 31.0225%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [159/225], Training Accuracy: 31.1026%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [160/225], Training Accuracy: 31.0840%, Training Loss: 0.6858%\n",
      "Epoch [66/100], Step [161/225], Training Accuracy: 31.1335%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [162/225], Training Accuracy: 31.1246%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [163/225], Training Accuracy: 31.1733%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [164/225], Training Accuracy: 31.1452%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [165/225], Training Accuracy: 31.0985%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [166/225], Training Accuracy: 31.0617%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [167/225], Training Accuracy: 31.1284%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [168/225], Training Accuracy: 31.0919%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [169/225], Training Accuracy: 31.0281%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [170/225], Training Accuracy: 31.0018%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [171/225], Training Accuracy: 31.0216%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [172/225], Training Accuracy: 31.0320%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [173/225], Training Accuracy: 31.0332%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [174/225], Training Accuracy: 31.0345%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [175/225], Training Accuracy: 31.0714%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [176/225], Training Accuracy: 31.0902%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [177/225], Training Accuracy: 31.0911%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [178/225], Training Accuracy: 31.0569%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [179/225], Training Accuracy: 31.0230%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [180/225], Training Accuracy: 31.0764%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [181/225], Training Accuracy: 31.0256%, Training Loss: 0.6857%\n",
      "Epoch [66/100], Step [182/225], Training Accuracy: 31.0268%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [183/225], Training Accuracy: 31.0365%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [184/225], Training Accuracy: 31.0037%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [185/225], Training Accuracy: 30.9797%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [186/225], Training Accuracy: 31.0148%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Step [187/225], Training Accuracy: 31.0160%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [188/225], Training Accuracy: 30.9924%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [189/225], Training Accuracy: 31.0020%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [190/225], Training Accuracy: 30.9704%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [191/225], Training Accuracy: 30.9473%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [192/225], Training Accuracy: 30.8675%, Training Loss: 0.6856%\n",
      "Epoch [66/100], Step [193/225], Training Accuracy: 30.8776%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [194/225], Training Accuracy: 30.8876%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [195/225], Training Accuracy: 30.8814%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [196/225], Training Accuracy: 30.8514%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [197/225], Training Accuracy: 30.8852%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [198/225], Training Accuracy: 30.9107%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [199/225], Training Accuracy: 30.8967%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [200/225], Training Accuracy: 30.8906%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [201/225], Training Accuracy: 30.9157%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [202/225], Training Accuracy: 30.9174%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [203/225], Training Accuracy: 30.9113%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [204/225], Training Accuracy: 30.9896%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [205/225], Training Accuracy: 30.9909%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [206/225], Training Accuracy: 30.9694%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [207/225], Training Accuracy: 30.9481%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [208/225], Training Accuracy: 30.9721%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [209/225], Training Accuracy: 31.0182%, Training Loss: 0.6855%\n",
      "Epoch [66/100], Step [210/225], Training Accuracy: 31.0417%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [211/225], Training Accuracy: 31.0352%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [212/225], Training Accuracy: 31.0952%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [213/225], Training Accuracy: 31.0739%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [214/225], Training Accuracy: 31.0894%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [215/225], Training Accuracy: 31.0610%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [216/225], Training Accuracy: 30.9968%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [217/225], Training Accuracy: 31.0124%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [218/225], Training Accuracy: 30.9705%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [219/225], Training Accuracy: 31.0288%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [220/225], Training Accuracy: 31.0369%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [221/225], Training Accuracy: 31.0167%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [222/225], Training Accuracy: 31.0177%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [223/225], Training Accuracy: 31.0538%, Training Loss: 0.6853%\n",
      "Epoch [66/100], Step [224/225], Training Accuracy: 31.0477%, Training Loss: 0.6854%\n",
      "Epoch [66/100], Step [225/225], Training Accuracy: 31.0450%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6939%\n",
      "Epoch [67/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6880%\n",
      "Epoch [67/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6904%\n",
      "Epoch [67/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6881%\n",
      "Epoch [67/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6870%\n",
      "Epoch [67/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6879%\n",
      "Epoch [67/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6866%\n",
      "Epoch [67/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6867%\n",
      "Epoch [67/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [12/225], Training Accuracy: 29.8177%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6864%\n",
      "Epoch [67/100], Step [14/225], Training Accuracy: 29.7991%, Training Loss: 0.6873%\n",
      "Epoch [67/100], Step [15/225], Training Accuracy: 30.3125%, Training Loss: 0.6874%\n",
      "Epoch [67/100], Step [16/225], Training Accuracy: 30.1758%, Training Loss: 0.6868%\n",
      "Epoch [67/100], Step [17/225], Training Accuracy: 30.0551%, Training Loss: 0.6869%\n",
      "Epoch [67/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6869%\n",
      "Epoch [67/100], Step [19/225], Training Accuracy: 30.7566%, Training Loss: 0.6870%\n",
      "Epoch [67/100], Step [20/225], Training Accuracy: 31.4062%, Training Loss: 0.6869%\n",
      "Epoch [67/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6866%\n",
      "Epoch [67/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6864%\n",
      "Epoch [67/100], Step [23/225], Training Accuracy: 31.5217%, Training Loss: 0.6864%\n",
      "Epoch [67/100], Step [24/225], Training Accuracy: 31.9010%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [26/225], Training Accuracy: 32.6322%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [27/225], Training Accuracy: 32.3495%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [28/225], Training Accuracy: 32.1429%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [29/225], Training Accuracy: 32.5970%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [30/225], Training Accuracy: 32.6562%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [31/225], Training Accuracy: 32.5101%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [32/225], Training Accuracy: 32.7148%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [33/225], Training Accuracy: 32.8598%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [34/225], Training Accuracy: 32.6287%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [35/225], Training Accuracy: 32.4554%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [36/225], Training Accuracy: 32.2483%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [37/225], Training Accuracy: 32.2213%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [38/225], Training Accuracy: 32.1135%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [39/225], Training Accuracy: 31.8109%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [40/225], Training Accuracy: 31.7188%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [41/225], Training Accuracy: 31.6311%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [42/225], Training Accuracy: 31.4360%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [43/225], Training Accuracy: 31.5407%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [44/225], Training Accuracy: 31.4986%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [46/225], Training Accuracy: 31.4538%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [47/225], Training Accuracy: 31.4162%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [48/225], Training Accuracy: 31.5104%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [49/225], Training Accuracy: 31.5689%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [50/225], Training Accuracy: 31.6250%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [51/225], Training Accuracy: 31.7708%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [52/225], Training Accuracy: 31.8209%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [53/225], Training Accuracy: 31.7512%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [54/225], Training Accuracy: 31.5972%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [55/225], Training Accuracy: 31.7330%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [56/225], Training Accuracy: 31.7801%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [57/225], Training Accuracy: 31.8257%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [58/225], Training Accuracy: 31.7619%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [59/225], Training Accuracy: 32.0975%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [60/225], Training Accuracy: 32.1875%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Step [61/225], Training Accuracy: 32.0953%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [62/225], Training Accuracy: 32.1573%, Training Loss: 0.6853%\n",
      "Epoch [67/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6853%\n",
      "Epoch [67/100], Step [64/225], Training Accuracy: 32.2510%, Training Loss: 0.6853%\n",
      "Epoch [67/100], Step [65/225], Training Accuracy: 32.2115%, Training Loss: 0.6853%\n",
      "Epoch [67/100], Step [66/225], Training Accuracy: 32.2443%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [67/225], Training Accuracy: 32.2528%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [68/225], Training Accuracy: 32.2610%, Training Loss: 0.6853%\n",
      "Epoch [67/100], Step [69/225], Training Accuracy: 32.2464%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [70/225], Training Accuracy: 32.1652%, Training Loss: 0.6852%\n",
      "Epoch [67/100], Step [71/225], Training Accuracy: 32.1303%, Training Loss: 0.6853%\n",
      "Epoch [67/100], Step [72/225], Training Accuracy: 31.8576%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [73/225], Training Accuracy: 31.7637%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [74/225], Training Accuracy: 31.8834%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [76/225], Training Accuracy: 31.8051%, Training Loss: 0.6854%\n",
      "Epoch [67/100], Step [77/225], Training Accuracy: 31.7370%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [78/225], Training Accuracy: 31.7708%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [79/225], Training Accuracy: 31.7049%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [80/225], Training Accuracy: 31.6992%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [81/225], Training Accuracy: 31.6165%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [82/225], Training Accuracy: 31.5930%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [83/225], Training Accuracy: 31.4947%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [84/225], Training Accuracy: 31.5290%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [85/225], Training Accuracy: 31.5074%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [86/225], Training Accuracy: 31.5407%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [87/225], Training Accuracy: 31.5553%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [88/225], Training Accuracy: 31.5163%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [89/225], Training Accuracy: 31.4256%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [90/225], Training Accuracy: 31.3368%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [91/225], Training Accuracy: 31.3874%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [92/225], Training Accuracy: 31.3859%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [93/225], Training Accuracy: 31.3844%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [94/225], Training Accuracy: 31.4495%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [95/225], Training Accuracy: 31.3322%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [96/225], Training Accuracy: 31.3965%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [97/225], Training Accuracy: 31.4272%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [98/225], Training Accuracy: 31.4413%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [99/225], Training Accuracy: 31.5657%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [100/225], Training Accuracy: 31.5312%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [101/225], Training Accuracy: 31.6522%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [102/225], Training Accuracy: 31.5564%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [103/225], Training Accuracy: 31.5989%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [104/225], Training Accuracy: 31.5655%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [105/225], Training Accuracy: 31.4732%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [106/225], Training Accuracy: 31.4416%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [107/225], Training Accuracy: 31.3668%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [108/225], Training Accuracy: 31.4091%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [109/225], Training Accuracy: 31.2930%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [110/225], Training Accuracy: 31.3068%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [111/225], Training Accuracy: 31.1937%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [112/225], Training Accuracy: 31.3058%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [113/225], Training Accuracy: 31.2915%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [114/225], Training Accuracy: 31.3322%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [115/225], Training Accuracy: 31.3043%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [116/225], Training Accuracy: 31.3173%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [117/225], Training Accuracy: 31.2767%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [118/225], Training Accuracy: 31.2632%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [119/225], Training Accuracy: 31.2106%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [120/225], Training Accuracy: 31.2630%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [121/225], Training Accuracy: 31.2371%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [122/225], Training Accuracy: 31.2244%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [123/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [124/225], Training Accuracy: 31.2122%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [125/225], Training Accuracy: 31.2125%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [126/225], Training Accuracy: 31.1508%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [127/225], Training Accuracy: 31.1024%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [128/225], Training Accuracy: 31.0913%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [129/225], Training Accuracy: 31.1410%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [130/225], Training Accuracy: 31.0697%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [131/225], Training Accuracy: 31.0353%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [132/225], Training Accuracy: 30.9777%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [133/225], Training Accuracy: 31.0033%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [134/225], Training Accuracy: 31.0751%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [135/225], Training Accuracy: 31.0764%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [136/225], Training Accuracy: 31.1006%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [137/225], Training Accuracy: 31.1245%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [138/225], Training Accuracy: 31.1141%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [139/225], Training Accuracy: 31.0701%, Training Loss: 0.6863%\n",
      "Epoch [67/100], Step [140/225], Training Accuracy: 31.0826%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [141/225], Training Accuracy: 31.0284%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [142/225], Training Accuracy: 31.0739%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [143/225], Training Accuracy: 31.0533%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [144/225], Training Accuracy: 31.0872%, Training Loss: 0.6862%\n",
      "Epoch [67/100], Step [145/225], Training Accuracy: 31.1530%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [146/225], Training Accuracy: 31.1537%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [147/225], Training Accuracy: 31.1756%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [148/225], Training Accuracy: 31.1339%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [149/225], Training Accuracy: 31.1661%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [150/225], Training Accuracy: 31.2083%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [151/225], Training Accuracy: 31.2603%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [152/225], Training Accuracy: 31.2603%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [153/225], Training Accuracy: 31.2296%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [154/225], Training Accuracy: 31.2601%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [155/225], Training Accuracy: 31.2601%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [156/225], Training Accuracy: 31.2901%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [157/225], Training Accuracy: 31.2201%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [158/225], Training Accuracy: 31.1907%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [159/225], Training Accuracy: 31.2598%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6861%\n",
      "Epoch [67/100], Step [161/225], Training Accuracy: 31.2403%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [162/225], Training Accuracy: 31.2114%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [163/225], Training Accuracy: 31.2883%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [164/225], Training Accuracy: 31.2595%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [165/225], Training Accuracy: 31.2027%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [166/225], Training Accuracy: 31.2029%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [167/225], Training Accuracy: 31.2406%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [168/225], Training Accuracy: 31.2035%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [169/225], Training Accuracy: 31.1206%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [170/225], Training Accuracy: 31.0754%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [171/225], Training Accuracy: 31.1038%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100], Step [172/225], Training Accuracy: 31.1228%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [173/225], Training Accuracy: 31.1236%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [174/225], Training Accuracy: 31.1333%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [175/225], Training Accuracy: 31.1786%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [176/225], Training Accuracy: 31.1967%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [177/225], Training Accuracy: 31.1970%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [178/225], Training Accuracy: 31.1973%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [179/225], Training Accuracy: 31.1627%, Training Loss: 0.6860%\n",
      "Epoch [67/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [181/225], Training Accuracy: 31.1637%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [182/225], Training Accuracy: 31.1727%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [183/225], Training Accuracy: 31.1817%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [184/225], Training Accuracy: 31.1481%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [185/225], Training Accuracy: 31.1149%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [186/225], Training Accuracy: 31.1576%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [187/225], Training Accuracy: 31.1414%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [190/225], Training Accuracy: 31.1184%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [191/225], Training Accuracy: 31.0946%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [192/225], Training Accuracy: 31.0303%, Training Loss: 0.6859%\n",
      "Epoch [67/100], Step [193/225], Training Accuracy: 31.0395%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [194/225], Training Accuracy: 31.0486%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [195/225], Training Accuracy: 31.0256%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [196/225], Training Accuracy: 31.0029%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [197/225], Training Accuracy: 31.0359%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [198/225], Training Accuracy: 31.0606%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [199/225], Training Accuracy: 31.0380%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [200/225], Training Accuracy: 31.0234%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [201/225], Training Accuracy: 31.0401%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [202/225], Training Accuracy: 31.0412%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [203/225], Training Accuracy: 31.0422%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [204/225], Training Accuracy: 31.0738%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [205/225], Training Accuracy: 31.0823%, Training Loss: 0.6858%\n",
      "Epoch [67/100], Step [206/225], Training Accuracy: 31.0680%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [207/225], Training Accuracy: 31.0688%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [208/225], Training Accuracy: 31.1073%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [209/225], Training Accuracy: 31.1603%, Training Loss: 0.6857%\n",
      "Epoch [67/100], Step [210/225], Training Accuracy: 31.1830%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [211/225], Training Accuracy: 31.1611%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [212/225], Training Accuracy: 31.1910%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [213/225], Training Accuracy: 31.1693%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [214/225], Training Accuracy: 31.1843%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [215/225], Training Accuracy: 31.1483%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [216/225], Training Accuracy: 31.0836%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [217/225], Training Accuracy: 31.0772%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [218/225], Training Accuracy: 31.0493%, Training Loss: 0.6856%\n",
      "Epoch [67/100], Step [219/225], Training Accuracy: 31.0859%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [220/225], Training Accuracy: 31.1080%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [221/225], Training Accuracy: 31.1015%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [222/225], Training Accuracy: 31.1163%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [223/225], Training Accuracy: 31.1589%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [224/225], Training Accuracy: 31.1523%, Training Loss: 0.6855%\n",
      "Epoch [67/100], Step [225/225], Training Accuracy: 31.1354%, Training Loss: 0.6855%\n",
      "Epoch [68/100], Step [1/225], Training Accuracy: 37.5000%, Training Loss: 0.6906%\n",
      "Epoch [68/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6856%\n",
      "Epoch [68/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6877%\n",
      "Epoch [68/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6858%\n",
      "Epoch [68/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6850%\n",
      "Epoch [68/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [18/225], Training Accuracy: 30.4688%, Training Loss: 0.6855%\n",
      "Epoch [68/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [20/225], Training Accuracy: 31.1719%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [21/225], Training Accuracy: 31.1756%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [22/225], Training Accuracy: 31.3920%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [24/225], Training Accuracy: 31.5755%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [27/225], Training Accuracy: 32.0023%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [30/225], Training Accuracy: 32.1354%, Training Loss: 0.6842%\n",
      "Epoch [68/100], Step [31/225], Training Accuracy: 32.1069%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6842%\n",
      "Epoch [68/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6840%\n",
      "Epoch [68/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6841%\n",
      "Epoch [68/100], Step [35/225], Training Accuracy: 32.1429%, Training Loss: 0.6841%\n",
      "Epoch [68/100], Step [36/225], Training Accuracy: 32.0312%, Training Loss: 0.6842%\n",
      "Epoch [68/100], Step [37/225], Training Accuracy: 32.0524%, Training Loss: 0.6842%\n",
      "Epoch [68/100], Step [38/225], Training Accuracy: 31.8668%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [39/225], Training Accuracy: 31.5705%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [42/225], Training Accuracy: 31.4732%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [43/225], Training Accuracy: 31.5770%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [44/225], Training Accuracy: 31.5696%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [46/225], Training Accuracy: 31.4538%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [47/225], Training Accuracy: 31.3497%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [48/225], Training Accuracy: 31.5104%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [49/225], Training Accuracy: 31.6008%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [50/225], Training Accuracy: 31.6250%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [51/225], Training Accuracy: 31.7402%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [52/225], Training Accuracy: 31.7308%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [53/225], Training Accuracy: 31.6627%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [54/225], Training Accuracy: 31.5972%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [55/225], Training Accuracy: 31.7330%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [56/225], Training Accuracy: 31.8080%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [57/225], Training Accuracy: 31.7982%, Training Loss: 0.6842%\n",
      "Epoch [68/100], Step [58/225], Training Accuracy: 31.7349%, Training Loss: 0.6841%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Step [59/225], Training Accuracy: 32.0710%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [60/225], Training Accuracy: 32.2135%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [61/225], Training Accuracy: 32.1209%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [62/225], Training Accuracy: 32.1825%, Training Loss: 0.6840%\n",
      "Epoch [68/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [64/225], Training Accuracy: 32.2266%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [65/225], Training Accuracy: 32.1635%, Training Loss: 0.6840%\n",
      "Epoch [68/100], Step [66/225], Training Accuracy: 32.2680%, Training Loss: 0.6841%\n",
      "Epoch [68/100], Step [67/225], Training Accuracy: 32.2295%, Training Loss: 0.6841%\n",
      "Epoch [68/100], Step [68/225], Training Accuracy: 32.3529%, Training Loss: 0.6841%\n",
      "Epoch [68/100], Step [69/225], Training Accuracy: 32.3822%, Training Loss: 0.6839%\n",
      "Epoch [68/100], Step [70/225], Training Accuracy: 32.3438%, Training Loss: 0.6840%\n",
      "Epoch [68/100], Step [71/225], Training Accuracy: 32.3504%, Training Loss: 0.6840%\n",
      "Epoch [68/100], Step [72/225], Training Accuracy: 32.0964%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [73/225], Training Accuracy: 32.0634%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [74/225], Training Accuracy: 32.1157%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [75/225], Training Accuracy: 32.0833%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [76/225], Training Accuracy: 32.0312%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [77/225], Training Accuracy: 31.9196%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [78/225], Training Accuracy: 31.9912%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [79/225], Training Accuracy: 31.8829%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [80/225], Training Accuracy: 31.8750%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [81/225], Training Accuracy: 31.7901%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [82/225], Training Accuracy: 31.7835%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [83/225], Training Accuracy: 31.7018%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [84/225], Training Accuracy: 31.7522%, Training Loss: 0.6842%\n",
      "Epoch [68/100], Step [85/225], Training Accuracy: 31.6728%, Training Loss: 0.6843%\n",
      "Epoch [68/100], Step [86/225], Training Accuracy: 31.7224%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [87/225], Training Accuracy: 31.7170%, Training Loss: 0.6844%\n",
      "Epoch [68/100], Step [88/225], Training Accuracy: 31.6584%, Training Loss: 0.6845%\n",
      "Epoch [68/100], Step [89/225], Training Accuracy: 31.5660%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [90/225], Training Accuracy: 31.4583%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [91/225], Training Accuracy: 31.4560%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [92/225], Training Accuracy: 31.4198%, Training Loss: 0.6846%\n",
      "Epoch [68/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [94/225], Training Accuracy: 31.4162%, Training Loss: 0.6847%\n",
      "Epoch [68/100], Step [95/225], Training Accuracy: 31.3158%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [96/225], Training Accuracy: 31.4128%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [97/225], Training Accuracy: 31.4272%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [98/225], Training Accuracy: 31.4573%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [99/225], Training Accuracy: 31.5814%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [100/225], Training Accuracy: 31.5469%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [101/225], Training Accuracy: 31.6677%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [102/225], Training Accuracy: 31.5564%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [103/225], Training Accuracy: 31.5837%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [104/225], Training Accuracy: 31.5655%, Training Loss: 0.6850%\n",
      "Epoch [68/100], Step [105/225], Training Accuracy: 31.5030%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [106/225], Training Accuracy: 31.5153%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [107/225], Training Accuracy: 31.4398%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [108/225], Training Accuracy: 31.4815%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [109/225], Training Accuracy: 31.3647%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [110/225], Training Accuracy: 31.3778%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [111/225], Training Accuracy: 31.2922%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [112/225], Training Accuracy: 31.3756%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [113/225], Training Accuracy: 31.3606%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [114/225], Training Accuracy: 31.4556%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [115/225], Training Accuracy: 31.4266%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [116/225], Training Accuracy: 31.4386%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [117/225], Training Accuracy: 31.3568%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [118/225], Training Accuracy: 31.3427%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [119/225], Training Accuracy: 31.3157%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [120/225], Training Accuracy: 31.3802%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [121/225], Training Accuracy: 31.3662%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [122/225], Training Accuracy: 31.3909%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [123/225], Training Accuracy: 31.3897%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [124/225], Training Accuracy: 31.3886%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [125/225], Training Accuracy: 31.3750%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [126/225], Training Accuracy: 31.2996%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [127/225], Training Accuracy: 31.2377%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [128/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [129/225], Training Accuracy: 31.2621%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [130/225], Training Accuracy: 31.2260%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [131/225], Training Accuracy: 31.2023%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [132/225], Training Accuracy: 31.1790%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [133/225], Training Accuracy: 31.2148%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [134/225], Training Accuracy: 31.2267%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [135/225], Training Accuracy: 31.2153%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [136/225], Training Accuracy: 31.2385%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [137/225], Training Accuracy: 31.2728%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [138/225], Training Accuracy: 31.2840%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [139/225], Training Accuracy: 31.2612%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [140/225], Training Accuracy: 31.2165%, Training Loss: 0.6855%\n",
      "Epoch [68/100], Step [141/225], Training Accuracy: 31.1724%, Training Loss: 0.6855%\n",
      "Epoch [68/100], Step [142/225], Training Accuracy: 31.2280%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [143/225], Training Accuracy: 31.2281%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [144/225], Training Accuracy: 31.2283%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [145/225], Training Accuracy: 31.3039%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [146/225], Training Accuracy: 31.3142%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [147/225], Training Accuracy: 31.3457%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [148/225], Training Accuracy: 31.3345%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [149/225], Training Accuracy: 31.3444%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [150/225], Training Accuracy: 31.3750%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [151/225], Training Accuracy: 31.4156%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [152/225], Training Accuracy: 31.4248%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [153/225], Training Accuracy: 31.4236%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [154/225], Training Accuracy: 31.4326%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [155/225], Training Accuracy: 31.4113%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [156/225], Training Accuracy: 31.4203%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [157/225], Training Accuracy: 31.3595%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Step [158/225], Training Accuracy: 31.3192%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [159/225], Training Accuracy: 31.3876%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [160/225], Training Accuracy: 31.3672%, Training Loss: 0.6855%\n",
      "Epoch [68/100], Step [161/225], Training Accuracy: 31.4344%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [162/225], Training Accuracy: 31.4140%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [163/225], Training Accuracy: 31.4609%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [164/225], Training Accuracy: 31.4310%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [165/225], Training Accuracy: 31.3731%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [166/225], Training Accuracy: 31.3724%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [167/225], Training Accuracy: 31.4184%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [168/225], Training Accuracy: 31.3709%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [169/225], Training Accuracy: 31.2962%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [170/225], Training Accuracy: 31.2408%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [171/225], Training Accuracy: 31.2591%, Training Loss: 0.6854%\n",
      "Epoch [68/100], Step [172/225], Training Accuracy: 31.2773%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [173/225], Training Accuracy: 31.2590%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [174/225], Training Accuracy: 31.2769%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [175/225], Training Accuracy: 31.3125%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [176/225], Training Accuracy: 31.3565%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [177/225], Training Accuracy: 31.3471%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [178/225], Training Accuracy: 31.3290%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [179/225], Training Accuracy: 31.3111%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [180/225], Training Accuracy: 31.3455%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [181/225], Training Accuracy: 31.3018%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [182/225], Training Accuracy: 31.2672%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [183/225], Training Accuracy: 31.2671%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [184/225], Training Accuracy: 31.2330%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [185/225], Training Accuracy: 31.2247%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [186/225], Training Accuracy: 31.2248%, Training Loss: 0.6853%\n",
      "Epoch [68/100], Step [187/225], Training Accuracy: 31.2416%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [188/225], Training Accuracy: 31.2168%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [189/225], Training Accuracy: 31.2417%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [190/225], Training Accuracy: 31.2007%, Training Loss: 0.6852%\n",
      "Epoch [68/100], Step [191/225], Training Accuracy: 31.1764%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [192/225], Training Accuracy: 31.1035%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [193/225], Training Accuracy: 31.1124%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [195/225], Training Accuracy: 31.1298%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [196/225], Training Accuracy: 31.1065%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [197/225], Training Accuracy: 31.1548%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [198/225], Training Accuracy: 31.1711%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [199/225], Training Accuracy: 31.1636%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [200/225], Training Accuracy: 31.1484%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [201/225], Training Accuracy: 31.1645%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [202/225], Training Accuracy: 31.1649%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [203/225], Training Accuracy: 31.1499%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [204/225], Training Accuracy: 31.1734%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [205/225], Training Accuracy: 31.1814%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [206/225], Training Accuracy: 31.1742%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [207/225], Training Accuracy: 31.1594%, Training Loss: 0.6851%\n",
      "Epoch [68/100], Step [208/225], Training Accuracy: 31.1824%, Training Loss: 0.6850%\n",
      "Epoch [68/100], Step [209/225], Training Accuracy: 31.2276%, Training Loss: 0.6850%\n",
      "Epoch [68/100], Step [210/225], Training Accuracy: 31.2426%, Training Loss: 0.6850%\n",
      "Epoch [68/100], Step [211/225], Training Accuracy: 31.2204%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [212/225], Training Accuracy: 31.2721%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [213/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [214/225], Training Accuracy: 31.2792%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [215/225], Training Accuracy: 31.2427%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [216/225], Training Accuracy: 31.1777%, Training Loss: 0.6850%\n",
      "Epoch [68/100], Step [217/225], Training Accuracy: 31.1780%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [218/225], Training Accuracy: 31.1497%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [219/225], Training Accuracy: 31.2143%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [220/225], Training Accuracy: 31.2287%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [221/225], Training Accuracy: 31.2217%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [222/225], Training Accuracy: 31.2289%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [223/225], Training Accuracy: 31.2710%, Training Loss: 0.6848%\n",
      "Epoch [68/100], Step [224/225], Training Accuracy: 31.2709%, Training Loss: 0.6849%\n",
      "Epoch [68/100], Step [225/225], Training Accuracy: 31.2535%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6948%\n",
      "Epoch [69/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6879%\n",
      "Epoch [69/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6896%\n",
      "Epoch [69/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6863%\n",
      "Epoch [69/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6863%\n",
      "Epoch [69/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [8/225], Training Accuracy: 32.0312%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6862%\n",
      "Epoch [69/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6858%\n",
      "Epoch [69/100], Step [11/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [69/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [13/225], Training Accuracy: 30.4087%, Training Loss: 0.6860%\n",
      "Epoch [69/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6866%\n",
      "Epoch [69/100], Step [15/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [69/100], Step [16/225], Training Accuracy: 31.3477%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [17/225], Training Accuracy: 30.9743%, Training Loss: 0.6859%\n",
      "Epoch [69/100], Step [18/225], Training Accuracy: 31.0764%, Training Loss: 0.6860%\n",
      "Epoch [69/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6860%\n",
      "Epoch [69/100], Step [20/225], Training Accuracy: 31.7188%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [22/225], Training Accuracy: 31.7472%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [23/225], Training Accuracy: 31.5897%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [24/225], Training Accuracy: 31.9010%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [25/225], Training Accuracy: 32.0000%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [26/225], Training Accuracy: 32.3317%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [27/225], Training Accuracy: 32.1759%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [29/225], Training Accuracy: 32.2737%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [30/225], Training Accuracy: 32.3438%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [31/225], Training Accuracy: 32.2077%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [32/225], Training Accuracy: 32.3730%, Training Loss: 0.6843%\n",
      "Epoch [69/100], Step [33/225], Training Accuracy: 32.4811%, Training Loss: 0.6842%\n",
      "Epoch [69/100], Step [34/225], Training Accuracy: 32.3529%, Training Loss: 0.6842%\n",
      "Epoch [69/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6843%\n",
      "Epoch [69/100], Step [36/225], Training Accuracy: 32.1181%, Training Loss: 0.6844%\n",
      "Epoch [69/100], Step [37/225], Training Accuracy: 32.1368%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [38/225], Training Accuracy: 31.9901%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [40/225], Training Accuracy: 31.6797%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [41/225], Training Accuracy: 31.7454%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100], Step [42/225], Training Accuracy: 31.6592%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [43/225], Training Accuracy: 31.9041%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [44/225], Training Accuracy: 31.8537%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [45/225], Training Accuracy: 31.8750%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [46/225], Training Accuracy: 31.7255%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [47/225], Training Accuracy: 31.6157%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [49/225], Training Accuracy: 31.7921%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [50/225], Training Accuracy: 31.7188%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [51/225], Training Accuracy: 31.8627%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [52/225], Training Accuracy: 31.8810%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [53/225], Training Accuracy: 31.7807%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [55/225], Training Accuracy: 31.7614%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [56/225], Training Accuracy: 31.8359%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [57/225], Training Accuracy: 31.8257%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [58/225], Training Accuracy: 31.7619%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [59/225], Training Accuracy: 32.0975%, Training Loss: 0.6845%\n",
      "Epoch [69/100], Step [60/225], Training Accuracy: 32.1875%, Training Loss: 0.6845%\n",
      "Epoch [69/100], Step [61/225], Training Accuracy: 32.1209%, Training Loss: 0.6845%\n",
      "Epoch [69/100], Step [62/225], Training Accuracy: 32.1069%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [63/225], Training Accuracy: 32.1429%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [64/225], Training Accuracy: 32.1289%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [65/225], Training Accuracy: 32.0433%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [66/225], Training Accuracy: 32.1259%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [67/225], Training Accuracy: 32.1129%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [68/225], Training Accuracy: 32.1691%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [69/225], Training Accuracy: 32.1105%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [70/225], Training Accuracy: 32.0536%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [71/225], Training Accuracy: 32.1303%, Training Loss: 0.6846%\n",
      "Epoch [69/100], Step [72/225], Training Accuracy: 31.9444%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [73/225], Training Accuracy: 31.8707%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [74/225], Training Accuracy: 31.9890%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [75/225], Training Accuracy: 31.9792%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [76/225], Training Accuracy: 31.9696%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [77/225], Training Accuracy: 31.8994%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [78/225], Training Accuracy: 31.9311%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [79/225], Training Accuracy: 31.8236%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [80/225], Training Accuracy: 31.8555%, Training Loss: 0.6847%\n",
      "Epoch [69/100], Step [81/225], Training Accuracy: 31.7323%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [82/225], Training Accuracy: 31.7454%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [83/225], Training Accuracy: 31.7018%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [84/225], Training Accuracy: 31.7336%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [85/225], Training Accuracy: 31.7096%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [86/225], Training Accuracy: 31.7042%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [87/225], Training Accuracy: 31.7170%, Training Loss: 0.6848%\n",
      "Epoch [69/100], Step [88/225], Training Accuracy: 31.6939%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [89/225], Training Accuracy: 31.6187%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [90/225], Training Accuracy: 31.5278%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [91/225], Training Accuracy: 31.5247%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [92/225], Training Accuracy: 31.4878%, Training Loss: 0.6850%\n",
      "Epoch [69/100], Step [93/225], Training Accuracy: 31.4684%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [94/225], Training Accuracy: 31.5492%, Training Loss: 0.6849%\n",
      "Epoch [69/100], Step [95/225], Training Accuracy: 31.4309%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [96/225], Training Accuracy: 31.5267%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [97/225], Training Accuracy: 31.5399%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [98/225], Training Accuracy: 31.5370%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [99/225], Training Accuracy: 31.6446%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [100/225], Training Accuracy: 31.6250%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [101/225], Training Accuracy: 31.7605%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [102/225], Training Accuracy: 31.6636%, Training Loss: 0.6851%\n",
      "Epoch [69/100], Step [103/225], Training Accuracy: 31.7354%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [104/225], Training Accuracy: 31.7007%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [105/225], Training Accuracy: 31.6667%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [106/225], Training Accuracy: 31.6333%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [107/225], Training Accuracy: 31.5275%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [109/225], Training Accuracy: 31.4077%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [110/225], Training Accuracy: 31.3920%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [111/225], Training Accuracy: 31.3204%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [112/225], Training Accuracy: 31.3616%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [113/225], Training Accuracy: 31.3744%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [114/225], Training Accuracy: 31.4556%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [115/225], Training Accuracy: 31.4266%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [116/225], Training Accuracy: 31.4251%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [117/225], Training Accuracy: 31.3702%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [118/225], Training Accuracy: 31.3559%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [119/225], Training Accuracy: 31.3025%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [120/225], Training Accuracy: 31.3411%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [121/225], Training Accuracy: 31.3146%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [122/225], Training Accuracy: 31.3268%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [123/225], Training Accuracy: 31.3516%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [124/225], Training Accuracy: 31.3382%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [125/225], Training Accuracy: 31.3000%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [126/225], Training Accuracy: 31.2376%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [127/225], Training Accuracy: 31.1639%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [128/225], Training Accuracy: 31.1523%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [129/225], Training Accuracy: 31.1894%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [130/225], Training Accuracy: 31.1418%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [131/225], Training Accuracy: 31.1188%, Training Loss: 0.6858%\n",
      "Epoch [69/100], Step [132/225], Training Accuracy: 31.1080%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [133/225], Training Accuracy: 31.1325%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [134/225], Training Accuracy: 31.1917%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [135/225], Training Accuracy: 31.2269%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [136/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [137/225], Training Accuracy: 31.2956%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [138/225], Training Accuracy: 31.2953%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [139/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [69/100], Step [140/225], Training Accuracy: 31.2054%, Training Loss: 0.6858%\n",
      "Epoch [69/100], Step [141/225], Training Accuracy: 31.1613%, Training Loss: 0.6858%\n",
      "Epoch [69/100], Step [142/225], Training Accuracy: 31.2170%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [143/225], Training Accuracy: 31.2281%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [144/225], Training Accuracy: 31.2609%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [145/225], Training Accuracy: 31.3362%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [146/225], Training Accuracy: 31.3570%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [147/225], Training Accuracy: 31.3776%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100], Step [148/225], Training Accuracy: 31.3450%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [149/225], Training Accuracy: 31.3758%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [150/225], Training Accuracy: 31.3958%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [151/225], Training Accuracy: 31.4570%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [152/225], Training Accuracy: 31.4453%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [153/225], Training Accuracy: 31.4032%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [154/225], Training Accuracy: 31.4022%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [155/225], Training Accuracy: 31.3911%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [156/225], Training Accuracy: 31.4203%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [157/225], Training Accuracy: 31.3396%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [158/225], Training Accuracy: 31.3093%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [159/225], Training Accuracy: 31.3483%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [160/225], Training Accuracy: 31.3184%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [161/225], Training Accuracy: 31.3665%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [162/225], Training Accuracy: 31.3368%, Training Loss: 0.6857%\n",
      "Epoch [69/100], Step [163/225], Training Accuracy: 31.4034%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [164/225], Training Accuracy: 31.3929%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [165/225], Training Accuracy: 31.3163%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [166/225], Training Accuracy: 31.3253%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [167/225], Training Accuracy: 31.3623%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [168/225], Training Accuracy: 31.3058%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [169/225], Training Accuracy: 31.2130%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [170/225], Training Accuracy: 31.1673%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [171/225], Training Accuracy: 31.1952%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [172/225], Training Accuracy: 31.2046%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [173/225], Training Accuracy: 31.2139%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [174/225], Training Accuracy: 31.2141%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [175/225], Training Accuracy: 31.2411%, Training Loss: 0.6856%\n",
      "Epoch [69/100], Step [176/225], Training Accuracy: 31.2766%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [177/225], Training Accuracy: 31.2677%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [178/225], Training Accuracy: 31.2588%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [179/225], Training Accuracy: 31.2413%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [180/225], Training Accuracy: 31.2760%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [181/225], Training Accuracy: 31.2327%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [182/225], Training Accuracy: 31.2071%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [183/225], Training Accuracy: 31.2244%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [184/225], Training Accuracy: 31.1990%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [185/225], Training Accuracy: 31.1571%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [186/225], Training Accuracy: 31.1828%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [187/225], Training Accuracy: 31.1915%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [188/225], Training Accuracy: 31.1835%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [189/225], Training Accuracy: 31.2252%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [190/225], Training Accuracy: 31.1760%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [191/225], Training Accuracy: 31.1437%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [192/225], Training Accuracy: 31.0710%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [193/225], Training Accuracy: 31.0881%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [194/225], Training Accuracy: 31.0889%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [195/225], Training Accuracy: 31.0657%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [196/225], Training Accuracy: 31.0348%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [197/225], Training Accuracy: 31.0676%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [198/225], Training Accuracy: 31.0922%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [199/225], Training Accuracy: 31.0537%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [200/225], Training Accuracy: 31.0391%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [201/225], Training Accuracy: 31.0479%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [202/225], Training Accuracy: 31.0412%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [203/225], Training Accuracy: 31.0268%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [204/225], Training Accuracy: 31.0738%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [205/225], Training Accuracy: 31.0899%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6855%\n",
      "Epoch [69/100], Step [207/225], Training Accuracy: 31.0537%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [208/225], Training Accuracy: 31.0998%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [209/225], Training Accuracy: 31.1603%, Training Loss: 0.6854%\n",
      "Epoch [69/100], Step [210/225], Training Accuracy: 31.1979%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [211/225], Training Accuracy: 31.1908%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [213/225], Training Accuracy: 31.2060%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [214/225], Training Accuracy: 31.2354%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [215/225], Training Accuracy: 31.2064%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [216/225], Training Accuracy: 31.1415%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [217/225], Training Accuracy: 31.1492%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [218/225], Training Accuracy: 31.1138%, Training Loss: 0.6853%\n",
      "Epoch [69/100], Step [219/225], Training Accuracy: 31.1787%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [220/225], Training Accuracy: 31.2074%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [221/225], Training Accuracy: 31.1934%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [222/225], Training Accuracy: 31.2078%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [223/225], Training Accuracy: 31.2570%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [224/225], Training Accuracy: 31.2570%, Training Loss: 0.6852%\n",
      "Epoch [69/100], Step [225/225], Training Accuracy: 31.2396%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6938%\n",
      "Epoch [70/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6884%\n",
      "Epoch [70/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6909%\n",
      "Epoch [70/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6884%\n",
      "Epoch [70/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6886%\n",
      "Epoch [70/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6883%\n",
      "Epoch [70/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6866%\n",
      "Epoch [70/100], Step [8/225], Training Accuracy: 32.0312%, Training Loss: 0.6863%\n",
      "Epoch [70/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6872%\n",
      "Epoch [70/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6864%\n",
      "Epoch [70/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6859%\n",
      "Epoch [70/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6861%\n",
      "Epoch [70/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6864%\n",
      "Epoch [70/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6871%\n",
      "Epoch [70/100], Step [15/225], Training Accuracy: 30.8333%, Training Loss: 0.6866%\n",
      "Epoch [70/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6864%\n",
      "Epoch [70/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6866%\n",
      "Epoch [70/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6869%\n",
      "Epoch [70/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6870%\n",
      "Epoch [70/100], Step [20/225], Training Accuracy: 31.2500%, Training Loss: 0.6869%\n",
      "Epoch [70/100], Step [21/225], Training Accuracy: 31.1756%, Training Loss: 0.6867%\n",
      "Epoch [70/100], Step [22/225], Training Accuracy: 31.3210%, Training Loss: 0.6866%\n",
      "Epoch [70/100], Step [23/225], Training Accuracy: 31.1821%, Training Loss: 0.6864%\n",
      "Epoch [70/100], Step [24/225], Training Accuracy: 31.2500%, Training Loss: 0.6866%\n",
      "Epoch [70/100], Step [25/225], Training Accuracy: 31.4375%, Training Loss: 0.6864%\n",
      "Epoch [70/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6862%\n",
      "Epoch [70/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6860%\n",
      "Epoch [70/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6859%\n",
      "Epoch [70/100], Step [29/225], Training Accuracy: 31.8966%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [31/225], Training Accuracy: 31.7540%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [33/225], Training Accuracy: 32.1970%, Training Loss: 0.6848%\n",
      "Epoch [70/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6847%\n",
      "Epoch [70/100], Step [35/225], Training Accuracy: 31.8304%, Training Loss: 0.6848%\n",
      "Epoch [70/100], Step [36/225], Training Accuracy: 31.6406%, Training Loss: 0.6850%\n",
      "Epoch [70/100], Step [37/225], Training Accuracy: 31.7568%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [38/225], Training Accuracy: 31.5378%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [39/225], Training Accuracy: 31.2901%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [40/225], Training Accuracy: 31.3281%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [41/225], Training Accuracy: 31.3643%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [42/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [43/225], Training Accuracy: 31.4317%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [44/225], Training Accuracy: 31.5341%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [45/225], Training Accuracy: 31.5625%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [46/225], Training Accuracy: 31.5557%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [47/225], Training Accuracy: 31.4495%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [48/225], Training Accuracy: 31.5104%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [49/225], Training Accuracy: 31.5689%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [50/225], Training Accuracy: 31.5938%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [51/225], Training Accuracy: 31.8015%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [52/225], Training Accuracy: 31.8209%, Training Loss: 0.6850%\n",
      "Epoch [70/100], Step [53/225], Training Accuracy: 31.7217%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [54/225], Training Accuracy: 31.5683%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [55/225], Training Accuracy: 31.6761%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [56/225], Training Accuracy: 31.7243%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [57/225], Training Accuracy: 31.7434%, Training Loss: 0.6847%\n",
      "Epoch [70/100], Step [58/225], Training Accuracy: 31.6810%, Training Loss: 0.6847%\n",
      "Epoch [70/100], Step [59/225], Training Accuracy: 31.9915%, Training Loss: 0.6845%\n",
      "Epoch [70/100], Step [60/225], Training Accuracy: 32.0833%, Training Loss: 0.6845%\n",
      "Epoch [70/100], Step [61/225], Training Accuracy: 32.0441%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [62/225], Training Accuracy: 32.0565%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [63/225], Training Accuracy: 32.1181%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [64/225], Training Accuracy: 32.1045%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [65/225], Training Accuracy: 31.9952%, Training Loss: 0.6845%\n",
      "Epoch [70/100], Step [66/225], Training Accuracy: 32.0786%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [67/225], Training Accuracy: 32.0896%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [68/225], Training Accuracy: 32.1691%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [69/225], Training Accuracy: 32.1105%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [70/225], Training Accuracy: 32.0536%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [71/225], Training Accuracy: 32.1083%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [72/225], Training Accuracy: 31.8793%, Training Loss: 0.6846%\n",
      "Epoch [70/100], Step [73/225], Training Accuracy: 31.8065%, Training Loss: 0.6845%\n",
      "Epoch [70/100], Step [74/225], Training Accuracy: 31.8834%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [75/225], Training Accuracy: 31.8542%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [76/225], Training Accuracy: 31.8051%, Training Loss: 0.6843%\n",
      "Epoch [70/100], Step [77/225], Training Accuracy: 31.7167%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [78/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [79/225], Training Accuracy: 31.7049%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [80/225], Training Accuracy: 31.7188%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [81/225], Training Accuracy: 31.6551%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [82/225], Training Accuracy: 31.6502%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [83/225], Training Accuracy: 31.5512%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [84/225], Training Accuracy: 31.5848%, Training Loss: 0.6844%\n",
      "Epoch [70/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6845%\n",
      "Epoch [70/100], Step [86/225], Training Accuracy: 31.6497%, Training Loss: 0.6846%\n",
      "Epoch [70/100], Step [87/225], Training Accuracy: 31.6451%, Training Loss: 0.6846%\n",
      "Epoch [70/100], Step [88/225], Training Accuracy: 31.5696%, Training Loss: 0.6847%\n",
      "Epoch [70/100], Step [89/225], Training Accuracy: 31.4958%, Training Loss: 0.6848%\n",
      "Epoch [70/100], Step [90/225], Training Accuracy: 31.3889%, Training Loss: 0.6848%\n",
      "Epoch [70/100], Step [91/225], Training Accuracy: 31.4560%, Training Loss: 0.6848%\n",
      "Epoch [70/100], Step [92/225], Training Accuracy: 31.4198%, Training Loss: 0.6848%\n",
      "Epoch [70/100], Step [93/225], Training Accuracy: 31.4012%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [94/225], Training Accuracy: 31.5326%, Training Loss: 0.6849%\n",
      "Epoch [70/100], Step [95/225], Training Accuracy: 31.4145%, Training Loss: 0.6850%\n",
      "Epoch [70/100], Step [96/225], Training Accuracy: 31.5104%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [97/225], Training Accuracy: 31.5238%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [98/225], Training Accuracy: 31.5529%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [100/225], Training Accuracy: 31.5938%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [101/225], Training Accuracy: 31.7296%, Training Loss: 0.6850%\n",
      "Epoch [70/100], Step [102/225], Training Accuracy: 31.6023%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [103/225], Training Accuracy: 31.6444%, Training Loss: 0.6851%\n",
      "Epoch [70/100], Step [104/225], Training Accuracy: 31.6106%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [105/225], Training Accuracy: 31.5476%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [106/225], Training Accuracy: 31.5301%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [107/225], Training Accuracy: 31.4544%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [109/225], Training Accuracy: 31.3790%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [110/225], Training Accuracy: 31.4062%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [111/225], Training Accuracy: 31.3063%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [112/225], Training Accuracy: 31.3616%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [113/225], Training Accuracy: 31.3191%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [114/225], Training Accuracy: 31.3871%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [115/225], Training Accuracy: 31.3587%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [116/225], Training Accuracy: 31.3578%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [117/225], Training Accuracy: 31.2901%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [118/225], Training Accuracy: 31.2368%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [119/225], Training Accuracy: 31.1712%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [120/225], Training Accuracy: 31.2109%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [121/225], Training Accuracy: 31.2629%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [122/225], Training Accuracy: 31.3012%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [123/225], Training Accuracy: 31.3262%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [124/225], Training Accuracy: 31.3130%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [125/225], Training Accuracy: 31.2875%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [126/225], Training Accuracy: 31.2128%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [127/225], Training Accuracy: 31.1393%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [128/225], Training Accuracy: 31.1279%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [129/225], Training Accuracy: 31.1773%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [130/225], Training Accuracy: 31.1178%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [131/225], Training Accuracy: 31.0830%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [132/225], Training Accuracy: 31.0606%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [133/225], Training Accuracy: 31.0855%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [134/225], Training Accuracy: 31.0984%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Step [135/225], Training Accuracy: 31.1227%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [136/225], Training Accuracy: 31.1236%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [137/225], Training Accuracy: 31.1588%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [138/225], Training Accuracy: 31.1594%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [139/225], Training Accuracy: 31.1376%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [140/225], Training Accuracy: 31.1161%, Training Loss: 0.6859%\n",
      "Epoch [70/100], Step [141/225], Training Accuracy: 31.0616%, Training Loss: 0.6858%\n",
      "Epoch [70/100], Step [142/225], Training Accuracy: 31.1180%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [143/225], Training Accuracy: 31.1189%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [144/225], Training Accuracy: 31.1523%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [145/225], Training Accuracy: 31.2284%, Training Loss: 0.6857%\n",
      "Epoch [70/100], Step [146/225], Training Accuracy: 31.2607%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [147/225], Training Accuracy: 31.2925%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [148/225], Training Accuracy: 31.2817%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [149/225], Training Accuracy: 31.2919%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [150/225], Training Accuracy: 31.3333%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [151/225], Training Accuracy: 31.3638%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [152/225], Training Accuracy: 31.3734%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [153/225], Training Accuracy: 31.3113%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [154/225], Training Accuracy: 31.3413%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [155/225], Training Accuracy: 31.3206%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [156/225], Training Accuracy: 31.3401%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [157/225], Training Accuracy: 31.2898%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [158/225], Training Accuracy: 31.2797%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [159/225], Training Accuracy: 31.3679%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [160/225], Training Accuracy: 31.3379%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [161/225], Training Accuracy: 31.4053%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [162/225], Training Accuracy: 31.3850%, Training Loss: 0.6856%\n",
      "Epoch [70/100], Step [163/225], Training Accuracy: 31.4609%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [164/225], Training Accuracy: 31.4501%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [165/225], Training Accuracy: 31.3731%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [166/225], Training Accuracy: 31.3630%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [167/225], Training Accuracy: 31.3997%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [168/225], Training Accuracy: 31.3616%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [169/225], Training Accuracy: 31.2592%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [170/225], Training Accuracy: 31.2132%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [171/225], Training Accuracy: 31.2135%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [172/225], Training Accuracy: 31.2227%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [173/225], Training Accuracy: 31.1958%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [174/225], Training Accuracy: 31.2141%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [175/225], Training Accuracy: 31.2589%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [176/225], Training Accuracy: 31.2855%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [177/225], Training Accuracy: 31.3030%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [178/225], Training Accuracy: 31.3027%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [179/225], Training Accuracy: 31.2675%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [180/225], Training Accuracy: 31.2934%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [181/225], Training Accuracy: 31.2414%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [182/225], Training Accuracy: 31.2328%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [183/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [184/225], Training Accuracy: 31.2245%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [185/225], Training Accuracy: 31.1824%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [186/225], Training Accuracy: 31.2080%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [187/225], Training Accuracy: 31.2082%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [188/225], Training Accuracy: 31.2334%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [189/225], Training Accuracy: 31.2665%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [190/225], Training Accuracy: 31.2007%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [191/225], Training Accuracy: 31.1764%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [192/225], Training Accuracy: 31.1117%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [193/225], Training Accuracy: 31.1205%, Training Loss: 0.6855%\n",
      "Epoch [70/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [195/225], Training Accuracy: 31.1218%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [196/225], Training Accuracy: 31.1065%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [197/225], Training Accuracy: 31.1469%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [198/225], Training Accuracy: 31.1632%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [199/225], Training Accuracy: 31.1322%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [200/225], Training Accuracy: 31.1094%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [201/225], Training Accuracy: 31.1101%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [202/225], Training Accuracy: 31.1185%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [203/225], Training Accuracy: 31.1038%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [204/225], Training Accuracy: 31.1581%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [205/225], Training Accuracy: 31.1509%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [206/225], Training Accuracy: 31.1286%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [207/225], Training Accuracy: 31.0990%, Training Loss: 0.6854%\n",
      "Epoch [70/100], Step [208/225], Training Accuracy: 31.1298%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [209/225], Training Accuracy: 31.1752%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [210/225], Training Accuracy: 31.1905%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [211/225], Training Accuracy: 31.1685%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [213/225], Training Accuracy: 31.1913%, Training Loss: 0.6853%\n",
      "Epoch [70/100], Step [214/225], Training Accuracy: 31.1989%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [215/225], Training Accuracy: 31.1701%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [216/225], Training Accuracy: 31.0981%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [217/225], Training Accuracy: 31.0988%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [218/225], Training Accuracy: 31.0636%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [219/225], Training Accuracy: 31.1358%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [220/225], Training Accuracy: 31.1577%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [221/225], Training Accuracy: 31.1439%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [222/225], Training Accuracy: 31.1655%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [223/225], Training Accuracy: 31.2150%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [224/225], Training Accuracy: 31.2151%, Training Loss: 0.6852%\n",
      "Epoch [70/100], Step [225/225], Training Accuracy: 31.1840%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [1/225], Training Accuracy: 29.6875%, Training Loss: 0.6913%\n",
      "Epoch [71/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6859%\n",
      "Epoch [71/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6875%\n",
      "Epoch [71/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6860%\n",
      "Epoch [71/100], Step [7/225], Training Accuracy: 31.0268%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [8/225], Training Accuracy: 30.6641%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [10/225], Training Accuracy: 30.3125%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [11/225], Training Accuracy: 30.3977%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [14/225], Training Accuracy: 29.6875%, Training Loss: 0.6864%\n",
      "Epoch [71/100], Step [15/225], Training Accuracy: 30.5208%, Training Loss: 0.6861%\n",
      "Epoch [71/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6862%\n",
      "Epoch [71/100], Step [18/225], Training Accuracy: 30.2951%, Training Loss: 0.6861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6863%\n",
      "Epoch [71/100], Step [20/225], Training Accuracy: 31.1719%, Training Loss: 0.6863%\n",
      "Epoch [71/100], Step [21/225], Training Accuracy: 31.1756%, Training Loss: 0.6861%\n",
      "Epoch [71/100], Step [22/225], Training Accuracy: 31.3920%, Training Loss: 0.6860%\n",
      "Epoch [71/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6860%\n",
      "Epoch [71/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6860%\n",
      "Epoch [71/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6861%\n",
      "Epoch [71/100], Step [26/225], Training Accuracy: 32.2115%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [27/225], Training Accuracy: 31.9444%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [29/225], Training Accuracy: 32.2198%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [30/225], Training Accuracy: 32.2396%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [31/225], Training Accuracy: 32.1573%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [32/225], Training Accuracy: 32.2754%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [36/225], Training Accuracy: 31.9878%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [38/225], Training Accuracy: 31.7845%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [39/225], Training Accuracy: 31.5304%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [40/225], Training Accuracy: 31.5234%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [41/225], Training Accuracy: 31.5549%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [42/225], Training Accuracy: 31.3616%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [43/225], Training Accuracy: 31.5770%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [44/225], Training Accuracy: 31.4986%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [45/225], Training Accuracy: 31.5278%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [46/225], Training Accuracy: 31.4198%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [47/225], Training Accuracy: 31.3830%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [48/225], Training Accuracy: 31.5104%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [49/225], Training Accuracy: 31.6008%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [50/225], Training Accuracy: 31.5938%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [51/225], Training Accuracy: 31.7402%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [52/225], Training Accuracy: 31.7007%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [53/225], Training Accuracy: 31.6038%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [54/225], Training Accuracy: 31.5104%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [55/225], Training Accuracy: 31.5909%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [56/225], Training Accuracy: 31.5848%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [57/225], Training Accuracy: 31.5789%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [58/225], Training Accuracy: 31.4925%, Training Loss: 0.6849%\n",
      "Epoch [71/100], Step [59/225], Training Accuracy: 31.8061%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [60/225], Training Accuracy: 31.9010%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [61/225], Training Accuracy: 31.8648%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [62/225], Training Accuracy: 31.9556%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [63/225], Training Accuracy: 32.0933%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [64/225], Training Accuracy: 32.1045%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [65/225], Training Accuracy: 31.9952%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [66/225], Training Accuracy: 32.1023%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [67/225], Training Accuracy: 32.0896%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [68/225], Training Accuracy: 32.1691%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [69/225], Training Accuracy: 32.1784%, Training Loss: 0.6845%\n",
      "Epoch [71/100], Step [70/225], Training Accuracy: 32.1205%, Training Loss: 0.6846%\n",
      "Epoch [71/100], Step [71/225], Training Accuracy: 32.1303%, Training Loss: 0.6846%\n",
      "Epoch [71/100], Step [72/225], Training Accuracy: 31.9227%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [73/225], Training Accuracy: 31.8707%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [74/225], Training Accuracy: 31.9890%, Training Loss: 0.6846%\n",
      "Epoch [71/100], Step [75/225], Training Accuracy: 31.9583%, Training Loss: 0.6846%\n",
      "Epoch [71/100], Step [76/225], Training Accuracy: 31.9285%, Training Loss: 0.6846%\n",
      "Epoch [71/100], Step [77/225], Training Accuracy: 31.8182%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [78/225], Training Accuracy: 31.8309%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [79/225], Training Accuracy: 31.7445%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [80/225], Training Accuracy: 31.7383%, Training Loss: 0.6846%\n",
      "Epoch [71/100], Step [81/225], Training Accuracy: 31.6551%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [82/225], Training Accuracy: 31.6883%, Training Loss: 0.6847%\n",
      "Epoch [71/100], Step [83/225], Training Accuracy: 31.6077%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [84/225], Training Accuracy: 31.6220%, Training Loss: 0.6848%\n",
      "Epoch [71/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6849%\n",
      "Epoch [71/100], Step [86/225], Training Accuracy: 31.6315%, Training Loss: 0.6849%\n",
      "Epoch [71/100], Step [87/225], Training Accuracy: 31.6631%, Training Loss: 0.6849%\n",
      "Epoch [71/100], Step [88/225], Training Accuracy: 31.6406%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [89/225], Training Accuracy: 31.5836%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [90/225], Training Accuracy: 31.4931%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [91/225], Training Accuracy: 31.5591%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [92/225], Training Accuracy: 31.5217%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [93/225], Training Accuracy: 31.5356%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [94/225], Training Accuracy: 31.6157%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [95/225], Training Accuracy: 31.4967%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [97/225], Training Accuracy: 31.6205%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [98/225], Training Accuracy: 31.6327%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [99/225], Training Accuracy: 31.7708%, Training Loss: 0.6850%\n",
      "Epoch [71/100], Step [100/225], Training Accuracy: 31.7656%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [101/225], Training Accuracy: 31.8688%, Training Loss: 0.6849%\n",
      "Epoch [71/100], Step [102/225], Training Accuracy: 31.7402%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [103/225], Training Accuracy: 31.7809%, Training Loss: 0.6851%\n",
      "Epoch [71/100], Step [104/225], Training Accuracy: 31.7308%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [105/225], Training Accuracy: 31.6964%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [106/225], Training Accuracy: 31.7070%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [107/225], Training Accuracy: 31.6151%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [108/225], Training Accuracy: 31.6985%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [109/225], Training Accuracy: 31.5797%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [110/225], Training Accuracy: 31.5625%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [111/225], Training Accuracy: 31.4752%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [112/225], Training Accuracy: 31.5430%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [113/225], Training Accuracy: 31.5404%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [114/225], Training Accuracy: 31.5927%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [115/225], Training Accuracy: 31.5897%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [116/225], Training Accuracy: 31.5733%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [117/225], Training Accuracy: 31.5171%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Step [118/225], Training Accuracy: 31.4619%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [119/225], Training Accuracy: 31.4338%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [120/225], Training Accuracy: 31.4193%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [121/225], Training Accuracy: 31.4179%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [122/225], Training Accuracy: 31.4293%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [123/225], Training Accuracy: 31.4660%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [124/225], Training Accuracy: 31.4516%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [126/225], Training Accuracy: 31.3492%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [127/225], Training Accuracy: 31.2992%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [128/225], Training Accuracy: 31.3110%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [129/225], Training Accuracy: 31.3711%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [130/225], Training Accuracy: 31.3341%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [131/225], Training Accuracy: 31.2977%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [132/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [133/225], Training Accuracy: 31.2735%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [134/225], Training Accuracy: 31.3316%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [135/225], Training Accuracy: 31.3310%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [136/225], Training Accuracy: 31.3649%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [137/225], Training Accuracy: 31.3869%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [138/225], Training Accuracy: 31.3745%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [139/225], Training Accuracy: 31.3399%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [140/225], Training Accuracy: 31.3281%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [141/225], Training Accuracy: 31.2722%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [142/225], Training Accuracy: 31.3270%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [143/225], Training Accuracy: 31.3265%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [144/225], Training Accuracy: 31.3585%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [145/225], Training Accuracy: 31.4116%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [146/225], Training Accuracy: 31.4319%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [147/225], Training Accuracy: 31.4201%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [148/225], Training Accuracy: 31.4084%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [149/225], Training Accuracy: 31.4283%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [150/225], Training Accuracy: 31.4792%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [151/225], Training Accuracy: 31.5087%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [152/225], Training Accuracy: 31.4967%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [153/225], Training Accuracy: 31.4542%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [154/225], Training Accuracy: 31.4631%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [155/225], Training Accuracy: 31.4415%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [156/225], Training Accuracy: 31.4804%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [157/225], Training Accuracy: 31.4291%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [158/225], Training Accuracy: 31.4181%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [159/225], Training Accuracy: 31.4564%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [160/225], Training Accuracy: 31.4160%, Training Loss: 0.6859%\n",
      "Epoch [71/100], Step [161/225], Training Accuracy: 31.4635%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [162/225], Training Accuracy: 31.4333%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [163/225], Training Accuracy: 31.4992%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [164/225], Training Accuracy: 31.4691%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [165/225], Training Accuracy: 31.4110%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [166/225], Training Accuracy: 31.3912%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [167/225], Training Accuracy: 31.4278%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [168/225], Training Accuracy: 31.3709%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [169/225], Training Accuracy: 31.2870%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [170/225], Training Accuracy: 31.2408%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [171/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [71/100], Step [172/225], Training Accuracy: 31.2682%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [173/225], Training Accuracy: 31.2590%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [174/225], Training Accuracy: 31.2680%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [175/225], Training Accuracy: 31.3125%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [176/225], Training Accuracy: 31.3477%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [177/225], Training Accuracy: 31.3383%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [178/225], Training Accuracy: 31.3202%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [179/225], Training Accuracy: 31.2936%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [180/225], Training Accuracy: 31.3368%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [181/225], Training Accuracy: 31.3018%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [182/225], Training Accuracy: 31.2929%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [183/225], Training Accuracy: 31.3012%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [184/225], Training Accuracy: 31.2840%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [185/225], Training Accuracy: 31.2669%, Training Loss: 0.6857%\n",
      "Epoch [71/100], Step [186/225], Training Accuracy: 31.2752%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [187/225], Training Accuracy: 31.2751%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [188/225], Training Accuracy: 31.2832%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [189/225], Training Accuracy: 31.2996%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [190/225], Training Accuracy: 31.2664%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [191/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [192/225], Training Accuracy: 31.1768%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [193/225], Training Accuracy: 31.1933%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [194/225], Training Accuracy: 31.2017%, Training Loss: 0.6856%\n",
      "Epoch [71/100], Step [195/225], Training Accuracy: 31.1859%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [196/225], Training Accuracy: 31.1464%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [197/225], Training Accuracy: 31.1865%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [198/225], Training Accuracy: 31.2027%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [199/225], Training Accuracy: 31.1872%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [200/225], Training Accuracy: 31.1641%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [201/225], Training Accuracy: 31.1723%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [202/225], Training Accuracy: 31.1804%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [203/225], Training Accuracy: 31.1653%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [204/225], Training Accuracy: 31.2270%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [205/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [206/225], Training Accuracy: 31.2424%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [207/225], Training Accuracy: 31.2123%, Training Loss: 0.6855%\n",
      "Epoch [71/100], Step [208/225], Training Accuracy: 31.2425%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [209/225], Training Accuracy: 31.2949%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [210/225], Training Accuracy: 31.3318%, Training Loss: 0.6854%\n",
      "Epoch [71/100], Step [211/225], Training Accuracy: 31.3166%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [212/225], Training Accuracy: 31.3532%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [213/225], Training Accuracy: 31.3307%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [214/225], Training Accuracy: 31.3303%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [215/225], Training Accuracy: 31.2936%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [216/225], Training Accuracy: 31.2355%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100], Step [217/225], Training Accuracy: 31.2356%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [218/225], Training Accuracy: 31.2070%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [219/225], Training Accuracy: 31.2714%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [220/225], Training Accuracy: 31.2855%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [221/225], Training Accuracy: 31.2641%, Training Loss: 0.6853%\n",
      "Epoch [71/100], Step [222/225], Training Accuracy: 31.2711%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [223/225], Training Accuracy: 31.3131%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [224/225], Training Accuracy: 31.3128%, Training Loss: 0.6852%\n",
      "Epoch [71/100], Step [225/225], Training Accuracy: 31.2952%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6920%\n",
      "Epoch [72/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [3/225], Training Accuracy: 33.3333%, Training Loss: 0.6869%\n",
      "Epoch [72/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [72/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6859%\n",
      "Epoch [72/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [11/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [12/225], Training Accuracy: 30.2083%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6863%\n",
      "Epoch [72/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6862%\n",
      "Epoch [72/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6859%\n",
      "Epoch [72/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6861%\n",
      "Epoch [72/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6864%\n",
      "Epoch [72/100], Step [19/225], Training Accuracy: 30.9211%, Training Loss: 0.6862%\n",
      "Epoch [72/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6861%\n",
      "Epoch [72/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6859%\n",
      "Epoch [72/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [26/225], Training Accuracy: 32.0913%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [27/225], Training Accuracy: 31.8287%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [28/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6851%\n",
      "Epoch [72/100], Step [31/225], Training Accuracy: 31.8548%, Training Loss: 0.6851%\n",
      "Epoch [72/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [33/225], Training Accuracy: 32.2443%, Training Loss: 0.6846%\n",
      "Epoch [72/100], Step [34/225], Training Accuracy: 32.0772%, Training Loss: 0.6847%\n",
      "Epoch [72/100], Step [35/225], Training Accuracy: 31.9643%, Training Loss: 0.6847%\n",
      "Epoch [72/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [37/225], Training Accuracy: 31.8412%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [38/225], Training Accuracy: 31.7023%, Training Loss: 0.6851%\n",
      "Epoch [72/100], Step [39/225], Training Accuracy: 31.3702%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [41/225], Training Accuracy: 31.4787%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [42/225], Training Accuracy: 31.2128%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [43/225], Training Accuracy: 31.3953%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [44/225], Training Accuracy: 31.3210%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [45/225], Training Accuracy: 31.2847%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [46/225], Training Accuracy: 31.1141%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [47/225], Training Accuracy: 31.0505%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [48/225], Training Accuracy: 31.2174%, Training Loss: 0.6851%\n",
      "Epoch [72/100], Step [49/225], Training Accuracy: 31.3138%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [51/225], Training Accuracy: 31.4645%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [52/225], Training Accuracy: 31.4904%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [54/225], Training Accuracy: 31.3079%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [55/225], Training Accuracy: 31.3920%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [56/225], Training Accuracy: 31.5569%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [57/225], Training Accuracy: 31.5789%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [58/225], Training Accuracy: 31.4925%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [59/225], Training Accuracy: 31.8326%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [60/225], Training Accuracy: 31.9531%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [61/225], Training Accuracy: 31.9160%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6846%\n",
      "Epoch [72/100], Step [63/225], Training Accuracy: 31.9444%, Training Loss: 0.6846%\n",
      "Epoch [72/100], Step [64/225], Training Accuracy: 31.9336%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6846%\n",
      "Epoch [72/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6847%\n",
      "Epoch [72/100], Step [67/225], Training Accuracy: 31.8563%, Training Loss: 0.6846%\n",
      "Epoch [72/100], Step [68/225], Training Accuracy: 31.8934%, Training Loss: 0.6846%\n",
      "Epoch [72/100], Step [69/225], Training Accuracy: 31.8614%, Training Loss: 0.6844%\n",
      "Epoch [72/100], Step [70/225], Training Accuracy: 31.7857%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [71/225], Training Accuracy: 31.8442%, Training Loss: 0.6845%\n",
      "Epoch [72/100], Step [72/225], Training Accuracy: 31.6189%, Training Loss: 0.6847%\n",
      "Epoch [72/100], Step [73/225], Training Accuracy: 31.5497%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [74/225], Training Accuracy: 31.6512%, Training Loss: 0.6847%\n",
      "Epoch [72/100], Step [75/225], Training Accuracy: 31.5833%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [76/225], Training Accuracy: 31.5584%, Training Loss: 0.6848%\n",
      "Epoch [72/100], Step [77/225], Training Accuracy: 31.4935%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [78/225], Training Accuracy: 31.5304%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [79/225], Training Accuracy: 31.4676%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [80/225], Training Accuracy: 31.4648%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [81/225], Training Accuracy: 31.3850%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [82/225], Training Accuracy: 31.4024%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [83/225], Training Accuracy: 31.3441%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [84/225], Training Accuracy: 31.4360%, Training Loss: 0.6849%\n",
      "Epoch [72/100], Step [85/225], Training Accuracy: 31.4338%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [86/225], Training Accuracy: 31.4862%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [87/225], Training Accuracy: 31.5014%, Training Loss: 0.6850%\n",
      "Epoch [72/100], Step [88/225], Training Accuracy: 31.4986%, Training Loss: 0.6851%\n",
      "Epoch [72/100], Step [89/225], Training Accuracy: 31.4256%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [90/225], Training Accuracy: 31.3368%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [91/225], Training Accuracy: 31.3874%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Step [92/225], Training Accuracy: 31.3859%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [93/225], Training Accuracy: 31.3844%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [94/225], Training Accuracy: 31.4495%, Training Loss: 0.6852%\n",
      "Epoch [72/100], Step [95/225], Training Accuracy: 31.3487%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [96/225], Training Accuracy: 31.4290%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [97/225], Training Accuracy: 31.4433%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [98/225], Training Accuracy: 31.4254%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [99/225], Training Accuracy: 31.5341%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [100/225], Training Accuracy: 31.4844%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [101/225], Training Accuracy: 31.6213%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [102/225], Training Accuracy: 31.5257%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [103/225], Training Accuracy: 31.5837%, Training Loss: 0.6853%\n",
      "Epoch [72/100], Step [104/225], Training Accuracy: 31.5655%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [105/225], Training Accuracy: 31.5179%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [106/225], Training Accuracy: 31.5006%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [107/225], Training Accuracy: 31.4106%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [108/225], Training Accuracy: 31.4959%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [109/225], Training Accuracy: 31.3503%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [110/225], Training Accuracy: 31.3778%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [111/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [112/225], Training Accuracy: 31.3198%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [113/225], Training Accuracy: 31.3191%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [114/225], Training Accuracy: 31.3596%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [115/225], Training Accuracy: 31.3315%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [116/225], Training Accuracy: 31.3308%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [117/225], Training Accuracy: 31.2767%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [118/225], Training Accuracy: 31.2368%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [119/225], Training Accuracy: 31.1975%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [120/225], Training Accuracy: 31.2240%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [121/225], Training Accuracy: 31.2113%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [122/225], Training Accuracy: 31.1988%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [123/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [124/225], Training Accuracy: 31.2752%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [125/225], Training Accuracy: 31.2625%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [126/225], Training Accuracy: 31.1880%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [127/225], Training Accuracy: 31.1639%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [128/225], Training Accuracy: 31.1523%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [129/225], Training Accuracy: 31.1773%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [130/225], Training Accuracy: 31.1178%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [131/225], Training Accuracy: 31.0711%, Training Loss: 0.6859%\n",
      "Epoch [72/100], Step [132/225], Training Accuracy: 31.0369%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [133/225], Training Accuracy: 31.0620%, Training Loss: 0.6859%\n",
      "Epoch [72/100], Step [134/225], Training Accuracy: 31.1101%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [135/225], Training Accuracy: 31.1227%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [136/225], Training Accuracy: 31.1581%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [137/225], Training Accuracy: 31.1816%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [138/225], Training Accuracy: 31.2047%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [139/225], Training Accuracy: 31.1713%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [140/225], Training Accuracy: 31.1607%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [141/225], Training Accuracy: 31.0949%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [142/225], Training Accuracy: 31.1620%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [143/225], Training Accuracy: 31.1407%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [144/225], Training Accuracy: 31.1523%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [145/225], Training Accuracy: 31.2284%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [146/225], Training Accuracy: 31.2072%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [147/225], Training Accuracy: 31.2287%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [148/225], Training Accuracy: 31.1972%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [149/225], Training Accuracy: 31.2185%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [151/225], Training Accuracy: 31.2707%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [152/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [153/225], Training Accuracy: 31.2194%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [154/225], Training Accuracy: 31.2601%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [155/225], Training Accuracy: 31.2198%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [156/225], Training Accuracy: 31.2300%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [157/225], Training Accuracy: 31.1704%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [158/225], Training Accuracy: 31.1511%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [159/225], Training Accuracy: 31.1812%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [160/225], Training Accuracy: 31.1523%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [161/225], Training Accuracy: 31.2015%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [162/225], Training Accuracy: 31.1728%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [163/225], Training Accuracy: 31.2308%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [164/225], Training Accuracy: 31.2024%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [165/225], Training Accuracy: 31.1364%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [166/225], Training Accuracy: 31.1088%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [167/225], Training Accuracy: 31.1284%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [168/225], Training Accuracy: 31.0919%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [169/225], Training Accuracy: 31.0189%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [170/225], Training Accuracy: 30.9926%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [171/225], Training Accuracy: 31.0033%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [172/225], Training Accuracy: 31.0320%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [173/225], Training Accuracy: 31.0242%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [174/225], Training Accuracy: 31.0165%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [175/225], Training Accuracy: 31.0357%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [176/225], Training Accuracy: 31.0813%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [177/225], Training Accuracy: 31.0823%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [178/225], Training Accuracy: 31.0920%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [179/225], Training Accuracy: 31.0667%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [180/225], Training Accuracy: 31.1024%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [181/225], Training Accuracy: 31.0601%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [182/225], Training Accuracy: 31.0697%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [183/225], Training Accuracy: 31.0707%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [184/225], Training Accuracy: 31.0462%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [185/225], Training Accuracy: 31.0135%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [186/225], Training Accuracy: 31.0400%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [187/225], Training Accuracy: 31.0495%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [188/225], Training Accuracy: 31.0339%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [189/225], Training Accuracy: 31.0351%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [190/225], Training Accuracy: 30.9951%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Step [191/225], Training Accuracy: 30.9637%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [192/225], Training Accuracy: 30.9001%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [193/225], Training Accuracy: 30.9181%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [194/225], Training Accuracy: 30.9359%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [195/225], Training Accuracy: 30.9215%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [196/225], Training Accuracy: 30.8913%, Training Loss: 0.6858%\n",
      "Epoch [72/100], Step [197/225], Training Accuracy: 30.9407%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [198/225], Training Accuracy: 30.9659%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [199/225], Training Accuracy: 30.9516%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [200/225], Training Accuracy: 30.9297%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [201/225], Training Accuracy: 30.9313%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [202/225], Training Accuracy: 30.9329%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [203/225], Training Accuracy: 30.9267%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [204/225], Training Accuracy: 30.9896%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [205/225], Training Accuracy: 30.9832%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [206/225], Training Accuracy: 30.9769%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [207/225], Training Accuracy: 30.9481%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [208/225], Training Accuracy: 30.9796%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [209/225], Training Accuracy: 31.0257%, Training Loss: 0.6857%\n",
      "Epoch [72/100], Step [210/225], Training Accuracy: 31.0565%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [211/225], Training Accuracy: 31.0352%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [212/225], Training Accuracy: 31.0805%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [213/225], Training Accuracy: 31.0593%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [214/225], Training Accuracy: 31.0967%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [215/225], Training Accuracy: 31.0683%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [216/225], Training Accuracy: 31.0041%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [217/225], Training Accuracy: 30.9980%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [218/225], Training Accuracy: 30.9705%, Training Loss: 0.6856%\n",
      "Epoch [72/100], Step [219/225], Training Accuracy: 31.0431%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [220/225], Training Accuracy: 31.0582%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [221/225], Training Accuracy: 31.0450%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [222/225], Training Accuracy: 31.0529%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [223/225], Training Accuracy: 31.1029%, Training Loss: 0.6855%\n",
      "Epoch [72/100], Step [224/225], Training Accuracy: 31.1175%, Training Loss: 0.6854%\n",
      "Epoch [72/100], Step [225/225], Training Accuracy: 31.0867%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6879%\n",
      "Epoch [73/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6864%\n",
      "Epoch [73/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6872%\n",
      "Epoch [73/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [9/225], Training Accuracy: 30.9028%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [10/225], Training Accuracy: 30.4688%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [11/225], Training Accuracy: 30.2557%, Training Loss: 0.6849%\n",
      "Epoch [73/100], Step [12/225], Training Accuracy: 29.5573%, Training Loss: 0.6847%\n",
      "Epoch [73/100], Step [13/225], Training Accuracy: 29.4471%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [14/225], Training Accuracy: 29.6875%, Training Loss: 0.6856%\n",
      "Epoch [73/100], Step [15/225], Training Accuracy: 30.2083%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6849%\n",
      "Epoch [73/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6850%\n",
      "Epoch [73/100], Step [18/225], Training Accuracy: 30.2951%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [19/225], Training Accuracy: 30.5921%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [20/225], Training Accuracy: 31.0938%, Training Loss: 0.6850%\n",
      "Epoch [73/100], Step [21/225], Training Accuracy: 31.1012%, Training Loss: 0.6849%\n",
      "Epoch [73/100], Step [22/225], Training Accuracy: 31.3210%, Training Loss: 0.6849%\n",
      "Epoch [73/100], Step [23/225], Training Accuracy: 31.1821%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [24/225], Training Accuracy: 31.2500%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [25/225], Training Accuracy: 31.5000%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6844%\n",
      "Epoch [73/100], Step [27/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [73/100], Step [28/225], Training Accuracy: 31.5290%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [29/225], Training Accuracy: 31.8427%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [30/225], Training Accuracy: 31.7708%, Training Loss: 0.6841%\n",
      "Epoch [73/100], Step [31/225], Training Accuracy: 31.6532%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [32/225], Training Accuracy: 31.9336%, Training Loss: 0.6839%\n",
      "Epoch [73/100], Step [33/225], Training Accuracy: 32.0549%, Training Loss: 0.6838%\n",
      "Epoch [73/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6838%\n",
      "Epoch [73/100], Step [35/225], Training Accuracy: 31.9643%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [36/225], Training Accuracy: 31.8576%, Training Loss: 0.6841%\n",
      "Epoch [73/100], Step [37/225], Training Accuracy: 31.9257%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [38/225], Training Accuracy: 31.7023%, Training Loss: 0.6844%\n",
      "Epoch [73/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [40/225], Training Accuracy: 31.4844%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [42/225], Training Accuracy: 31.4360%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6847%\n",
      "Epoch [73/100], Step [44/225], Training Accuracy: 31.5696%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [45/225], Training Accuracy: 31.6667%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [46/225], Training Accuracy: 31.5217%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [47/225], Training Accuracy: 31.4495%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [48/225], Training Accuracy: 31.5430%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [49/225], Training Accuracy: 31.5051%, Training Loss: 0.6844%\n",
      "Epoch [73/100], Step [50/225], Training Accuracy: 31.5312%, Training Loss: 0.6844%\n",
      "Epoch [73/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [52/225], Training Accuracy: 31.7308%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [53/225], Training Accuracy: 31.6922%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [54/225], Training Accuracy: 31.5972%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [55/225], Training Accuracy: 31.6761%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [56/225], Training Accuracy: 31.7243%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [57/225], Training Accuracy: 31.7434%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [58/225], Training Accuracy: 31.6272%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [59/225], Training Accuracy: 32.0445%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [60/225], Training Accuracy: 32.1354%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [61/225], Training Accuracy: 32.0953%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [62/225], Training Accuracy: 32.1321%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [64/225], Training Accuracy: 32.2510%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [65/225], Training Accuracy: 32.2115%, Training Loss: 0.6840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Step [66/225], Training Accuracy: 32.2680%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [67/225], Training Accuracy: 32.2295%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [68/225], Training Accuracy: 32.2381%, Training Loss: 0.6839%\n",
      "Epoch [73/100], Step [69/225], Training Accuracy: 32.2237%, Training Loss: 0.6837%\n",
      "Epoch [73/100], Step [70/225], Training Accuracy: 32.1429%, Training Loss: 0.6838%\n",
      "Epoch [73/100], Step [71/225], Training Accuracy: 32.2183%, Training Loss: 0.6838%\n",
      "Epoch [73/100], Step [72/225], Training Accuracy: 32.0312%, Training Loss: 0.6841%\n",
      "Epoch [73/100], Step [73/225], Training Accuracy: 31.9349%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [74/225], Training Accuracy: 32.0312%, Training Loss: 0.6839%\n",
      "Epoch [73/100], Step [75/225], Training Accuracy: 32.0000%, Training Loss: 0.6839%\n",
      "Epoch [73/100], Step [76/225], Training Accuracy: 31.9490%, Training Loss: 0.6839%\n",
      "Epoch [73/100], Step [77/225], Training Accuracy: 31.8385%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [78/225], Training Accuracy: 31.8710%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [79/225], Training Accuracy: 31.8038%, Training Loss: 0.6841%\n",
      "Epoch [73/100], Step [80/225], Training Accuracy: 31.7773%, Training Loss: 0.6840%\n",
      "Epoch [73/100], Step [81/225], Training Accuracy: 31.6937%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [82/225], Training Accuracy: 31.7073%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [83/225], Training Accuracy: 31.6642%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [84/225], Training Accuracy: 31.7150%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [85/225], Training Accuracy: 31.7096%, Training Loss: 0.6842%\n",
      "Epoch [73/100], Step [86/225], Training Accuracy: 31.7406%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [87/225], Training Accuracy: 31.7708%, Training Loss: 0.6843%\n",
      "Epoch [73/100], Step [88/225], Training Accuracy: 31.7472%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [89/225], Training Accuracy: 31.6362%, Training Loss: 0.6845%\n",
      "Epoch [73/100], Step [90/225], Training Accuracy: 31.5451%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [91/225], Training Accuracy: 31.6277%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [92/225], Training Accuracy: 31.6067%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [93/225], Training Accuracy: 31.6028%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [94/225], Training Accuracy: 31.6822%, Training Loss: 0.6846%\n",
      "Epoch [73/100], Step [95/225], Training Accuracy: 31.5625%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [96/225], Training Accuracy: 31.6569%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [97/225], Training Accuracy: 31.6688%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [98/225], Training Accuracy: 31.6805%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [99/225], Training Accuracy: 31.8182%, Training Loss: 0.6847%\n",
      "Epoch [73/100], Step [100/225], Training Accuracy: 31.7969%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [101/225], Training Accuracy: 31.8998%, Training Loss: 0.6847%\n",
      "Epoch [73/100], Step [102/225], Training Accuracy: 31.8015%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [103/225], Training Accuracy: 31.8416%, Training Loss: 0.6848%\n",
      "Epoch [73/100], Step [104/225], Training Accuracy: 31.7758%, Training Loss: 0.6849%\n",
      "Epoch [73/100], Step [105/225], Training Accuracy: 31.7411%, Training Loss: 0.6849%\n",
      "Epoch [73/100], Step [106/225], Training Accuracy: 31.7364%, Training Loss: 0.6850%\n",
      "Epoch [73/100], Step [107/225], Training Accuracy: 31.6443%, Training Loss: 0.6850%\n",
      "Epoch [73/100], Step [108/225], Training Accuracy: 31.6985%, Training Loss: 0.6850%\n",
      "Epoch [73/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [110/225], Training Accuracy: 31.5625%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [111/225], Training Accuracy: 31.4471%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [112/225], Training Accuracy: 31.5151%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [113/225], Training Accuracy: 31.4989%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [114/225], Training Accuracy: 31.5378%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [115/225], Training Accuracy: 31.5082%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [116/225], Training Accuracy: 31.5059%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [117/225], Training Accuracy: 31.4503%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [118/225], Training Accuracy: 31.4221%, Training Loss: 0.6851%\n",
      "Epoch [73/100], Step [119/225], Training Accuracy: 31.3682%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [120/225], Training Accuracy: 31.4062%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [121/225], Training Accuracy: 31.3662%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [122/225], Training Accuracy: 31.3909%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [123/225], Training Accuracy: 31.4151%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [124/225], Training Accuracy: 31.4138%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [125/225], Training Accuracy: 31.4250%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [126/225], Training Accuracy: 31.3616%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [127/225], Training Accuracy: 31.3361%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [128/225], Training Accuracy: 31.3354%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [129/225], Training Accuracy: 31.3711%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [130/225], Training Accuracy: 31.2981%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [131/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [132/225], Training Accuracy: 31.2382%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [133/225], Training Accuracy: 31.2617%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [134/225], Training Accuracy: 31.3083%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [135/225], Training Accuracy: 31.3079%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [136/225], Training Accuracy: 31.3419%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [137/225], Training Accuracy: 31.3869%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [138/225], Training Accuracy: 31.4198%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [139/225], Training Accuracy: 31.3849%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [140/225], Training Accuracy: 31.3393%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [141/225], Training Accuracy: 31.2832%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [142/225], Training Accuracy: 31.3380%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [143/225], Training Accuracy: 31.3374%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [144/225], Training Accuracy: 31.3260%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [145/225], Training Accuracy: 31.3901%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [146/225], Training Accuracy: 31.4105%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [147/225], Training Accuracy: 31.4094%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [148/225], Training Accuracy: 31.3767%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [149/225], Training Accuracy: 31.4073%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [150/225], Training Accuracy: 31.4479%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [151/225], Training Accuracy: 31.4880%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [152/225], Training Accuracy: 31.4659%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [153/225], Training Accuracy: 31.4032%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [154/225], Training Accuracy: 31.4326%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [155/225], Training Accuracy: 31.4012%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [156/225], Training Accuracy: 31.4303%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [157/225], Training Accuracy: 31.3694%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [158/225], Training Accuracy: 31.3489%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [159/225], Training Accuracy: 31.4269%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [160/225], Training Accuracy: 31.4160%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [161/225], Training Accuracy: 31.4538%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [162/225], Training Accuracy: 31.4333%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [163/225], Training Accuracy: 31.4992%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [164/225], Training Accuracy: 31.4882%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [165/225], Training Accuracy: 31.4394%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Step [166/225], Training Accuracy: 31.4288%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [167/225], Training Accuracy: 31.4746%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [168/225], Training Accuracy: 31.4174%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [169/225], Training Accuracy: 31.3517%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [170/225], Training Accuracy: 31.3051%, Training Loss: 0.6856%\n",
      "Epoch [73/100], Step [171/225], Training Accuracy: 31.3140%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [172/225], Training Accuracy: 31.3136%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [173/225], Training Accuracy: 31.2952%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [174/225], Training Accuracy: 31.3039%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [175/225], Training Accuracy: 31.3393%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [176/225], Training Accuracy: 31.3388%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [177/225], Training Accuracy: 31.3294%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [178/225], Training Accuracy: 31.3290%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [179/225], Training Accuracy: 31.3024%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [180/225], Training Accuracy: 31.3542%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [181/225], Training Accuracy: 31.3104%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [182/225], Training Accuracy: 31.3101%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [183/225], Training Accuracy: 31.3183%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [184/225], Training Accuracy: 31.2840%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [185/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [186/225], Training Accuracy: 31.2752%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [187/225], Training Accuracy: 31.2834%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [188/225], Training Accuracy: 31.2832%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [189/225], Training Accuracy: 31.2996%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [190/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [191/225], Training Accuracy: 31.2091%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [192/225], Training Accuracy: 31.1361%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [193/225], Training Accuracy: 31.1609%, Training Loss: 0.6855%\n",
      "Epoch [73/100], Step [194/225], Training Accuracy: 31.1775%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [195/225], Training Accuracy: 31.1699%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [196/225], Training Accuracy: 31.1384%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [197/225], Training Accuracy: 31.1865%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [198/225], Training Accuracy: 31.2184%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [199/225], Training Accuracy: 31.1950%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [200/225], Training Accuracy: 31.1797%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [201/225], Training Accuracy: 31.1956%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [202/225], Training Accuracy: 31.1959%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [203/225], Training Accuracy: 31.1653%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [204/225], Training Accuracy: 31.2040%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [205/225], Training Accuracy: 31.1966%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [206/225], Training Accuracy: 31.1893%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [207/225], Training Accuracy: 31.1519%, Training Loss: 0.6854%\n",
      "Epoch [73/100], Step [208/225], Training Accuracy: 31.1674%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [209/225], Training Accuracy: 31.1977%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [210/225], Training Accuracy: 31.2202%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [211/225], Training Accuracy: 31.1834%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [213/225], Training Accuracy: 31.2060%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [214/225], Training Accuracy: 31.2208%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [215/225], Training Accuracy: 31.1991%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [216/225], Training Accuracy: 31.1415%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [217/225], Training Accuracy: 31.1276%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [218/225], Training Accuracy: 31.0923%, Training Loss: 0.6853%\n",
      "Epoch [73/100], Step [219/225], Training Accuracy: 31.1430%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [220/225], Training Accuracy: 31.1506%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [221/225], Training Accuracy: 31.1581%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [222/225], Training Accuracy: 31.1726%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [223/225], Training Accuracy: 31.2010%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [224/225], Training Accuracy: 31.2012%, Training Loss: 0.6852%\n",
      "Epoch [73/100], Step [225/225], Training Accuracy: 31.1770%, Training Loss: 0.6852%\n",
      "Epoch [74/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6935%\n",
      "Epoch [74/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6880%\n",
      "Epoch [74/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6892%\n",
      "Epoch [74/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6862%\n",
      "Epoch [74/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6864%\n",
      "Epoch [74/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6865%\n",
      "Epoch [74/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6852%\n",
      "Epoch [74/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6854%\n",
      "Epoch [74/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6854%\n",
      "Epoch [74/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6862%\n",
      "Epoch [74/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6861%\n",
      "Epoch [74/100], Step [16/225], Training Accuracy: 31.3477%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [17/225], Training Accuracy: 31.0662%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [18/225], Training Accuracy: 30.8160%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [19/225], Training Accuracy: 31.3322%, Training Loss: 0.6861%\n",
      "Epoch [74/100], Step [20/225], Training Accuracy: 31.7969%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [21/225], Training Accuracy: 31.8452%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [22/225], Training Accuracy: 31.9602%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [24/225], Training Accuracy: 31.9661%, Training Loss: 0.6854%\n",
      "Epoch [74/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6854%\n",
      "Epoch [74/100], Step [26/225], Training Accuracy: 32.4519%, Training Loss: 0.6852%\n",
      "Epoch [74/100], Step [27/225], Training Accuracy: 32.2338%, Training Loss: 0.6851%\n",
      "Epoch [74/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6852%\n",
      "Epoch [74/100], Step [29/225], Training Accuracy: 32.3276%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [30/225], Training Accuracy: 32.2396%, Training Loss: 0.6848%\n",
      "Epoch [74/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6847%\n",
      "Epoch [74/100], Step [32/225], Training Accuracy: 32.1777%, Training Loss: 0.6845%\n",
      "Epoch [74/100], Step [33/225], Training Accuracy: 32.3390%, Training Loss: 0.6844%\n",
      "Epoch [74/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6844%\n",
      "Epoch [74/100], Step [35/225], Training Accuracy: 32.0982%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [36/225], Training Accuracy: 31.8576%, Training Loss: 0.6848%\n",
      "Epoch [74/100], Step [37/225], Training Accuracy: 31.8834%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [38/225], Training Accuracy: 31.7023%, Training Loss: 0.6850%\n",
      "Epoch [74/100], Step [39/225], Training Accuracy: 31.3301%, Training Loss: 0.6851%\n",
      "Epoch [74/100], Step [40/225], Training Accuracy: 31.3672%, Training Loss: 0.6852%\n",
      "Epoch [74/100], Step [41/225], Training Accuracy: 31.4405%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [42/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [43/225], Training Accuracy: 31.4317%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [44/225], Training Accuracy: 31.4276%, Training Loss: 0.6852%\n",
      "Epoch [74/100], Step [45/225], Training Accuracy: 31.4236%, Training Loss: 0.6851%\n",
      "Epoch [74/100], Step [46/225], Training Accuracy: 31.3179%, Training Loss: 0.6850%\n",
      "Epoch [74/100], Step [47/225], Training Accuracy: 31.3165%, Training Loss: 0.6850%\n",
      "Epoch [74/100], Step [48/225], Training Accuracy: 31.4128%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [49/225], Training Accuracy: 31.4413%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [50/225], Training Accuracy: 31.4688%, Training Loss: 0.6851%\n",
      "Epoch [74/100], Step [51/225], Training Accuracy: 31.6176%, Training Loss: 0.6850%\n",
      "Epoch [74/100], Step [52/225], Training Accuracy: 31.6707%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Step [53/225], Training Accuracy: 31.5743%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [54/225], Training Accuracy: 31.4236%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [55/225], Training Accuracy: 31.5341%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [56/225], Training Accuracy: 31.6964%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [57/225], Training Accuracy: 31.7160%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [58/225], Training Accuracy: 31.6272%, Training Loss: 0.6848%\n",
      "Epoch [74/100], Step [59/225], Training Accuracy: 32.0180%, Training Loss: 0.6845%\n",
      "Epoch [74/100], Step [60/225], Training Accuracy: 32.0833%, Training Loss: 0.6845%\n",
      "Epoch [74/100], Step [61/225], Training Accuracy: 32.0441%, Training Loss: 0.6845%\n",
      "Epoch [74/100], Step [62/225], Training Accuracy: 32.0565%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [63/225], Training Accuracy: 32.0933%, Training Loss: 0.6847%\n",
      "Epoch [74/100], Step [64/225], Training Accuracy: 32.0068%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [65/225], Training Accuracy: 31.9471%, Training Loss: 0.6847%\n",
      "Epoch [74/100], Step [66/225], Training Accuracy: 32.0076%, Training Loss: 0.6847%\n",
      "Epoch [74/100], Step [67/225], Training Accuracy: 31.9263%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [68/225], Training Accuracy: 31.9853%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [69/225], Training Accuracy: 31.9973%, Training Loss: 0.6844%\n",
      "Epoch [74/100], Step [70/225], Training Accuracy: 31.9196%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [71/225], Training Accuracy: 31.9982%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [72/225], Training Accuracy: 31.7708%, Training Loss: 0.6848%\n",
      "Epoch [74/100], Step [73/225], Training Accuracy: 31.7423%, Training Loss: 0.6848%\n",
      "Epoch [74/100], Step [74/225], Training Accuracy: 31.8412%, Training Loss: 0.6847%\n",
      "Epoch [74/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6846%\n",
      "Epoch [74/100], Step [76/225], Training Accuracy: 31.8051%, Training Loss: 0.6847%\n",
      "Epoch [74/100], Step [77/225], Training Accuracy: 31.6964%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [78/225], Training Accuracy: 31.7308%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [79/225], Training Accuracy: 31.6456%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [80/225], Training Accuracy: 31.6211%, Training Loss: 0.6848%\n",
      "Epoch [74/100], Step [81/225], Training Accuracy: 31.5394%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [82/225], Training Accuracy: 31.5549%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [83/225], Training Accuracy: 31.4947%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [84/225], Training Accuracy: 31.5662%, Training Loss: 0.6849%\n",
      "Epoch [74/100], Step [85/225], Training Accuracy: 31.5257%, Training Loss: 0.6850%\n",
      "Epoch [74/100], Step [86/225], Training Accuracy: 31.5589%, Training Loss: 0.6851%\n",
      "Epoch [74/100], Step [87/225], Training Accuracy: 31.5553%, Training Loss: 0.6851%\n",
      "Epoch [74/100], Step [88/225], Training Accuracy: 31.5341%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [89/225], Training Accuracy: 31.4607%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [90/225], Training Accuracy: 31.3715%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [91/225], Training Accuracy: 31.4389%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [92/225], Training Accuracy: 31.4029%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [94/225], Training Accuracy: 31.4495%, Training Loss: 0.6853%\n",
      "Epoch [74/100], Step [95/225], Training Accuracy: 31.2993%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [96/225], Training Accuracy: 31.4128%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [97/225], Training Accuracy: 31.4272%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [98/225], Training Accuracy: 31.4413%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [99/225], Training Accuracy: 31.5499%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [100/225], Training Accuracy: 31.5156%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [101/225], Training Accuracy: 31.6368%, Training Loss: 0.6854%\n",
      "Epoch [74/100], Step [102/225], Training Accuracy: 31.5104%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [103/225], Training Accuracy: 31.5837%, Training Loss: 0.6854%\n",
      "Epoch [74/100], Step [104/225], Training Accuracy: 31.5655%, Training Loss: 0.6855%\n",
      "Epoch [74/100], Step [105/225], Training Accuracy: 31.5179%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [106/225], Training Accuracy: 31.5153%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [107/225], Training Accuracy: 31.4398%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [108/225], Training Accuracy: 31.5394%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [109/225], Training Accuracy: 31.4077%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [110/225], Training Accuracy: 31.4347%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [111/225], Training Accuracy: 31.3345%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [112/225], Training Accuracy: 31.3895%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [113/225], Training Accuracy: 31.3744%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [114/225], Training Accuracy: 31.4419%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [115/225], Training Accuracy: 31.3995%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [116/225], Training Accuracy: 31.3982%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [117/225], Training Accuracy: 31.3168%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [118/225], Training Accuracy: 31.2897%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [119/225], Training Accuracy: 31.2369%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [120/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [121/225], Training Accuracy: 31.2371%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [122/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [123/225], Training Accuracy: 31.2627%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [124/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [125/225], Training Accuracy: 31.2375%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [126/225], Training Accuracy: 31.1508%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [127/225], Training Accuracy: 31.1024%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [128/225], Training Accuracy: 31.1035%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [129/225], Training Accuracy: 31.1289%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [130/225], Training Accuracy: 31.0577%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [131/225], Training Accuracy: 31.0234%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [132/225], Training Accuracy: 30.9896%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [133/225], Training Accuracy: 31.0033%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [134/225], Training Accuracy: 31.0634%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [135/225], Training Accuracy: 31.0764%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [136/225], Training Accuracy: 31.1006%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [137/225], Training Accuracy: 31.1359%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [138/225], Training Accuracy: 31.1707%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [139/225], Training Accuracy: 31.1376%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [140/225], Training Accuracy: 31.1049%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [141/225], Training Accuracy: 31.0616%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [142/225], Training Accuracy: 31.1070%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [143/225], Training Accuracy: 31.1080%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [144/225], Training Accuracy: 31.1306%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [145/225], Training Accuracy: 31.1961%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [146/225], Training Accuracy: 31.2179%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [147/225], Training Accuracy: 31.2181%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [148/225], Training Accuracy: 31.1761%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [149/225], Training Accuracy: 31.2081%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [150/225], Training Accuracy: 31.2292%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [151/225], Training Accuracy: 31.2914%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [152/225], Training Accuracy: 31.2808%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [153/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [154/225], Training Accuracy: 31.2906%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [155/225], Training Accuracy: 31.2702%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [156/225], Training Accuracy: 31.2901%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [157/225], Training Accuracy: 31.2400%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [158/225], Training Accuracy: 31.2302%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [159/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [161/225], Training Accuracy: 31.2791%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [162/225], Training Accuracy: 31.2596%, Training Loss: 0.6860%\n",
      "Epoch [74/100], Step [163/225], Training Accuracy: 31.3171%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [164/225], Training Accuracy: 31.2976%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Step [165/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [166/225], Training Accuracy: 31.2594%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [167/225], Training Accuracy: 31.2874%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [168/225], Training Accuracy: 31.2593%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [169/225], Training Accuracy: 31.1853%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [170/225], Training Accuracy: 31.1213%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [171/225], Training Accuracy: 31.1495%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [172/225], Training Accuracy: 31.1682%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [173/225], Training Accuracy: 31.1507%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [174/225], Training Accuracy: 31.1512%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [175/225], Training Accuracy: 31.1875%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [176/225], Training Accuracy: 31.2145%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [177/225], Training Accuracy: 31.2323%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [178/225], Training Accuracy: 31.1973%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [179/225], Training Accuracy: 31.1627%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [180/225], Training Accuracy: 31.1892%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [181/225], Training Accuracy: 31.1464%, Training Loss: 0.6859%\n",
      "Epoch [74/100], Step [182/225], Training Accuracy: 31.1470%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [183/225], Training Accuracy: 31.1561%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [184/225], Training Accuracy: 31.1311%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [185/225], Training Accuracy: 31.0811%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [186/225], Training Accuracy: 31.0988%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [187/225], Training Accuracy: 31.1163%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [188/225], Training Accuracy: 31.1004%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [189/225], Training Accuracy: 31.1177%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [190/225], Training Accuracy: 31.0691%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [191/225], Training Accuracy: 31.0537%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [192/225], Training Accuracy: 30.9814%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [193/225], Training Accuracy: 30.9828%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [194/225], Training Accuracy: 31.0003%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [195/225], Training Accuracy: 30.9936%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [196/225], Training Accuracy: 30.9630%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [197/225], Training Accuracy: 30.9962%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [198/225], Training Accuracy: 31.0133%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [199/225], Training Accuracy: 30.9830%, Training Loss: 0.6858%\n",
      "Epoch [74/100], Step [200/225], Training Accuracy: 30.9609%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [201/225], Training Accuracy: 30.9779%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [202/225], Training Accuracy: 30.9715%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [203/225], Training Accuracy: 30.9498%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [204/225], Training Accuracy: 31.0049%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [205/225], Training Accuracy: 31.0061%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [206/225], Training Accuracy: 30.9921%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [207/225], Training Accuracy: 30.9783%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [208/225], Training Accuracy: 31.0096%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [209/225], Training Accuracy: 31.0481%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [210/225], Training Accuracy: 31.1086%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [211/225], Training Accuracy: 31.1093%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [212/225], Training Accuracy: 31.1468%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [213/225], Training Accuracy: 31.1253%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [214/225], Training Accuracy: 31.1405%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [215/225], Training Accuracy: 31.1192%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [216/225], Training Accuracy: 31.0764%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [217/225], Training Accuracy: 31.0700%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [218/225], Training Accuracy: 31.0278%, Training Loss: 0.6857%\n",
      "Epoch [74/100], Step [219/225], Training Accuracy: 31.0645%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [220/225], Training Accuracy: 31.0724%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [221/225], Training Accuracy: 31.0520%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [222/225], Training Accuracy: 31.0600%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [223/225], Training Accuracy: 31.1099%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [224/225], Training Accuracy: 31.1035%, Training Loss: 0.6856%\n",
      "Epoch [74/100], Step [225/225], Training Accuracy: 31.0798%, Training Loss: 0.6856%\n",
      "Epoch [75/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6921%\n",
      "Epoch [75/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6884%\n",
      "Epoch [75/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6899%\n",
      "Epoch [75/100], Step [4/225], Training Accuracy: 30.8594%, Training Loss: 0.6880%\n",
      "Epoch [75/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6873%\n",
      "Epoch [75/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6871%\n",
      "Epoch [75/100], Step [7/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [75/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6865%\n",
      "Epoch [75/100], Step [10/225], Training Accuracy: 30.6250%, Training Loss: 0.6862%\n",
      "Epoch [75/100], Step [11/225], Training Accuracy: 30.3977%, Training Loss: 0.6862%\n",
      "Epoch [75/100], Step [12/225], Training Accuracy: 29.8177%, Training Loss: 0.6861%\n",
      "Epoch [75/100], Step [13/225], Training Accuracy: 29.8077%, Training Loss: 0.6866%\n",
      "Epoch [75/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6867%\n",
      "Epoch [75/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6866%\n",
      "Epoch [75/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6863%\n",
      "Epoch [75/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6866%\n",
      "Epoch [75/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6867%\n",
      "Epoch [75/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6867%\n",
      "Epoch [75/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6865%\n",
      "Epoch [75/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6864%\n",
      "Epoch [75/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6863%\n",
      "Epoch [75/100], Step [23/225], Training Accuracy: 31.6576%, Training Loss: 0.6861%\n",
      "Epoch [75/100], Step [24/225], Training Accuracy: 31.8359%, Training Loss: 0.6860%\n",
      "Epoch [75/100], Step [25/225], Training Accuracy: 31.9375%, Training Loss: 0.6859%\n",
      "Epoch [75/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [27/225], Training Accuracy: 32.0602%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [31/225], Training Accuracy: 32.0060%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [33/225], Training Accuracy: 32.3390%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [34/225], Training Accuracy: 32.2610%, Training Loss: 0.6846%\n",
      "Epoch [75/100], Step [35/225], Training Accuracy: 32.1429%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [36/225], Training Accuracy: 32.0312%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [37/225], Training Accuracy: 31.9257%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [38/225], Training Accuracy: 31.7434%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [39/225], Training Accuracy: 31.4904%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [40/225], Training Accuracy: 31.5625%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [41/225], Training Accuracy: 31.5930%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [42/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [44/225], Training Accuracy: 31.6761%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [45/225], Training Accuracy: 31.7014%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [46/225], Training Accuracy: 31.6576%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [48/225], Training Accuracy: 31.6732%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [49/225], Training Accuracy: 31.6964%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [50/225], Training Accuracy: 31.6562%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Step [51/225], Training Accuracy: 31.7708%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [52/225], Training Accuracy: 31.7608%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [53/225], Training Accuracy: 31.6627%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [54/225], Training Accuracy: 31.5394%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [55/225], Training Accuracy: 31.6193%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [56/225], Training Accuracy: 31.6964%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [57/225], Training Accuracy: 31.6612%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [58/225], Training Accuracy: 31.5194%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [59/225], Training Accuracy: 31.8856%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [60/225], Training Accuracy: 31.9531%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [61/225], Training Accuracy: 31.8904%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [62/225], Training Accuracy: 31.9304%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [63/225], Training Accuracy: 31.9940%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [64/225], Training Accuracy: 32.0068%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [65/225], Training Accuracy: 31.8750%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [66/225], Training Accuracy: 31.9366%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [67/225], Training Accuracy: 31.9729%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [68/225], Training Accuracy: 32.0312%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [69/225], Training Accuracy: 32.0426%, Training Loss: 0.6845%\n",
      "Epoch [75/100], Step [70/225], Training Accuracy: 31.9866%, Training Loss: 0.6845%\n",
      "Epoch [75/100], Step [71/225], Training Accuracy: 32.0202%, Training Loss: 0.6845%\n",
      "Epoch [75/100], Step [72/225], Training Accuracy: 31.8359%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [73/225], Training Accuracy: 31.7637%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [74/225], Training Accuracy: 31.8412%, Training Loss: 0.6846%\n",
      "Epoch [75/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6846%\n",
      "Epoch [75/100], Step [76/225], Training Accuracy: 31.7845%, Training Loss: 0.6846%\n",
      "Epoch [75/100], Step [77/225], Training Accuracy: 31.6761%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [78/225], Training Accuracy: 31.6907%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [79/225], Training Accuracy: 31.5862%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [80/225], Training Accuracy: 31.5625%, Training Loss: 0.6847%\n",
      "Epoch [75/100], Step [81/225], Training Accuracy: 31.5008%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [82/225], Training Accuracy: 31.5168%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [83/225], Training Accuracy: 31.4383%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [84/225], Training Accuracy: 31.4732%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [85/225], Training Accuracy: 31.4890%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [86/225], Training Accuracy: 31.4862%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [87/225], Training Accuracy: 31.5014%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [88/225], Training Accuracy: 31.4986%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [89/225], Training Accuracy: 31.4080%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [90/225], Training Accuracy: 31.3194%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [91/225], Training Accuracy: 31.3874%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [92/225], Training Accuracy: 31.4029%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [94/225], Training Accuracy: 31.4661%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [95/225], Training Accuracy: 31.3487%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [96/225], Training Accuracy: 31.4128%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [97/225], Training Accuracy: 31.4594%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [98/225], Training Accuracy: 31.5051%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [100/225], Training Accuracy: 31.6094%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [101/225], Training Accuracy: 31.7296%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [102/225], Training Accuracy: 31.6023%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [103/225], Training Accuracy: 31.6444%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [104/225], Training Accuracy: 31.6106%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [105/225], Training Accuracy: 31.5774%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [107/225], Training Accuracy: 31.4836%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [108/225], Training Accuracy: 31.5683%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [109/225], Training Accuracy: 31.4507%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [110/225], Training Accuracy: 31.4773%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [111/225], Training Accuracy: 31.3626%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [112/225], Training Accuracy: 31.4453%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [113/225], Training Accuracy: 31.4436%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [114/225], Training Accuracy: 31.4967%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [115/225], Training Accuracy: 31.4538%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [116/225], Training Accuracy: 31.4520%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [117/225], Training Accuracy: 31.3835%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [118/225], Training Accuracy: 31.3824%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [119/225], Training Accuracy: 31.3550%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [120/225], Training Accuracy: 31.3802%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [121/225], Training Accuracy: 31.3275%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [122/225], Training Accuracy: 31.3140%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [123/225], Training Accuracy: 31.3389%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [124/225], Training Accuracy: 31.3508%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [125/225], Training Accuracy: 31.3500%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [126/225], Training Accuracy: 31.2872%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [127/225], Training Accuracy: 31.2377%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [128/225], Training Accuracy: 31.2256%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [129/225], Training Accuracy: 31.2621%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [130/225], Training Accuracy: 31.1779%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [131/225], Training Accuracy: 31.1307%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [132/225], Training Accuracy: 31.0843%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [133/225], Training Accuracy: 31.0973%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [134/225], Training Accuracy: 31.1567%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [135/225], Training Accuracy: 31.1343%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [136/225], Training Accuracy: 31.1466%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [137/225], Training Accuracy: 31.2158%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [138/225], Training Accuracy: 31.2387%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [139/225], Training Accuracy: 31.2163%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [140/225], Training Accuracy: 31.1719%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [141/225], Training Accuracy: 31.1170%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [142/225], Training Accuracy: 31.1840%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [143/225], Training Accuracy: 31.1844%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [144/225], Training Accuracy: 31.1849%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [145/225], Training Accuracy: 31.2392%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [146/225], Training Accuracy: 31.2286%, Training Loss: 0.6855%\n",
      "Epoch [75/100], Step [147/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [148/225], Training Accuracy: 31.2289%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100], Step [150/225], Training Accuracy: 31.2708%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [151/225], Training Accuracy: 31.3224%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [152/225], Training Accuracy: 31.3220%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [153/225], Training Accuracy: 31.3011%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [154/225], Training Accuracy: 31.3109%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [155/225], Training Accuracy: 31.2903%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [156/225], Training Accuracy: 31.3001%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [157/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [158/225], Training Accuracy: 31.2302%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [159/225], Training Accuracy: 31.2893%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [160/225], Training Accuracy: 31.2695%, Training Loss: 0.6854%\n",
      "Epoch [75/100], Step [161/225], Training Accuracy: 31.3179%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [162/225], Training Accuracy: 31.2886%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [163/225], Training Accuracy: 31.3650%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [164/225], Training Accuracy: 31.3357%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [165/225], Training Accuracy: 31.2595%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [166/225], Training Accuracy: 31.2594%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [167/225], Training Accuracy: 31.3155%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [168/225], Training Accuracy: 31.2686%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [169/225], Training Accuracy: 31.1945%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [170/225], Training Accuracy: 31.1397%, Training Loss: 0.6853%\n",
      "Epoch [75/100], Step [171/225], Training Accuracy: 31.1769%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [172/225], Training Accuracy: 31.2137%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [173/225], Training Accuracy: 31.2048%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [174/225], Training Accuracy: 31.2231%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [175/225], Training Accuracy: 31.2679%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [176/225], Training Accuracy: 31.3033%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [177/225], Training Accuracy: 31.3118%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [178/225], Training Accuracy: 31.3114%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [179/225], Training Accuracy: 31.2762%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [180/225], Training Accuracy: 31.3108%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [181/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [182/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [183/225], Training Accuracy: 31.2671%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [184/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [185/225], Training Accuracy: 31.2162%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [186/225], Training Accuracy: 31.2416%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [187/225], Training Accuracy: 31.2416%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [188/225], Training Accuracy: 31.2251%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [189/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [190/225], Training Accuracy: 31.2171%, Training Loss: 0.6852%\n",
      "Epoch [75/100], Step [191/225], Training Accuracy: 31.1927%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [192/225], Training Accuracy: 31.1198%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [193/225], Training Accuracy: 31.1286%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [195/225], Training Accuracy: 31.1138%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [196/225], Training Accuracy: 31.0746%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [197/225], Training Accuracy: 31.0993%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [198/225], Training Accuracy: 31.1158%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [199/225], Training Accuracy: 31.0930%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [200/225], Training Accuracy: 31.0703%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [201/225], Training Accuracy: 31.1023%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [202/225], Training Accuracy: 31.1108%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [203/225], Training Accuracy: 31.1115%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [204/225], Training Accuracy: 31.1428%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [205/225], Training Accuracy: 31.1585%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [206/225], Training Accuracy: 31.1514%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [207/225], Training Accuracy: 31.1292%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [208/225], Training Accuracy: 31.1523%, Training Loss: 0.6851%\n",
      "Epoch [75/100], Step [209/225], Training Accuracy: 31.2051%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [210/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [211/225], Training Accuracy: 31.2426%, Training Loss: 0.6850%\n",
      "Epoch [75/100], Step [212/225], Training Accuracy: 31.3016%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [213/225], Training Accuracy: 31.2793%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [214/225], Training Accuracy: 31.3084%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [215/225], Training Accuracy: 31.2863%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [216/225], Training Accuracy: 31.2138%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [217/225], Training Accuracy: 31.2356%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [218/225], Training Accuracy: 31.2070%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [219/225], Training Accuracy: 31.2714%, Training Loss: 0.6849%\n",
      "Epoch [75/100], Step [220/225], Training Accuracy: 31.2855%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [221/225], Training Accuracy: 31.2783%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [222/225], Training Accuracy: 31.2852%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [223/225], Training Accuracy: 31.3201%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [224/225], Training Accuracy: 31.3128%, Training Loss: 0.6848%\n",
      "Epoch [75/100], Step [225/225], Training Accuracy: 31.3021%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6887%\n",
      "Epoch [76/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6908%\n",
      "Epoch [76/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6900%\n",
      "Epoch [76/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6871%\n",
      "Epoch [76/100], Step [5/225], Training Accuracy: 34.0625%, Training Loss: 0.6876%\n",
      "Epoch [76/100], Step [6/225], Training Accuracy: 33.0729%, Training Loss: 0.6882%\n",
      "Epoch [76/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6869%\n",
      "Epoch [76/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6863%\n",
      "Epoch [76/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6868%\n",
      "Epoch [76/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6861%\n",
      "Epoch [76/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6861%\n",
      "Epoch [76/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6859%\n",
      "Epoch [76/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6860%\n",
      "Epoch [76/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6864%\n",
      "Epoch [76/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6860%\n",
      "Epoch [76/100], Step [16/225], Training Accuracy: 31.1523%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6858%\n",
      "Epoch [76/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6859%\n",
      "Epoch [76/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6862%\n",
      "Epoch [76/100], Step [20/225], Training Accuracy: 31.3281%, Training Loss: 0.6858%\n",
      "Epoch [76/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6856%\n",
      "Epoch [76/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6854%\n",
      "Epoch [76/100], Step [23/225], Training Accuracy: 31.1821%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Step [24/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [76/100], Step [25/225], Training Accuracy: 31.4375%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [26/225], Training Accuracy: 31.7308%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [27/225], Training Accuracy: 31.4815%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [28/225], Training Accuracy: 31.3058%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [29/225], Training Accuracy: 31.7888%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [30/225], Training Accuracy: 31.7188%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [32/225], Training Accuracy: 31.7871%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [33/225], Training Accuracy: 31.8655%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [34/225], Training Accuracy: 31.8015%, Training Loss: 0.6843%\n",
      "Epoch [76/100], Step [35/225], Training Accuracy: 31.6964%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [36/225], Training Accuracy: 31.5972%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [37/225], Training Accuracy: 31.6301%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [38/225], Training Accuracy: 31.4145%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [39/225], Training Accuracy: 31.0897%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [40/225], Training Accuracy: 31.1719%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [41/225], Training Accuracy: 31.2119%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [42/225], Training Accuracy: 30.9896%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [43/225], Training Accuracy: 31.1773%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [45/225], Training Accuracy: 31.2153%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [46/225], Training Accuracy: 31.1141%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [47/225], Training Accuracy: 31.0838%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [48/225], Training Accuracy: 31.1849%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [49/225], Training Accuracy: 31.2500%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [50/225], Training Accuracy: 31.1875%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [51/225], Training Accuracy: 31.2806%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [52/225], Training Accuracy: 31.3401%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [53/225], Training Accuracy: 31.2795%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [54/225], Training Accuracy: 31.1632%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [55/225], Training Accuracy: 31.3068%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [56/225], Training Accuracy: 31.4174%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [57/225], Training Accuracy: 31.4419%, Training Loss: 0.6843%\n",
      "Epoch [76/100], Step [58/225], Training Accuracy: 31.3578%, Training Loss: 0.6843%\n",
      "Epoch [76/100], Step [59/225], Training Accuracy: 31.7267%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [60/225], Training Accuracy: 31.8490%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [62/225], Training Accuracy: 31.8044%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [63/225], Training Accuracy: 31.8700%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [64/225], Training Accuracy: 31.8604%, Training Loss: 0.6840%\n",
      "Epoch [76/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6842%\n",
      "Epoch [76/100], Step [67/225], Training Accuracy: 31.8330%, Training Loss: 0.6842%\n",
      "Epoch [76/100], Step [68/225], Training Accuracy: 31.8474%, Training Loss: 0.6842%\n",
      "Epoch [76/100], Step [69/225], Training Accuracy: 31.8388%, Training Loss: 0.6840%\n",
      "Epoch [76/100], Step [70/225], Training Accuracy: 31.8080%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [71/225], Training Accuracy: 31.8442%, Training Loss: 0.6841%\n",
      "Epoch [76/100], Step [72/225], Training Accuracy: 31.6406%, Training Loss: 0.6843%\n",
      "Epoch [76/100], Step [73/225], Training Accuracy: 31.5711%, Training Loss: 0.6843%\n",
      "Epoch [76/100], Step [74/225], Training Accuracy: 31.6723%, Training Loss: 0.6842%\n",
      "Epoch [76/100], Step [75/225], Training Accuracy: 31.5625%, Training Loss: 0.6842%\n",
      "Epoch [76/100], Step [76/225], Training Accuracy: 31.5173%, Training Loss: 0.6842%\n",
      "Epoch [76/100], Step [77/225], Training Accuracy: 31.3920%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [78/225], Training Accuracy: 31.4103%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [79/225], Training Accuracy: 31.3489%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [80/225], Training Accuracy: 31.3477%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [81/225], Training Accuracy: 31.2886%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [82/225], Training Accuracy: 31.3072%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [83/225], Training Accuracy: 31.2312%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [84/225], Training Accuracy: 31.2686%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [85/225], Training Accuracy: 31.2500%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [86/225], Training Accuracy: 31.3227%, Training Loss: 0.6844%\n",
      "Epoch [76/100], Step [87/225], Training Accuracy: 31.3398%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [88/225], Training Accuracy: 31.2855%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [89/225], Training Accuracy: 31.1798%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [90/225], Training Accuracy: 31.0938%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [92/225], Training Accuracy: 31.2500%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [93/225], Training Accuracy: 31.2332%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [94/225], Training Accuracy: 31.3331%, Training Loss: 0.6845%\n",
      "Epoch [76/100], Step [95/225], Training Accuracy: 31.2336%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [96/225], Training Accuracy: 31.3314%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [97/225], Training Accuracy: 31.3466%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [98/225], Training Accuracy: 31.3297%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [99/225], Training Accuracy: 31.4394%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [100/225], Training Accuracy: 31.3750%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [101/225], Training Accuracy: 31.4821%, Training Loss: 0.6846%\n",
      "Epoch [76/100], Step [102/225], Training Accuracy: 31.3725%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [103/225], Training Accuracy: 31.4320%, Training Loss: 0.6847%\n",
      "Epoch [76/100], Step [104/225], Training Accuracy: 31.4002%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [105/225], Training Accuracy: 31.3393%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [106/225], Training Accuracy: 31.3679%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [107/225], Training Accuracy: 31.2646%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [108/225], Training Accuracy: 31.3368%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [109/225], Training Accuracy: 31.1640%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [110/225], Training Accuracy: 31.1648%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [111/225], Training Accuracy: 31.0529%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [112/225], Training Accuracy: 31.1523%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [113/225], Training Accuracy: 31.1532%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [114/225], Training Accuracy: 31.1815%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [115/225], Training Accuracy: 31.1413%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [116/225], Training Accuracy: 31.1557%, Training Loss: 0.6848%\n",
      "Epoch [76/100], Step [117/225], Training Accuracy: 31.1031%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [118/225], Training Accuracy: 31.0911%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [119/225], Training Accuracy: 31.0399%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [120/225], Training Accuracy: 31.0417%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [121/225], Training Accuracy: 31.0305%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [122/225], Training Accuracy: 31.0195%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [123/225], Training Accuracy: 31.0595%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Step [124/225], Training Accuracy: 31.0484%, Training Loss: 0.6849%\n",
      "Epoch [76/100], Step [125/225], Training Accuracy: 31.0125%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [126/225], Training Accuracy: 30.9524%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [127/225], Training Accuracy: 30.9055%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [128/225], Training Accuracy: 30.8960%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [129/225], Training Accuracy: 30.9351%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [130/225], Training Accuracy: 30.9014%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [131/225], Training Accuracy: 30.8683%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [132/225], Training Accuracy: 30.8002%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [133/225], Training Accuracy: 30.8271%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [134/225], Training Accuracy: 30.9002%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [135/225], Training Accuracy: 30.9028%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [136/225], Training Accuracy: 30.9398%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [137/225], Training Accuracy: 30.9991%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [138/225], Training Accuracy: 31.0122%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [139/225], Training Accuracy: 30.9353%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [140/225], Training Accuracy: 30.9375%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [141/225], Training Accuracy: 30.8621%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [142/225], Training Accuracy: 30.9309%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [143/225], Training Accuracy: 30.9441%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [144/225], Training Accuracy: 30.9787%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [145/225], Training Accuracy: 31.0345%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [146/225], Training Accuracy: 31.0788%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [147/225], Training Accuracy: 31.1224%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [148/225], Training Accuracy: 31.0811%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [149/225], Training Accuracy: 31.1242%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [150/225], Training Accuracy: 31.1667%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [151/225], Training Accuracy: 31.2190%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [152/225], Training Accuracy: 31.2294%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [153/225], Training Accuracy: 31.1989%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [154/225], Training Accuracy: 31.2196%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [155/225], Training Accuracy: 31.1895%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [156/225], Training Accuracy: 31.1999%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [157/225], Training Accuracy: 31.1604%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [158/225], Training Accuracy: 31.1511%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [159/225], Training Accuracy: 31.1812%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [160/225], Training Accuracy: 31.1523%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [161/225], Training Accuracy: 31.1918%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [162/225], Training Accuracy: 31.1535%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [163/225], Training Accuracy: 31.2117%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [164/225], Training Accuracy: 31.1833%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [165/225], Training Accuracy: 31.1174%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [166/225], Training Accuracy: 31.1088%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [167/225], Training Accuracy: 31.1658%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [168/225], Training Accuracy: 31.1105%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [169/225], Training Accuracy: 31.0558%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [170/225], Training Accuracy: 31.0294%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [171/225], Training Accuracy: 31.0398%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [172/225], Training Accuracy: 31.0592%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [173/225], Training Accuracy: 31.0513%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [174/225], Training Accuracy: 31.0704%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [175/225], Training Accuracy: 31.1161%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [176/225], Training Accuracy: 31.1523%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [177/225], Training Accuracy: 31.1617%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [178/225], Training Accuracy: 31.1534%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [179/225], Training Accuracy: 31.1365%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [180/225], Training Accuracy: 31.1806%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [181/225], Training Accuracy: 31.1378%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [182/225], Training Accuracy: 31.1298%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [183/225], Training Accuracy: 31.1390%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [184/225], Training Accuracy: 31.1141%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [185/225], Training Accuracy: 31.0811%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [186/225], Training Accuracy: 31.1072%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [187/225], Training Accuracy: 31.1247%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [188/225], Training Accuracy: 31.1170%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [189/225], Training Accuracy: 31.1508%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [190/225], Training Accuracy: 31.1266%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [191/225], Training Accuracy: 31.1109%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [192/225], Training Accuracy: 31.0547%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [193/225], Training Accuracy: 31.0395%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [194/225], Training Accuracy: 31.0486%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [195/225], Training Accuracy: 31.0417%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [196/225], Training Accuracy: 31.0188%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [197/225], Training Accuracy: 31.0596%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [198/225], Training Accuracy: 31.0843%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [199/225], Training Accuracy: 31.0694%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [200/225], Training Accuracy: 31.0547%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [201/225], Training Accuracy: 31.0868%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [202/225], Training Accuracy: 31.0876%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [203/225], Training Accuracy: 31.0730%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [204/225], Training Accuracy: 31.1428%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [205/225], Training Accuracy: 31.1280%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [206/225], Training Accuracy: 31.1211%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [207/225], Training Accuracy: 31.1217%, Training Loss: 0.6853%\n",
      "Epoch [76/100], Step [208/225], Training Accuracy: 31.1448%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [209/225], Training Accuracy: 31.1827%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [210/225], Training Accuracy: 31.1905%, Training Loss: 0.6852%\n",
      "Epoch [76/100], Step [211/225], Training Accuracy: 31.1759%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [212/225], Training Accuracy: 31.2205%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [213/225], Training Accuracy: 31.1913%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [214/225], Training Accuracy: 31.1989%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [215/225], Training Accuracy: 31.1701%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [216/225], Training Accuracy: 31.1126%, Training Loss: 0.6851%\n",
      "Epoch [76/100], Step [217/225], Training Accuracy: 31.1060%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [218/225], Training Accuracy: 31.0851%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [219/225], Training Accuracy: 31.1430%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [220/225], Training Accuracy: 31.1577%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [221/225], Training Accuracy: 31.1510%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [222/225], Training Accuracy: 31.1655%, Training Loss: 0.6850%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Step [223/225], Training Accuracy: 31.2080%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [224/225], Training Accuracy: 31.2012%, Training Loss: 0.6850%\n",
      "Epoch [76/100], Step [225/225], Training Accuracy: 31.1770%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6944%\n",
      "Epoch [77/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6878%\n",
      "Epoch [77/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6884%\n",
      "Epoch [77/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6864%\n",
      "Epoch [77/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6874%\n",
      "Epoch [77/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6873%\n",
      "Epoch [77/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [11/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [14/225], Training Accuracy: 30.9152%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [15/225], Training Accuracy: 31.4583%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [16/225], Training Accuracy: 31.4453%, Training Loss: 0.6858%\n",
      "Epoch [77/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [18/225], Training Accuracy: 30.9028%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [22/225], Training Accuracy: 31.8182%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [24/225], Training Accuracy: 31.9661%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [25/225], Training Accuracy: 32.1250%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [26/225], Training Accuracy: 32.5120%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [27/225], Training Accuracy: 32.2917%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [28/225], Training Accuracy: 32.1987%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [29/225], Training Accuracy: 32.4892%, Training Loss: 0.6854%\n",
      "Epoch [77/100], Step [30/225], Training Accuracy: 32.3958%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [31/225], Training Accuracy: 32.2077%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [33/225], Training Accuracy: 32.4337%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [36/225], Training Accuracy: 32.0747%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [38/225], Training Accuracy: 31.7845%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [39/225], Training Accuracy: 31.5304%, Training Loss: 0.6854%\n",
      "Epoch [77/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [41/225], Training Accuracy: 31.6692%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [43/225], Training Accuracy: 31.7951%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [44/225], Training Accuracy: 31.7472%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [45/225], Training Accuracy: 31.7361%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [46/225], Training Accuracy: 31.5897%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [47/225], Training Accuracy: 31.5160%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [48/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [49/225], Training Accuracy: 31.6327%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [50/225], Training Accuracy: 31.5938%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [51/225], Training Accuracy: 31.7402%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [52/225], Training Accuracy: 31.7608%, Training Loss: 0.6854%\n",
      "Epoch [77/100], Step [53/225], Training Accuracy: 31.6627%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [54/225], Training Accuracy: 31.5394%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [55/225], Training Accuracy: 31.6761%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [56/225], Training Accuracy: 31.7243%, Training Loss: 0.6854%\n",
      "Epoch [77/100], Step [57/225], Training Accuracy: 31.7160%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [58/225], Training Accuracy: 31.6272%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [59/225], Training Accuracy: 31.9121%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [60/225], Training Accuracy: 32.0052%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [61/225], Training Accuracy: 31.9416%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [62/225], Training Accuracy: 31.9808%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [63/225], Training Accuracy: 32.0437%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [64/225], Training Accuracy: 32.0068%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [65/225], Training Accuracy: 31.9712%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [66/225], Training Accuracy: 32.0786%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [67/225], Training Accuracy: 32.0662%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [68/225], Training Accuracy: 32.1691%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [69/225], Training Accuracy: 32.2237%, Training Loss: 0.6848%\n",
      "Epoch [77/100], Step [70/225], Training Accuracy: 32.1875%, Training Loss: 0.6849%\n",
      "Epoch [77/100], Step [71/225], Training Accuracy: 32.2843%, Training Loss: 0.6849%\n",
      "Epoch [77/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [73/225], Training Accuracy: 31.9991%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [74/225], Training Accuracy: 32.0946%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [75/225], Training Accuracy: 32.0208%, Training Loss: 0.6850%\n",
      "Epoch [77/100], Step [76/225], Training Accuracy: 32.0107%, Training Loss: 0.6851%\n",
      "Epoch [77/100], Step [77/225], Training Accuracy: 31.9602%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [78/225], Training Accuracy: 31.9511%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [79/225], Training Accuracy: 31.8631%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [80/225], Training Accuracy: 31.8164%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [81/225], Training Accuracy: 31.7708%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [82/225], Training Accuracy: 31.7835%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [83/225], Training Accuracy: 31.7206%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [84/225], Training Accuracy: 31.7522%, Training Loss: 0.6852%\n",
      "Epoch [77/100], Step [85/225], Training Accuracy: 31.7279%, Training Loss: 0.6853%\n",
      "Epoch [77/100], Step [86/225], Training Accuracy: 31.7587%, Training Loss: 0.6854%\n",
      "Epoch [77/100], Step [87/225], Training Accuracy: 31.7170%, Training Loss: 0.6854%\n",
      "Epoch [77/100], Step [88/225], Training Accuracy: 31.6939%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [89/225], Training Accuracy: 31.6011%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [90/225], Training Accuracy: 31.5104%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [91/225], Training Accuracy: 31.5076%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [92/225], Training Accuracy: 31.5048%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [93/225], Training Accuracy: 31.5188%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [94/225], Training Accuracy: 31.5824%, Training Loss: 0.6855%\n",
      "Epoch [77/100], Step [95/225], Training Accuracy: 31.4803%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [97/225], Training Accuracy: 31.5883%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [98/225], Training Accuracy: 31.5848%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [99/225], Training Accuracy: 31.7077%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [100/225], Training Accuracy: 31.6562%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [101/225], Training Accuracy: 31.7760%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [102/225], Training Accuracy: 31.6789%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [103/225], Training Accuracy: 31.7203%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [104/225], Training Accuracy: 31.6707%, Training Loss: 0.6858%\n",
      "Epoch [77/100], Step [105/225], Training Accuracy: 31.6369%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [106/225], Training Accuracy: 31.6333%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [107/225], Training Accuracy: 31.5129%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [108/225], Training Accuracy: 31.5828%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [109/225], Training Accuracy: 31.4794%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Step [110/225], Training Accuracy: 31.4915%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [111/225], Training Accuracy: 31.4330%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [112/225], Training Accuracy: 31.4732%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [113/225], Training Accuracy: 31.4159%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [114/225], Training Accuracy: 31.4830%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [115/225], Training Accuracy: 31.4810%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [116/225], Training Accuracy: 31.4790%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [117/225], Training Accuracy: 31.4236%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [118/225], Training Accuracy: 31.3957%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [119/225], Training Accuracy: 31.3419%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [120/225], Training Accuracy: 31.3542%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [121/225], Training Accuracy: 31.3017%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [122/225], Training Accuracy: 31.2884%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [123/225], Training Accuracy: 31.3135%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [124/225], Training Accuracy: 31.2878%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [125/225], Training Accuracy: 31.2625%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [126/225], Training Accuracy: 31.2128%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [127/225], Training Accuracy: 31.1516%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [128/225], Training Accuracy: 31.1523%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [129/225], Training Accuracy: 31.2016%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [130/225], Training Accuracy: 31.1178%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [131/225], Training Accuracy: 31.0592%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [132/225], Training Accuracy: 31.0251%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [133/225], Training Accuracy: 31.0503%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [134/225], Training Accuracy: 31.1217%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [135/225], Training Accuracy: 31.1111%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [136/225], Training Accuracy: 31.1581%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [137/225], Training Accuracy: 31.1930%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [138/225], Training Accuracy: 31.2047%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [139/225], Training Accuracy: 31.1826%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [140/225], Training Accuracy: 31.1719%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [141/225], Training Accuracy: 31.1170%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [142/225], Training Accuracy: 31.1730%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [143/225], Training Accuracy: 31.1517%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [144/225], Training Accuracy: 31.1849%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [145/225], Training Accuracy: 31.2392%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [146/225], Training Accuracy: 31.2928%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [147/225], Training Accuracy: 31.3244%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [148/225], Training Accuracy: 31.2711%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [149/225], Training Accuracy: 31.3024%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [150/225], Training Accuracy: 31.3125%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [151/225], Training Accuracy: 31.3431%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [152/225], Training Accuracy: 31.3528%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [153/225], Training Accuracy: 31.3317%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [154/225], Training Accuracy: 31.3312%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [155/225], Training Accuracy: 31.3004%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [156/225], Training Accuracy: 31.3201%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [157/225], Training Accuracy: 31.2600%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [158/225], Training Accuracy: 31.2401%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [159/225], Training Accuracy: 31.2991%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [160/225], Training Accuracy: 31.2793%, Training Loss: 0.6863%\n",
      "Epoch [77/100], Step [161/225], Training Accuracy: 31.3373%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [162/225], Training Accuracy: 31.3272%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [163/225], Training Accuracy: 31.3842%, Training Loss: 0.6862%\n",
      "Epoch [77/100], Step [164/225], Training Accuracy: 31.3739%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [165/225], Training Accuracy: 31.3068%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [166/225], Training Accuracy: 31.3065%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [167/225], Training Accuracy: 31.3342%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [168/225], Training Accuracy: 31.2686%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [169/225], Training Accuracy: 31.2038%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [170/225], Training Accuracy: 31.1673%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [171/225], Training Accuracy: 31.1769%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [172/225], Training Accuracy: 31.1955%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [174/225], Training Accuracy: 31.2231%, Training Loss: 0.6861%\n",
      "Epoch [77/100], Step [175/225], Training Accuracy: 31.2411%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [176/225], Training Accuracy: 31.2678%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [177/225], Training Accuracy: 31.2765%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [178/225], Training Accuracy: 31.2676%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [179/225], Training Accuracy: 31.2238%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [180/225], Training Accuracy: 31.2587%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [181/225], Training Accuracy: 31.2068%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [182/225], Training Accuracy: 31.2157%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [183/225], Training Accuracy: 31.2244%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [184/225], Training Accuracy: 31.1821%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [185/225], Training Accuracy: 31.1486%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [186/225], Training Accuracy: 31.1912%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [187/225], Training Accuracy: 31.1999%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [188/225], Training Accuracy: 31.1918%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [189/225], Training Accuracy: 31.2169%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [190/225], Training Accuracy: 31.1842%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [191/225], Training Accuracy: 31.1518%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [192/225], Training Accuracy: 31.0791%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [193/225], Training Accuracy: 31.0557%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [194/225], Training Accuracy: 31.0728%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [195/225], Training Accuracy: 31.0497%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [196/225], Training Accuracy: 31.0348%, Training Loss: 0.6860%\n",
      "Epoch [77/100], Step [197/225], Training Accuracy: 31.0914%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [198/225], Training Accuracy: 31.1080%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [199/225], Training Accuracy: 31.0773%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [200/225], Training Accuracy: 31.0625%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [201/225], Training Accuracy: 31.0868%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [202/225], Training Accuracy: 31.0721%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [203/225], Training Accuracy: 31.0653%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [204/225], Training Accuracy: 31.1351%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [205/225], Training Accuracy: 31.1585%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [206/225], Training Accuracy: 31.1666%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [207/225], Training Accuracy: 31.1443%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [208/225], Training Accuracy: 31.1674%, Training Loss: 0.6859%\n",
      "Epoch [77/100], Step [209/225], Training Accuracy: 31.2276%, Training Loss: 0.6858%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100], Step [210/225], Training Accuracy: 31.2649%, Training Loss: 0.6858%\n",
      "Epoch [77/100], Step [211/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [77/100], Step [212/225], Training Accuracy: 31.3016%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [213/225], Training Accuracy: 31.2793%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [214/225], Training Accuracy: 31.2792%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [215/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [216/225], Training Accuracy: 31.1921%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [217/225], Training Accuracy: 31.1852%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [218/225], Training Accuracy: 31.1568%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [219/225], Training Accuracy: 31.2215%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [220/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [221/225], Training Accuracy: 31.2288%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [222/225], Training Accuracy: 31.2430%, Training Loss: 0.6857%\n",
      "Epoch [77/100], Step [223/225], Training Accuracy: 31.2780%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [224/225], Training Accuracy: 31.2849%, Training Loss: 0.6856%\n",
      "Epoch [77/100], Step [225/225], Training Accuracy: 31.2674%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6904%\n",
      "Epoch [78/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6861%\n",
      "Epoch [78/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6870%\n",
      "Epoch [78/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6844%\n",
      "Epoch [78/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6844%\n",
      "Epoch [78/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6859%\n",
      "Epoch [78/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6864%\n",
      "Epoch [78/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6863%\n",
      "Epoch [78/100], Step [16/225], Training Accuracy: 30.6641%, Training Loss: 0.6861%\n",
      "Epoch [78/100], Step [17/225], Training Accuracy: 30.2390%, Training Loss: 0.6862%\n",
      "Epoch [78/100], Step [18/225], Training Accuracy: 30.3819%, Training Loss: 0.6862%\n",
      "Epoch [78/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6864%\n",
      "Epoch [78/100], Step [20/225], Training Accuracy: 31.2500%, Training Loss: 0.6866%\n",
      "Epoch [78/100], Step [21/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [78/100], Step [22/225], Training Accuracy: 31.3920%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [23/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [25/225], Training Accuracy: 31.6250%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [27/225], Training Accuracy: 31.7130%, Training Loss: 0.6851%\n",
      "Epoch [78/100], Step [28/225], Training Accuracy: 31.6964%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6843%\n",
      "Epoch [78/100], Step [34/225], Training Accuracy: 32.1232%, Training Loss: 0.6843%\n",
      "Epoch [78/100], Step [35/225], Training Accuracy: 32.0089%, Training Loss: 0.6844%\n",
      "Epoch [78/100], Step [36/225], Training Accuracy: 31.9010%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [37/225], Training Accuracy: 32.0101%, Training Loss: 0.6847%\n",
      "Epoch [78/100], Step [38/225], Training Accuracy: 31.8668%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [39/225], Training Accuracy: 31.5705%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [40/225], Training Accuracy: 31.6406%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [41/225], Training Accuracy: 31.7835%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [42/225], Training Accuracy: 31.5104%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [43/225], Training Accuracy: 31.6497%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [44/225], Training Accuracy: 31.6051%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [45/225], Training Accuracy: 31.5278%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [46/225], Training Accuracy: 31.4198%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [47/225], Training Accuracy: 31.3497%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [48/225], Training Accuracy: 31.4779%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [49/225], Training Accuracy: 31.5689%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [50/225], Training Accuracy: 31.5625%, Training Loss: 0.6851%\n",
      "Epoch [78/100], Step [51/225], Training Accuracy: 31.6789%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [52/225], Training Accuracy: 31.6406%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [53/225], Training Accuracy: 31.5743%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [54/225], Training Accuracy: 31.4815%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [55/225], Training Accuracy: 31.6193%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [56/225], Training Accuracy: 31.6685%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [57/225], Training Accuracy: 31.6886%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [58/225], Training Accuracy: 31.5733%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [59/225], Training Accuracy: 31.8856%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [60/225], Training Accuracy: 32.0052%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [61/225], Training Accuracy: 31.9416%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [62/225], Training Accuracy: 31.9808%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [63/225], Training Accuracy: 32.0685%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [64/225], Training Accuracy: 32.0557%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [65/225], Training Accuracy: 31.9712%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [66/225], Training Accuracy: 32.0549%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [67/225], Training Accuracy: 32.0429%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [68/225], Training Accuracy: 32.1232%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [69/225], Training Accuracy: 32.1105%, Training Loss: 0.6844%\n",
      "Epoch [78/100], Step [70/225], Training Accuracy: 32.0312%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [71/225], Training Accuracy: 32.0202%, Training Loss: 0.6845%\n",
      "Epoch [78/100], Step [72/225], Training Accuracy: 31.8142%, Training Loss: 0.6847%\n",
      "Epoch [78/100], Step [73/225], Training Accuracy: 31.7423%, Training Loss: 0.6847%\n",
      "Epoch [78/100], Step [74/225], Training Accuracy: 31.7990%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [75/225], Training Accuracy: 31.7292%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [77/225], Training Accuracy: 31.5544%, Training Loss: 0.6847%\n",
      "Epoch [78/100], Step [78/225], Training Accuracy: 31.5705%, Training Loss: 0.6846%\n",
      "Epoch [78/100], Step [79/225], Training Accuracy: 31.5071%, Training Loss: 0.6847%\n",
      "Epoch [78/100], Step [80/225], Training Accuracy: 31.4844%, Training Loss: 0.6847%\n",
      "Epoch [78/100], Step [81/225], Training Accuracy: 31.3850%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [82/225], Training Accuracy: 31.4024%, Training Loss: 0.6848%\n",
      "Epoch [78/100], Step [83/225], Training Accuracy: 31.3253%, Training Loss: 0.6849%\n",
      "Epoch [78/100], Step [84/225], Training Accuracy: 31.3988%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Step [85/225], Training Accuracy: 31.3971%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [86/225], Training Accuracy: 31.4317%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [87/225], Training Accuracy: 31.4655%, Training Loss: 0.6850%\n",
      "Epoch [78/100], Step [88/225], Training Accuracy: 31.4453%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [89/225], Training Accuracy: 31.3729%, Training Loss: 0.6852%\n",
      "Epoch [78/100], Step [90/225], Training Accuracy: 31.2847%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [91/225], Training Accuracy: 31.4045%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [92/225], Training Accuracy: 31.3519%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [93/225], Training Accuracy: 31.3508%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [94/225], Training Accuracy: 31.4328%, Training Loss: 0.6853%\n",
      "Epoch [78/100], Step [95/225], Training Accuracy: 31.3322%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [96/225], Training Accuracy: 31.4290%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [97/225], Training Accuracy: 31.4433%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [98/225], Training Accuracy: 31.4413%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [99/225], Training Accuracy: 31.5814%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [100/225], Training Accuracy: 31.5469%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [101/225], Training Accuracy: 31.6832%, Training Loss: 0.6854%\n",
      "Epoch [78/100], Step [102/225], Training Accuracy: 31.5717%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [103/225], Training Accuracy: 31.6141%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [104/225], Training Accuracy: 31.6106%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [105/225], Training Accuracy: 31.5774%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [107/225], Training Accuracy: 31.4544%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [108/225], Training Accuracy: 31.5249%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [109/225], Training Accuracy: 31.4077%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [110/225], Training Accuracy: 31.4489%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [111/225], Training Accuracy: 31.3626%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [112/225], Training Accuracy: 31.4593%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [113/225], Training Accuracy: 31.4436%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [114/225], Training Accuracy: 31.5515%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [115/225], Training Accuracy: 31.4946%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [116/225], Training Accuracy: 31.5059%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [117/225], Training Accuracy: 31.4637%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [118/225], Training Accuracy: 31.4486%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [119/225], Training Accuracy: 31.3944%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [120/225], Training Accuracy: 31.4323%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [121/225], Training Accuracy: 31.3920%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [122/225], Training Accuracy: 31.3909%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [123/225], Training Accuracy: 31.4278%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [124/225], Training Accuracy: 31.4138%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [125/225], Training Accuracy: 31.3750%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [126/225], Training Accuracy: 31.3244%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [127/225], Training Accuracy: 31.2746%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [128/225], Training Accuracy: 31.2744%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [129/225], Training Accuracy: 31.3106%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [131/225], Training Accuracy: 31.2261%, Training Loss: 0.6859%\n",
      "Epoch [78/100], Step [132/225], Training Accuracy: 31.2145%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [133/225], Training Accuracy: 31.2383%, Training Loss: 0.6859%\n",
      "Epoch [78/100], Step [134/225], Training Accuracy: 31.3083%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [135/225], Training Accuracy: 31.2963%, Training Loss: 0.6859%\n",
      "Epoch [78/100], Step [136/225], Training Accuracy: 31.3304%, Training Loss: 0.6859%\n",
      "Epoch [78/100], Step [137/225], Training Accuracy: 31.3526%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [138/225], Training Accuracy: 31.3519%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [139/225], Training Accuracy: 31.2837%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [140/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [141/225], Training Accuracy: 31.1835%, Training Loss: 0.6859%\n",
      "Epoch [78/100], Step [142/225], Training Accuracy: 31.2390%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [143/225], Training Accuracy: 31.2172%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [144/225], Training Accuracy: 31.2391%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [145/225], Training Accuracy: 31.3147%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [146/225], Training Accuracy: 31.3249%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [147/225], Training Accuracy: 31.3457%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [148/225], Training Accuracy: 31.3239%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [149/225], Training Accuracy: 31.3654%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [150/225], Training Accuracy: 31.3958%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [151/225], Training Accuracy: 31.4259%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [152/225], Training Accuracy: 31.4248%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [153/225], Training Accuracy: 31.3725%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [154/225], Training Accuracy: 31.3819%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [155/225], Training Accuracy: 31.3710%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [156/225], Training Accuracy: 31.4002%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [157/225], Training Accuracy: 31.3396%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [158/225], Training Accuracy: 31.3291%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [159/225], Training Accuracy: 31.3778%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [160/225], Training Accuracy: 31.3379%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [161/225], Training Accuracy: 31.3762%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [162/225], Training Accuracy: 31.3465%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [163/225], Training Accuracy: 31.4130%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [164/225], Training Accuracy: 31.3834%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [165/225], Training Accuracy: 31.3068%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [166/225], Training Accuracy: 31.3159%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [167/225], Training Accuracy: 31.3529%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [168/225], Training Accuracy: 31.3058%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [169/225], Training Accuracy: 31.2408%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [170/225], Training Accuracy: 31.2040%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [171/225], Training Accuracy: 31.2043%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [172/225], Training Accuracy: 31.2409%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [173/225], Training Accuracy: 31.2319%, Training Loss: 0.6858%\n",
      "Epoch [78/100], Step [174/225], Training Accuracy: 31.2410%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [175/225], Training Accuracy: 31.2768%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [176/225], Training Accuracy: 31.2944%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [177/225], Training Accuracy: 31.3118%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [178/225], Training Accuracy: 31.3027%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [179/225], Training Accuracy: 31.2675%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [180/225], Training Accuracy: 31.3108%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [181/225], Training Accuracy: 31.2586%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [182/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [183/225], Training Accuracy: 31.2585%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [184/225], Training Accuracy: 31.2245%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Step [185/225], Training Accuracy: 31.2078%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [186/225], Training Accuracy: 31.2164%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [187/225], Training Accuracy: 31.2166%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [188/225], Training Accuracy: 31.2168%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [189/225], Training Accuracy: 31.2417%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [190/225], Training Accuracy: 31.2007%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [191/225], Training Accuracy: 31.1764%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [192/225], Training Accuracy: 31.1117%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [193/225], Training Accuracy: 31.1286%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [194/225], Training Accuracy: 31.1372%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [195/225], Training Accuracy: 31.1058%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [196/225], Training Accuracy: 31.0826%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [197/225], Training Accuracy: 31.1231%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [198/225], Training Accuracy: 31.1395%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [199/225], Training Accuracy: 31.1244%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [200/225], Training Accuracy: 31.1016%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [201/225], Training Accuracy: 31.1256%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [202/225], Training Accuracy: 31.1262%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [203/225], Training Accuracy: 31.1038%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [204/225], Training Accuracy: 31.1657%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [205/225], Training Accuracy: 31.1585%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [206/225], Training Accuracy: 31.1514%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [207/225], Training Accuracy: 31.1368%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [208/225], Training Accuracy: 31.1523%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [209/225], Training Accuracy: 31.1977%, Training Loss: 0.6857%\n",
      "Epoch [78/100], Step [210/225], Training Accuracy: 31.2128%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [211/225], Training Accuracy: 31.1982%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [213/225], Training Accuracy: 31.2133%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [214/225], Training Accuracy: 31.2427%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [215/225], Training Accuracy: 31.2137%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [216/225], Training Accuracy: 31.1487%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [217/225], Training Accuracy: 31.1564%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [218/225], Training Accuracy: 31.1282%, Training Loss: 0.6856%\n",
      "Epoch [78/100], Step [219/225], Training Accuracy: 31.1858%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [220/225], Training Accuracy: 31.2074%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [221/225], Training Accuracy: 31.2005%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [222/225], Training Accuracy: 31.2007%, Training Loss: 0.6854%\n",
      "Epoch [78/100], Step [223/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [224/225], Training Accuracy: 31.2360%, Training Loss: 0.6855%\n",
      "Epoch [78/100], Step [225/225], Training Accuracy: 31.2118%, Training Loss: 0.6855%\n",
      "Epoch [79/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6889%\n",
      "Epoch [79/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6865%\n",
      "Epoch [79/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6858%\n",
      "Epoch [79/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6855%\n",
      "Epoch [79/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6855%\n",
      "Epoch [79/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6856%\n",
      "Epoch [79/100], Step [14/225], Training Accuracy: 29.7991%, Training Loss: 0.6863%\n",
      "Epoch [79/100], Step [15/225], Training Accuracy: 30.2083%, Training Loss: 0.6863%\n",
      "Epoch [79/100], Step [16/225], Training Accuracy: 30.1758%, Training Loss: 0.6860%\n",
      "Epoch [79/100], Step [17/225], Training Accuracy: 30.0551%, Training Loss: 0.6860%\n",
      "Epoch [79/100], Step [18/225], Training Accuracy: 30.1215%, Training Loss: 0.6859%\n",
      "Epoch [79/100], Step [19/225], Training Accuracy: 30.6743%, Training Loss: 0.6859%\n",
      "Epoch [79/100], Step [20/225], Training Accuracy: 31.1719%, Training Loss: 0.6859%\n",
      "Epoch [79/100], Step [21/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [79/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6855%\n",
      "Epoch [79/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [24/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [25/225], Training Accuracy: 31.8750%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [27/225], Training Accuracy: 31.9444%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [29/225], Training Accuracy: 32.2198%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [30/225], Training Accuracy: 32.1354%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [31/225], Training Accuracy: 32.0060%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [32/225], Training Accuracy: 32.2266%, Training Loss: 0.6841%\n",
      "Epoch [79/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6841%\n",
      "Epoch [79/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6841%\n",
      "Epoch [79/100], Step [35/225], Training Accuracy: 32.1875%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [36/225], Training Accuracy: 32.1615%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [37/225], Training Accuracy: 32.1791%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [38/225], Training Accuracy: 32.0724%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [39/225], Training Accuracy: 31.8109%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [40/225], Training Accuracy: 31.8750%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [41/225], Training Accuracy: 31.8216%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [42/225], Training Accuracy: 31.6592%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [43/225], Training Accuracy: 31.7587%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [44/225], Training Accuracy: 31.7116%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [45/225], Training Accuracy: 31.7014%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [46/225], Training Accuracy: 31.5557%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [47/225], Training Accuracy: 31.4827%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [48/225], Training Accuracy: 31.6081%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [49/225], Training Accuracy: 31.6008%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [50/225], Training Accuracy: 31.6250%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [51/225], Training Accuracy: 31.8015%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [52/225], Training Accuracy: 31.7909%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [53/225], Training Accuracy: 31.7217%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [54/225], Training Accuracy: 31.5972%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [55/225], Training Accuracy: 31.7330%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [56/225], Training Accuracy: 31.7801%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [57/225], Training Accuracy: 31.7708%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [58/225], Training Accuracy: 31.7349%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [59/225], Training Accuracy: 32.0975%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [60/225], Training Accuracy: 32.1875%, Training Loss: 0.6842%\n",
      "Epoch [79/100], Step [61/225], Training Accuracy: 32.1721%, Training Loss: 0.6842%\n",
      "Epoch [79/100], Step [62/225], Training Accuracy: 32.2077%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [63/225], Training Accuracy: 32.3165%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [64/225], Training Accuracy: 32.2754%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [65/225], Training Accuracy: 32.1875%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [66/225], Training Accuracy: 32.2917%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [67/225], Training Accuracy: 32.2295%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [68/225], Training Accuracy: 32.3300%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [69/225], Training Accuracy: 32.3596%, Training Loss: 0.6842%\n",
      "Epoch [79/100], Step [70/225], Training Accuracy: 32.2991%, Training Loss: 0.6842%\n",
      "Epoch [79/100], Step [71/225], Training Accuracy: 32.3283%, Training Loss: 0.6842%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Step [72/225], Training Accuracy: 32.0964%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [73/225], Training Accuracy: 32.0205%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [74/225], Training Accuracy: 32.1368%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [75/225], Training Accuracy: 32.1250%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [76/225], Training Accuracy: 32.0518%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [77/225], Training Accuracy: 31.9196%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [78/225], Training Accuracy: 31.9712%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [79/225], Training Accuracy: 31.8829%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [80/225], Training Accuracy: 31.8555%, Training Loss: 0.6843%\n",
      "Epoch [79/100], Step [81/225], Training Accuracy: 31.7708%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [82/225], Training Accuracy: 31.7835%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [83/225], Training Accuracy: 31.7206%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [84/225], Training Accuracy: 31.7894%, Training Loss: 0.6844%\n",
      "Epoch [79/100], Step [85/225], Training Accuracy: 31.8015%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [86/225], Training Accuracy: 31.8314%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [87/225], Training Accuracy: 31.8427%, Training Loss: 0.6845%\n",
      "Epoch [79/100], Step [88/225], Training Accuracy: 31.8359%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [89/225], Training Accuracy: 31.7240%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [90/225], Training Accuracy: 31.6319%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [91/225], Training Accuracy: 31.7136%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [92/225], Training Accuracy: 31.6746%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [93/225], Training Accuracy: 31.6532%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [94/225], Training Accuracy: 31.6988%, Training Loss: 0.6846%\n",
      "Epoch [79/100], Step [95/225], Training Accuracy: 31.5789%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [96/225], Training Accuracy: 31.6732%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [97/225], Training Accuracy: 31.7010%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [98/225], Training Accuracy: 31.6964%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [99/225], Training Accuracy: 31.7866%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [100/225], Training Accuracy: 31.7812%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [101/225], Training Accuracy: 31.9152%, Training Loss: 0.6847%\n",
      "Epoch [79/100], Step [102/225], Training Accuracy: 31.8015%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [103/225], Training Accuracy: 31.8720%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [104/225], Training Accuracy: 31.8510%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [105/225], Training Accuracy: 31.8155%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [106/225], Training Accuracy: 31.7954%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [107/225], Training Accuracy: 31.7027%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [108/225], Training Accuracy: 31.7708%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [109/225], Training Accuracy: 31.6370%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [110/225], Training Accuracy: 31.6761%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [111/225], Training Accuracy: 31.6019%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [112/225], Training Accuracy: 31.6825%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [113/225], Training Accuracy: 31.6372%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [114/225], Training Accuracy: 31.7023%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [115/225], Training Accuracy: 31.6576%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [116/225], Training Accuracy: 31.6676%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [117/225], Training Accuracy: 31.6239%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [118/225], Training Accuracy: 31.6075%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [119/225], Training Accuracy: 31.5651%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [120/225], Training Accuracy: 31.5755%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [121/225], Training Accuracy: 31.5341%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [122/225], Training Accuracy: 31.5061%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [123/225], Training Accuracy: 31.5295%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [124/225], Training Accuracy: 31.4768%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [125/225], Training Accuracy: 31.4625%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [126/225], Training Accuracy: 31.4112%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [127/225], Training Accuracy: 31.3853%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [128/225], Training Accuracy: 31.3843%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [129/225], Training Accuracy: 31.4438%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [130/225], Training Accuracy: 31.3221%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [131/225], Training Accuracy: 31.3096%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [132/225], Training Accuracy: 31.2737%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [133/225], Training Accuracy: 31.3087%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [134/225], Training Accuracy: 31.3666%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [135/225], Training Accuracy: 31.3542%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [136/225], Training Accuracy: 31.3649%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [137/225], Training Accuracy: 31.4097%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [138/225], Training Accuracy: 31.4538%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [139/225], Training Accuracy: 31.4299%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [140/225], Training Accuracy: 31.4062%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [141/225], Training Accuracy: 31.3608%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [142/225], Training Accuracy: 31.4040%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [143/225], Training Accuracy: 31.3811%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [144/225], Training Accuracy: 31.4019%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [145/225], Training Accuracy: 31.4763%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [146/225], Training Accuracy: 31.5283%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [147/225], Training Accuracy: 31.5476%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [148/225], Training Accuracy: 31.5139%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [149/225], Training Accuracy: 31.5331%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [150/225], Training Accuracy: 31.5417%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [151/225], Training Accuracy: 31.5811%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [152/225], Training Accuracy: 31.5892%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [153/225], Training Accuracy: 31.5257%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [154/225], Training Accuracy: 31.5645%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [155/225], Training Accuracy: 31.5121%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [156/225], Training Accuracy: 31.5405%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [157/225], Training Accuracy: 31.4789%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [158/225], Training Accuracy: 31.4577%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [159/225], Training Accuracy: 31.5252%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [160/225], Training Accuracy: 31.4746%, Training Loss: 0.6854%\n",
      "Epoch [79/100], Step [161/225], Training Accuracy: 31.5509%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [162/225], Training Accuracy: 31.5201%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [163/225], Training Accuracy: 31.5759%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [164/225], Training Accuracy: 31.5739%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [165/225], Training Accuracy: 31.5057%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [166/225], Training Accuracy: 31.4853%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [167/225], Training Accuracy: 31.5120%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [168/225], Training Accuracy: 31.4546%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [169/225], Training Accuracy: 31.4072%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [170/225], Training Accuracy: 31.3603%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [171/225], Training Accuracy: 31.3779%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Step [172/225], Training Accuracy: 31.3772%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [173/225], Training Accuracy: 31.3584%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [174/225], Training Accuracy: 31.3757%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [175/225], Training Accuracy: 31.4107%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [176/225], Training Accuracy: 31.4364%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [177/225], Training Accuracy: 31.4619%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [178/225], Training Accuracy: 31.4519%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [179/225], Training Accuracy: 31.4246%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [180/225], Training Accuracy: 31.4583%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [181/225], Training Accuracy: 31.4140%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [182/225], Training Accuracy: 31.3959%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [183/225], Training Accuracy: 31.3952%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [184/225], Training Accuracy: 31.3519%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [185/225], Training Accuracy: 31.3260%, Training Loss: 0.6853%\n",
      "Epoch [79/100], Step [186/225], Training Accuracy: 31.3340%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [187/225], Training Accuracy: 31.3753%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [188/225], Training Accuracy: 31.3664%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [189/225], Training Accuracy: 31.3905%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [190/225], Training Accuracy: 31.3405%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [191/225], Training Accuracy: 31.3073%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [192/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [193/225], Training Accuracy: 31.2338%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [194/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [195/225], Training Accuracy: 31.2340%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [196/225], Training Accuracy: 31.2101%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [197/225], Training Accuracy: 31.2262%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [198/225], Training Accuracy: 31.2421%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [199/225], Training Accuracy: 31.2421%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [200/225], Training Accuracy: 31.2188%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [201/225], Training Accuracy: 31.2189%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [202/225], Training Accuracy: 31.2191%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [203/225], Training Accuracy: 31.2115%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [204/225], Training Accuracy: 31.2730%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [205/225], Training Accuracy: 31.2652%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [206/225], Training Accuracy: 31.2576%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [207/225], Training Accuracy: 31.2425%, Training Loss: 0.6852%\n",
      "Epoch [79/100], Step [208/225], Training Accuracy: 31.2425%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [209/225], Training Accuracy: 31.3023%, Training Loss: 0.6851%\n",
      "Epoch [79/100], Step [210/225], Training Accuracy: 31.3095%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [211/225], Training Accuracy: 31.2944%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [212/225], Training Accuracy: 31.3679%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [213/225], Training Accuracy: 31.3454%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [214/225], Training Accuracy: 31.3668%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [215/225], Training Accuracy: 31.3445%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [216/225], Training Accuracy: 31.2862%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [217/225], Training Accuracy: 31.2860%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [218/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [79/100], Step [219/225], Training Accuracy: 31.3213%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [220/225], Training Accuracy: 31.3565%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [221/225], Training Accuracy: 31.3490%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [222/225], Training Accuracy: 31.3626%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [223/225], Training Accuracy: 31.4041%, Training Loss: 0.6849%\n",
      "Epoch [79/100], Step [224/225], Training Accuracy: 31.4035%, Training Loss: 0.6848%\n",
      "Epoch [79/100], Step [225/225], Training Accuracy: 31.3785%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6892%\n",
      "Epoch [80/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [80/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6895%\n",
      "Epoch [80/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [10/225], Training Accuracy: 30.3125%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [11/225], Training Accuracy: 30.5398%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6848%\n",
      "Epoch [80/100], Step [14/225], Training Accuracy: 30.0223%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [18/225], Training Accuracy: 30.8160%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [22/225], Training Accuracy: 31.8892%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6847%\n",
      "Epoch [80/100], Step [27/225], Training Accuracy: 31.8287%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [31/225], Training Accuracy: 31.8548%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [32/225], Training Accuracy: 32.1777%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [33/225], Training Accuracy: 32.2443%, Training Loss: 0.6839%\n",
      "Epoch [80/100], Step [34/225], Training Accuracy: 32.0312%, Training Loss: 0.6839%\n",
      "Epoch [80/100], Step [35/225], Training Accuracy: 31.9196%, Training Loss: 0.6840%\n",
      "Epoch [80/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [37/225], Training Accuracy: 31.7145%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [38/225], Training Accuracy: 31.5789%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [39/225], Training Accuracy: 31.2901%, Training Loss: 0.6845%\n",
      "Epoch [80/100], Step [40/225], Training Accuracy: 31.3672%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [41/225], Training Accuracy: 31.5168%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [42/225], Training Accuracy: 31.2128%, Training Loss: 0.6848%\n",
      "Epoch [80/100], Step [43/225], Training Accuracy: 31.3953%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [44/225], Training Accuracy: 31.3920%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [45/225], Training Accuracy: 31.4236%, Training Loss: 0.6845%\n",
      "Epoch [80/100], Step [46/225], Training Accuracy: 31.3179%, Training Loss: 0.6844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Step [47/225], Training Accuracy: 31.1835%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [48/225], Training Accuracy: 31.3151%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [49/225], Training Accuracy: 31.3138%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6845%\n",
      "Epoch [80/100], Step [51/225], Training Accuracy: 31.4338%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [52/225], Training Accuracy: 31.4904%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [53/225], Training Accuracy: 31.4269%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [55/225], Training Accuracy: 31.3920%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [56/225], Training Accuracy: 31.4732%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [57/225], Training Accuracy: 31.4967%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [58/225], Training Accuracy: 31.4116%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [59/225], Training Accuracy: 31.8061%, Training Loss: 0.6840%\n",
      "Epoch [80/100], Step [60/225], Training Accuracy: 31.9010%, Training Loss: 0.6839%\n",
      "Epoch [80/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6840%\n",
      "Epoch [80/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [63/225], Training Accuracy: 31.9196%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [64/225], Training Accuracy: 31.9092%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [66/225], Training Accuracy: 31.9839%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [67/225], Training Accuracy: 31.9263%, Training Loss: 0.6842%\n",
      "Epoch [80/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [69/225], Training Accuracy: 31.9293%, Training Loss: 0.6840%\n",
      "Epoch [80/100], Step [70/225], Training Accuracy: 31.8750%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [71/225], Training Accuracy: 31.9542%, Training Loss: 0.6841%\n",
      "Epoch [80/100], Step [72/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [73/225], Training Accuracy: 31.6781%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [74/225], Training Accuracy: 31.7568%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [75/225], Training Accuracy: 31.6875%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [76/225], Training Accuracy: 31.6612%, Training Loss: 0.6843%\n",
      "Epoch [80/100], Step [77/225], Training Accuracy: 31.5950%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [78/225], Training Accuracy: 31.6306%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [79/225], Training Accuracy: 31.5665%, Training Loss: 0.6845%\n",
      "Epoch [80/100], Step [80/225], Training Accuracy: 31.5625%, Training Loss: 0.6844%\n",
      "Epoch [80/100], Step [81/225], Training Accuracy: 31.4815%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [82/225], Training Accuracy: 31.4787%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [83/225], Training Accuracy: 31.4194%, Training Loss: 0.6847%\n",
      "Epoch [80/100], Step [84/225], Training Accuracy: 31.4918%, Training Loss: 0.6846%\n",
      "Epoch [80/100], Step [85/225], Training Accuracy: 31.4338%, Training Loss: 0.6847%\n",
      "Epoch [80/100], Step [86/225], Training Accuracy: 31.5044%, Training Loss: 0.6847%\n",
      "Epoch [80/100], Step [87/225], Training Accuracy: 31.5374%, Training Loss: 0.6848%\n",
      "Epoch [80/100], Step [88/225], Training Accuracy: 31.5163%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [89/225], Training Accuracy: 31.4431%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [90/225], Training Accuracy: 31.3542%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [91/225], Training Accuracy: 31.4389%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [92/225], Training Accuracy: 31.4368%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [93/225], Training Accuracy: 31.4012%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [94/225], Training Accuracy: 31.4993%, Training Loss: 0.6849%\n",
      "Epoch [80/100], Step [95/225], Training Accuracy: 31.3816%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [96/225], Training Accuracy: 31.4779%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [97/225], Training Accuracy: 31.5077%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [98/225], Training Accuracy: 31.5370%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [99/225], Training Accuracy: 31.6446%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [100/225], Training Accuracy: 31.6094%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [101/225], Training Accuracy: 31.7450%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [102/225], Training Accuracy: 31.6330%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [103/225], Training Accuracy: 31.6899%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [104/225], Training Accuracy: 31.6556%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [105/225], Training Accuracy: 31.5774%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [106/225], Training Accuracy: 31.5890%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [107/225], Training Accuracy: 31.4982%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [108/225], Training Accuracy: 31.5828%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [109/225], Training Accuracy: 31.4507%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [110/225], Training Accuracy: 31.4773%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [111/225], Training Accuracy: 31.3767%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [112/225], Training Accuracy: 31.4732%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [113/225], Training Accuracy: 31.4712%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [114/225], Training Accuracy: 31.5241%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [115/225], Training Accuracy: 31.5082%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [116/225], Training Accuracy: 31.4790%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [117/225], Training Accuracy: 31.4236%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [118/225], Training Accuracy: 31.3692%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [119/225], Training Accuracy: 31.3157%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [120/225], Training Accuracy: 31.3281%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [121/225], Training Accuracy: 31.3146%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [122/225], Training Accuracy: 31.2884%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [123/225], Training Accuracy: 31.3135%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [124/225], Training Accuracy: 31.2878%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [125/225], Training Accuracy: 31.2750%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [126/225], Training Accuracy: 31.2004%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [127/225], Training Accuracy: 31.1516%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [128/225], Training Accuracy: 31.1401%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [129/225], Training Accuracy: 31.1652%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [130/225], Training Accuracy: 31.1058%, Training Loss: 0.6859%\n",
      "Epoch [80/100], Step [131/225], Training Accuracy: 31.0592%, Training Loss: 0.6859%\n",
      "Epoch [80/100], Step [132/225], Training Accuracy: 31.0251%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [133/225], Training Accuracy: 31.0385%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [134/225], Training Accuracy: 31.0984%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [135/225], Training Accuracy: 31.0995%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [136/225], Training Accuracy: 31.0662%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [137/225], Training Accuracy: 31.0675%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [138/225], Training Accuracy: 31.0802%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [139/225], Training Accuracy: 31.0701%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [140/225], Training Accuracy: 31.0826%, Training Loss: 0.6859%\n",
      "Epoch [80/100], Step [141/225], Training Accuracy: 31.0284%, Training Loss: 0.6859%\n",
      "Epoch [80/100], Step [142/225], Training Accuracy: 31.0849%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [143/225], Training Accuracy: 31.0752%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [144/225], Training Accuracy: 31.0872%, Training Loss: 0.6858%\n",
      "Epoch [80/100], Step [145/225], Training Accuracy: 31.1746%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [146/225], Training Accuracy: 31.2072%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [147/225], Training Accuracy: 31.2287%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [148/225], Training Accuracy: 31.1972%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [150/225], Training Accuracy: 31.2604%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [151/225], Training Accuracy: 31.3328%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [152/225], Training Accuracy: 31.3425%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [153/225], Training Accuracy: 31.3011%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [154/225], Training Accuracy: 31.3109%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [155/225], Training Accuracy: 31.2903%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [156/225], Training Accuracy: 31.3301%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [157/225], Training Accuracy: 31.2600%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [158/225], Training Accuracy: 31.2401%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Step [159/225], Training Accuracy: 31.3090%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [160/225], Training Accuracy: 31.2891%, Training Loss: 0.6857%\n",
      "Epoch [80/100], Step [161/225], Training Accuracy: 31.3470%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [162/225], Training Accuracy: 31.3175%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [163/225], Training Accuracy: 31.3746%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [164/225], Training Accuracy: 31.3548%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [165/225], Training Accuracy: 31.2973%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [166/225], Training Accuracy: 31.2877%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [167/225], Training Accuracy: 31.3529%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [168/225], Training Accuracy: 31.3058%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [169/225], Training Accuracy: 31.2315%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [170/225], Training Accuracy: 31.1949%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [171/225], Training Accuracy: 31.2226%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [172/225], Training Accuracy: 31.2318%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6856%\n",
      "Epoch [80/100], Step [174/225], Training Accuracy: 31.1961%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [175/225], Training Accuracy: 31.2411%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [176/225], Training Accuracy: 31.2678%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [177/225], Training Accuracy: 31.2677%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [178/225], Training Accuracy: 31.2588%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [179/225], Training Accuracy: 31.2325%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [180/225], Training Accuracy: 31.2760%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [181/225], Training Accuracy: 31.2241%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [182/225], Training Accuracy: 31.2242%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [183/225], Training Accuracy: 31.2329%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [184/225], Training Accuracy: 31.2160%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [185/225], Training Accuracy: 31.1824%, Training Loss: 0.6855%\n",
      "Epoch [80/100], Step [186/225], Training Accuracy: 31.1996%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [187/225], Training Accuracy: 31.2166%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [188/225], Training Accuracy: 31.2001%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [189/225], Training Accuracy: 31.2335%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [190/225], Training Accuracy: 31.1760%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [191/225], Training Accuracy: 31.1518%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [192/225], Training Accuracy: 31.0872%, Training Loss: 0.6854%\n",
      "Epoch [80/100], Step [193/225], Training Accuracy: 31.0962%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [194/225], Training Accuracy: 31.1131%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [195/225], Training Accuracy: 31.0897%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [196/225], Training Accuracy: 31.0746%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [197/225], Training Accuracy: 31.1231%, Training Loss: 0.6853%\n",
      "Epoch [80/100], Step [198/225], Training Accuracy: 31.1395%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [199/225], Training Accuracy: 31.1165%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [200/225], Training Accuracy: 31.1016%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [201/225], Training Accuracy: 31.1178%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [202/225], Training Accuracy: 31.1108%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [203/225], Training Accuracy: 31.1038%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [204/225], Training Accuracy: 31.1504%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [205/225], Training Accuracy: 31.1662%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [206/225], Training Accuracy: 31.1590%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [207/225], Training Accuracy: 31.1292%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [208/225], Training Accuracy: 31.1599%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [209/225], Training Accuracy: 31.1977%, Training Loss: 0.6852%\n",
      "Epoch [80/100], Step [210/225], Training Accuracy: 31.2426%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [211/225], Training Accuracy: 31.2352%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [212/225], Training Accuracy: 31.3016%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [213/225], Training Accuracy: 31.2720%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [214/225], Training Accuracy: 31.2938%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [215/225], Training Accuracy: 31.2645%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [216/225], Training Accuracy: 31.2066%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [217/225], Training Accuracy: 31.2212%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [218/225], Training Accuracy: 31.1855%, Training Loss: 0.6851%\n",
      "Epoch [80/100], Step [219/225], Training Accuracy: 31.2429%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [220/225], Training Accuracy: 31.2642%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [221/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [222/225], Training Accuracy: 31.2641%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [223/225], Training Accuracy: 31.3061%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [224/225], Training Accuracy: 31.3058%, Training Loss: 0.6850%\n",
      "Epoch [80/100], Step [225/225], Training Accuracy: 31.2813%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6853%\n",
      "Epoch [81/100], Step [2/225], Training Accuracy: 30.4688%, Training Loss: 0.6824%\n",
      "Epoch [81/100], Step [3/225], Training Accuracy: 30.7292%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [4/225], Training Accuracy: 31.2500%, Training Loss: 0.6831%\n",
      "Epoch [81/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6835%\n",
      "Epoch [81/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6834%\n",
      "Epoch [81/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6836%\n",
      "Epoch [81/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [12/225], Training Accuracy: 30.5990%, Training Loss: 0.6837%\n",
      "Epoch [81/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [16/225], Training Accuracy: 30.5664%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [17/225], Training Accuracy: 30.4228%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [23/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [24/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [25/225], Training Accuracy: 31.1875%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [26/225], Training Accuracy: 31.4904%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [27/225], Training Accuracy: 31.3079%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [28/225], Training Accuracy: 31.1384%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [29/225], Training Accuracy: 31.6272%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [30/225], Training Accuracy: 31.5625%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [31/225], Training Accuracy: 31.4516%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [32/225], Training Accuracy: 31.7383%, Training Loss: 0.6838%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Step [33/225], Training Accuracy: 31.8182%, Training Loss: 0.6837%\n",
      "Epoch [81/100], Step [34/225], Training Accuracy: 31.6636%, Training Loss: 0.6838%\n",
      "Epoch [81/100], Step [35/225], Training Accuracy: 31.5179%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [36/225], Training Accuracy: 31.2934%, Training Loss: 0.6842%\n",
      "Epoch [81/100], Step [37/225], Training Accuracy: 31.3767%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [38/225], Training Accuracy: 31.1678%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [39/225], Training Accuracy: 30.9295%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [40/225], Training Accuracy: 30.9375%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [41/225], Training Accuracy: 30.8308%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [42/225], Training Accuracy: 30.7292%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [43/225], Training Accuracy: 30.9593%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [44/225], Training Accuracy: 31.0014%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [45/225], Training Accuracy: 31.0069%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [46/225], Training Accuracy: 30.9103%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [47/225], Training Accuracy: 30.8511%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [48/225], Training Accuracy: 31.0221%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [49/225], Training Accuracy: 31.0587%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [50/225], Training Accuracy: 31.0000%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [51/225], Training Accuracy: 31.1887%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [52/225], Training Accuracy: 31.2200%, Training Loss: 0.6841%\n",
      "Epoch [81/100], Step [53/225], Training Accuracy: 31.1616%, Training Loss: 0.6841%\n",
      "Epoch [81/100], Step [54/225], Training Accuracy: 31.0185%, Training Loss: 0.6842%\n",
      "Epoch [81/100], Step [55/225], Training Accuracy: 31.1648%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [56/225], Training Accuracy: 31.2221%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [57/225], Training Accuracy: 31.2226%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [58/225], Training Accuracy: 31.0884%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [59/225], Training Accuracy: 31.4619%, Training Loss: 0.6841%\n",
      "Epoch [81/100], Step [60/225], Training Accuracy: 31.6146%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [61/225], Training Accuracy: 31.5574%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [62/225], Training Accuracy: 31.5776%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [63/225], Training Accuracy: 31.6468%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [64/225], Training Accuracy: 31.6895%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [65/225], Training Accuracy: 31.5625%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [66/225], Training Accuracy: 31.6525%, Training Loss: 0.6841%\n",
      "Epoch [81/100], Step [67/225], Training Accuracy: 31.6931%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [68/225], Training Accuracy: 31.7325%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [69/225], Training Accuracy: 31.7255%, Training Loss: 0.6837%\n",
      "Epoch [81/100], Step [70/225], Training Accuracy: 31.6964%, Training Loss: 0.6837%\n",
      "Epoch [81/100], Step [71/225], Training Accuracy: 31.7342%, Training Loss: 0.6838%\n",
      "Epoch [81/100], Step [72/225], Training Accuracy: 31.5104%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [73/225], Training Accuracy: 31.4640%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [74/225], Training Accuracy: 31.5667%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [75/225], Training Accuracy: 31.5417%, Training Loss: 0.6838%\n",
      "Epoch [81/100], Step [76/225], Training Accuracy: 31.5378%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [77/225], Training Accuracy: 31.4529%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [78/225], Training Accuracy: 31.4904%, Training Loss: 0.6839%\n",
      "Epoch [81/100], Step [79/225], Training Accuracy: 31.4280%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [80/225], Training Accuracy: 31.4062%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [81/225], Training Accuracy: 31.3079%, Training Loss: 0.6841%\n",
      "Epoch [81/100], Step [82/225], Training Accuracy: 31.3262%, Training Loss: 0.6840%\n",
      "Epoch [81/100], Step [83/225], Training Accuracy: 31.2500%, Training Loss: 0.6842%\n",
      "Epoch [81/100], Step [84/225], Training Accuracy: 31.3058%, Training Loss: 0.6842%\n",
      "Epoch [81/100], Step [85/225], Training Accuracy: 31.2684%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [86/225], Training Accuracy: 31.3227%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [87/225], Training Accuracy: 31.3398%, Training Loss: 0.6843%\n",
      "Epoch [81/100], Step [88/225], Training Accuracy: 31.3033%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [89/225], Training Accuracy: 31.2324%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [90/225], Training Accuracy: 31.1458%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [91/225], Training Accuracy: 31.2328%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [92/225], Training Accuracy: 31.2330%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [93/225], Training Accuracy: 31.2164%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [94/225], Training Accuracy: 31.2832%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [95/225], Training Accuracy: 31.1678%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [96/225], Training Accuracy: 31.2826%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [97/225], Training Accuracy: 31.2983%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [98/225], Training Accuracy: 31.3297%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [99/225], Training Accuracy: 31.4867%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [100/225], Training Accuracy: 31.4688%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [101/225], Training Accuracy: 31.5903%, Training Loss: 0.6844%\n",
      "Epoch [81/100], Step [102/225], Training Accuracy: 31.4951%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [103/225], Training Accuracy: 31.5686%, Training Loss: 0.6845%\n",
      "Epoch [81/100], Step [104/225], Training Accuracy: 31.5355%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [105/225], Training Accuracy: 31.4881%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [106/225], Training Accuracy: 31.5006%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [107/225], Training Accuracy: 31.3668%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [108/225], Training Accuracy: 31.4670%, Training Loss: 0.6846%\n",
      "Epoch [81/100], Step [109/225], Training Accuracy: 31.3647%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [110/225], Training Accuracy: 31.3778%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [111/225], Training Accuracy: 31.2922%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [112/225], Training Accuracy: 31.3337%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [113/225], Training Accuracy: 31.3330%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [114/225], Training Accuracy: 31.3734%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [115/225], Training Accuracy: 31.3315%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [116/225], Training Accuracy: 31.3173%, Training Loss: 0.6847%\n",
      "Epoch [81/100], Step [117/225], Training Accuracy: 31.2366%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [118/225], Training Accuracy: 31.1970%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [119/225], Training Accuracy: 31.1187%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [120/225], Training Accuracy: 31.1328%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [121/225], Training Accuracy: 31.1338%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [123/225], Training Accuracy: 31.1611%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [124/225], Training Accuracy: 31.1618%, Training Loss: 0.6848%\n",
      "Epoch [81/100], Step [125/225], Training Accuracy: 31.1125%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [126/225], Training Accuracy: 31.0640%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [127/225], Training Accuracy: 31.0039%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [128/225], Training Accuracy: 30.9814%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [129/225], Training Accuracy: 30.9956%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [130/225], Training Accuracy: 30.9014%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [131/225], Training Accuracy: 30.8922%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [132/225], Training Accuracy: 30.8594%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100], Step [133/225], Training Accuracy: 30.8741%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [134/225], Training Accuracy: 30.9352%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [135/225], Training Accuracy: 30.9491%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [136/225], Training Accuracy: 30.9513%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [137/225], Training Accuracy: 30.9877%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [138/225], Training Accuracy: 31.0236%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [139/225], Training Accuracy: 31.0139%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [140/225], Training Accuracy: 30.9933%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [141/225], Training Accuracy: 30.9397%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [142/225], Training Accuracy: 31.0079%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [143/225], Training Accuracy: 30.9987%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [144/225], Training Accuracy: 31.0004%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [145/225], Training Accuracy: 31.0560%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [146/225], Training Accuracy: 31.0681%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [147/225], Training Accuracy: 31.0906%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [148/225], Training Accuracy: 31.0811%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [149/225], Training Accuracy: 31.1032%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [150/225], Training Accuracy: 31.1250%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [151/225], Training Accuracy: 31.1569%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [152/225], Training Accuracy: 31.1472%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [153/225], Training Accuracy: 31.1172%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [154/225], Training Accuracy: 31.1384%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [155/225], Training Accuracy: 31.1391%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [156/225], Training Accuracy: 31.1699%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [157/225], Training Accuracy: 31.1007%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [158/225], Training Accuracy: 31.1017%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [159/225], Training Accuracy: 31.1714%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [160/225], Training Accuracy: 31.1621%, Training Loss: 0.6853%\n",
      "Epoch [81/100], Step [161/225], Training Accuracy: 31.2015%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [162/225], Training Accuracy: 31.1728%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [163/225], Training Accuracy: 31.2308%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [164/225], Training Accuracy: 31.2024%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [165/225], Training Accuracy: 31.1458%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [166/225], Training Accuracy: 31.1465%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [167/225], Training Accuracy: 31.1751%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [168/225], Training Accuracy: 31.1291%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [169/225], Training Accuracy: 31.0558%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [170/225], Training Accuracy: 31.0202%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [171/225], Training Accuracy: 31.0398%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [172/225], Training Accuracy: 31.0501%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [173/225], Training Accuracy: 31.0152%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [174/225], Training Accuracy: 31.0345%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [175/225], Training Accuracy: 31.0536%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [176/225], Training Accuracy: 31.0902%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [177/225], Training Accuracy: 31.0911%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [178/225], Training Accuracy: 31.0744%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [179/225], Training Accuracy: 31.0580%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [180/225], Training Accuracy: 31.0590%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [181/225], Training Accuracy: 31.0169%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [182/225], Training Accuracy: 31.0096%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [183/225], Training Accuracy: 31.0195%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [184/225], Training Accuracy: 30.9868%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [185/225], Training Accuracy: 30.9459%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [186/225], Training Accuracy: 30.9812%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [187/225], Training Accuracy: 30.9993%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [188/225], Training Accuracy: 31.0007%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [189/225], Training Accuracy: 31.0185%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [190/225], Training Accuracy: 30.9786%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [191/225], Training Accuracy: 30.9555%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [192/225], Training Accuracy: 30.8919%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [193/225], Training Accuracy: 30.9019%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [194/225], Training Accuracy: 30.9117%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [195/225], Training Accuracy: 30.8974%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [196/225], Training Accuracy: 30.8753%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [197/225], Training Accuracy: 30.9248%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [198/225], Training Accuracy: 30.9422%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [199/225], Training Accuracy: 30.9438%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [200/225], Training Accuracy: 30.9219%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [201/225], Training Accuracy: 30.9235%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [202/225], Training Accuracy: 30.9329%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [203/225], Training Accuracy: 30.9267%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [204/225], Training Accuracy: 30.9819%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [205/225], Training Accuracy: 30.9756%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [206/225], Training Accuracy: 30.9542%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [207/225], Training Accuracy: 30.9405%, Training Loss: 0.6852%\n",
      "Epoch [81/100], Step [208/225], Training Accuracy: 30.9645%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [209/225], Training Accuracy: 31.0108%, Training Loss: 0.6851%\n",
      "Epoch [81/100], Step [210/225], Training Accuracy: 31.0565%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [211/225], Training Accuracy: 31.0278%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [212/225], Training Accuracy: 31.0805%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [213/225], Training Accuracy: 31.0519%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [214/225], Training Accuracy: 31.0602%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [215/225], Training Accuracy: 31.0320%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [216/225], Training Accuracy: 30.9751%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [217/225], Training Accuracy: 30.9836%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [218/225], Training Accuracy: 30.9633%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [219/225], Training Accuracy: 31.0288%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [220/225], Training Accuracy: 31.0440%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [221/225], Training Accuracy: 31.0379%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [222/225], Training Accuracy: 31.0670%, Training Loss: 0.6850%\n",
      "Epoch [81/100], Step [223/225], Training Accuracy: 31.1099%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [224/225], Training Accuracy: 31.1105%, Training Loss: 0.6849%\n",
      "Epoch [81/100], Step [225/225], Training Accuracy: 31.0937%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6936%\n",
      "Epoch [82/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6883%\n",
      "Epoch [82/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6897%\n",
      "Epoch [82/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6867%\n",
      "Epoch [82/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6871%\n",
      "Epoch [82/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6871%\n",
      "Epoch [82/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6862%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6860%\n",
      "Epoch [82/100], Step [9/225], Training Accuracy: 31.7708%, Training Loss: 0.6866%\n",
      "Epoch [82/100], Step [10/225], Training Accuracy: 31.4062%, Training Loss: 0.6863%\n",
      "Epoch [82/100], Step [11/225], Training Accuracy: 31.2500%, Training Loss: 0.6864%\n",
      "Epoch [82/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6867%\n",
      "Epoch [82/100], Step [13/225], Training Accuracy: 30.4087%, Training Loss: 0.6870%\n",
      "Epoch [82/100], Step [14/225], Training Accuracy: 30.5804%, Training Loss: 0.6875%\n",
      "Epoch [82/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6871%\n",
      "Epoch [82/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6871%\n",
      "Epoch [82/100], Step [17/225], Training Accuracy: 30.7904%, Training Loss: 0.6873%\n",
      "Epoch [82/100], Step [18/225], Training Accuracy: 30.9896%, Training Loss: 0.6873%\n",
      "Epoch [82/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6876%\n",
      "Epoch [82/100], Step [20/225], Training Accuracy: 31.9531%, Training Loss: 0.6875%\n",
      "Epoch [82/100], Step [21/225], Training Accuracy: 31.7708%, Training Loss: 0.6873%\n",
      "Epoch [82/100], Step [22/225], Training Accuracy: 31.8892%, Training Loss: 0.6872%\n",
      "Epoch [82/100], Step [23/225], Training Accuracy: 31.8614%, Training Loss: 0.6870%\n",
      "Epoch [82/100], Step [24/225], Training Accuracy: 32.0312%, Training Loss: 0.6870%\n",
      "Epoch [82/100], Step [25/225], Training Accuracy: 32.3125%, Training Loss: 0.6869%\n",
      "Epoch [82/100], Step [26/225], Training Accuracy: 32.6322%, Training Loss: 0.6867%\n",
      "Epoch [82/100], Step [27/225], Training Accuracy: 32.4074%, Training Loss: 0.6865%\n",
      "Epoch [82/100], Step [28/225], Training Accuracy: 32.0871%, Training Loss: 0.6867%\n",
      "Epoch [82/100], Step [29/225], Training Accuracy: 32.4353%, Training Loss: 0.6863%\n",
      "Epoch [82/100], Step [30/225], Training Accuracy: 32.3438%, Training Loss: 0.6864%\n",
      "Epoch [82/100], Step [31/225], Training Accuracy: 32.2581%, Training Loss: 0.6863%\n",
      "Epoch [82/100], Step [32/225], Training Accuracy: 32.4219%, Training Loss: 0.6860%\n",
      "Epoch [82/100], Step [33/225], Training Accuracy: 32.5284%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [34/225], Training Accuracy: 32.3989%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [35/225], Training Accuracy: 32.2321%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [36/225], Training Accuracy: 32.0747%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [37/225], Training Accuracy: 32.0524%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6860%\n",
      "Epoch [82/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [40/225], Training Accuracy: 31.7188%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [41/225], Training Accuracy: 31.7835%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [42/225], Training Accuracy: 31.6220%, Training Loss: 0.6859%\n",
      "Epoch [82/100], Step [43/225], Training Accuracy: 31.7951%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [44/225], Training Accuracy: 31.7827%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [45/225], Training Accuracy: 31.8056%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [46/225], Training Accuracy: 31.6916%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [47/225], Training Accuracy: 31.6489%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [48/225], Training Accuracy: 31.8034%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [50/225], Training Accuracy: 31.8125%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [51/225], Training Accuracy: 31.9853%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [52/225], Training Accuracy: 32.0312%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [53/225], Training Accuracy: 31.9281%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [54/225], Training Accuracy: 31.8287%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [55/225], Training Accuracy: 31.9318%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [56/225], Training Accuracy: 32.0312%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [57/225], Training Accuracy: 32.0175%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [58/225], Training Accuracy: 31.9504%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [59/225], Training Accuracy: 32.3093%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [60/225], Training Accuracy: 32.4479%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [61/225], Training Accuracy: 32.3770%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [62/225], Training Accuracy: 32.4093%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [63/225], Training Accuracy: 32.4901%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [64/225], Training Accuracy: 32.5195%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [65/225], Training Accuracy: 32.3798%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [66/225], Training Accuracy: 32.4811%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [67/225], Training Accuracy: 32.4860%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [68/225], Training Accuracy: 32.5368%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [69/225], Training Accuracy: 32.5634%, Training Loss: 0.6848%\n",
      "Epoch [82/100], Step [70/225], Training Accuracy: 32.5223%, Training Loss: 0.6848%\n",
      "Epoch [82/100], Step [71/225], Training Accuracy: 32.5484%, Training Loss: 0.6848%\n",
      "Epoch [82/100], Step [72/225], Training Accuracy: 32.3568%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [73/225], Training Accuracy: 32.2988%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [74/225], Training Accuracy: 32.3691%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [75/225], Training Accuracy: 32.3333%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [76/225], Training Accuracy: 32.2985%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [77/225], Training Accuracy: 32.2443%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [78/225], Training Accuracy: 32.2917%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [79/225], Training Accuracy: 32.2191%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [80/225], Training Accuracy: 32.1680%, Training Loss: 0.6849%\n",
      "Epoch [82/100], Step [81/225], Training Accuracy: 32.1181%, Training Loss: 0.6851%\n",
      "Epoch [82/100], Step [82/225], Training Accuracy: 32.1265%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [83/225], Training Accuracy: 32.0407%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [84/225], Training Accuracy: 32.0871%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [85/225], Training Accuracy: 32.0404%, Training Loss: 0.6850%\n",
      "Epoch [82/100], Step [86/225], Training Accuracy: 32.0858%, Training Loss: 0.6851%\n",
      "Epoch [82/100], Step [87/225], Training Accuracy: 32.1300%, Training Loss: 0.6851%\n",
      "Epoch [82/100], Step [88/225], Training Accuracy: 32.0845%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [89/225], Training Accuracy: 31.9874%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [90/225], Training Accuracy: 31.8924%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [91/225], Training Accuracy: 31.9368%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [92/225], Training Accuracy: 31.9293%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [93/225], Training Accuracy: 31.9220%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [94/225], Training Accuracy: 31.9980%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [95/225], Training Accuracy: 31.8586%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [96/225], Training Accuracy: 31.9336%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [97/225], Training Accuracy: 31.9427%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [98/225], Training Accuracy: 31.9834%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [99/225], Training Accuracy: 32.1181%, Training Loss: 0.6852%\n",
      "Epoch [82/100], Step [100/225], Training Accuracy: 32.1250%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [101/225], Training Accuracy: 32.2401%, Training Loss: 0.6853%\n",
      "Epoch [82/100], Step [102/225], Training Accuracy: 32.1385%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [103/225], Training Accuracy: 32.1905%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [104/225], Training Accuracy: 32.1514%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [105/225], Training Accuracy: 32.0833%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [106/225], Training Accuracy: 32.0607%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [107/225], Training Accuracy: 31.9947%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [108/225], Training Accuracy: 32.0747%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [109/225], Training Accuracy: 31.9237%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [110/225], Training Accuracy: 31.9460%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [111/225], Training Accuracy: 31.8412%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [112/225], Training Accuracy: 31.9196%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [113/225], Training Accuracy: 31.9414%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [114/225], Training Accuracy: 31.9901%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [115/225], Training Accuracy: 31.9701%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [116/225], Training Accuracy: 31.9639%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [117/225], Training Accuracy: 31.9044%, Training Loss: 0.6857%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Step [118/225], Training Accuracy: 31.8856%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [119/225], Training Accuracy: 31.8277%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [120/225], Training Accuracy: 31.8620%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [121/225], Training Accuracy: 31.8311%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [122/225], Training Accuracy: 31.8519%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [123/225], Training Accuracy: 31.8852%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [124/225], Training Accuracy: 31.8548%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [125/225], Training Accuracy: 31.8250%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [126/225], Training Accuracy: 31.7584%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [127/225], Training Accuracy: 31.6929%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [128/225], Training Accuracy: 31.6650%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [129/225], Training Accuracy: 31.7103%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [130/225], Training Accuracy: 31.6707%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [131/225], Training Accuracy: 31.6198%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [132/225], Training Accuracy: 31.5578%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [133/225], Training Accuracy: 31.5789%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [134/225], Training Accuracy: 31.6465%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [135/225], Training Accuracy: 31.6551%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [136/225], Training Accuracy: 31.6751%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [137/225], Training Accuracy: 31.7176%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [138/225], Training Accuracy: 31.7255%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [139/225], Training Accuracy: 31.6884%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [140/225], Training Accuracy: 31.6853%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [141/225], Training Accuracy: 31.6268%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [142/225], Training Accuracy: 31.6901%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [143/225], Training Accuracy: 31.6761%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [144/225], Training Accuracy: 31.6949%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [145/225], Training Accuracy: 31.7565%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [146/225], Training Accuracy: 31.7744%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [147/225], Training Accuracy: 31.7921%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [148/225], Training Accuracy: 31.7568%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [149/225], Training Accuracy: 31.7743%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [150/225], Training Accuracy: 31.8021%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [151/225], Training Accuracy: 31.8502%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [152/225], Training Accuracy: 31.8257%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [153/225], Training Accuracy: 31.7606%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [154/225], Training Accuracy: 31.7877%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [155/225], Training Accuracy: 31.7540%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [156/225], Training Accuracy: 31.7708%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [157/225], Training Accuracy: 31.6979%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [158/225], Training Accuracy: 31.6752%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [159/225], Training Accuracy: 31.7610%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [160/225], Training Accuracy: 31.7480%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [161/225], Training Accuracy: 31.8032%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [162/225], Training Accuracy: 31.7612%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [163/225], Training Accuracy: 31.8156%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [164/225], Training Accuracy: 31.8121%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [165/225], Training Accuracy: 31.7614%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [166/225], Training Accuracy: 31.7677%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [167/225], Training Accuracy: 31.8114%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [168/225], Training Accuracy: 31.7708%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [169/225], Training Accuracy: 31.6938%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [170/225], Training Accuracy: 31.6544%, Training Loss: 0.6858%\n",
      "Epoch [82/100], Step [171/225], Training Accuracy: 31.6886%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [172/225], Training Accuracy: 31.7042%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [173/225], Training Accuracy: 31.6926%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [174/225], Training Accuracy: 31.7170%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [175/225], Training Accuracy: 31.7679%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [176/225], Training Accuracy: 31.7738%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [177/225], Training Accuracy: 31.7885%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [178/225], Training Accuracy: 31.7855%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [179/225], Training Accuracy: 31.7388%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [180/225], Training Accuracy: 31.7708%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [181/225], Training Accuracy: 31.7162%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [182/225], Training Accuracy: 31.7050%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [183/225], Training Accuracy: 31.7025%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [184/225], Training Accuracy: 31.6491%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [185/225], Training Accuracy: 31.6132%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [186/225], Training Accuracy: 31.6532%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [187/225], Training Accuracy: 31.6511%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [188/225], Training Accuracy: 31.6572%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [189/225], Training Accuracy: 31.6799%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [190/225], Training Accuracy: 31.6447%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [191/225], Training Accuracy: 31.6181%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [192/225], Training Accuracy: 31.5348%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [193/225], Training Accuracy: 31.5253%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [194/225], Training Accuracy: 31.5399%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [195/225], Training Accuracy: 31.5144%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [196/225], Training Accuracy: 31.4892%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [197/225], Training Accuracy: 31.5355%, Training Loss: 0.6857%\n",
      "Epoch [82/100], Step [198/225], Training Accuracy: 31.5578%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [199/225], Training Accuracy: 31.5405%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [200/225], Training Accuracy: 31.5312%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [201/225], Training Accuracy: 31.5454%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [202/225], Training Accuracy: 31.5362%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [203/225], Training Accuracy: 31.5348%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [204/225], Training Accuracy: 31.6023%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [205/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [206/225], Training Accuracy: 31.5458%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [207/225], Training Accuracy: 31.5217%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [208/225], Training Accuracy: 31.5355%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [209/225], Training Accuracy: 31.5565%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [210/225], Training Accuracy: 31.5923%, Training Loss: 0.6856%\n",
      "Epoch [82/100], Step [211/225], Training Accuracy: 31.5610%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [212/225], Training Accuracy: 31.6259%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [213/225], Training Accuracy: 31.5948%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [214/225], Training Accuracy: 31.6151%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [215/225], Training Accuracy: 31.5843%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [216/225], Training Accuracy: 31.5177%, Training Loss: 0.6855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Step [217/225], Training Accuracy: 31.4948%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [218/225], Training Accuracy: 31.4794%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [219/225], Training Accuracy: 31.5354%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [220/225], Training Accuracy: 31.5412%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [221/225], Training Accuracy: 31.5328%, Training Loss: 0.6855%\n",
      "Epoch [82/100], Step [222/225], Training Accuracy: 31.5526%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [223/225], Training Accuracy: 31.5863%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [224/225], Training Accuracy: 31.5848%, Training Loss: 0.6854%\n",
      "Epoch [82/100], Step [225/225], Training Accuracy: 31.5731%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [1/225], Training Accuracy: 31.2500%, Training Loss: 0.6882%\n",
      "Epoch [83/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6873%\n",
      "Epoch [83/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6846%\n",
      "Epoch [83/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6863%\n",
      "Epoch [83/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6842%\n",
      "Epoch [83/100], Step [9/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6858%\n",
      "Epoch [83/100], Step [15/225], Training Accuracy: 30.8333%, Training Loss: 0.6859%\n",
      "Epoch [83/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6858%\n",
      "Epoch [83/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6860%\n",
      "Epoch [83/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6862%\n",
      "Epoch [83/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6863%\n",
      "Epoch [83/100], Step [20/225], Training Accuracy: 31.4062%, Training Loss: 0.6860%\n",
      "Epoch [83/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6858%\n",
      "Epoch [83/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6856%\n",
      "Epoch [83/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6856%\n",
      "Epoch [83/100], Step [25/225], Training Accuracy: 31.8750%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [26/225], Training Accuracy: 32.2115%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [27/225], Training Accuracy: 32.0023%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [29/225], Training Accuracy: 32.1659%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [30/225], Training Accuracy: 32.0312%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [31/225], Training Accuracy: 31.8548%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [32/225], Training Accuracy: 32.0801%, Training Loss: 0.6846%\n",
      "Epoch [83/100], Step [33/225], Training Accuracy: 32.1970%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [34/225], Training Accuracy: 32.1691%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [35/225], Training Accuracy: 32.0536%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [36/225], Training Accuracy: 32.0747%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [37/225], Training Accuracy: 32.0946%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [38/225], Training Accuracy: 31.9490%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [39/225], Training Accuracy: 31.7308%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [40/225], Training Accuracy: 31.7969%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [41/225], Training Accuracy: 31.8598%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [42/225], Training Accuracy: 31.7336%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [43/225], Training Accuracy: 31.8677%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [44/225], Training Accuracy: 31.8537%, Training Loss: 0.6856%\n",
      "Epoch [83/100], Step [45/225], Training Accuracy: 31.7708%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [46/225], Training Accuracy: 31.6576%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [47/225], Training Accuracy: 31.6157%, Training Loss: 0.6857%\n",
      "Epoch [83/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [49/225], Training Accuracy: 31.7921%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [50/225], Training Accuracy: 31.7500%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [51/225], Training Accuracy: 31.8934%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [52/225], Training Accuracy: 31.8810%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [53/225], Training Accuracy: 31.8101%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [54/225], Training Accuracy: 31.7130%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [55/225], Training Accuracy: 31.7614%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [56/225], Training Accuracy: 31.8638%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [57/225], Training Accuracy: 31.9079%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [58/225], Training Accuracy: 31.8157%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [59/225], Training Accuracy: 32.1239%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [60/225], Training Accuracy: 32.2135%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [61/225], Training Accuracy: 32.1721%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [62/225], Training Accuracy: 32.1573%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [63/225], Training Accuracy: 32.1925%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [64/225], Training Accuracy: 32.2021%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [65/225], Training Accuracy: 32.1154%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [66/225], Training Accuracy: 32.1733%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [67/225], Training Accuracy: 32.1595%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [68/225], Training Accuracy: 32.2151%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [69/225], Training Accuracy: 32.1784%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [70/225], Training Accuracy: 32.1429%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [71/225], Training Accuracy: 32.1523%, Training Loss: 0.6846%\n",
      "Epoch [83/100], Step [72/225], Training Accuracy: 31.9444%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [73/225], Training Accuracy: 31.8493%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [74/225], Training Accuracy: 31.9046%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [75/225], Training Accuracy: 31.8125%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [76/225], Training Accuracy: 31.8257%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [77/225], Training Accuracy: 31.7370%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [78/225], Training Accuracy: 31.7909%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [79/225], Training Accuracy: 31.7247%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [80/225], Training Accuracy: 31.7188%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [81/225], Training Accuracy: 31.6358%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [82/225], Training Accuracy: 31.6502%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [83/225], Training Accuracy: 31.6077%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [84/225], Training Accuracy: 31.6592%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [85/225], Training Accuracy: 31.6544%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [86/225], Training Accuracy: 31.6860%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [87/225], Training Accuracy: 31.7349%, Training Loss: 0.6847%\n",
      "Epoch [83/100], Step [88/225], Training Accuracy: 31.7116%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [89/225], Training Accuracy: 31.6362%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [90/225], Training Accuracy: 31.5451%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [91/225], Training Accuracy: 31.6277%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Step [92/225], Training Accuracy: 31.6067%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [93/225], Training Accuracy: 31.6364%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [94/225], Training Accuracy: 31.7154%, Training Loss: 0.6848%\n",
      "Epoch [83/100], Step [95/225], Training Accuracy: 31.5954%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [96/225], Training Accuracy: 31.7220%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [97/225], Training Accuracy: 31.7332%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [98/225], Training Accuracy: 31.7283%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [99/225], Training Accuracy: 31.8497%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [100/225], Training Accuracy: 31.8438%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [101/225], Training Accuracy: 31.9462%, Training Loss: 0.6849%\n",
      "Epoch [83/100], Step [102/225], Training Accuracy: 31.8474%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [103/225], Training Accuracy: 31.9175%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [104/225], Training Accuracy: 31.8810%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [105/225], Training Accuracy: 31.8304%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [106/225], Training Accuracy: 31.8691%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [107/225], Training Accuracy: 31.7903%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [108/225], Training Accuracy: 31.8866%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [109/225], Training Accuracy: 31.7661%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [110/225], Training Accuracy: 31.7756%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [111/225], Training Accuracy: 31.6582%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [112/225], Training Accuracy: 31.7383%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [113/225], Training Accuracy: 31.7616%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [114/225], Training Accuracy: 31.7845%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [115/225], Training Accuracy: 31.7527%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [116/225], Training Accuracy: 31.7349%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [117/225], Training Accuracy: 31.6640%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [118/225], Training Accuracy: 31.6075%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [119/225], Training Accuracy: 31.5257%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [120/225], Training Accuracy: 31.5495%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [121/225], Training Accuracy: 31.5599%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [122/225], Training Accuracy: 31.5830%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [123/225], Training Accuracy: 31.6057%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [124/225], Training Accuracy: 31.5902%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [125/225], Training Accuracy: 31.5625%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [126/225], Training Accuracy: 31.4856%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [127/225], Training Accuracy: 31.4222%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [128/225], Training Accuracy: 31.4209%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [129/225], Training Accuracy: 31.4680%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [130/225], Training Accuracy: 31.3822%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [131/225], Training Accuracy: 31.3454%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [132/225], Training Accuracy: 31.3092%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [133/225], Training Accuracy: 31.3322%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [134/225], Training Accuracy: 31.3783%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [135/225], Training Accuracy: 31.3773%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [136/225], Training Accuracy: 31.3994%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [137/225], Training Accuracy: 31.4325%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [138/225], Training Accuracy: 31.4538%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [139/225], Training Accuracy: 31.4074%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [140/225], Training Accuracy: 31.3616%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [141/225], Training Accuracy: 31.2832%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [142/225], Training Accuracy: 31.3050%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [143/225], Training Accuracy: 31.3156%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [144/225], Training Accuracy: 31.3368%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [145/225], Training Accuracy: 31.4224%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [146/225], Training Accuracy: 31.4533%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [147/225], Training Accuracy: 31.4626%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [148/225], Training Accuracy: 31.4400%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [149/225], Training Accuracy: 31.4702%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [150/225], Training Accuracy: 31.5208%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [151/225], Training Accuracy: 31.5397%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [152/225], Training Accuracy: 31.5481%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [153/225], Training Accuracy: 31.5155%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [154/225], Training Accuracy: 31.5138%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [155/225], Training Accuracy: 31.4919%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [156/225], Training Accuracy: 31.5004%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [157/225], Training Accuracy: 31.4291%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [158/225], Training Accuracy: 31.4280%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [159/225], Training Accuracy: 31.4564%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [160/225], Training Accuracy: 31.4355%, Training Loss: 0.6855%\n",
      "Epoch [83/100], Step [161/225], Training Accuracy: 31.4926%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [162/225], Training Accuracy: 31.4525%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [163/225], Training Accuracy: 31.5088%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [164/225], Training Accuracy: 31.4787%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [165/225], Training Accuracy: 31.4205%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [166/225], Training Accuracy: 31.4100%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [167/225], Training Accuracy: 31.4371%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [168/225], Training Accuracy: 31.3988%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [169/225], Training Accuracy: 31.3332%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [170/225], Training Accuracy: 31.2960%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [171/225], Training Accuracy: 31.3048%, Training Loss: 0.6854%\n",
      "Epoch [83/100], Step [172/225], Training Accuracy: 31.3227%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [173/225], Training Accuracy: 31.3042%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [174/225], Training Accuracy: 31.3308%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [175/225], Training Accuracy: 31.3661%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [176/225], Training Accuracy: 31.3832%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [177/225], Training Accuracy: 31.3912%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [178/225], Training Accuracy: 31.3817%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [179/225], Training Accuracy: 31.3460%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [180/225], Training Accuracy: 31.3802%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [181/225], Training Accuracy: 31.3363%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [182/225], Training Accuracy: 31.3187%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [183/225], Training Accuracy: 31.3268%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [184/225], Training Accuracy: 31.2925%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [185/225], Training Accuracy: 31.2584%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [186/225], Training Accuracy: 31.2920%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [187/225], Training Accuracy: 31.2751%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [188/225], Training Accuracy: 31.2583%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [189/225], Training Accuracy: 31.2913%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [190/225], Training Accuracy: 31.2582%, Training Loss: 0.6853%\n",
      "Epoch [83/100], Step [191/225], Training Accuracy: 31.2336%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100], Step [192/225], Training Accuracy: 31.1605%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [193/225], Training Accuracy: 31.1690%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [194/225], Training Accuracy: 31.1856%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [195/225], Training Accuracy: 31.1779%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [196/225], Training Accuracy: 31.1623%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [197/225], Training Accuracy: 31.2024%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [198/225], Training Accuracy: 31.2105%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [199/225], Training Accuracy: 31.1950%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [200/225], Training Accuracy: 31.1797%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [201/225], Training Accuracy: 31.1723%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [202/225], Training Accuracy: 31.1804%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [203/225], Training Accuracy: 31.1576%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [204/225], Training Accuracy: 31.2270%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [205/225], Training Accuracy: 31.2348%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [206/225], Training Accuracy: 31.2197%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [207/225], Training Accuracy: 31.1972%, Training Loss: 0.6852%\n",
      "Epoch [83/100], Step [208/225], Training Accuracy: 31.2275%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [209/225], Training Accuracy: 31.2575%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [210/225], Training Accuracy: 31.2798%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [211/225], Training Accuracy: 31.2722%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [212/225], Training Accuracy: 31.3163%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [213/225], Training Accuracy: 31.2940%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [214/225], Training Accuracy: 31.3157%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [215/225], Training Accuracy: 31.2718%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [216/225], Training Accuracy: 31.2138%, Training Loss: 0.6851%\n",
      "Epoch [83/100], Step [217/225], Training Accuracy: 31.1996%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [218/225], Training Accuracy: 31.1783%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [219/225], Training Accuracy: 31.2429%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [220/225], Training Accuracy: 31.2571%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [221/225], Training Accuracy: 31.2288%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [222/225], Training Accuracy: 31.2359%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [223/225], Training Accuracy: 31.2780%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [224/225], Training Accuracy: 31.2570%, Training Loss: 0.6850%\n",
      "Epoch [83/100], Step [225/225], Training Accuracy: 31.2465%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6887%\n",
      "Epoch [84/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6840%\n",
      "Epoch [84/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6859%\n",
      "Epoch [84/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6837%\n",
      "Epoch [84/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [6/225], Training Accuracy: 33.0729%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6843%\n",
      "Epoch [84/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6841%\n",
      "Epoch [84/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [11/225], Training Accuracy: 31.5341%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [12/225], Training Accuracy: 30.8594%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6855%\n",
      "Epoch [84/100], Step [15/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [84/100], Step [16/225], Training Accuracy: 31.3477%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6855%\n",
      "Epoch [84/100], Step [18/225], Training Accuracy: 30.9896%, Training Loss: 0.6855%\n",
      "Epoch [84/100], Step [19/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [84/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6858%\n",
      "Epoch [84/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6855%\n",
      "Epoch [84/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [23/225], Training Accuracy: 31.4538%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [25/225], Training Accuracy: 31.9375%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [26/225], Training Accuracy: 32.1514%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [27/225], Training Accuracy: 31.8866%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [28/225], Training Accuracy: 31.6964%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [29/225], Training Accuracy: 32.1659%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6843%\n",
      "Epoch [84/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [32/225], Training Accuracy: 32.3242%, Training Loss: 0.6840%\n",
      "Epoch [84/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6841%\n",
      "Epoch [84/100], Step [34/225], Training Accuracy: 32.3070%, Training Loss: 0.6840%\n",
      "Epoch [84/100], Step [35/225], Training Accuracy: 32.1429%, Training Loss: 0.6842%\n",
      "Epoch [84/100], Step [36/225], Training Accuracy: 32.0312%, Training Loss: 0.6843%\n",
      "Epoch [84/100], Step [37/225], Training Accuracy: 32.0524%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [38/225], Training Accuracy: 31.8668%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [39/225], Training Accuracy: 31.5705%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [40/225], Training Accuracy: 31.6797%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [41/225], Training Accuracy: 31.7454%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [43/225], Training Accuracy: 31.7587%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [44/225], Training Accuracy: 31.7827%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [45/225], Training Accuracy: 31.7708%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [46/225], Training Accuracy: 31.6576%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [47/225], Training Accuracy: 31.6157%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [50/225], Training Accuracy: 31.8438%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [51/225], Training Accuracy: 32.0159%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [52/225], Training Accuracy: 32.0012%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [53/225], Training Accuracy: 31.9281%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [54/225], Training Accuracy: 31.7998%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [55/225], Training Accuracy: 31.8750%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [56/225], Training Accuracy: 31.9754%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [57/225], Training Accuracy: 32.0175%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [58/225], Training Accuracy: 31.8966%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [59/225], Training Accuracy: 32.2564%, Training Loss: 0.6843%\n",
      "Epoch [84/100], Step [60/225], Training Accuracy: 32.3698%, Training Loss: 0.6843%\n",
      "Epoch [84/100], Step [61/225], Training Accuracy: 32.3514%, Training Loss: 0.6842%\n",
      "Epoch [84/100], Step [62/225], Training Accuracy: 32.4093%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [63/225], Training Accuracy: 32.4901%, Training Loss: 0.6843%\n",
      "Epoch [84/100], Step [64/225], Training Accuracy: 32.4707%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [65/225], Training Accuracy: 32.3317%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [66/225], Training Accuracy: 32.3864%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [67/225], Training Accuracy: 32.3694%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [68/225], Training Accuracy: 32.4908%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [69/225], Training Accuracy: 32.4502%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [70/225], Training Accuracy: 32.4330%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [71/225], Training Accuracy: 32.4384%, Training Loss: 0.6844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Step [72/225], Training Accuracy: 32.2266%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [73/225], Training Accuracy: 32.1276%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [74/225], Training Accuracy: 32.2002%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [75/225], Training Accuracy: 32.1458%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [76/225], Training Accuracy: 32.0724%, Training Loss: 0.6844%\n",
      "Epoch [84/100], Step [77/225], Training Accuracy: 32.0211%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [78/225], Training Accuracy: 32.0312%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [79/225], Training Accuracy: 31.9422%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [80/225], Training Accuracy: 31.8945%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [81/225], Training Accuracy: 31.7708%, Training Loss: 0.6846%\n",
      "Epoch [84/100], Step [82/225], Training Accuracy: 31.7835%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [83/225], Training Accuracy: 31.7018%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [84/225], Training Accuracy: 31.7522%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [85/225], Training Accuracy: 31.7279%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [86/225], Training Accuracy: 31.7769%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [87/225], Training Accuracy: 31.7708%, Training Loss: 0.6845%\n",
      "Epoch [84/100], Step [88/225], Training Accuracy: 31.7472%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [89/225], Training Accuracy: 31.6713%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [90/225], Training Accuracy: 31.5799%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [91/225], Training Accuracy: 31.6449%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [92/225], Training Accuracy: 31.5897%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [93/225], Training Accuracy: 31.6196%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [94/225], Training Accuracy: 31.6988%, Training Loss: 0.6847%\n",
      "Epoch [84/100], Step [95/225], Training Accuracy: 31.5789%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [96/225], Training Accuracy: 31.7057%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [97/225], Training Accuracy: 31.7171%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [98/225], Training Accuracy: 31.7283%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [99/225], Training Accuracy: 31.8182%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [100/225], Training Accuracy: 31.8281%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [101/225], Training Accuracy: 31.8998%, Training Loss: 0.6848%\n",
      "Epoch [84/100], Step [102/225], Training Accuracy: 31.8015%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [103/225], Training Accuracy: 31.8416%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [104/225], Training Accuracy: 31.8059%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [105/225], Training Accuracy: 31.7560%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [106/225], Training Accuracy: 31.7659%, Training Loss: 0.6849%\n",
      "Epoch [84/100], Step [107/225], Training Accuracy: 31.6589%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [108/225], Training Accuracy: 31.7419%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [109/225], Training Accuracy: 31.6084%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [110/225], Training Accuracy: 31.5909%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [111/225], Training Accuracy: 31.5034%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [112/225], Training Accuracy: 31.5848%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [113/225], Training Accuracy: 31.5542%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [114/225], Training Accuracy: 31.6064%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [115/225], Training Accuracy: 31.5761%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [116/225], Training Accuracy: 31.5867%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [117/225], Training Accuracy: 31.5304%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [118/225], Training Accuracy: 31.4883%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [119/225], Training Accuracy: 31.4076%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [120/225], Training Accuracy: 31.4062%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [121/225], Training Accuracy: 31.4179%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [122/225], Training Accuracy: 31.4293%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [123/225], Training Accuracy: 31.4660%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [124/225], Training Accuracy: 31.4390%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [126/225], Training Accuracy: 31.3492%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [127/225], Training Accuracy: 31.3361%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [128/225], Training Accuracy: 31.3110%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [129/225], Training Accuracy: 31.3711%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [130/225], Training Accuracy: 31.2981%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [131/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [132/225], Training Accuracy: 31.2263%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [133/225], Training Accuracy: 31.2265%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [134/225], Training Accuracy: 31.2850%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [135/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [136/225], Training Accuracy: 31.2730%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [137/225], Training Accuracy: 31.2956%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [138/225], Training Accuracy: 31.3406%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [139/225], Training Accuracy: 31.3287%, Training Loss: 0.6854%\n",
      "Epoch [84/100], Step [140/225], Training Accuracy: 31.2946%, Training Loss: 0.6854%\n",
      "Epoch [84/100], Step [141/225], Training Accuracy: 31.2389%, Training Loss: 0.6854%\n",
      "Epoch [84/100], Step [142/225], Training Accuracy: 31.2830%, Training Loss: 0.6854%\n",
      "Epoch [84/100], Step [143/225], Training Accuracy: 31.2719%, Training Loss: 0.6854%\n",
      "Epoch [84/100], Step [144/225], Training Accuracy: 31.2934%, Training Loss: 0.6854%\n",
      "Epoch [84/100], Step [145/225], Training Accuracy: 31.3578%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [146/225], Training Accuracy: 31.3784%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [147/225], Training Accuracy: 31.3776%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [148/225], Training Accuracy: 31.3450%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [149/225], Training Accuracy: 31.3654%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [150/225], Training Accuracy: 31.3958%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [151/225], Training Accuracy: 31.4570%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [152/225], Training Accuracy: 31.4659%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [153/225], Training Accuracy: 31.4134%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [154/225], Training Accuracy: 31.4326%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [155/225], Training Accuracy: 31.4012%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [156/225], Training Accuracy: 31.4203%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [157/225], Training Accuracy: 31.3694%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [158/225], Training Accuracy: 31.3390%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [159/225], Training Accuracy: 31.3974%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [160/225], Training Accuracy: 31.3770%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [161/225], Training Accuracy: 31.4053%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [162/225], Training Accuracy: 31.3657%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [163/225], Training Accuracy: 31.4034%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [164/225], Training Accuracy: 31.3834%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [165/225], Training Accuracy: 31.3258%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [166/225], Training Accuracy: 31.3253%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [167/225], Training Accuracy: 31.3716%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [168/225], Training Accuracy: 31.3337%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [169/225], Training Accuracy: 31.2592%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [170/225], Training Accuracy: 31.2132%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [171/225], Training Accuracy: 31.2226%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [172/225], Training Accuracy: 31.2409%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [173/225], Training Accuracy: 31.2319%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [174/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [175/225], Training Accuracy: 31.2768%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [176/225], Training Accuracy: 31.2944%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Step [177/225], Training Accuracy: 31.2941%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [178/225], Training Accuracy: 31.2763%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [179/225], Training Accuracy: 31.2151%, Training Loss: 0.6853%\n",
      "Epoch [84/100], Step [180/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [181/225], Training Accuracy: 31.2068%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [182/225], Training Accuracy: 31.1899%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [183/225], Training Accuracy: 31.1902%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [185/225], Training Accuracy: 31.1318%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [186/225], Training Accuracy: 31.1828%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [187/225], Training Accuracy: 31.1999%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [188/225], Training Accuracy: 31.2001%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [189/225], Training Accuracy: 31.2252%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [190/225], Training Accuracy: 31.1760%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [191/225], Training Accuracy: 31.1518%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [192/225], Training Accuracy: 31.0791%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [193/225], Training Accuracy: 31.0800%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [194/225], Training Accuracy: 31.0889%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [195/225], Training Accuracy: 31.0817%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [196/225], Training Accuracy: 31.0507%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [197/225], Training Accuracy: 31.0676%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [198/225], Training Accuracy: 31.0922%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [199/225], Training Accuracy: 31.0851%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [200/225], Training Accuracy: 31.0703%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [201/225], Training Accuracy: 31.0945%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [202/225], Training Accuracy: 31.0798%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [203/225], Training Accuracy: 31.0653%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [204/225], Training Accuracy: 31.1275%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [205/225], Training Accuracy: 31.1357%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [206/225], Training Accuracy: 31.1211%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [207/225], Training Accuracy: 31.0990%, Training Loss: 0.6852%\n",
      "Epoch [84/100], Step [208/225], Training Accuracy: 31.1073%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [209/225], Training Accuracy: 31.1603%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [210/225], Training Accuracy: 31.1533%, Training Loss: 0.6851%\n",
      "Epoch [84/100], Step [211/225], Training Accuracy: 31.1611%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [212/225], Training Accuracy: 31.2131%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [213/225], Training Accuracy: 31.1913%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [214/225], Training Accuracy: 31.2208%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [215/225], Training Accuracy: 31.1846%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [216/225], Training Accuracy: 31.1270%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [217/225], Training Accuracy: 31.1204%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [218/225], Training Accuracy: 31.0851%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [219/225], Training Accuracy: 31.1358%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [220/225], Training Accuracy: 31.1577%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [221/225], Training Accuracy: 31.1510%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [222/225], Training Accuracy: 31.1655%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [223/225], Training Accuracy: 31.2150%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [224/225], Training Accuracy: 31.2151%, Training Loss: 0.6850%\n",
      "Epoch [84/100], Step [225/225], Training Accuracy: 31.2048%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6906%\n",
      "Epoch [85/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6883%\n",
      "Epoch [85/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6864%\n",
      "Epoch [85/100], Step [5/225], Training Accuracy: 33.7500%, Training Loss: 0.6865%\n",
      "Epoch [85/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6870%\n",
      "Epoch [85/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [8/225], Training Accuracy: 32.0312%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [9/225], Training Accuracy: 32.1181%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [10/225], Training Accuracy: 31.5625%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [11/225], Training Accuracy: 31.1080%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6866%\n",
      "Epoch [85/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6866%\n",
      "Epoch [85/100], Step [16/225], Training Accuracy: 30.5664%, Training Loss: 0.6865%\n",
      "Epoch [85/100], Step [17/225], Training Accuracy: 30.1471%, Training Loss: 0.6870%\n",
      "Epoch [85/100], Step [18/225], Training Accuracy: 30.1215%, Training Loss: 0.6869%\n",
      "Epoch [85/100], Step [19/225], Training Accuracy: 30.5921%, Training Loss: 0.6869%\n",
      "Epoch [85/100], Step [20/225], Training Accuracy: 31.0156%, Training Loss: 0.6866%\n",
      "Epoch [85/100], Step [21/225], Training Accuracy: 30.9524%, Training Loss: 0.6864%\n",
      "Epoch [85/100], Step [22/225], Training Accuracy: 31.2500%, Training Loss: 0.6862%\n",
      "Epoch [85/100], Step [23/225], Training Accuracy: 31.1141%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [24/225], Training Accuracy: 31.3151%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [25/225], Training Accuracy: 31.5000%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [27/225], Training Accuracy: 31.6551%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [29/225], Training Accuracy: 31.9504%, Training Loss: 0.6856%\n",
      "Epoch [85/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [31/225], Training Accuracy: 31.7540%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [32/225], Training Accuracy: 31.9824%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [33/225], Training Accuracy: 32.1023%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [34/225], Training Accuracy: 31.9853%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [35/225], Training Accuracy: 31.8750%, Training Loss: 0.6852%\n",
      "Epoch [85/100], Step [36/225], Training Accuracy: 31.7274%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [37/225], Training Accuracy: 31.7990%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [39/225], Training Accuracy: 31.4103%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [40/225], Training Accuracy: 31.4844%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [41/225], Training Accuracy: 31.5168%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [42/225], Training Accuracy: 31.4360%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [43/225], Training Accuracy: 31.6134%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [44/225], Training Accuracy: 31.5341%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [45/225], Training Accuracy: 31.4583%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [46/225], Training Accuracy: 31.3519%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [47/225], Training Accuracy: 31.2832%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [48/225], Training Accuracy: 31.4128%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [49/225], Training Accuracy: 31.5370%, Training Loss: 0.6851%\n",
      "Epoch [85/100], Step [50/225], Training Accuracy: 31.5000%, Training Loss: 0.6852%\n",
      "Epoch [85/100], Step [51/225], Training Accuracy: 31.6176%, Training Loss: 0.6851%\n",
      "Epoch [85/100], Step [52/225], Training Accuracy: 31.5805%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [53/225], Training Accuracy: 31.5153%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [54/225], Training Accuracy: 31.3947%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [55/225], Training Accuracy: 31.4773%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [56/225], Training Accuracy: 31.5011%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [57/225], Training Accuracy: 31.5515%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [58/225], Training Accuracy: 31.4925%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [59/225], Training Accuracy: 31.7797%, Training Loss: 0.6846%\n",
      "Epoch [85/100], Step [60/225], Training Accuracy: 31.9010%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [61/225], Training Accuracy: 31.8391%, Training Loss: 0.6846%\n",
      "Epoch [85/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Step [63/225], Training Accuracy: 31.9444%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [64/225], Training Accuracy: 31.8848%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [65/225], Training Accuracy: 31.7067%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [66/225], Training Accuracy: 31.8182%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [67/225], Training Accuracy: 31.7864%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [68/225], Training Accuracy: 31.8244%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [69/225], Training Accuracy: 31.8614%, Training Loss: 0.6846%\n",
      "Epoch [85/100], Step [70/225], Training Accuracy: 31.8304%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [71/225], Training Accuracy: 31.8662%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [72/225], Training Accuracy: 31.6623%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [73/225], Training Accuracy: 31.6139%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [74/225], Training Accuracy: 31.7356%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [75/225], Training Accuracy: 31.6875%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [76/225], Training Accuracy: 31.6201%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [77/225], Training Accuracy: 31.5747%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [78/225], Training Accuracy: 31.6306%, Training Loss: 0.6847%\n",
      "Epoch [85/100], Step [79/225], Training Accuracy: 31.5269%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [80/225], Training Accuracy: 31.5039%, Training Loss: 0.6848%\n",
      "Epoch [85/100], Step [81/225], Training Accuracy: 31.4043%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [82/225], Training Accuracy: 31.4215%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [83/225], Training Accuracy: 31.3630%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [84/225], Training Accuracy: 31.4360%, Training Loss: 0.6849%\n",
      "Epoch [85/100], Step [85/225], Training Accuracy: 31.3971%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [86/225], Training Accuracy: 31.4680%, Training Loss: 0.6850%\n",
      "Epoch [85/100], Step [87/225], Training Accuracy: 31.4835%, Training Loss: 0.6851%\n",
      "Epoch [85/100], Step [88/225], Training Accuracy: 31.4808%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [89/225], Training Accuracy: 31.4080%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [90/225], Training Accuracy: 31.3021%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6853%\n",
      "Epoch [85/100], Step [92/225], Training Accuracy: 31.3349%, Training Loss: 0.6852%\n",
      "Epoch [85/100], Step [93/225], Training Accuracy: 31.3340%, Training Loss: 0.6852%\n",
      "Epoch [85/100], Step [94/225], Training Accuracy: 31.3830%, Training Loss: 0.6852%\n",
      "Epoch [85/100], Step [95/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [96/225], Training Accuracy: 31.3314%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [97/225], Training Accuracy: 31.3305%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [98/225], Training Accuracy: 31.3616%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [99/225], Training Accuracy: 31.4710%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [100/225], Training Accuracy: 31.4531%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [101/225], Training Accuracy: 31.5130%, Training Loss: 0.6854%\n",
      "Epoch [85/100], Step [102/225], Training Accuracy: 31.4032%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [103/225], Training Accuracy: 31.4320%, Training Loss: 0.6855%\n",
      "Epoch [85/100], Step [104/225], Training Accuracy: 31.4153%, Training Loss: 0.6856%\n",
      "Epoch [85/100], Step [105/225], Training Accuracy: 31.3690%, Training Loss: 0.6856%\n",
      "Epoch [85/100], Step [106/225], Training Accuracy: 31.3827%, Training Loss: 0.6856%\n",
      "Epoch [85/100], Step [107/225], Training Accuracy: 31.2792%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [108/225], Training Accuracy: 31.3368%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [109/225], Training Accuracy: 31.2357%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [110/225], Training Accuracy: 31.2784%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [111/225], Training Accuracy: 31.1796%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [112/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [113/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [114/225], Training Accuracy: 31.3048%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [115/225], Training Accuracy: 31.2908%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [116/225], Training Accuracy: 31.2635%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [117/225], Training Accuracy: 31.1832%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [118/225], Training Accuracy: 31.1573%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [119/225], Training Accuracy: 31.1187%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [120/225], Training Accuracy: 31.1719%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [121/225], Training Accuracy: 31.1338%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [122/225], Training Accuracy: 31.1732%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [123/225], Training Accuracy: 31.2119%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [124/225], Training Accuracy: 31.1996%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [125/225], Training Accuracy: 31.1750%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [126/225], Training Accuracy: 31.1136%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [127/225], Training Accuracy: 31.0778%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [128/225], Training Accuracy: 31.0669%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [129/225], Training Accuracy: 31.1168%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [130/225], Training Accuracy: 31.0457%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [131/225], Training Accuracy: 31.0234%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [132/225], Training Accuracy: 31.0133%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [133/225], Training Accuracy: 31.0268%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [134/225], Training Accuracy: 31.0868%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [135/225], Training Accuracy: 31.0648%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [136/225], Training Accuracy: 31.0892%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [137/225], Training Accuracy: 31.1245%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [138/225], Training Accuracy: 31.1481%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [139/225], Training Accuracy: 31.1151%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [140/225], Training Accuracy: 31.0938%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [141/225], Training Accuracy: 31.0395%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [142/225], Training Accuracy: 31.0849%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [143/225], Training Accuracy: 31.0642%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [144/225], Training Accuracy: 31.0764%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [145/225], Training Accuracy: 31.1530%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [146/225], Training Accuracy: 31.1751%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [147/225], Training Accuracy: 31.1862%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [148/225], Training Accuracy: 31.1550%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [149/225], Training Accuracy: 31.1556%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [150/225], Training Accuracy: 31.1771%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [151/225], Training Accuracy: 31.2190%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [152/225], Training Accuracy: 31.2294%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [153/225], Training Accuracy: 31.2092%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [154/225], Training Accuracy: 31.2804%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [155/225], Training Accuracy: 31.2601%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [156/225], Training Accuracy: 31.2800%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [157/225], Training Accuracy: 31.2002%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [158/225], Training Accuracy: 31.1808%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [159/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [161/225], Training Accuracy: 31.2694%, Training Loss: 0.6861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100], Step [162/225], Training Accuracy: 31.2500%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [163/225], Training Accuracy: 31.2979%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [164/225], Training Accuracy: 31.2691%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [165/225], Training Accuracy: 31.2121%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [166/225], Training Accuracy: 31.2218%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [167/225], Training Accuracy: 31.2687%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [168/225], Training Accuracy: 31.2221%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [169/225], Training Accuracy: 31.1483%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [170/225], Training Accuracy: 31.0938%, Training Loss: 0.6862%\n",
      "Epoch [85/100], Step [171/225], Training Accuracy: 31.1221%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [172/225], Training Accuracy: 31.1410%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [173/225], Training Accuracy: 31.1326%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [174/225], Training Accuracy: 31.1602%, Training Loss: 0.6861%\n",
      "Epoch [85/100], Step [175/225], Training Accuracy: 31.2054%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [176/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [177/225], Training Accuracy: 31.2588%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [178/225], Training Accuracy: 31.2324%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [179/225], Training Accuracy: 31.1976%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [180/225], Training Accuracy: 31.2240%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [181/225], Training Accuracy: 31.1809%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [182/225], Training Accuracy: 31.1727%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [183/225], Training Accuracy: 31.1817%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [185/225], Training Accuracy: 31.1149%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [186/225], Training Accuracy: 31.1576%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [187/225], Training Accuracy: 31.1748%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [188/225], Training Accuracy: 31.1752%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [189/225], Training Accuracy: 31.2004%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [190/225], Training Accuracy: 31.1513%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [191/225], Training Accuracy: 31.1355%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [192/225], Training Accuracy: 31.0710%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [193/225], Training Accuracy: 31.0638%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [194/225], Training Accuracy: 31.0728%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [195/225], Training Accuracy: 31.0497%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [196/225], Training Accuracy: 31.0268%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [197/225], Training Accuracy: 31.0834%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [198/225], Training Accuracy: 31.1080%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [199/225], Training Accuracy: 31.0851%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [200/225], Training Accuracy: 31.0703%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [201/225], Training Accuracy: 31.0868%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [202/225], Training Accuracy: 31.0876%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [203/225], Training Accuracy: 31.0730%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [204/225], Training Accuracy: 31.1275%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [205/225], Training Accuracy: 31.1204%, Training Loss: 0.6860%\n",
      "Epoch [85/100], Step [206/225], Training Accuracy: 31.1059%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [207/225], Training Accuracy: 31.0688%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [208/225], Training Accuracy: 31.1223%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [209/225], Training Accuracy: 31.1678%, Training Loss: 0.6859%\n",
      "Epoch [85/100], Step [210/225], Training Accuracy: 31.1979%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [211/225], Training Accuracy: 31.1908%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [212/225], Training Accuracy: 31.2574%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [213/225], Training Accuracy: 31.2353%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [214/225], Training Accuracy: 31.2573%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [215/225], Training Accuracy: 31.2282%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [216/225], Training Accuracy: 31.1632%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [217/225], Training Accuracy: 31.1708%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [218/225], Training Accuracy: 31.1497%, Training Loss: 0.6858%\n",
      "Epoch [85/100], Step [219/225], Training Accuracy: 31.2072%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [220/225], Training Accuracy: 31.2145%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [221/225], Training Accuracy: 31.2005%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [222/225], Training Accuracy: 31.2148%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [223/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [224/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [85/100], Step [225/225], Training Accuracy: 31.2396%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [1/225], Training Accuracy: 29.6875%, Training Loss: 0.6906%\n",
      "Epoch [86/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6865%\n",
      "Epoch [86/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6871%\n",
      "Epoch [86/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6848%\n",
      "Epoch [86/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6845%\n",
      "Epoch [86/100], Step [9/225], Training Accuracy: 30.7292%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [11/225], Training Accuracy: 30.6818%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6848%\n",
      "Epoch [86/100], Step [13/225], Training Accuracy: 29.6875%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [14/225], Training Accuracy: 29.7991%, Training Loss: 0.6860%\n",
      "Epoch [86/100], Step [15/225], Training Accuracy: 30.4167%, Training Loss: 0.6860%\n",
      "Epoch [86/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [17/225], Training Accuracy: 30.1471%, Training Loss: 0.6860%\n",
      "Epoch [86/100], Step [18/225], Training Accuracy: 29.7743%, Training Loss: 0.6862%\n",
      "Epoch [86/100], Step [19/225], Training Accuracy: 30.1809%, Training Loss: 0.6865%\n",
      "Epoch [86/100], Step [20/225], Training Accuracy: 30.5469%, Training Loss: 0.6863%\n",
      "Epoch [86/100], Step [21/225], Training Accuracy: 30.5804%, Training Loss: 0.6863%\n",
      "Epoch [86/100], Step [22/225], Training Accuracy: 30.7528%, Training Loss: 0.6861%\n",
      "Epoch [86/100], Step [23/225], Training Accuracy: 30.7065%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [24/225], Training Accuracy: 30.9245%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [25/225], Training Accuracy: 31.3125%, Training Loss: 0.6860%\n",
      "Epoch [86/100], Step [26/225], Training Accuracy: 31.7308%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [27/225], Training Accuracy: 31.4815%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [28/225], Training Accuracy: 31.3616%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [29/225], Training Accuracy: 31.6810%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [30/225], Training Accuracy: 31.5625%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [31/225], Training Accuracy: 31.3508%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [32/225], Training Accuracy: 31.5918%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [33/225], Training Accuracy: 31.6761%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [34/225], Training Accuracy: 31.5257%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [35/225], Training Accuracy: 31.4732%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Step [36/225], Training Accuracy: 31.3802%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [37/225], Training Accuracy: 31.4189%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [38/225], Training Accuracy: 31.2911%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [39/225], Training Accuracy: 31.0497%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [40/225], Training Accuracy: 31.1328%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [41/225], Training Accuracy: 31.1738%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [42/225], Training Accuracy: 31.0268%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [43/225], Training Accuracy: 31.2137%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [45/225], Training Accuracy: 31.1458%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [46/225], Training Accuracy: 31.0462%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [47/225], Training Accuracy: 30.9840%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [48/225], Training Accuracy: 31.1849%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [49/225], Training Accuracy: 31.2181%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [50/225], Training Accuracy: 31.2188%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [51/225], Training Accuracy: 31.4032%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [52/225], Training Accuracy: 31.3702%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [53/225], Training Accuracy: 31.3384%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [54/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [55/225], Training Accuracy: 31.3920%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [56/225], Training Accuracy: 31.5569%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [57/225], Training Accuracy: 31.5241%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [58/225], Training Accuracy: 31.4386%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [61/225], Training Accuracy: 31.8391%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [62/225], Training Accuracy: 31.8296%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [63/225], Training Accuracy: 31.9444%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [64/225], Training Accuracy: 31.9336%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [66/225], Training Accuracy: 31.9366%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [67/225], Training Accuracy: 31.9263%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [69/225], Training Accuracy: 31.9293%, Training Loss: 0.6848%\n",
      "Epoch [86/100], Step [70/225], Training Accuracy: 31.9196%, Training Loss: 0.6848%\n",
      "Epoch [86/100], Step [71/225], Training Accuracy: 31.9762%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [72/225], Training Accuracy: 31.7708%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [73/225], Training Accuracy: 31.6995%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [74/225], Training Accuracy: 31.8201%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [75/225], Training Accuracy: 31.7708%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [76/225], Training Accuracy: 31.7640%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [77/225], Training Accuracy: 31.6558%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [78/225], Training Accuracy: 31.7107%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [79/225], Training Accuracy: 31.6456%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [80/225], Training Accuracy: 31.6406%, Training Loss: 0.6848%\n",
      "Epoch [86/100], Step [81/225], Training Accuracy: 31.5201%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [82/225], Training Accuracy: 31.4977%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [83/225], Training Accuracy: 31.4194%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [84/225], Training Accuracy: 31.4732%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [85/225], Training Accuracy: 31.4522%, Training Loss: 0.6849%\n",
      "Epoch [86/100], Step [86/225], Training Accuracy: 31.5225%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [87/225], Training Accuracy: 31.5194%, Training Loss: 0.6850%\n",
      "Epoch [86/100], Step [88/225], Training Accuracy: 31.4631%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [89/225], Training Accuracy: 31.3904%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [90/225], Training Accuracy: 31.2847%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [92/225], Training Accuracy: 31.3689%, Training Loss: 0.6851%\n",
      "Epoch [86/100], Step [93/225], Training Accuracy: 31.3340%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [94/225], Training Accuracy: 31.3996%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [95/225], Training Accuracy: 31.2829%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [96/225], Training Accuracy: 31.3802%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [97/225], Training Accuracy: 31.4111%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [98/225], Training Accuracy: 31.4094%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [99/225], Training Accuracy: 31.5341%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [100/225], Training Accuracy: 31.4844%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [101/225], Training Accuracy: 31.6058%, Training Loss: 0.6852%\n",
      "Epoch [86/100], Step [102/225], Training Accuracy: 31.4798%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [103/225], Training Accuracy: 31.5534%, Training Loss: 0.6853%\n",
      "Epoch [86/100], Step [104/225], Training Accuracy: 31.5355%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [105/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [106/225], Training Accuracy: 31.4416%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [107/225], Training Accuracy: 31.3522%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [108/225], Training Accuracy: 31.4236%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [109/225], Training Accuracy: 31.2930%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [110/225], Training Accuracy: 31.3352%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [111/225], Training Accuracy: 31.2359%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [112/225], Training Accuracy: 31.3198%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [113/225], Training Accuracy: 31.3191%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [114/225], Training Accuracy: 31.4008%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [115/225], Training Accuracy: 31.3451%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [116/225], Training Accuracy: 31.3578%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [117/225], Training Accuracy: 31.2901%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [118/225], Training Accuracy: 31.2632%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [119/225], Training Accuracy: 31.2237%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [120/225], Training Accuracy: 31.2630%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [121/225], Training Accuracy: 31.2371%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [122/225], Training Accuracy: 31.2372%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [123/225], Training Accuracy: 31.2754%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [124/225], Training Accuracy: 31.2626%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [125/225], Training Accuracy: 31.2375%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [126/225], Training Accuracy: 31.1756%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [127/225], Training Accuracy: 31.1270%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [128/225], Training Accuracy: 31.1279%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [129/225], Training Accuracy: 31.1652%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [130/225], Training Accuracy: 31.0938%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [131/225], Training Accuracy: 31.0353%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [132/225], Training Accuracy: 30.9777%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [133/225], Training Accuracy: 31.0268%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [134/225], Training Accuracy: 31.0518%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [135/225], Training Accuracy: 31.0648%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Step [136/225], Training Accuracy: 31.0662%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [137/225], Training Accuracy: 31.1131%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [138/225], Training Accuracy: 31.1255%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [139/225], Training Accuracy: 31.1039%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [140/225], Training Accuracy: 31.0491%, Training Loss: 0.6860%\n",
      "Epoch [86/100], Step [141/225], Training Accuracy: 31.0173%, Training Loss: 0.6860%\n",
      "Epoch [86/100], Step [142/225], Training Accuracy: 31.0409%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [143/225], Training Accuracy: 31.0315%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [144/225], Training Accuracy: 31.0438%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [145/225], Training Accuracy: 31.1099%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [146/225], Training Accuracy: 31.1323%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [147/225], Training Accuracy: 31.1543%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [148/225], Training Accuracy: 31.1233%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [149/225], Training Accuracy: 31.1346%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [150/225], Training Accuracy: 31.1667%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [151/225], Training Accuracy: 31.2293%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [152/225], Training Accuracy: 31.2397%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [153/225], Training Accuracy: 31.2194%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [154/225], Training Accuracy: 31.2196%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [155/225], Training Accuracy: 31.2097%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [156/225], Training Accuracy: 31.2300%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [157/225], Training Accuracy: 31.1405%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [158/225], Training Accuracy: 31.1313%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [159/225], Training Accuracy: 31.2107%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [160/225], Training Accuracy: 31.1914%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [161/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [162/225], Training Accuracy: 31.2114%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [163/225], Training Accuracy: 31.2788%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [164/225], Training Accuracy: 31.2691%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [165/225], Training Accuracy: 31.2027%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [166/225], Training Accuracy: 31.2123%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [167/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [168/225], Training Accuracy: 31.2035%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [169/225], Training Accuracy: 31.1298%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [170/225], Training Accuracy: 31.0938%, Training Loss: 0.6859%\n",
      "Epoch [86/100], Step [171/225], Training Accuracy: 31.1038%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [172/225], Training Accuracy: 31.1592%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [173/225], Training Accuracy: 31.1507%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [174/225], Training Accuracy: 31.1602%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [175/225], Training Accuracy: 31.1875%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [176/225], Training Accuracy: 31.2056%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [177/225], Training Accuracy: 31.2059%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [178/225], Training Accuracy: 31.1710%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [179/225], Training Accuracy: 31.1365%, Training Loss: 0.6858%\n",
      "Epoch [86/100], Step [180/225], Training Accuracy: 31.1632%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [181/225], Training Accuracy: 31.1205%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [182/225], Training Accuracy: 31.1126%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [183/225], Training Accuracy: 31.1219%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [184/225], Training Accuracy: 31.0971%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [185/225], Training Accuracy: 31.0642%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [186/225], Training Accuracy: 31.0820%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [187/225], Training Accuracy: 31.0996%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [188/225], Training Accuracy: 31.1087%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [189/225], Training Accuracy: 31.1425%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [190/225], Training Accuracy: 31.0938%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [191/225], Training Accuracy: 31.0782%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [192/225], Training Accuracy: 31.0059%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [193/225], Training Accuracy: 31.0152%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [194/225], Training Accuracy: 31.0325%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [195/225], Training Accuracy: 31.0256%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [196/225], Training Accuracy: 31.0108%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [197/225], Training Accuracy: 31.0755%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [198/225], Training Accuracy: 31.0922%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [199/225], Training Accuracy: 31.0773%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [200/225], Training Accuracy: 31.0547%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [201/225], Training Accuracy: 31.0479%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [202/225], Training Accuracy: 31.0489%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [203/225], Training Accuracy: 31.0499%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [204/225], Training Accuracy: 31.0892%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [205/225], Training Accuracy: 31.0976%, Training Loss: 0.6857%\n",
      "Epoch [86/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [207/225], Training Accuracy: 31.0688%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [208/225], Training Accuracy: 31.1148%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [209/225], Training Accuracy: 31.1603%, Training Loss: 0.6856%\n",
      "Epoch [86/100], Step [210/225], Training Accuracy: 31.2128%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [211/225], Training Accuracy: 31.1908%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [212/225], Training Accuracy: 31.2426%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [213/225], Training Accuracy: 31.2280%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [214/225], Training Accuracy: 31.2573%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [215/225], Training Accuracy: 31.2209%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [216/225], Training Accuracy: 31.1560%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [217/225], Training Accuracy: 31.1564%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [218/225], Training Accuracy: 31.1282%, Training Loss: 0.6855%\n",
      "Epoch [86/100], Step [219/225], Training Accuracy: 31.1858%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [220/225], Training Accuracy: 31.2003%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [221/225], Training Accuracy: 31.1864%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [222/225], Training Accuracy: 31.1937%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [223/225], Training Accuracy: 31.2360%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [224/225], Training Accuracy: 31.2291%, Training Loss: 0.6854%\n",
      "Epoch [86/100], Step [225/225], Training Accuracy: 31.2118%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6826%\n",
      "Epoch [87/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6836%\n",
      "Epoch [87/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6845%\n",
      "Epoch [87/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Step [9/225], Training Accuracy: 30.9028%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [10/225], Training Accuracy: 30.6250%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [11/225], Training Accuracy: 30.3977%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6860%\n",
      "Epoch [87/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6857%\n",
      "Epoch [87/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6856%\n",
      "Epoch [87/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6858%\n",
      "Epoch [87/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6860%\n",
      "Epoch [87/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6860%\n",
      "Epoch [87/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6858%\n",
      "Epoch [87/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6858%\n",
      "Epoch [87/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6857%\n",
      "Epoch [87/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6856%\n",
      "Epoch [87/100], Step [24/225], Training Accuracy: 31.5755%, Training Loss: 0.6856%\n",
      "Epoch [87/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6857%\n",
      "Epoch [87/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6855%\n",
      "Epoch [87/100], Step [27/225], Training Accuracy: 31.7708%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [28/225], Training Accuracy: 31.5848%, Training Loss: 0.6855%\n",
      "Epoch [87/100], Step [29/225], Training Accuracy: 31.9504%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [30/225], Training Accuracy: 31.8750%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [31/225], Training Accuracy: 31.7036%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [32/225], Training Accuracy: 31.9824%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [33/225], Training Accuracy: 32.1496%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [34/225], Training Accuracy: 32.0772%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [35/225], Training Accuracy: 31.9643%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [36/225], Training Accuracy: 31.8142%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [37/225], Training Accuracy: 31.7990%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [39/225], Training Accuracy: 31.3301%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [40/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [41/225], Training Accuracy: 31.3262%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [42/225], Training Accuracy: 31.1384%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [43/225], Training Accuracy: 31.2863%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [44/225], Training Accuracy: 31.3210%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [45/225], Training Accuracy: 31.3542%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [46/225], Training Accuracy: 31.1821%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [47/225], Training Accuracy: 31.1170%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [48/225], Training Accuracy: 31.2174%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [49/225], Training Accuracy: 31.2819%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [51/225], Training Accuracy: 31.4951%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [52/225], Training Accuracy: 31.5505%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [53/225], Training Accuracy: 31.5448%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [54/225], Training Accuracy: 31.3947%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [55/225], Training Accuracy: 31.4773%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [56/225], Training Accuracy: 31.5569%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [57/225], Training Accuracy: 31.5789%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [58/225], Training Accuracy: 31.5194%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [59/225], Training Accuracy: 31.8591%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [60/225], Training Accuracy: 32.0052%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [61/225], Training Accuracy: 31.8904%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [62/225], Training Accuracy: 31.9304%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [63/225], Training Accuracy: 31.9692%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [64/225], Training Accuracy: 31.9580%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [65/225], Training Accuracy: 31.8510%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [66/225], Training Accuracy: 31.9839%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [67/225], Training Accuracy: 31.9729%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [68/225], Training Accuracy: 32.0312%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [69/225], Training Accuracy: 32.0426%, Training Loss: 0.6845%\n",
      "Epoch [87/100], Step [70/225], Training Accuracy: 31.9866%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [71/225], Training Accuracy: 32.0202%, Training Loss: 0.6846%\n",
      "Epoch [87/100], Step [72/225], Training Accuracy: 31.7925%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [73/225], Training Accuracy: 31.7209%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [74/225], Training Accuracy: 31.7990%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [75/225], Training Accuracy: 31.7500%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [76/225], Training Accuracy: 31.7229%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [77/225], Training Accuracy: 31.5747%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [78/225], Training Accuracy: 31.6306%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [79/225], Training Accuracy: 31.5269%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [80/225], Training Accuracy: 31.4844%, Training Loss: 0.6847%\n",
      "Epoch [87/100], Step [81/225], Training Accuracy: 31.4043%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [82/225], Training Accuracy: 31.4215%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [83/225], Training Accuracy: 31.3818%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [84/225], Training Accuracy: 31.4174%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [85/225], Training Accuracy: 31.3787%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [86/225], Training Accuracy: 31.4499%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [87/225], Training Accuracy: 31.4476%, Training Loss: 0.6848%\n",
      "Epoch [87/100], Step [88/225], Training Accuracy: 31.4276%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [89/225], Training Accuracy: 31.3378%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [90/225], Training Accuracy: 31.2326%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [91/225], Training Accuracy: 31.3187%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [92/225], Training Accuracy: 31.3349%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [93/225], Training Accuracy: 31.3172%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [94/225], Training Accuracy: 31.3830%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [95/225], Training Accuracy: 31.2664%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [96/225], Training Accuracy: 31.3314%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [97/225], Training Accuracy: 31.3466%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [98/225], Training Accuracy: 31.3616%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [99/225], Training Accuracy: 31.4867%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [100/225], Training Accuracy: 31.4844%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [101/225], Training Accuracy: 31.6368%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [102/225], Training Accuracy: 31.5257%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [103/225], Training Accuracy: 31.5686%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [104/225], Training Accuracy: 31.5355%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [105/225], Training Accuracy: 31.4881%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [106/225], Training Accuracy: 31.4416%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [107/225], Training Accuracy: 31.3668%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [108/225], Training Accuracy: 31.4525%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Step [109/225], Training Accuracy: 31.3360%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [110/225], Training Accuracy: 31.3068%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [111/225], Training Accuracy: 31.2078%, Training Loss: 0.6855%\n",
      "Epoch [87/100], Step [112/225], Training Accuracy: 31.2779%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [113/225], Training Accuracy: 31.2915%, Training Loss: 0.6855%\n",
      "Epoch [87/100], Step [114/225], Training Accuracy: 31.3459%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [115/225], Training Accuracy: 31.2908%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [116/225], Training Accuracy: 31.2635%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [117/225], Training Accuracy: 31.2233%, Training Loss: 0.6855%\n",
      "Epoch [87/100], Step [118/225], Training Accuracy: 31.1706%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [119/225], Training Accuracy: 31.1318%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [120/225], Training Accuracy: 31.1589%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [121/225], Training Accuracy: 31.1596%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [122/225], Training Accuracy: 31.1732%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [123/225], Training Accuracy: 31.2119%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [124/225], Training Accuracy: 31.1996%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [125/225], Training Accuracy: 31.1750%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [126/225], Training Accuracy: 31.1260%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [127/225], Training Accuracy: 31.1147%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [128/225], Training Accuracy: 31.1157%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [129/225], Training Accuracy: 31.1652%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [130/225], Training Accuracy: 31.0938%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [131/225], Training Accuracy: 31.0472%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [132/225], Training Accuracy: 31.0133%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [133/225], Training Accuracy: 31.0385%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [134/225], Training Accuracy: 31.0868%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [135/225], Training Accuracy: 31.0764%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [136/225], Training Accuracy: 31.0892%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [137/225], Training Accuracy: 31.1245%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [138/225], Training Accuracy: 31.1368%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [139/225], Training Accuracy: 31.1488%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [140/225], Training Accuracy: 31.1272%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [141/225], Training Accuracy: 31.0616%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [142/225], Training Accuracy: 31.1400%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [143/225], Training Accuracy: 31.1080%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [144/225], Training Accuracy: 31.1306%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [145/225], Training Accuracy: 31.1746%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [146/225], Training Accuracy: 31.1965%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [147/225], Training Accuracy: 31.2075%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [149/225], Training Accuracy: 31.1871%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [150/225], Training Accuracy: 31.2188%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [151/225], Training Accuracy: 31.2707%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [152/225], Training Accuracy: 31.2706%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [153/225], Training Accuracy: 31.2296%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [154/225], Training Accuracy: 31.2399%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [155/225], Training Accuracy: 31.2298%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [156/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [157/225], Training Accuracy: 31.1803%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [158/225], Training Accuracy: 31.1709%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [159/225], Training Accuracy: 31.2107%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [160/225], Training Accuracy: 31.1816%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [161/225], Training Accuracy: 31.2306%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [162/225], Training Accuracy: 31.2018%, Training Loss: 0.6854%\n",
      "Epoch [87/100], Step [163/225], Training Accuracy: 31.2692%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [164/225], Training Accuracy: 31.2786%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [165/225], Training Accuracy: 31.2121%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [166/225], Training Accuracy: 31.2123%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [167/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [168/225], Training Accuracy: 31.1849%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [169/225], Training Accuracy: 31.1113%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [170/225], Training Accuracy: 31.0662%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [171/225], Training Accuracy: 31.0947%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [172/225], Training Accuracy: 31.1137%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [173/225], Training Accuracy: 31.1145%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [174/225], Training Accuracy: 31.1422%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [175/225], Training Accuracy: 31.1607%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [176/225], Training Accuracy: 31.1701%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [177/225], Training Accuracy: 31.1706%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [178/225], Training Accuracy: 31.1622%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [179/225], Training Accuracy: 31.1191%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [180/225], Training Accuracy: 31.1458%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [181/225], Training Accuracy: 31.1032%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [182/225], Training Accuracy: 31.0869%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [183/225], Training Accuracy: 31.0963%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [184/225], Training Accuracy: 31.0632%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [185/225], Training Accuracy: 31.0473%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [186/225], Training Accuracy: 31.0904%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [187/225], Training Accuracy: 31.1080%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [188/225], Training Accuracy: 31.1004%, Training Loss: 0.6853%\n",
      "Epoch [87/100], Step [189/225], Training Accuracy: 31.1425%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [190/225], Training Accuracy: 31.1020%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [191/225], Training Accuracy: 31.0864%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [192/225], Training Accuracy: 31.0140%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [193/225], Training Accuracy: 31.0152%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [194/225], Training Accuracy: 31.0325%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [195/225], Training Accuracy: 31.0176%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [196/225], Training Accuracy: 30.9869%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [197/225], Training Accuracy: 31.0121%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [198/225], Training Accuracy: 31.0290%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [199/225], Training Accuracy: 31.0144%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [200/225], Training Accuracy: 30.9922%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [201/225], Training Accuracy: 30.9857%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [202/225], Training Accuracy: 30.9870%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [203/225], Training Accuracy: 30.9575%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [204/225], Training Accuracy: 31.0049%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [205/225], Training Accuracy: 31.0137%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [206/225], Training Accuracy: 31.0225%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [207/225], Training Accuracy: 31.0085%, Training Loss: 0.6852%\n",
      "Epoch [87/100], Step [208/225], Training Accuracy: 31.0322%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100], Step [209/225], Training Accuracy: 31.0706%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [210/225], Training Accuracy: 31.1012%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [211/225], Training Accuracy: 31.0797%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [212/225], Training Accuracy: 31.1173%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [213/225], Training Accuracy: 31.0960%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [214/225], Training Accuracy: 31.1186%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [215/225], Training Accuracy: 31.0756%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [216/225], Training Accuracy: 31.0185%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [217/225], Training Accuracy: 31.0124%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [218/225], Training Accuracy: 30.9848%, Training Loss: 0.6851%\n",
      "Epoch [87/100], Step [219/225], Training Accuracy: 31.0502%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [220/225], Training Accuracy: 31.0653%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [221/225], Training Accuracy: 31.0520%, Training Loss: 0.6850%\n",
      "Epoch [87/100], Step [222/225], Training Accuracy: 31.0670%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [223/225], Training Accuracy: 31.1169%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [224/225], Training Accuracy: 31.1105%, Training Loss: 0.6849%\n",
      "Epoch [87/100], Step [225/225], Training Accuracy: 31.0937%, Training Loss: 0.6850%\n",
      "Epoch [88/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6935%\n",
      "Epoch [88/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6865%\n",
      "Epoch [88/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6865%\n",
      "Epoch [88/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6866%\n",
      "Epoch [88/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6861%\n",
      "Epoch [88/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6864%\n",
      "Epoch [88/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6865%\n",
      "Epoch [88/100], Step [12/225], Training Accuracy: 30.0781%, Training Loss: 0.6864%\n",
      "Epoch [88/100], Step [13/225], Training Accuracy: 30.2885%, Training Loss: 0.6867%\n",
      "Epoch [88/100], Step [14/225], Training Accuracy: 30.4688%, Training Loss: 0.6872%\n",
      "Epoch [88/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6872%\n",
      "Epoch [88/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6870%\n",
      "Epoch [88/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6873%\n",
      "Epoch [88/100], Step [18/225], Training Accuracy: 30.9028%, Training Loss: 0.6871%\n",
      "Epoch [88/100], Step [19/225], Training Accuracy: 31.3322%, Training Loss: 0.6869%\n",
      "Epoch [88/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6868%\n",
      "Epoch [88/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6867%\n",
      "Epoch [88/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6863%\n",
      "Epoch [88/100], Step [23/225], Training Accuracy: 31.6576%, Training Loss: 0.6862%\n",
      "Epoch [88/100], Step [24/225], Training Accuracy: 31.8359%, Training Loss: 0.6862%\n",
      "Epoch [88/100], Step [25/225], Training Accuracy: 32.0000%, Training Loss: 0.6860%\n",
      "Epoch [88/100], Step [26/225], Training Accuracy: 32.3918%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [27/225], Training Accuracy: 32.0023%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [28/225], Training Accuracy: 31.6964%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [29/225], Training Accuracy: 31.9504%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [31/225], Training Accuracy: 31.7540%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6851%\n",
      "Epoch [88/100], Step [33/225], Training Accuracy: 32.1970%, Training Loss: 0.6850%\n",
      "Epoch [88/100], Step [34/225], Training Accuracy: 32.1691%, Training Loss: 0.6850%\n",
      "Epoch [88/100], Step [35/225], Training Accuracy: 32.0089%, Training Loss: 0.6851%\n",
      "Epoch [88/100], Step [36/225], Training Accuracy: 31.9010%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [37/225], Training Accuracy: 31.8412%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [40/225], Training Accuracy: 31.5234%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [41/225], Training Accuracy: 31.5930%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [42/225], Training Accuracy: 31.3988%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [43/225], Training Accuracy: 31.6134%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [44/225], Training Accuracy: 31.6406%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [45/225], Training Accuracy: 31.6667%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [46/225], Training Accuracy: 31.5557%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [47/225], Training Accuracy: 31.4827%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [48/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [49/225], Training Accuracy: 31.6008%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [50/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [51/225], Training Accuracy: 31.7402%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [52/225], Training Accuracy: 31.7308%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [53/225], Training Accuracy: 31.6922%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [54/225], Training Accuracy: 31.5972%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [55/225], Training Accuracy: 31.7045%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [56/225], Training Accuracy: 31.7243%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [57/225], Training Accuracy: 31.6886%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [58/225], Training Accuracy: 31.6002%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [59/225], Training Accuracy: 31.8856%, Training Loss: 0.6851%\n",
      "Epoch [88/100], Step [60/225], Training Accuracy: 31.9271%, Training Loss: 0.6851%\n",
      "Epoch [88/100], Step [61/225], Training Accuracy: 31.8391%, Training Loss: 0.6851%\n",
      "Epoch [88/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [63/225], Training Accuracy: 31.9444%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [64/225], Training Accuracy: 31.9336%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [65/225], Training Accuracy: 31.8269%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [66/225], Training Accuracy: 31.8419%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [67/225], Training Accuracy: 31.8563%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [68/225], Training Accuracy: 31.9164%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [69/225], Training Accuracy: 31.9067%, Training Loss: 0.6850%\n",
      "Epoch [88/100], Step [70/225], Training Accuracy: 31.9196%, Training Loss: 0.6850%\n",
      "Epoch [88/100], Step [71/225], Training Accuracy: 31.9542%, Training Loss: 0.6851%\n",
      "Epoch [88/100], Step [72/225], Training Accuracy: 31.7274%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [73/225], Training Accuracy: 31.6567%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [74/225], Training Accuracy: 31.7356%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [75/225], Training Accuracy: 31.6875%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [76/225], Training Accuracy: 31.5995%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [77/225], Training Accuracy: 31.4935%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [78/225], Training Accuracy: 31.5905%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [79/225], Training Accuracy: 31.5269%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [80/225], Training Accuracy: 31.5039%, Training Loss: 0.6852%\n",
      "Epoch [88/100], Step [81/225], Training Accuracy: 31.4236%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [82/225], Training Accuracy: 31.4405%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Step [83/225], Training Accuracy: 31.3818%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [84/225], Training Accuracy: 31.4360%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [85/225], Training Accuracy: 31.4154%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [86/225], Training Accuracy: 31.4680%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [87/225], Training Accuracy: 31.4476%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [88/225], Training Accuracy: 31.4276%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [89/225], Training Accuracy: 31.3729%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [90/225], Training Accuracy: 31.2847%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [91/225], Training Accuracy: 31.3702%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [92/225], Training Accuracy: 31.3689%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [93/225], Training Accuracy: 31.3676%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [94/225], Training Accuracy: 31.4495%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [95/225], Training Accuracy: 31.3322%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [96/225], Training Accuracy: 31.4290%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [97/225], Training Accuracy: 31.4594%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [98/225], Training Accuracy: 31.5051%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [99/225], Training Accuracy: 31.6288%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [100/225], Training Accuracy: 31.5938%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [101/225], Training Accuracy: 31.7296%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [102/225], Training Accuracy: 31.6176%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [103/225], Training Accuracy: 31.6748%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [104/225], Training Accuracy: 31.6556%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [105/225], Training Accuracy: 31.6071%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [106/225], Training Accuracy: 31.5890%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [107/225], Training Accuracy: 31.4982%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [108/225], Training Accuracy: 31.5972%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [109/225], Training Accuracy: 31.4650%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [110/225], Training Accuracy: 31.4915%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [111/225], Training Accuracy: 31.4048%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [112/225], Training Accuracy: 31.5011%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [113/225], Training Accuracy: 31.4989%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [114/225], Training Accuracy: 31.5515%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [115/225], Training Accuracy: 31.5217%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [116/225], Training Accuracy: 31.5059%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [117/225], Training Accuracy: 31.4770%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [118/225], Training Accuracy: 31.4486%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [119/225], Training Accuracy: 31.4076%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [120/225], Training Accuracy: 31.4193%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [121/225], Training Accuracy: 31.3791%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [122/225], Training Accuracy: 31.3653%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [123/225], Training Accuracy: 31.4278%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [124/225], Training Accuracy: 31.3886%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [125/225], Training Accuracy: 31.3750%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [126/225], Training Accuracy: 31.3120%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [127/225], Training Accuracy: 31.2746%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [128/225], Training Accuracy: 31.2622%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [129/225], Training Accuracy: 31.2984%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [130/225], Training Accuracy: 31.2380%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [131/225], Training Accuracy: 31.2142%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [132/225], Training Accuracy: 31.1790%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [133/225], Training Accuracy: 31.1913%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [134/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [135/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [136/225], Training Accuracy: 31.2730%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [137/225], Training Accuracy: 31.3070%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [138/225], Training Accuracy: 31.3293%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [139/225], Training Accuracy: 31.2837%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [140/225], Training Accuracy: 31.2612%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [141/225], Training Accuracy: 31.1946%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [142/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [88/100], Step [143/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [144/225], Training Accuracy: 31.2717%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [145/225], Training Accuracy: 31.3254%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [146/225], Training Accuracy: 31.3463%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [147/225], Training Accuracy: 31.3669%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [148/225], Training Accuracy: 31.3450%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [149/225], Training Accuracy: 31.3549%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [150/225], Training Accuracy: 31.3542%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [151/225], Training Accuracy: 31.4156%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [152/225], Training Accuracy: 31.4145%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [153/225], Training Accuracy: 31.3725%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [154/225], Training Accuracy: 31.3920%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [155/225], Training Accuracy: 31.3810%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [156/225], Training Accuracy: 31.4002%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [157/225], Training Accuracy: 31.3396%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [158/225], Training Accuracy: 31.2994%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [159/225], Training Accuracy: 31.3581%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [160/225], Training Accuracy: 31.3281%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [161/225], Training Accuracy: 31.3956%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [162/225], Training Accuracy: 31.3657%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [163/225], Training Accuracy: 31.4225%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [164/225], Training Accuracy: 31.3929%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [165/225], Training Accuracy: 31.3352%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [166/225], Training Accuracy: 31.3441%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [167/225], Training Accuracy: 31.3716%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [168/225], Training Accuracy: 31.3151%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [169/225], Training Accuracy: 31.2408%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [170/225], Training Accuracy: 31.1857%, Training Loss: 0.6858%\n",
      "Epoch [88/100], Step [171/225], Training Accuracy: 31.2135%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [172/225], Training Accuracy: 31.2318%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [173/225], Training Accuracy: 31.2139%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [174/225], Training Accuracy: 31.2410%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [175/225], Training Accuracy: 31.2679%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [176/225], Training Accuracy: 31.3121%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [177/225], Training Accuracy: 31.3206%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [178/225], Training Accuracy: 31.3378%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [179/225], Training Accuracy: 31.3024%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [180/225], Training Accuracy: 31.3368%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Step [181/225], Training Accuracy: 31.3018%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [182/225], Training Accuracy: 31.2929%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [183/225], Training Accuracy: 31.3098%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [184/225], Training Accuracy: 31.2585%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [185/225], Training Accuracy: 31.2162%, Training Loss: 0.6857%\n",
      "Epoch [88/100], Step [186/225], Training Accuracy: 31.2668%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [187/225], Training Accuracy: 31.2751%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [188/225], Training Accuracy: 31.2749%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [189/225], Training Accuracy: 31.2996%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [190/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [191/225], Training Accuracy: 31.2173%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [192/225], Training Accuracy: 31.1442%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [193/225], Training Accuracy: 31.1528%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [194/225], Training Accuracy: 31.1695%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [195/225], Training Accuracy: 31.1538%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [196/225], Training Accuracy: 31.1224%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [197/225], Training Accuracy: 31.1628%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [198/225], Training Accuracy: 31.1711%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [199/225], Training Accuracy: 31.1715%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [200/225], Training Accuracy: 31.1406%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [201/225], Training Accuracy: 31.1489%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [202/225], Training Accuracy: 31.1417%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [203/225], Training Accuracy: 31.1268%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [204/225], Training Accuracy: 31.1811%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [205/225], Training Accuracy: 31.1966%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [206/225], Training Accuracy: 31.1817%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [207/225], Training Accuracy: 31.1519%, Training Loss: 0.6856%\n",
      "Epoch [88/100], Step [208/225], Training Accuracy: 31.1824%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [209/225], Training Accuracy: 31.2276%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [210/225], Training Accuracy: 31.2649%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [211/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [88/100], Step [212/225], Training Accuracy: 31.3090%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [213/225], Training Accuracy: 31.2867%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [214/225], Training Accuracy: 31.3230%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [215/225], Training Accuracy: 31.3009%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [216/225], Training Accuracy: 31.2428%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [217/225], Training Accuracy: 31.2428%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [218/225], Training Accuracy: 31.2070%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [219/225], Training Accuracy: 31.2643%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [220/225], Training Accuracy: 31.2997%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [221/225], Training Accuracy: 31.2854%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [222/225], Training Accuracy: 31.3063%, Training Loss: 0.6854%\n",
      "Epoch [88/100], Step [223/225], Training Accuracy: 31.3341%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [224/225], Training Accuracy: 31.3337%, Training Loss: 0.6853%\n",
      "Epoch [88/100], Step [225/225], Training Accuracy: 31.3160%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6886%\n",
      "Epoch [89/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6898%\n",
      "Epoch [89/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6916%\n",
      "Epoch [89/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6886%\n",
      "Epoch [89/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6893%\n",
      "Epoch [89/100], Step [6/225], Training Accuracy: 31.7708%, Training Loss: 0.6888%\n",
      "Epoch [89/100], Step [7/225], Training Accuracy: 31.0268%, Training Loss: 0.6883%\n",
      "Epoch [89/100], Step [8/225], Training Accuracy: 30.6641%, Training Loss: 0.6877%\n",
      "Epoch [89/100], Step [9/225], Training Accuracy: 30.3819%, Training Loss: 0.6881%\n",
      "Epoch [89/100], Step [10/225], Training Accuracy: 29.6875%, Training Loss: 0.6878%\n",
      "Epoch [89/100], Step [11/225], Training Accuracy: 29.5455%, Training Loss: 0.6877%\n",
      "Epoch [89/100], Step [12/225], Training Accuracy: 28.7760%, Training Loss: 0.6875%\n",
      "Epoch [89/100], Step [13/225], Training Accuracy: 28.6058%, Training Loss: 0.6876%\n",
      "Epoch [89/100], Step [14/225], Training Accuracy: 28.7946%, Training Loss: 0.6882%\n",
      "Epoch [89/100], Step [15/225], Training Accuracy: 29.1667%, Training Loss: 0.6882%\n",
      "Epoch [89/100], Step [16/225], Training Accuracy: 29.2969%, Training Loss: 0.6878%\n",
      "Epoch [89/100], Step [17/225], Training Accuracy: 29.0441%, Training Loss: 0.6878%\n",
      "Epoch [89/100], Step [18/225], Training Accuracy: 29.5139%, Training Loss: 0.6878%\n",
      "Epoch [89/100], Step [19/225], Training Accuracy: 29.6875%, Training Loss: 0.6879%\n",
      "Epoch [89/100], Step [20/225], Training Accuracy: 30.3125%, Training Loss: 0.6876%\n",
      "Epoch [89/100], Step [21/225], Training Accuracy: 30.3571%, Training Loss: 0.6873%\n",
      "Epoch [89/100], Step [22/225], Training Accuracy: 30.4688%, Training Loss: 0.6870%\n",
      "Epoch [89/100], Step [23/225], Training Accuracy: 30.4348%, Training Loss: 0.6868%\n",
      "Epoch [89/100], Step [24/225], Training Accuracy: 30.7292%, Training Loss: 0.6868%\n",
      "Epoch [89/100], Step [25/225], Training Accuracy: 30.8750%, Training Loss: 0.6865%\n",
      "Epoch [89/100], Step [26/225], Training Accuracy: 31.3101%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [27/225], Training Accuracy: 31.0764%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [28/225], Training Accuracy: 30.8594%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [29/225], Training Accuracy: 31.3578%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [30/225], Training Accuracy: 31.4583%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [31/225], Training Accuracy: 31.4012%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [32/225], Training Accuracy: 31.6406%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [33/225], Training Accuracy: 31.7235%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [34/225], Training Accuracy: 31.5717%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [35/225], Training Accuracy: 31.4732%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [36/225], Training Accuracy: 31.3368%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [37/225], Training Accuracy: 31.3345%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [38/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [39/225], Training Accuracy: 30.9295%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [40/225], Training Accuracy: 30.9375%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [41/225], Training Accuracy: 31.0213%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [42/225], Training Accuracy: 30.8780%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [43/225], Training Accuracy: 31.1047%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [44/225], Training Accuracy: 31.0369%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [45/225], Training Accuracy: 31.1111%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [46/225], Training Accuracy: 31.0462%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [47/225], Training Accuracy: 31.0505%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [48/225], Training Accuracy: 31.1849%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [49/225], Training Accuracy: 31.2181%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [50/225], Training Accuracy: 31.1250%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [51/225], Training Accuracy: 31.2194%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [52/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [53/225], Training Accuracy: 31.2205%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [54/225], Training Accuracy: 31.1053%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100], Step [55/225], Training Accuracy: 31.1648%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [56/225], Training Accuracy: 31.1663%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [57/225], Training Accuracy: 31.1129%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [58/225], Training Accuracy: 31.0075%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [59/225], Training Accuracy: 31.3824%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [60/225], Training Accuracy: 31.4844%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [61/225], Training Accuracy: 31.4293%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [62/225], Training Accuracy: 31.4768%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [63/225], Training Accuracy: 31.5724%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [64/225], Training Accuracy: 31.5674%, Training Loss: 0.6853%\n",
      "Epoch [89/100], Step [65/225], Training Accuracy: 31.4663%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [66/225], Training Accuracy: 31.5814%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [67/225], Training Accuracy: 31.5765%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [68/225], Training Accuracy: 31.6176%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [69/225], Training Accuracy: 31.5897%, Training Loss: 0.6853%\n",
      "Epoch [89/100], Step [70/225], Training Accuracy: 31.5402%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [71/225], Training Accuracy: 31.6461%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [72/225], Training Accuracy: 31.4236%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [73/225], Training Accuracy: 31.3784%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [74/225], Training Accuracy: 31.5034%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [75/225], Training Accuracy: 31.4375%, Training Loss: 0.6854%\n",
      "Epoch [89/100], Step [76/225], Training Accuracy: 31.3939%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [77/225], Training Accuracy: 31.3312%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [78/225], Training Accuracy: 31.3702%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [79/225], Training Accuracy: 31.3093%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [80/225], Training Accuracy: 31.3086%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [81/225], Training Accuracy: 31.2307%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [82/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [83/225], Training Accuracy: 31.1559%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [84/225], Training Accuracy: 31.1756%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [85/225], Training Accuracy: 31.1765%, Training Loss: 0.6855%\n",
      "Epoch [89/100], Step [86/225], Training Accuracy: 31.2318%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [87/225], Training Accuracy: 31.2320%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [88/225], Training Accuracy: 31.1967%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [89/225], Training Accuracy: 31.1447%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [90/225], Training Accuracy: 31.0590%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [91/225], Training Accuracy: 31.1298%, Training Loss: 0.6857%\n",
      "Epoch [89/100], Step [92/225], Training Accuracy: 31.1311%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [93/225], Training Accuracy: 31.1156%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [94/225], Training Accuracy: 31.1669%, Training Loss: 0.6856%\n",
      "Epoch [89/100], Step [95/225], Training Accuracy: 31.0526%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [96/225], Training Accuracy: 31.1198%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [97/225], Training Accuracy: 31.1372%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [98/225], Training Accuracy: 31.1703%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [99/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [100/225], Training Accuracy: 31.2188%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [101/225], Training Accuracy: 31.3583%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [102/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [103/225], Training Accuracy: 31.3107%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [104/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [105/225], Training Accuracy: 31.2054%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [106/225], Training Accuracy: 31.1763%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [107/225], Training Accuracy: 31.1040%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [108/225], Training Accuracy: 31.1632%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [109/225], Training Accuracy: 31.0493%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [110/225], Training Accuracy: 31.0795%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [111/225], Training Accuracy: 30.9685%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [112/225], Training Accuracy: 31.0268%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [113/225], Training Accuracy: 31.0149%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [114/225], Training Accuracy: 31.0992%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [115/225], Training Accuracy: 31.0734%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [116/225], Training Accuracy: 31.1018%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [117/225], Training Accuracy: 31.0630%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [118/225], Training Accuracy: 31.0117%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [119/225], Training Accuracy: 30.9480%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [120/225], Training Accuracy: 31.0156%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [121/225], Training Accuracy: 30.9917%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [122/225], Training Accuracy: 30.9939%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [123/225], Training Accuracy: 31.0340%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [124/225], Training Accuracy: 31.0358%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [125/225], Training Accuracy: 31.0250%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [126/225], Training Accuracy: 30.9648%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [127/225], Training Accuracy: 30.9547%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [128/225], Training Accuracy: 30.9570%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [129/225], Training Accuracy: 31.0078%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [130/225], Training Accuracy: 30.9736%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [131/225], Training Accuracy: 30.9399%, Training Loss: 0.6864%\n",
      "Epoch [89/100], Step [132/225], Training Accuracy: 30.9067%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [133/225], Training Accuracy: 30.9328%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [134/225], Training Accuracy: 30.9935%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [135/225], Training Accuracy: 30.9838%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [136/225], Training Accuracy: 30.9972%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [137/225], Training Accuracy: 31.0333%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [138/225], Training Accuracy: 31.0462%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [139/225], Training Accuracy: 31.0027%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [140/225], Training Accuracy: 31.0045%, Training Loss: 0.6864%\n",
      "Epoch [89/100], Step [141/225], Training Accuracy: 30.9397%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [142/225], Training Accuracy: 30.9859%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [143/225], Training Accuracy: 30.9878%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [144/225], Training Accuracy: 31.0113%, Training Loss: 0.6864%\n",
      "Epoch [89/100], Step [145/225], Training Accuracy: 31.0884%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [146/225], Training Accuracy: 31.0895%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [147/225], Training Accuracy: 31.1118%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [148/225], Training Accuracy: 31.0705%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [149/225], Training Accuracy: 31.1137%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [150/225], Training Accuracy: 31.1562%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [151/225], Training Accuracy: 31.1983%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [152/225], Training Accuracy: 31.2192%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [153/225], Training Accuracy: 31.1785%, Training Loss: 0.6862%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100], Step [154/225], Training Accuracy: 31.2094%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [155/225], Training Accuracy: 31.2097%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [156/225], Training Accuracy: 31.2200%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [157/225], Training Accuracy: 31.1604%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [158/225], Training Accuracy: 31.1511%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [159/225], Training Accuracy: 31.2402%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [160/225], Training Accuracy: 31.2012%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [161/225], Training Accuracy: 31.2209%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [162/225], Training Accuracy: 31.1825%, Training Loss: 0.6863%\n",
      "Epoch [89/100], Step [163/225], Training Accuracy: 31.2404%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [164/225], Training Accuracy: 31.2214%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [165/225], Training Accuracy: 31.1742%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [166/225], Training Accuracy: 31.1559%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [167/225], Training Accuracy: 31.2032%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [168/225], Training Accuracy: 31.1570%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [169/225], Training Accuracy: 31.0928%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [170/225], Training Accuracy: 31.0570%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [171/225], Training Accuracy: 31.0764%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [172/225], Training Accuracy: 31.0956%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [173/225], Training Accuracy: 31.0965%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [174/225], Training Accuracy: 31.1243%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [175/225], Training Accuracy: 31.1786%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [176/225], Training Accuracy: 31.2056%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [177/225], Training Accuracy: 31.2059%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [178/225], Training Accuracy: 31.1973%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [179/225], Training Accuracy: 31.1714%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [181/225], Training Accuracy: 31.1723%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [182/225], Training Accuracy: 31.1556%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [183/225], Training Accuracy: 31.1646%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [184/225], Training Accuracy: 31.1226%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [185/225], Training Accuracy: 31.0895%, Training Loss: 0.6862%\n",
      "Epoch [89/100], Step [186/225], Training Accuracy: 31.1072%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [187/225], Training Accuracy: 31.0996%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [188/225], Training Accuracy: 31.1004%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [189/225], Training Accuracy: 31.1343%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [190/225], Training Accuracy: 31.0938%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [191/225], Training Accuracy: 31.0618%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [192/225], Training Accuracy: 30.9896%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [193/225], Training Accuracy: 30.9909%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [194/225], Training Accuracy: 31.0003%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [195/225], Training Accuracy: 30.9696%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [196/225], Training Accuracy: 30.9391%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [197/225], Training Accuracy: 30.9724%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [198/225], Training Accuracy: 30.9738%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [199/225], Training Accuracy: 30.9595%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [200/225], Training Accuracy: 30.9297%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [201/225], Training Accuracy: 30.9391%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [202/225], Training Accuracy: 30.9406%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [203/225], Training Accuracy: 30.9344%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [204/225], Training Accuracy: 30.9972%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [205/225], Training Accuracy: 31.0061%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [206/225], Training Accuracy: 30.9997%, Training Loss: 0.6861%\n",
      "Epoch [89/100], Step [207/225], Training Accuracy: 30.9707%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [208/225], Training Accuracy: 30.9946%, Training Loss: 0.6860%\n",
      "Epoch [89/100], Step [209/225], Training Accuracy: 31.0407%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [210/225], Training Accuracy: 31.0714%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [211/225], Training Accuracy: 31.0649%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [212/225], Training Accuracy: 31.1100%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [213/225], Training Accuracy: 31.0886%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [214/225], Training Accuracy: 31.1113%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [215/225], Training Accuracy: 31.0828%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [216/225], Training Accuracy: 31.0258%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [217/225], Training Accuracy: 31.0196%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [218/225], Training Accuracy: 30.9848%, Training Loss: 0.6859%\n",
      "Epoch [89/100], Step [219/225], Training Accuracy: 31.0431%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [220/225], Training Accuracy: 31.0653%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [221/225], Training Accuracy: 31.0591%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [222/225], Training Accuracy: 31.0811%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [223/225], Training Accuracy: 31.1239%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [224/225], Training Accuracy: 31.1384%, Training Loss: 0.6858%\n",
      "Epoch [89/100], Step [225/225], Training Accuracy: 31.1145%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6898%\n",
      "Epoch [90/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6865%\n",
      "Epoch [90/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6871%\n",
      "Epoch [90/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6852%\n",
      "Epoch [90/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6860%\n",
      "Epoch [90/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6852%\n",
      "Epoch [90/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [14/225], Training Accuracy: 30.0223%, Training Loss: 0.6861%\n",
      "Epoch [90/100], Step [15/225], Training Accuracy: 30.6250%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [25/225], Training Accuracy: 31.6250%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [27/225], Training Accuracy: 31.5972%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [28/225], Training Accuracy: 31.3616%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Step [29/225], Training Accuracy: 31.6272%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [30/225], Training Accuracy: 31.5625%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [31/225], Training Accuracy: 31.5020%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [32/225], Training Accuracy: 31.8359%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [33/225], Training Accuracy: 31.9129%, Training Loss: 0.6846%\n",
      "Epoch [90/100], Step [34/225], Training Accuracy: 31.8474%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [35/225], Training Accuracy: 31.6964%, Training Loss: 0.6849%\n",
      "Epoch [90/100], Step [36/225], Training Accuracy: 31.5104%, Training Loss: 0.6849%\n",
      "Epoch [90/100], Step [37/225], Training Accuracy: 31.5456%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [38/225], Training Accuracy: 31.3734%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [39/225], Training Accuracy: 31.1298%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [40/225], Training Accuracy: 31.2109%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [41/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [90/100], Step [42/225], Training Accuracy: 31.1012%, Training Loss: 0.6854%\n",
      "Epoch [90/100], Step [43/225], Training Accuracy: 31.2863%, Training Loss: 0.6854%\n",
      "Epoch [90/100], Step [44/225], Training Accuracy: 31.2855%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [45/225], Training Accuracy: 31.3194%, Training Loss: 0.6852%\n",
      "Epoch [90/100], Step [46/225], Training Accuracy: 31.1141%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [47/225], Training Accuracy: 31.0505%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [48/225], Training Accuracy: 31.1849%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [49/225], Training Accuracy: 31.2181%, Training Loss: 0.6849%\n",
      "Epoch [90/100], Step [50/225], Training Accuracy: 31.2188%, Training Loss: 0.6852%\n",
      "Epoch [90/100], Step [51/225], Training Accuracy: 31.3725%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [52/225], Training Accuracy: 31.3702%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [53/225], Training Accuracy: 31.3384%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [54/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [56/225], Training Accuracy: 31.5011%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [57/225], Training Accuracy: 31.5241%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [58/225], Training Accuracy: 31.4655%, Training Loss: 0.6849%\n",
      "Epoch [90/100], Step [59/225], Training Accuracy: 31.8326%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [60/225], Training Accuracy: 31.9531%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [61/225], Training Accuracy: 31.8648%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [62/225], Training Accuracy: 31.9304%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [63/225], Training Accuracy: 32.0188%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [64/225], Training Accuracy: 32.0068%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [65/225], Training Accuracy: 31.9231%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [66/225], Training Accuracy: 31.9602%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [67/225], Training Accuracy: 31.9263%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [68/225], Training Accuracy: 32.0083%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [69/225], Training Accuracy: 31.9746%, Training Loss: 0.6847%\n",
      "Epoch [90/100], Step [70/225], Training Accuracy: 31.9420%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [71/225], Training Accuracy: 31.9542%, Training Loss: 0.6848%\n",
      "Epoch [90/100], Step [72/225], Training Accuracy: 31.7491%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [73/225], Training Accuracy: 31.7209%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [74/225], Training Accuracy: 31.8201%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [75/225], Training Accuracy: 31.7292%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [76/225], Training Accuracy: 31.7023%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [77/225], Training Accuracy: 31.6558%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [78/225], Training Accuracy: 31.7107%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [79/225], Training Accuracy: 31.6456%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [80/225], Training Accuracy: 31.6016%, Training Loss: 0.6849%\n",
      "Epoch [90/100], Step [81/225], Training Accuracy: 31.5394%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [82/225], Training Accuracy: 31.5549%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [83/225], Training Accuracy: 31.4571%, Training Loss: 0.6850%\n",
      "Epoch [90/100], Step [84/225], Training Accuracy: 31.5290%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [85/225], Training Accuracy: 31.5257%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [86/225], Training Accuracy: 31.5589%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [87/225], Training Accuracy: 31.5912%, Training Loss: 0.6851%\n",
      "Epoch [90/100], Step [88/225], Training Accuracy: 31.5341%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [89/225], Training Accuracy: 31.4607%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [90/225], Training Accuracy: 31.3715%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [91/225], Training Accuracy: 31.4389%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [92/225], Training Accuracy: 31.4198%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [93/225], Training Accuracy: 31.3844%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [94/225], Training Accuracy: 31.4827%, Training Loss: 0.6853%\n",
      "Epoch [90/100], Step [95/225], Training Accuracy: 31.3980%, Training Loss: 0.6854%\n",
      "Epoch [90/100], Step [96/225], Training Accuracy: 31.5267%, Training Loss: 0.6854%\n",
      "Epoch [90/100], Step [97/225], Training Accuracy: 31.5561%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [98/225], Training Accuracy: 31.5529%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [99/225], Training Accuracy: 31.6604%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [100/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [101/225], Training Accuracy: 31.7605%, Training Loss: 0.6854%\n",
      "Epoch [90/100], Step [102/225], Training Accuracy: 31.6636%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [103/225], Training Accuracy: 31.7051%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [104/225], Training Accuracy: 31.6406%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [105/225], Training Accuracy: 31.5923%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [106/225], Training Accuracy: 31.5743%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [107/225], Training Accuracy: 31.5129%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [108/225], Training Accuracy: 31.5972%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [109/225], Training Accuracy: 31.4794%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [110/225], Training Accuracy: 31.4631%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [111/225], Training Accuracy: 31.3485%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [112/225], Training Accuracy: 31.4453%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [113/225], Training Accuracy: 31.4298%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [114/225], Training Accuracy: 31.4830%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [115/225], Training Accuracy: 31.4946%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [116/225], Training Accuracy: 31.4925%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [117/225], Training Accuracy: 31.4503%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [118/225], Training Accuracy: 31.4619%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [119/225], Training Accuracy: 31.4207%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [120/225], Training Accuracy: 31.4453%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [121/225], Training Accuracy: 31.4308%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [122/225], Training Accuracy: 31.4293%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [123/225], Training Accuracy: 31.4660%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [124/225], Training Accuracy: 31.4390%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [126/225], Training Accuracy: 31.3616%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [127/225], Training Accuracy: 31.3238%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [128/225], Training Accuracy: 31.3232%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [129/225], Training Accuracy: 31.3953%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [130/225], Training Accuracy: 31.3341%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [131/225], Training Accuracy: 31.2858%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [132/225], Training Accuracy: 31.2263%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [133/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [134/225], Training Accuracy: 31.3083%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [135/225], Training Accuracy: 31.2963%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [136/225], Training Accuracy: 31.2845%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [137/225], Training Accuracy: 31.3184%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [138/225], Training Accuracy: 31.3293%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [139/225], Training Accuracy: 31.2725%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [140/225], Training Accuracy: 31.2500%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [141/225], Training Accuracy: 31.1835%, Training Loss: 0.6859%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Step [142/225], Training Accuracy: 31.2390%, Training Loss: 0.6859%\n",
      "Epoch [90/100], Step [143/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [144/225], Training Accuracy: 31.2826%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [145/225], Training Accuracy: 31.3470%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [146/225], Training Accuracy: 31.3570%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [147/225], Training Accuracy: 31.3776%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [148/225], Training Accuracy: 31.3661%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [149/225], Training Accuracy: 31.3863%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [150/225], Training Accuracy: 31.4062%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [151/225], Training Accuracy: 31.4363%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [152/225], Training Accuracy: 31.4350%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [153/225], Training Accuracy: 31.4032%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [154/225], Training Accuracy: 31.3920%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [155/225], Training Accuracy: 31.3710%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [156/225], Training Accuracy: 31.3802%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [157/225], Training Accuracy: 31.3197%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [158/225], Training Accuracy: 31.2896%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [159/225], Training Accuracy: 31.3679%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [160/225], Training Accuracy: 31.3574%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [161/225], Training Accuracy: 31.4150%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [162/225], Training Accuracy: 31.3754%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [163/225], Training Accuracy: 31.4321%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [164/225], Training Accuracy: 31.4120%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [165/225], Training Accuracy: 31.3352%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [166/225], Training Accuracy: 31.3441%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [167/225], Training Accuracy: 31.4091%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [168/225], Training Accuracy: 31.3523%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [169/225], Training Accuracy: 31.2962%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [170/225], Training Accuracy: 31.2500%, Training Loss: 0.6858%\n",
      "Epoch [90/100], Step [171/225], Training Accuracy: 31.2591%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [172/225], Training Accuracy: 31.2773%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [173/225], Training Accuracy: 31.2771%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [174/225], Training Accuracy: 31.3129%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [175/225], Training Accuracy: 31.3482%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [176/225], Training Accuracy: 31.3477%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [177/225], Training Accuracy: 31.3559%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [178/225], Training Accuracy: 31.3466%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [179/225], Training Accuracy: 31.3024%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [180/225], Training Accuracy: 31.3542%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [181/225], Training Accuracy: 31.2932%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [182/225], Training Accuracy: 31.2843%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [183/225], Training Accuracy: 31.3012%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [184/225], Training Accuracy: 31.2585%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [185/225], Training Accuracy: 31.2331%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [186/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [187/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [188/225], Training Accuracy: 31.2417%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [189/225], Training Accuracy: 31.2583%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [190/225], Training Accuracy: 31.2253%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [191/225], Training Accuracy: 31.2009%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [192/225], Training Accuracy: 31.1361%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [193/225], Training Accuracy: 31.1205%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [194/225], Training Accuracy: 31.1292%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [195/225], Training Accuracy: 31.1138%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [196/225], Training Accuracy: 31.0826%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [197/225], Training Accuracy: 31.1072%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [198/225], Training Accuracy: 31.1316%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [199/225], Training Accuracy: 31.1087%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [200/225], Training Accuracy: 31.0859%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [201/225], Training Accuracy: 31.0945%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [202/225], Training Accuracy: 31.0721%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [203/225], Training Accuracy: 31.0422%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [204/225], Training Accuracy: 31.0892%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [205/225], Training Accuracy: 31.0976%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [206/225], Training Accuracy: 31.0983%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [207/225], Training Accuracy: 31.0839%, Training Loss: 0.6857%\n",
      "Epoch [90/100], Step [208/225], Training Accuracy: 31.0998%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [209/225], Training Accuracy: 31.1379%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [210/225], Training Accuracy: 31.1533%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [211/225], Training Accuracy: 31.1389%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [212/225], Training Accuracy: 31.1910%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [213/225], Training Accuracy: 31.1620%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [214/225], Training Accuracy: 31.1989%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [215/225], Training Accuracy: 31.1701%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [216/225], Training Accuracy: 31.1053%, Training Loss: 0.6856%\n",
      "Epoch [90/100], Step [217/225], Training Accuracy: 31.1132%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [218/225], Training Accuracy: 31.0708%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [219/225], Training Accuracy: 31.1358%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [220/225], Training Accuracy: 31.1364%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [221/225], Training Accuracy: 31.1227%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [222/225], Training Accuracy: 31.1303%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [223/225], Training Accuracy: 31.1659%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [224/225], Training Accuracy: 31.1663%, Training Loss: 0.6855%\n",
      "Epoch [90/100], Step [225/225], Training Accuracy: 31.1492%, Training Loss: 0.6855%\n",
      "Epoch [91/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6892%\n",
      "Epoch [91/100], Step [2/225], Training Accuracy: 32.0312%, Training Loss: 0.6873%\n",
      "Epoch [91/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6887%\n",
      "Epoch [91/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6857%\n",
      "Epoch [91/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6853%\n",
      "Epoch [91/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6854%\n",
      "Epoch [91/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [8/225], Training Accuracy: 31.0547%, Training Loss: 0.6839%\n",
      "Epoch [91/100], Step [9/225], Training Accuracy: 30.9028%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [10/225], Training Accuracy: 30.4688%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [11/225], Training Accuracy: 30.1136%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [12/225], Training Accuracy: 29.4271%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [13/225], Training Accuracy: 29.3269%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [14/225], Training Accuracy: 29.6875%, Training Loss: 0.6853%\n",
      "Epoch [91/100], Step [15/225], Training Accuracy: 30.3125%, Training Loss: 0.6852%\n",
      "Epoch [91/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Step [17/225], Training Accuracy: 30.1471%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [18/225], Training Accuracy: 30.2951%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [19/225], Training Accuracy: 30.8388%, Training Loss: 0.6851%\n",
      "Epoch [91/100], Step [20/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [21/225], Training Accuracy: 31.3244%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [24/225], Training Accuracy: 31.7057%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [25/225], Training Accuracy: 31.9375%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [26/225], Training Accuracy: 32.2716%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [27/225], Training Accuracy: 32.1181%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [28/225], Training Accuracy: 31.8080%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [29/225], Training Accuracy: 32.1659%, Training Loss: 0.6841%\n",
      "Epoch [91/100], Step [30/225], Training Accuracy: 32.1875%, Training Loss: 0.6839%\n",
      "Epoch [91/100], Step [31/225], Training Accuracy: 32.1069%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [32/225], Training Accuracy: 32.4219%, Training Loss: 0.6835%\n",
      "Epoch [91/100], Step [33/225], Training Accuracy: 32.5758%, Training Loss: 0.6834%\n",
      "Epoch [91/100], Step [34/225], Training Accuracy: 32.4449%, Training Loss: 0.6833%\n",
      "Epoch [91/100], Step [35/225], Training Accuracy: 32.3214%, Training Loss: 0.6834%\n",
      "Epoch [91/100], Step [36/225], Training Accuracy: 32.1615%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [37/225], Training Accuracy: 32.1791%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [38/225], Training Accuracy: 31.9490%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [39/225], Training Accuracy: 31.7308%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [40/225], Training Accuracy: 31.7969%, Training Loss: 0.6841%\n",
      "Epoch [91/100], Step [41/225], Training Accuracy: 31.7835%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [42/225], Training Accuracy: 31.6964%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [43/225], Training Accuracy: 31.9041%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [44/225], Training Accuracy: 31.9247%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [45/225], Training Accuracy: 31.9444%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [46/225], Training Accuracy: 31.8274%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [47/225], Training Accuracy: 31.7487%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [48/225], Training Accuracy: 31.8685%, Training Loss: 0.6841%\n",
      "Epoch [91/100], Step [49/225], Training Accuracy: 31.8559%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [50/225], Training Accuracy: 31.8438%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [51/225], Training Accuracy: 31.9853%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [52/225], Training Accuracy: 32.0613%, Training Loss: 0.6839%\n",
      "Epoch [91/100], Step [53/225], Training Accuracy: 32.0165%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [54/225], Training Accuracy: 31.8576%, Training Loss: 0.6839%\n",
      "Epoch [91/100], Step [55/225], Training Accuracy: 31.9602%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [56/225], Training Accuracy: 32.0592%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [57/225], Training Accuracy: 32.0998%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [58/225], Training Accuracy: 31.9774%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [59/225], Training Accuracy: 32.2828%, Training Loss: 0.6835%\n",
      "Epoch [91/100], Step [60/225], Training Accuracy: 32.3438%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [61/225], Training Accuracy: 32.2746%, Training Loss: 0.6835%\n",
      "Epoch [91/100], Step [62/225], Training Accuracy: 32.2329%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [63/225], Training Accuracy: 32.2669%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [64/225], Training Accuracy: 32.2510%, Training Loss: 0.6835%\n",
      "Epoch [91/100], Step [65/225], Training Accuracy: 32.1635%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [66/225], Training Accuracy: 32.2680%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [67/225], Training Accuracy: 32.2528%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [68/225], Training Accuracy: 32.3070%, Training Loss: 0.6835%\n",
      "Epoch [91/100], Step [69/225], Training Accuracy: 32.3143%, Training Loss: 0.6833%\n",
      "Epoch [91/100], Step [70/225], Training Accuracy: 32.2321%, Training Loss: 0.6834%\n",
      "Epoch [91/100], Step [71/225], Training Accuracy: 32.2843%, Training Loss: 0.6834%\n",
      "Epoch [91/100], Step [72/225], Training Accuracy: 32.0747%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [73/225], Training Accuracy: 32.0205%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [74/225], Training Accuracy: 32.0946%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [75/225], Training Accuracy: 32.0417%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [76/225], Training Accuracy: 31.9901%, Training Loss: 0.6836%\n",
      "Epoch [91/100], Step [77/225], Training Accuracy: 31.9196%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [78/225], Training Accuracy: 31.9712%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [79/225], Training Accuracy: 31.9027%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [80/225], Training Accuracy: 31.8359%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [81/225], Training Accuracy: 31.6744%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [82/225], Training Accuracy: 31.6692%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [83/225], Training Accuracy: 31.6077%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [84/225], Training Accuracy: 31.6592%, Training Loss: 0.6837%\n",
      "Epoch [91/100], Step [85/225], Training Accuracy: 31.6544%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [86/225], Training Accuracy: 31.7042%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [87/225], Training Accuracy: 31.6990%, Training Loss: 0.6838%\n",
      "Epoch [91/100], Step [88/225], Training Accuracy: 31.6584%, Training Loss: 0.6839%\n",
      "Epoch [91/100], Step [89/225], Training Accuracy: 31.5660%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [90/225], Training Accuracy: 31.4757%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [91/225], Training Accuracy: 31.5247%, Training Loss: 0.6841%\n",
      "Epoch [91/100], Step [92/225], Training Accuracy: 31.5217%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [93/225], Training Accuracy: 31.5020%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [94/225], Training Accuracy: 31.5824%, Training Loss: 0.6840%\n",
      "Epoch [91/100], Step [95/225], Training Accuracy: 31.4638%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [96/225], Training Accuracy: 31.5755%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [97/225], Training Accuracy: 31.6044%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [98/225], Training Accuracy: 31.5848%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [99/225], Training Accuracy: 31.7077%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [100/225], Training Accuracy: 31.6875%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [101/225], Training Accuracy: 31.8224%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [102/225], Training Accuracy: 31.7249%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [103/225], Training Accuracy: 31.7961%, Training Loss: 0.6842%\n",
      "Epoch [91/100], Step [104/225], Training Accuracy: 31.7608%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [105/225], Training Accuracy: 31.7262%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [106/225], Training Accuracy: 31.7217%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [107/225], Training Accuracy: 31.6297%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [108/225], Training Accuracy: 31.7274%, Training Loss: 0.6843%\n",
      "Epoch [91/100], Step [109/225], Training Accuracy: 31.5940%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [110/225], Training Accuracy: 31.5909%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [111/225], Training Accuracy: 31.5175%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [112/225], Training Accuracy: 31.5709%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [113/225], Training Accuracy: 31.5542%, Training Loss: 0.6845%\n",
      "Epoch [91/100], Step [114/225], Training Accuracy: 31.5515%, Training Loss: 0.6845%\n",
      "Epoch [91/100], Step [115/225], Training Accuracy: 31.5217%, Training Loss: 0.6845%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Step [116/225], Training Accuracy: 31.5194%, Training Loss: 0.6844%\n",
      "Epoch [91/100], Step [117/225], Training Accuracy: 31.4503%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [118/225], Training Accuracy: 31.4089%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [119/225], Training Accuracy: 31.3813%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [120/225], Training Accuracy: 31.4323%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [121/225], Training Accuracy: 31.4179%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [122/225], Training Accuracy: 31.3781%, Training Loss: 0.6845%\n",
      "Epoch [91/100], Step [123/225], Training Accuracy: 31.3897%, Training Loss: 0.6845%\n",
      "Epoch [91/100], Step [124/225], Training Accuracy: 31.3886%, Training Loss: 0.6846%\n",
      "Epoch [91/100], Step [125/225], Training Accuracy: 31.3500%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [126/225], Training Accuracy: 31.3120%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [127/225], Training Accuracy: 31.2623%, Training Loss: 0.6847%\n",
      "Epoch [91/100], Step [128/225], Training Accuracy: 31.2622%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [129/225], Training Accuracy: 31.3106%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [130/225], Training Accuracy: 31.2380%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [131/225], Training Accuracy: 31.1904%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [132/225], Training Accuracy: 31.1553%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [133/225], Training Accuracy: 31.1795%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [134/225], Training Accuracy: 31.2383%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [135/225], Training Accuracy: 31.2153%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [136/225], Training Accuracy: 31.2385%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [137/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [138/225], Training Accuracy: 31.2613%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [139/225], Training Accuracy: 31.2275%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [140/225], Training Accuracy: 31.2165%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [141/225], Training Accuracy: 31.1392%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [142/225], Training Accuracy: 31.1730%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [143/225], Training Accuracy: 31.1517%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [144/225], Training Accuracy: 31.1740%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [145/225], Training Accuracy: 31.2392%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [146/225], Training Accuracy: 31.2607%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [147/225], Training Accuracy: 31.2606%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [148/225], Training Accuracy: 31.2289%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [149/225], Training Accuracy: 31.2395%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [150/225], Training Accuracy: 31.2812%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [151/225], Training Accuracy: 31.3017%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [152/225], Training Accuracy: 31.2911%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [153/225], Training Accuracy: 31.2296%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [154/225], Training Accuracy: 31.2601%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [155/225], Training Accuracy: 31.2298%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [156/225], Training Accuracy: 31.2700%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [157/225], Training Accuracy: 31.2102%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [158/225], Training Accuracy: 31.2104%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [159/225], Training Accuracy: 31.2402%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [160/225], Training Accuracy: 31.2207%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [161/225], Training Accuracy: 31.2791%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [162/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [163/225], Training Accuracy: 31.3075%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [164/225], Training Accuracy: 31.3072%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [165/225], Training Accuracy: 31.2595%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [166/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [167/225], Training Accuracy: 31.2968%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [168/225], Training Accuracy: 31.2221%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [169/225], Training Accuracy: 31.1575%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [170/225], Training Accuracy: 31.1121%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [171/225], Training Accuracy: 31.1221%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [172/225], Training Accuracy: 31.1319%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [173/225], Training Accuracy: 31.1507%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [174/225], Training Accuracy: 31.1692%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [175/225], Training Accuracy: 31.1875%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [176/225], Training Accuracy: 31.2056%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [177/225], Training Accuracy: 31.2059%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [178/225], Training Accuracy: 31.1973%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [179/225], Training Accuracy: 31.1714%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [180/225], Training Accuracy: 31.2153%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [181/225], Training Accuracy: 31.1723%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [182/225], Training Accuracy: 31.1641%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [183/225], Training Accuracy: 31.1732%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [184/225], Training Accuracy: 31.1566%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [185/225], Training Accuracy: 31.1233%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [186/225], Training Accuracy: 31.1576%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [187/225], Training Accuracy: 31.1497%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [188/225], Training Accuracy: 31.1420%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [189/225], Training Accuracy: 31.1673%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [190/225], Training Accuracy: 31.1266%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [191/225], Training Accuracy: 31.1027%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [192/225], Training Accuracy: 31.0303%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [193/225], Training Accuracy: 31.0314%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [194/225], Training Accuracy: 31.0486%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [195/225], Training Accuracy: 31.0417%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [196/225], Training Accuracy: 31.0268%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [197/225], Training Accuracy: 31.0755%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [198/225], Training Accuracy: 31.0843%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [199/225], Training Accuracy: 31.0694%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [200/225], Training Accuracy: 31.0469%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [201/225], Training Accuracy: 31.0634%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [202/225], Training Accuracy: 31.0489%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [203/225], Training Accuracy: 31.0268%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [204/225], Training Accuracy: 31.0815%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [205/225], Training Accuracy: 31.0823%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [206/225], Training Accuracy: 31.0680%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [207/225], Training Accuracy: 31.0386%, Training Loss: 0.6850%\n",
      "Epoch [91/100], Step [208/225], Training Accuracy: 31.0772%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [209/225], Training Accuracy: 31.1154%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [210/225], Training Accuracy: 31.1458%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [211/225], Training Accuracy: 31.1241%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [212/225], Training Accuracy: 31.1763%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [213/225], Training Accuracy: 31.1546%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [214/225], Training Accuracy: 31.1405%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [215/225], Training Accuracy: 31.1047%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [216/225], Training Accuracy: 31.0475%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [217/225], Training Accuracy: 31.0340%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [218/225], Training Accuracy: 31.0135%, Training Loss: 0.6849%\n",
      "Epoch [91/100], Step [219/225], Training Accuracy: 31.0788%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [220/225], Training Accuracy: 31.1009%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [221/225], Training Accuracy: 31.0945%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [222/225], Training Accuracy: 31.1022%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [223/225], Training Accuracy: 31.1379%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100], Step [224/225], Training Accuracy: 31.1384%, Training Loss: 0.6848%\n",
      "Epoch [91/100], Step [225/225], Training Accuracy: 31.1215%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6917%\n",
      "Epoch [92/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6826%\n",
      "Epoch [92/100], Step [3/225], Training Accuracy: 32.2917%, Training Loss: 0.6864%\n",
      "Epoch [92/100], Step [4/225], Training Accuracy: 32.4219%, Training Loss: 0.6827%\n",
      "Epoch [92/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6836%\n",
      "Epoch [92/100], Step [6/225], Training Accuracy: 32.2917%, Training Loss: 0.6841%\n",
      "Epoch [92/100], Step [7/225], Training Accuracy: 32.3661%, Training Loss: 0.6832%\n",
      "Epoch [92/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6831%\n",
      "Epoch [92/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [10/225], Training Accuracy: 30.9375%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [11/225], Training Accuracy: 30.8239%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [13/225], Training Accuracy: 30.5288%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6858%\n",
      "Epoch [92/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6857%\n",
      "Epoch [92/100], Step [16/225], Training Accuracy: 31.0547%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6859%\n",
      "Epoch [92/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6860%\n",
      "Epoch [92/100], Step [19/225], Training Accuracy: 31.0033%, Training Loss: 0.6861%\n",
      "Epoch [92/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6862%\n",
      "Epoch [92/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6859%\n",
      "Epoch [92/100], Step [22/225], Training Accuracy: 31.4631%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [24/225], Training Accuracy: 31.3802%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [25/225], Training Accuracy: 31.5625%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [26/225], Training Accuracy: 31.7909%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [27/225], Training Accuracy: 31.5394%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [28/225], Training Accuracy: 31.3616%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [29/225], Training Accuracy: 31.8427%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [30/225], Training Accuracy: 31.9792%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [31/225], Training Accuracy: 31.9052%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [32/225], Training Accuracy: 32.2266%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [34/225], Training Accuracy: 32.1232%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [35/225], Training Accuracy: 32.0536%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [36/225], Training Accuracy: 31.9878%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [37/225], Training Accuracy: 31.8834%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [38/225], Training Accuracy: 31.6612%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [39/225], Training Accuracy: 31.3702%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [40/225], Training Accuracy: 31.4453%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [41/225], Training Accuracy: 31.4787%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [42/225], Training Accuracy: 31.3244%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [43/225], Training Accuracy: 31.5044%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [44/225], Training Accuracy: 31.3920%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [45/225], Training Accuracy: 31.3542%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [46/225], Training Accuracy: 31.3179%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [47/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [48/225], Training Accuracy: 31.3802%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [49/225], Training Accuracy: 31.3776%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [50/225], Training Accuracy: 31.4062%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [51/225], Training Accuracy: 31.4951%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [52/225], Training Accuracy: 31.4904%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [53/225], Training Accuracy: 31.3974%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [54/225], Training Accuracy: 31.3079%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [55/225], Training Accuracy: 31.3636%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [56/225], Training Accuracy: 31.3895%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [57/225], Training Accuracy: 31.3596%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [58/225], Training Accuracy: 31.2769%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [59/225], Training Accuracy: 31.6208%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [60/225], Training Accuracy: 31.6927%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [61/225], Training Accuracy: 31.6342%, Training Loss: 0.6843%\n",
      "Epoch [92/100], Step [62/225], Training Accuracy: 31.7036%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [63/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [64/225], Training Accuracy: 31.7871%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [65/225], Training Accuracy: 31.7308%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [66/225], Training Accuracy: 31.7945%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [67/225], Training Accuracy: 31.7864%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [68/225], Training Accuracy: 31.8474%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [69/225], Training Accuracy: 31.8388%, Training Loss: 0.6844%\n",
      "Epoch [92/100], Step [70/225], Training Accuracy: 31.7857%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [71/225], Training Accuracy: 31.8662%, Training Loss: 0.6845%\n",
      "Epoch [92/100], Step [72/225], Training Accuracy: 31.6406%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [73/225], Training Accuracy: 31.6139%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [74/225], Training Accuracy: 31.6934%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [75/225], Training Accuracy: 31.6250%, Training Loss: 0.6846%\n",
      "Epoch [92/100], Step [76/225], Training Accuracy: 31.5995%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [77/225], Training Accuracy: 31.4935%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [78/225], Training Accuracy: 31.5304%, Training Loss: 0.6847%\n",
      "Epoch [92/100], Step [79/225], Training Accuracy: 31.4873%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [80/225], Training Accuracy: 31.4453%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [81/225], Training Accuracy: 31.3657%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [82/225], Training Accuracy: 31.3834%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [83/225], Training Accuracy: 31.3441%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [84/225], Training Accuracy: 31.4174%, Training Loss: 0.6848%\n",
      "Epoch [92/100], Step [85/225], Training Accuracy: 31.3971%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [86/225], Training Accuracy: 31.4680%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [87/225], Training Accuracy: 31.5014%, Training Loss: 0.6849%\n",
      "Epoch [92/100], Step [88/225], Training Accuracy: 31.4986%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [89/225], Training Accuracy: 31.4080%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [90/225], Training Accuracy: 31.3194%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [91/225], Training Accuracy: 31.4045%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [92/225], Training Accuracy: 31.3859%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [93/225], Training Accuracy: 31.3508%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [94/225], Training Accuracy: 31.4495%, Training Loss: 0.6850%\n",
      "Epoch [92/100], Step [95/225], Training Accuracy: 31.3322%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [96/225], Training Accuracy: 31.4290%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [97/225], Training Accuracy: 31.4433%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [98/225], Training Accuracy: 31.4732%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [99/225], Training Accuracy: 31.5499%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [100/225], Training Accuracy: 31.5312%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [101/225], Training Accuracy: 31.6677%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [102/225], Training Accuracy: 31.5257%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [103/225], Training Accuracy: 31.5686%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [104/225], Training Accuracy: 31.5355%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [105/225], Training Accuracy: 31.4732%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [106/225], Training Accuracy: 31.4858%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [107/225], Training Accuracy: 31.3668%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [108/225], Training Accuracy: 31.4525%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [109/225], Training Accuracy: 31.3073%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [110/225], Training Accuracy: 31.3210%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [111/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [112/225], Training Accuracy: 31.2779%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [113/225], Training Accuracy: 31.2777%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [114/225], Training Accuracy: 31.3322%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [115/225], Training Accuracy: 31.2908%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [116/225], Training Accuracy: 31.2769%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [117/225], Training Accuracy: 31.2099%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [118/225], Training Accuracy: 31.1970%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [119/225], Training Accuracy: 31.1450%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [120/225], Training Accuracy: 31.1849%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [121/225], Training Accuracy: 31.1338%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [122/225], Training Accuracy: 31.1091%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [123/225], Training Accuracy: 31.1230%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [124/225], Training Accuracy: 31.1240%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [125/225], Training Accuracy: 31.1125%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [126/225], Training Accuracy: 31.0516%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [127/225], Training Accuracy: 31.0285%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [128/225], Training Accuracy: 31.0303%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [129/225], Training Accuracy: 31.0804%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [130/225], Training Accuracy: 31.0337%, Training Loss: 0.6857%\n",
      "Epoch [92/100], Step [131/225], Training Accuracy: 30.9876%, Training Loss: 0.6857%\n",
      "Epoch [92/100], Step [132/225], Training Accuracy: 30.9541%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [133/225], Training Accuracy: 30.9798%, Training Loss: 0.6857%\n",
      "Epoch [92/100], Step [134/225], Training Accuracy: 31.0518%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [135/225], Training Accuracy: 31.0648%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [136/225], Training Accuracy: 31.0662%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [137/225], Training Accuracy: 31.0789%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [138/225], Training Accuracy: 31.1141%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [139/225], Training Accuracy: 31.0926%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [140/225], Training Accuracy: 31.0379%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [141/225], Training Accuracy: 30.9840%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [142/225], Training Accuracy: 31.0299%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [143/225], Training Accuracy: 31.0096%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [144/225], Training Accuracy: 31.0330%, Training Loss: 0.6856%\n",
      "Epoch [92/100], Step [145/225], Training Accuracy: 31.0991%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [146/225], Training Accuracy: 31.1216%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [147/225], Training Accuracy: 31.1224%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [148/225], Training Accuracy: 31.0916%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [149/225], Training Accuracy: 31.1242%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [150/225], Training Accuracy: 31.1562%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [151/225], Training Accuracy: 31.1983%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [152/225], Training Accuracy: 31.1986%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [153/225], Training Accuracy: 31.1785%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [154/225], Training Accuracy: 31.1891%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [155/225], Training Accuracy: 31.1593%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [156/225], Training Accuracy: 31.1999%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [157/225], Training Accuracy: 31.1206%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [158/225], Training Accuracy: 31.0918%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [159/225], Training Accuracy: 31.1419%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [160/225], Training Accuracy: 31.1230%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [161/225], Training Accuracy: 31.1627%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [162/225], Training Accuracy: 31.1246%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [163/225], Training Accuracy: 31.2021%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [164/225], Training Accuracy: 31.1738%, Training Loss: 0.6855%\n",
      "Epoch [92/100], Step [165/225], Training Accuracy: 31.0985%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [166/225], Training Accuracy: 31.0806%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [167/225], Training Accuracy: 31.1284%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [168/225], Training Accuracy: 31.0640%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [169/225], Training Accuracy: 30.9911%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [170/225], Training Accuracy: 30.9559%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [171/225], Training Accuracy: 30.9667%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [172/225], Training Accuracy: 30.9956%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [173/225], Training Accuracy: 30.9971%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [174/225], Training Accuracy: 31.0165%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [175/225], Training Accuracy: 31.0536%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [176/225], Training Accuracy: 31.0724%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [177/225], Training Accuracy: 31.0823%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [178/225], Training Accuracy: 31.0657%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [179/225], Training Accuracy: 31.0143%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [180/225], Training Accuracy: 31.0503%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [181/225], Training Accuracy: 31.0169%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [182/225], Training Accuracy: 31.0182%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [183/225], Training Accuracy: 31.0280%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [184/225], Training Accuracy: 31.0037%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [185/225], Training Accuracy: 30.9797%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [186/225], Training Accuracy: 31.0232%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [187/225], Training Accuracy: 31.0244%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [188/225], Training Accuracy: 31.0256%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [189/225], Training Accuracy: 31.0433%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [190/225], Training Accuracy: 31.0115%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [191/225], Training Accuracy: 30.9800%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [192/225], Training Accuracy: 30.9245%, Training Loss: 0.6854%\n",
      "Epoch [92/100], Step [193/225], Training Accuracy: 30.9262%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [194/225], Training Accuracy: 30.9439%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [195/225], Training Accuracy: 30.9215%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [196/225], Training Accuracy: 30.8992%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [197/225], Training Accuracy: 30.9486%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [198/225], Training Accuracy: 30.9659%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [199/225], Training Accuracy: 30.9202%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [200/225], Training Accuracy: 30.8984%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [201/225], Training Accuracy: 30.9313%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [202/225], Training Accuracy: 30.9329%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [203/225], Training Accuracy: 30.9267%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [204/225], Training Accuracy: 30.9666%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [205/225], Training Accuracy: 30.9680%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [206/225], Training Accuracy: 30.9466%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [207/225], Training Accuracy: 30.9254%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [208/225], Training Accuracy: 30.9721%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [209/225], Training Accuracy: 31.0257%, Training Loss: 0.6853%\n",
      "Epoch [92/100], Step [210/225], Training Accuracy: 31.0565%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [211/225], Training Accuracy: 31.0278%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [212/225], Training Accuracy: 31.0657%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [213/225], Training Accuracy: 31.0446%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [214/225], Training Accuracy: 31.0602%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [215/225], Training Accuracy: 31.0320%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [216/225], Training Accuracy: 30.9679%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [217/225], Training Accuracy: 30.9692%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [218/225], Training Accuracy: 30.9346%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [219/225], Training Accuracy: 31.0003%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [220/225], Training Accuracy: 31.0298%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [221/225], Training Accuracy: 31.0167%, Training Loss: 0.6852%\n",
      "Epoch [92/100], Step [222/225], Training Accuracy: 31.0318%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [223/225], Training Accuracy: 31.0748%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [224/225], Training Accuracy: 31.0686%, Training Loss: 0.6851%\n",
      "Epoch [92/100], Step [225/225], Training Accuracy: 31.0520%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6916%\n",
      "Epoch [93/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6892%\n",
      "Epoch [93/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6916%\n",
      "Epoch [93/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6867%\n",
      "Epoch [93/100], Step [5/225], Training Accuracy: 32.5000%, Training Loss: 0.6874%\n",
      "Epoch [93/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6872%\n",
      "Epoch [93/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6863%\n",
      "Epoch [93/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6861%\n",
      "Epoch [93/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6858%\n",
      "Epoch [93/100], Step [11/225], Training Accuracy: 30.5398%, Training Loss: 0.6863%\n",
      "Epoch [93/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6859%\n",
      "Epoch [93/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6864%\n",
      "Epoch [93/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6869%\n",
      "Epoch [93/100], Step [15/225], Training Accuracy: 30.7292%, Training Loss: 0.6864%\n",
      "Epoch [93/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6861%\n",
      "Epoch [93/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6862%\n",
      "Epoch [93/100], Step [18/225], Training Accuracy: 30.5556%, Training Loss: 0.6863%\n",
      "Epoch [93/100], Step [19/225], Training Accuracy: 31.0855%, Training Loss: 0.6864%\n",
      "Epoch [93/100], Step [20/225], Training Accuracy: 31.4844%, Training Loss: 0.6864%\n",
      "Epoch [93/100], Step [21/225], Training Accuracy: 31.4732%, Training Loss: 0.6860%\n",
      "Epoch [93/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6861%\n",
      "Epoch [93/100], Step [23/225], Training Accuracy: 31.3179%, Training Loss: 0.6861%\n",
      "Epoch [93/100], Step [24/225], Training Accuracy: 31.5755%, Training Loss: 0.6862%\n",
      "Epoch [93/100], Step [25/225], Training Accuracy: 31.7500%, Training Loss: 0.6861%\n",
      "Epoch [93/100], Step [26/225], Training Accuracy: 32.0312%, Training Loss: 0.6862%\n",
      "Epoch [93/100], Step [27/225], Training Accuracy: 31.7708%, Training Loss: 0.6860%\n",
      "Epoch [93/100], Step [28/225], Training Accuracy: 31.4174%, Training Loss: 0.6861%\n",
      "Epoch [93/100], Step [29/225], Training Accuracy: 31.7349%, Training Loss: 0.6858%\n",
      "Epoch [93/100], Step [30/225], Training Accuracy: 31.6667%, Training Loss: 0.6856%\n",
      "Epoch [93/100], Step [31/225], Training Accuracy: 31.5524%, Training Loss: 0.6855%\n",
      "Epoch [93/100], Step [32/225], Training Accuracy: 31.7871%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [33/225], Training Accuracy: 31.8182%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [34/225], Training Accuracy: 31.6636%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [35/225], Training Accuracy: 31.5625%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [36/225], Training Accuracy: 31.4670%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [37/225], Training Accuracy: 31.4611%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [38/225], Training Accuracy: 31.2911%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [39/225], Training Accuracy: 31.0096%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [40/225], Training Accuracy: 31.0938%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [41/225], Training Accuracy: 31.1738%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [42/225], Training Accuracy: 31.0268%, Training Loss: 0.6856%\n",
      "Epoch [93/100], Step [43/225], Training Accuracy: 31.2137%, Training Loss: 0.6856%\n",
      "Epoch [93/100], Step [44/225], Training Accuracy: 31.1790%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [45/225], Training Accuracy: 31.2153%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [46/225], Training Accuracy: 31.1821%, Training Loss: 0.6854%\n",
      "Epoch [93/100], Step [47/225], Training Accuracy: 31.1503%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [48/225], Training Accuracy: 31.2826%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [49/225], Training Accuracy: 31.3457%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [51/225], Training Accuracy: 31.5257%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [52/225], Training Accuracy: 31.5204%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [53/225], Training Accuracy: 31.4564%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [55/225], Training Accuracy: 31.4489%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [56/225], Training Accuracy: 31.5011%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [57/225], Training Accuracy: 31.4967%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [58/225], Training Accuracy: 31.3847%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [60/225], Training Accuracy: 31.8750%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [62/225], Training Accuracy: 31.8296%, Training Loss: 0.6844%\n",
      "Epoch [93/100], Step [63/225], Training Accuracy: 31.9196%, Training Loss: 0.6844%\n",
      "Epoch [93/100], Step [64/225], Training Accuracy: 31.9336%, Training Loss: 0.6844%\n",
      "Epoch [93/100], Step [65/225], Training Accuracy: 31.8029%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [66/225], Training Accuracy: 31.8892%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [68/225], Training Accuracy: 31.9393%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [69/225], Training Accuracy: 31.9293%, Training Loss: 0.6843%\n",
      "Epoch [93/100], Step [70/225], Training Accuracy: 31.8973%, Training Loss: 0.6843%\n",
      "Epoch [93/100], Step [71/225], Training Accuracy: 31.8882%, Training Loss: 0.6843%\n",
      "Epoch [93/100], Step [72/225], Training Accuracy: 31.6623%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [73/225], Training Accuracy: 31.6139%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [74/225], Training Accuracy: 31.7356%, Training Loss: 0.6844%\n",
      "Epoch [93/100], Step [75/225], Training Accuracy: 31.6667%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [76/225], Training Accuracy: 31.5995%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [77/225], Training Accuracy: 31.5341%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [78/225], Training Accuracy: 31.5505%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [79/225], Training Accuracy: 31.5071%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [80/225], Training Accuracy: 31.4453%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [81/225], Training Accuracy: 31.4043%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [82/225], Training Accuracy: 31.4024%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [83/225], Training Accuracy: 31.3441%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [84/225], Training Accuracy: 31.3430%, Training Loss: 0.6845%\n",
      "Epoch [93/100], Step [85/225], Training Accuracy: 31.3235%, Training Loss: 0.6846%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100], Step [86/225], Training Accuracy: 31.3045%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [87/225], Training Accuracy: 31.3757%, Training Loss: 0.6846%\n",
      "Epoch [93/100], Step [88/225], Training Accuracy: 31.3388%, Training Loss: 0.6847%\n",
      "Epoch [93/100], Step [89/225], Training Accuracy: 31.2851%, Training Loss: 0.6847%\n",
      "Epoch [93/100], Step [90/225], Training Accuracy: 31.1632%, Training Loss: 0.6847%\n",
      "Epoch [93/100], Step [91/225], Training Accuracy: 31.1985%, Training Loss: 0.6847%\n",
      "Epoch [93/100], Step [92/225], Training Accuracy: 31.1990%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [93/225], Training Accuracy: 31.1996%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [94/225], Training Accuracy: 31.2666%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [95/225], Training Accuracy: 31.1349%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [96/225], Training Accuracy: 31.2174%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [97/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [98/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [99/225], Training Accuracy: 31.4078%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [100/225], Training Accuracy: 31.4062%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [101/225], Training Accuracy: 31.5285%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [102/225], Training Accuracy: 31.4032%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [103/225], Training Accuracy: 31.4472%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [104/225], Training Accuracy: 31.4453%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [105/225], Training Accuracy: 31.3988%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [106/225], Training Accuracy: 31.4121%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [107/225], Training Accuracy: 31.3230%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [108/225], Training Accuracy: 31.3802%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [109/225], Training Accuracy: 31.2643%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [110/225], Training Accuracy: 31.2784%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [111/225], Training Accuracy: 31.1655%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [112/225], Training Accuracy: 31.2360%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [113/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [114/225], Training Accuracy: 31.3048%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [115/225], Training Accuracy: 31.2772%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [116/225], Training Accuracy: 31.2635%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [117/225], Training Accuracy: 31.2099%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [118/225], Training Accuracy: 31.1573%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [119/225], Training Accuracy: 31.1187%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [120/225], Training Accuracy: 31.1719%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [121/225], Training Accuracy: 31.1338%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [122/225], Training Accuracy: 31.1219%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [123/225], Training Accuracy: 31.1865%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [124/225], Training Accuracy: 31.1996%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [125/225], Training Accuracy: 31.1875%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [126/225], Training Accuracy: 31.1260%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [127/225], Training Accuracy: 31.1147%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [128/225], Training Accuracy: 31.1279%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [129/225], Training Accuracy: 31.1894%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [130/225], Training Accuracy: 31.1178%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [131/225], Training Accuracy: 31.0711%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [132/225], Training Accuracy: 31.0369%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [133/225], Training Accuracy: 31.0620%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [134/225], Training Accuracy: 31.1334%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [135/225], Training Accuracy: 31.1343%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [136/225], Training Accuracy: 31.1581%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [137/225], Training Accuracy: 31.2044%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [138/225], Training Accuracy: 31.2160%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [139/225], Training Accuracy: 31.1601%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [140/225], Training Accuracy: 31.1496%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [141/225], Training Accuracy: 31.0838%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [142/225], Training Accuracy: 31.1510%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [143/225], Training Accuracy: 31.1517%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [144/225], Training Accuracy: 31.1849%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [145/225], Training Accuracy: 31.2177%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [146/225], Training Accuracy: 31.2714%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [147/225], Training Accuracy: 31.2819%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [148/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [149/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [150/225], Training Accuracy: 31.2917%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [151/225], Training Accuracy: 31.3535%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [152/225], Training Accuracy: 31.3425%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [153/225], Training Accuracy: 31.2806%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [154/225], Training Accuracy: 31.3109%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [155/225], Training Accuracy: 31.2903%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [156/225], Training Accuracy: 31.3201%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [157/225], Training Accuracy: 31.2600%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [158/225], Training Accuracy: 31.2599%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [159/225], Training Accuracy: 31.2893%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [160/225], Training Accuracy: 31.2402%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [161/225], Training Accuracy: 31.3082%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [162/225], Training Accuracy: 31.2789%, Training Loss: 0.6853%\n",
      "Epoch [93/100], Step [163/225], Training Accuracy: 31.3459%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [164/225], Training Accuracy: 31.3357%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [165/225], Training Accuracy: 31.2784%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [166/225], Training Accuracy: 31.2312%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [167/225], Training Accuracy: 31.2781%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [168/225], Training Accuracy: 31.2221%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [169/225], Training Accuracy: 31.1668%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [170/225], Training Accuracy: 31.1305%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [171/225], Training Accuracy: 31.1404%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [172/225], Training Accuracy: 31.1682%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [173/225], Training Accuracy: 31.1777%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [174/225], Training Accuracy: 31.2051%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [175/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [176/225], Training Accuracy: 31.2855%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [177/225], Training Accuracy: 31.2765%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [178/225], Training Accuracy: 31.2851%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [179/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [180/225], Training Accuracy: 31.2847%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [181/225], Training Accuracy: 31.2414%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [182/225], Training Accuracy: 31.2157%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [183/225], Training Accuracy: 31.2244%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [184/225], Training Accuracy: 31.1906%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [185/225], Training Accuracy: 31.1571%, Training Loss: 0.6852%\n",
      "Epoch [93/100], Step [186/225], Training Accuracy: 31.1996%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [187/225], Training Accuracy: 31.1999%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [188/225], Training Accuracy: 31.1918%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [189/225], Training Accuracy: 31.1921%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [190/225], Training Accuracy: 31.1431%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [191/225], Training Accuracy: 31.1191%, Training Loss: 0.6851%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100], Step [192/225], Training Accuracy: 31.0547%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [193/225], Training Accuracy: 31.0557%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [194/225], Training Accuracy: 31.0648%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [195/225], Training Accuracy: 31.0497%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [196/225], Training Accuracy: 31.0348%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [197/225], Training Accuracy: 31.0834%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [198/225], Training Accuracy: 31.1001%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [199/225], Training Accuracy: 31.0851%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [200/225], Training Accuracy: 31.0781%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [201/225], Training Accuracy: 31.0945%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [202/225], Training Accuracy: 31.0876%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [203/225], Training Accuracy: 31.0807%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [204/225], Training Accuracy: 31.1351%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [205/225], Training Accuracy: 31.1280%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [206/225], Training Accuracy: 31.1059%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [207/225], Training Accuracy: 31.0839%, Training Loss: 0.6851%\n",
      "Epoch [93/100], Step [208/225], Training Accuracy: 31.0998%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [209/225], Training Accuracy: 31.1453%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [210/225], Training Accuracy: 31.1830%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [211/225], Training Accuracy: 31.1685%, Training Loss: 0.6850%\n",
      "Epoch [93/100], Step [212/225], Training Accuracy: 31.2205%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [213/225], Training Accuracy: 31.1913%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [214/225], Training Accuracy: 31.2135%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [215/225], Training Accuracy: 31.1773%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [216/225], Training Accuracy: 31.1126%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [217/225], Training Accuracy: 31.1060%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [218/225], Training Accuracy: 31.0780%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [219/225], Training Accuracy: 31.1501%, Training Loss: 0.6849%\n",
      "Epoch [93/100], Step [220/225], Training Accuracy: 31.1648%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [221/225], Training Accuracy: 31.1652%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [222/225], Training Accuracy: 31.1726%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [223/225], Training Accuracy: 31.2150%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [224/225], Training Accuracy: 31.2151%, Training Loss: 0.6848%\n",
      "Epoch [93/100], Step [225/225], Training Accuracy: 31.2048%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [1/225], Training Accuracy: 35.9375%, Training Loss: 0.6866%\n",
      "Epoch [94/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6833%\n",
      "Epoch [94/100], Step [3/225], Training Accuracy: 32.8125%, Training Loss: 0.6871%\n",
      "Epoch [94/100], Step [4/225], Training Accuracy: 32.8125%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6849%\n",
      "Epoch [94/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6849%\n",
      "Epoch [94/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [8/225], Training Accuracy: 32.2266%, Training Loss: 0.6831%\n",
      "Epoch [94/100], Step [9/225], Training Accuracy: 31.9444%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [10/225], Training Accuracy: 31.2500%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [12/225], Training Accuracy: 30.4688%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [13/225], Training Accuracy: 30.1683%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [14/225], Training Accuracy: 30.4688%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [15/225], Training Accuracy: 31.0417%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [17/225], Training Accuracy: 30.6066%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6850%\n",
      "Epoch [94/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [21/225], Training Accuracy: 31.3988%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [22/225], Training Accuracy: 31.5341%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [24/225], Training Accuracy: 31.4453%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [25/225], Training Accuracy: 31.6875%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [26/225], Training Accuracy: 31.9111%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [27/225], Training Accuracy: 31.6551%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [28/225], Training Accuracy: 31.5290%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [29/225], Training Accuracy: 31.9504%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [30/225], Training Accuracy: 31.9271%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [31/225], Training Accuracy: 31.8044%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [32/225], Training Accuracy: 32.1289%, Training Loss: 0.6834%\n",
      "Epoch [94/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6833%\n",
      "Epoch [94/100], Step [34/225], Training Accuracy: 32.2151%, Training Loss: 0.6834%\n",
      "Epoch [94/100], Step [35/225], Training Accuracy: 32.0982%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [36/225], Training Accuracy: 31.9010%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [37/225], Training Accuracy: 31.8834%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [38/225], Training Accuracy: 31.7434%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [39/225], Training Accuracy: 31.4503%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [40/225], Training Accuracy: 31.5625%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [41/225], Training Accuracy: 31.7073%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [42/225], Training Accuracy: 31.5476%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [43/225], Training Accuracy: 31.7224%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [44/225], Training Accuracy: 31.7116%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [45/225], Training Accuracy: 31.7708%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [46/225], Training Accuracy: 31.5897%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [47/225], Training Accuracy: 31.5160%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [48/225], Training Accuracy: 31.6081%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [49/225], Training Accuracy: 31.6964%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [50/225], Training Accuracy: 31.7188%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [51/225], Training Accuracy: 31.8934%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [52/225], Training Accuracy: 31.9411%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [53/225], Training Accuracy: 31.8691%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [54/225], Training Accuracy: 31.7419%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [55/225], Training Accuracy: 31.8750%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [56/225], Training Accuracy: 31.9475%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [57/225], Training Accuracy: 31.9901%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [58/225], Training Accuracy: 31.8966%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [59/225], Training Accuracy: 32.3358%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [60/225], Training Accuracy: 32.4479%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [61/225], Training Accuracy: 32.4027%, Training Loss: 0.6834%\n",
      "Epoch [94/100], Step [62/225], Training Accuracy: 32.4597%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [63/225], Training Accuracy: 32.5149%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [64/225], Training Accuracy: 32.4707%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [65/225], Training Accuracy: 32.3558%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [66/225], Training Accuracy: 32.4337%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [67/225], Training Accuracy: 32.4160%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [68/225], Training Accuracy: 32.4908%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [69/225], Training Accuracy: 32.5181%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [70/225], Training Accuracy: 32.4554%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [71/225], Training Accuracy: 32.4604%, Training Loss: 0.6835%\n",
      "Epoch [94/100], Step [72/225], Training Accuracy: 32.2266%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [73/225], Training Accuracy: 32.1704%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [74/225], Training Accuracy: 32.2635%, Training Loss: 0.6837%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Step [75/225], Training Accuracy: 32.1875%, Training Loss: 0.6836%\n",
      "Epoch [94/100], Step [76/225], Training Accuracy: 32.1135%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [77/225], Training Accuracy: 32.0008%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [78/225], Training Accuracy: 32.0112%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [79/225], Training Accuracy: 31.9422%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [80/225], Training Accuracy: 31.9336%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [81/225], Training Accuracy: 31.8673%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [82/225], Training Accuracy: 31.8788%, Training Loss: 0.6837%\n",
      "Epoch [94/100], Step [83/225], Training Accuracy: 31.8148%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [84/225], Training Accuracy: 31.8452%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [85/225], Training Accuracy: 31.8199%, Training Loss: 0.6838%\n",
      "Epoch [94/100], Step [86/225], Training Accuracy: 31.8496%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [87/225], Training Accuracy: 31.8606%, Training Loss: 0.6839%\n",
      "Epoch [94/100], Step [88/225], Training Accuracy: 31.8359%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [89/225], Training Accuracy: 31.7416%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [90/225], Training Accuracy: 31.6319%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [91/225], Training Accuracy: 31.6964%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [92/225], Training Accuracy: 31.6916%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [93/225], Training Accuracy: 31.6700%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [94/225], Training Accuracy: 31.7653%, Training Loss: 0.6840%\n",
      "Epoch [94/100], Step [95/225], Training Accuracy: 31.6283%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [96/225], Training Accuracy: 31.7220%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [97/225], Training Accuracy: 31.7171%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [98/225], Training Accuracy: 31.7283%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [99/225], Training Accuracy: 31.8340%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [100/225], Training Accuracy: 31.8438%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [101/225], Training Accuracy: 31.9926%, Training Loss: 0.6841%\n",
      "Epoch [94/100], Step [102/225], Training Accuracy: 31.8934%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [103/225], Training Accuracy: 31.9630%, Training Loss: 0.6842%\n",
      "Epoch [94/100], Step [104/225], Training Accuracy: 31.9411%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [105/225], Training Accuracy: 31.8899%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [106/225], Training Accuracy: 31.8838%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [107/225], Training Accuracy: 31.7903%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [108/225], Training Accuracy: 31.8866%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [109/225], Training Accuracy: 31.7517%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [110/225], Training Accuracy: 31.7330%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [111/225], Training Accuracy: 31.6160%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [112/225], Training Accuracy: 31.7104%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [113/225], Training Accuracy: 31.7063%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [114/225], Training Accuracy: 31.7708%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [115/225], Training Accuracy: 31.7391%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [116/225], Training Accuracy: 31.7484%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [117/225], Training Accuracy: 31.6907%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [118/225], Training Accuracy: 31.6472%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [119/225], Training Accuracy: 31.5914%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [120/225], Training Accuracy: 31.6276%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [121/225], Training Accuracy: 31.6116%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [122/225], Training Accuracy: 31.6342%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [123/225], Training Accuracy: 31.6692%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [124/225], Training Accuracy: 31.6532%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [125/225], Training Accuracy: 31.6250%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [126/225], Training Accuracy: 31.5724%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [127/225], Training Accuracy: 31.5207%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [128/225], Training Accuracy: 31.5308%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [129/225], Training Accuracy: 31.5770%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [130/225], Training Accuracy: 31.4904%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [131/225], Training Accuracy: 31.4289%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [132/225], Training Accuracy: 31.4039%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [133/225], Training Accuracy: 31.4262%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [134/225], Training Accuracy: 31.4832%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [135/225], Training Accuracy: 31.4931%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [136/225], Training Accuracy: 31.4798%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [137/225], Training Accuracy: 31.5237%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [138/225], Training Accuracy: 31.5217%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [139/225], Training Accuracy: 31.4861%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [140/225], Training Accuracy: 31.4732%, Training Loss: 0.6849%\n",
      "Epoch [94/100], Step [141/225], Training Accuracy: 31.4162%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [142/225], Training Accuracy: 31.4591%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [143/225], Training Accuracy: 31.4358%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [144/225], Training Accuracy: 31.4562%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [145/225], Training Accuracy: 31.5302%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [146/225], Training Accuracy: 31.5390%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [147/225], Training Accuracy: 31.5582%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [148/225], Training Accuracy: 31.5456%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [149/225], Training Accuracy: 31.5541%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [150/225], Training Accuracy: 31.5729%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [151/225], Training Accuracy: 31.6122%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [152/225], Training Accuracy: 31.6201%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [153/225], Training Accuracy: 31.5666%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [154/225], Training Accuracy: 31.5848%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [155/225], Training Accuracy: 31.5524%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [156/225], Training Accuracy: 31.5605%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [157/225], Training Accuracy: 31.4789%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [158/225], Training Accuracy: 31.4478%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [159/225], Training Accuracy: 31.5252%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [160/225], Training Accuracy: 31.4941%, Training Loss: 0.6849%\n",
      "Epoch [94/100], Step [161/225], Training Accuracy: 31.5411%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [162/225], Training Accuracy: 31.5394%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [163/225], Training Accuracy: 31.5951%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [164/225], Training Accuracy: 31.5739%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [165/225], Training Accuracy: 31.5057%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [166/225], Training Accuracy: 31.4853%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [167/225], Training Accuracy: 31.5400%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [168/225], Training Accuracy: 31.4918%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [169/225], Training Accuracy: 31.4072%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [170/225], Training Accuracy: 31.3511%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [171/225], Training Accuracy: 31.3688%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [172/225], Training Accuracy: 31.3772%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [173/225], Training Accuracy: 31.3584%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [174/225], Training Accuracy: 31.3667%, Training Loss: 0.6848%\n",
      "Epoch [94/100], Step [175/225], Training Accuracy: 31.4018%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [176/225], Training Accuracy: 31.4364%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [177/225], Training Accuracy: 31.4354%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [178/225], Training Accuracy: 31.4256%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [179/225], Training Accuracy: 31.3809%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [180/225], Training Accuracy: 31.3976%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [181/225], Training Accuracy: 31.3536%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [182/225], Training Accuracy: 31.3530%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [183/225], Training Accuracy: 31.3525%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Step [184/225], Training Accuracy: 31.3264%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [185/225], Training Accuracy: 31.2922%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [186/225], Training Accuracy: 31.3088%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [187/225], Training Accuracy: 31.3168%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [188/225], Training Accuracy: 31.3082%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [189/225], Training Accuracy: 31.3327%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [190/225], Training Accuracy: 31.2911%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [191/225], Training Accuracy: 31.2827%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [192/225], Training Accuracy: 31.2093%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [193/225], Training Accuracy: 31.2176%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [194/225], Training Accuracy: 31.2339%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [195/225], Training Accuracy: 31.2099%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [196/225], Training Accuracy: 31.2022%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [197/225], Training Accuracy: 31.2421%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [198/225], Training Accuracy: 31.2658%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [199/225], Training Accuracy: 31.2579%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [200/225], Training Accuracy: 31.2422%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [201/225], Training Accuracy: 31.2500%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [202/225], Training Accuracy: 31.2500%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [203/225], Training Accuracy: 31.2423%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [204/225], Training Accuracy: 31.2960%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [205/225], Training Accuracy: 31.3034%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [206/225], Training Accuracy: 31.2955%, Training Loss: 0.6847%\n",
      "Epoch [94/100], Step [207/225], Training Accuracy: 31.2953%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [208/225], Training Accuracy: 31.3326%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [209/225], Training Accuracy: 31.3771%, Training Loss: 0.6846%\n",
      "Epoch [94/100], Step [210/225], Training Accuracy: 31.4137%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [211/225], Training Accuracy: 31.4129%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [212/225], Training Accuracy: 31.4711%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [213/225], Training Accuracy: 31.4481%, Training Loss: 0.6845%\n",
      "Epoch [94/100], Step [214/225], Training Accuracy: 31.4763%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [215/225], Training Accuracy: 31.4535%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [216/225], Training Accuracy: 31.3947%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [217/225], Training Accuracy: 31.3940%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [218/225], Training Accuracy: 31.3718%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [219/225], Training Accuracy: 31.4355%, Training Loss: 0.6844%\n",
      "Epoch [94/100], Step [220/225], Training Accuracy: 31.4489%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [221/225], Training Accuracy: 31.4268%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [222/225], Training Accuracy: 31.4400%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [223/225], Training Accuracy: 31.4812%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [224/225], Training Accuracy: 31.4802%, Training Loss: 0.6843%\n",
      "Epoch [94/100], Step [225/225], Training Accuracy: 31.4689%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [1/225], Training Accuracy: 32.8125%, Training Loss: 0.6889%\n",
      "Epoch [95/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6874%\n",
      "Epoch [95/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6883%\n",
      "Epoch [95/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [5/225], Training Accuracy: 33.1250%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [7/225], Training Accuracy: 31.6964%, Training Loss: 0.6830%\n",
      "Epoch [95/100], Step [8/225], Training Accuracy: 31.2500%, Training Loss: 0.6831%\n",
      "Epoch [95/100], Step [9/225], Training Accuracy: 31.0764%, Training Loss: 0.6839%\n",
      "Epoch [95/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [11/225], Training Accuracy: 30.5398%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [12/225], Training Accuracy: 29.9479%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [13/225], Training Accuracy: 29.9279%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [14/225], Training Accuracy: 30.3571%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [15/225], Training Accuracy: 30.8333%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [16/225], Training Accuracy: 30.8594%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [17/225], Training Accuracy: 30.5147%, Training Loss: 0.6848%\n",
      "Epoch [95/100], Step [18/225], Training Accuracy: 30.7292%, Training Loss: 0.6848%\n",
      "Epoch [95/100], Step [19/225], Training Accuracy: 31.2500%, Training Loss: 0.6846%\n",
      "Epoch [95/100], Step [20/225], Training Accuracy: 31.8750%, Training Loss: 0.6846%\n",
      "Epoch [95/100], Step [21/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [22/225], Training Accuracy: 31.9602%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [23/225], Training Accuracy: 31.7255%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [24/225], Training Accuracy: 31.9010%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [25/225], Training Accuracy: 32.0625%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [26/225], Training Accuracy: 32.4519%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [27/225], Training Accuracy: 32.1759%, Training Loss: 0.6838%\n",
      "Epoch [95/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6839%\n",
      "Epoch [95/100], Step [29/225], Training Accuracy: 32.0582%, Training Loss: 0.6838%\n",
      "Epoch [95/100], Step [30/225], Training Accuracy: 32.1875%, Training Loss: 0.6837%\n",
      "Epoch [95/100], Step [31/225], Training Accuracy: 32.0565%, Training Loss: 0.6836%\n",
      "Epoch [95/100], Step [32/225], Training Accuracy: 32.4219%, Training Loss: 0.6833%\n",
      "Epoch [95/100], Step [33/225], Training Accuracy: 32.5758%, Training Loss: 0.6833%\n",
      "Epoch [95/100], Step [34/225], Training Accuracy: 32.3989%, Training Loss: 0.6835%\n",
      "Epoch [95/100], Step [35/225], Training Accuracy: 32.3214%, Training Loss: 0.6836%\n",
      "Epoch [95/100], Step [36/225], Training Accuracy: 32.0747%, Training Loss: 0.6838%\n",
      "Epoch [95/100], Step [37/225], Training Accuracy: 32.0946%, Training Loss: 0.6839%\n",
      "Epoch [95/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [39/225], Training Accuracy: 31.6106%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [40/225], Training Accuracy: 31.7578%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [41/225], Training Accuracy: 31.8979%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [42/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [43/225], Training Accuracy: 31.9404%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [44/225], Training Accuracy: 31.8892%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [45/225], Training Accuracy: 31.8403%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [46/225], Training Accuracy: 31.7255%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [47/225], Training Accuracy: 31.6489%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [49/225], Training Accuracy: 31.7921%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [50/225], Training Accuracy: 31.8125%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [51/225], Training Accuracy: 31.9240%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [52/225], Training Accuracy: 31.9712%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [53/225], Training Accuracy: 31.8986%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [55/225], Training Accuracy: 31.7898%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [56/225], Training Accuracy: 31.8638%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [57/225], Training Accuracy: 31.8805%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [58/225], Training Accuracy: 31.8157%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [59/225], Training Accuracy: 32.1504%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [60/225], Training Accuracy: 32.2917%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [61/225], Training Accuracy: 32.2234%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [62/225], Training Accuracy: 32.2581%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [63/225], Training Accuracy: 32.3165%, Training Loss: 0.6840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Step [64/225], Training Accuracy: 32.3242%, Training Loss: 0.6839%\n",
      "Epoch [95/100], Step [65/225], Training Accuracy: 32.2356%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [66/225], Training Accuracy: 32.3390%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [67/225], Training Accuracy: 32.3228%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [68/225], Training Accuracy: 32.3759%, Training Loss: 0.6840%\n",
      "Epoch [95/100], Step [69/225], Training Accuracy: 32.3822%, Training Loss: 0.6838%\n",
      "Epoch [95/100], Step [70/225], Training Accuracy: 32.3661%, Training Loss: 0.6839%\n",
      "Epoch [95/100], Step [71/225], Training Accuracy: 32.3724%, Training Loss: 0.6839%\n",
      "Epoch [95/100], Step [72/225], Training Accuracy: 32.1615%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [73/225], Training Accuracy: 32.1062%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [74/225], Training Accuracy: 32.2213%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [75/225], Training Accuracy: 32.1667%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [76/225], Training Accuracy: 32.0929%, Training Loss: 0.6841%\n",
      "Epoch [95/100], Step [77/225], Training Accuracy: 31.9602%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [78/225], Training Accuracy: 31.9912%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [79/225], Training Accuracy: 31.9422%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [80/225], Training Accuracy: 31.8945%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [81/225], Training Accuracy: 31.8094%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [82/225], Training Accuracy: 31.8407%, Training Loss: 0.6842%\n",
      "Epoch [95/100], Step [83/225], Training Accuracy: 31.7771%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [84/225], Training Accuracy: 31.8452%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [85/225], Training Accuracy: 31.8566%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [86/225], Training Accuracy: 31.9222%, Training Loss: 0.6843%\n",
      "Epoch [95/100], Step [87/225], Training Accuracy: 31.9325%, Training Loss: 0.6844%\n",
      "Epoch [95/100], Step [88/225], Training Accuracy: 31.9247%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [89/225], Training Accuracy: 31.8645%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [90/225], Training Accuracy: 31.7708%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [91/225], Training Accuracy: 31.8166%, Training Loss: 0.6846%\n",
      "Epoch [95/100], Step [92/225], Training Accuracy: 31.7765%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [93/225], Training Accuracy: 31.7708%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [94/225], Training Accuracy: 31.8650%, Training Loss: 0.6845%\n",
      "Epoch [95/100], Step [95/225], Training Accuracy: 31.7434%, Training Loss: 0.6846%\n",
      "Epoch [95/100], Step [96/225], Training Accuracy: 31.8359%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [97/225], Training Accuracy: 31.8299%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [98/225], Training Accuracy: 31.8878%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [99/225], Training Accuracy: 32.0076%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [100/225], Training Accuracy: 31.9531%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [101/225], Training Accuracy: 32.0854%, Training Loss: 0.6846%\n",
      "Epoch [95/100], Step [102/225], Training Accuracy: 31.9700%, Training Loss: 0.6847%\n",
      "Epoch [95/100], Step [103/225], Training Accuracy: 32.0237%, Training Loss: 0.6848%\n",
      "Epoch [95/100], Step [104/225], Training Accuracy: 31.9862%, Training Loss: 0.6848%\n",
      "Epoch [95/100], Step [105/225], Training Accuracy: 31.9345%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [106/225], Training Accuracy: 31.9281%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [107/225], Training Accuracy: 31.8195%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [108/225], Training Accuracy: 31.9155%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [109/225], Training Accuracy: 31.8234%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [110/225], Training Accuracy: 31.8182%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [111/225], Training Accuracy: 31.7286%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [112/225], Training Accuracy: 31.7941%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [113/225], Training Accuracy: 31.8031%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [114/225], Training Accuracy: 31.8257%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [115/225], Training Accuracy: 31.8207%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [116/225], Training Accuracy: 31.8023%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [117/225], Training Accuracy: 31.7441%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [118/225], Training Accuracy: 31.7135%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [119/225], Training Accuracy: 31.6570%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [120/225], Training Accuracy: 31.6667%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [121/225], Training Accuracy: 31.6761%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [122/225], Training Accuracy: 31.6855%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [123/225], Training Accuracy: 31.7327%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [124/225], Training Accuracy: 31.7162%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [125/225], Training Accuracy: 31.7000%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [126/225], Training Accuracy: 31.6220%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [127/225], Training Accuracy: 31.5945%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [128/225], Training Accuracy: 31.5918%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [129/225], Training Accuracy: 31.6376%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [130/225], Training Accuracy: 31.5385%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [131/225], Training Accuracy: 31.4885%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [132/225], Training Accuracy: 31.4276%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [133/225], Training Accuracy: 31.4380%, Training Loss: 0.6855%\n",
      "Epoch [95/100], Step [134/225], Training Accuracy: 31.4715%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [135/225], Training Accuracy: 31.4583%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [136/225], Training Accuracy: 31.4683%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [137/225], Training Accuracy: 31.5009%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [138/225], Training Accuracy: 31.5217%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [139/225], Training Accuracy: 31.4861%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [140/225], Training Accuracy: 31.4509%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [141/225], Training Accuracy: 31.4162%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [142/225], Training Accuracy: 31.4591%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [143/225], Training Accuracy: 31.4576%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [144/225], Training Accuracy: 31.4887%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [145/225], Training Accuracy: 31.5625%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [146/225], Training Accuracy: 31.5925%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [147/225], Training Accuracy: 31.6114%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [148/225], Training Accuracy: 31.5667%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [149/225], Training Accuracy: 31.5856%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [150/225], Training Accuracy: 31.6250%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [151/225], Training Accuracy: 31.6743%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [152/225], Training Accuracy: 31.6715%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [153/225], Training Accuracy: 31.6279%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [154/225], Training Accuracy: 31.6457%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [155/225], Training Accuracy: 31.6331%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [156/225], Training Accuracy: 31.6607%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [157/225], Training Accuracy: 31.5983%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [158/225], Training Accuracy: 31.5665%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [159/225], Training Accuracy: 31.6038%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [160/225], Training Accuracy: 31.5820%, Training Loss: 0.6855%\n",
      "Epoch [95/100], Step [161/225], Training Accuracy: 31.6479%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [162/225], Training Accuracy: 31.6262%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [163/225], Training Accuracy: 31.6718%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [164/225], Training Accuracy: 31.6597%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [165/225], Training Accuracy: 31.5814%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [166/225], Training Accuracy: 31.5700%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [167/225], Training Accuracy: 31.5962%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [168/225], Training Accuracy: 31.5476%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [169/225], Training Accuracy: 31.4626%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [170/225], Training Accuracy: 31.4154%, Training Loss: 0.6854%\n",
      "Epoch [95/100], Step [171/225], Training Accuracy: 31.4236%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [172/225], Training Accuracy: 31.4317%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [173/225], Training Accuracy: 31.4216%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100], Step [174/225], Training Accuracy: 31.4296%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [175/225], Training Accuracy: 31.4464%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [176/225], Training Accuracy: 31.4719%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [177/225], Training Accuracy: 31.4795%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [178/225], Training Accuracy: 31.4782%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [179/225], Training Accuracy: 31.4420%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [180/225], Training Accuracy: 31.4757%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [181/225], Training Accuracy: 31.4140%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [182/225], Training Accuracy: 31.4045%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [183/225], Training Accuracy: 31.4037%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [184/225], Training Accuracy: 31.3689%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [185/225], Training Accuracy: 31.3429%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [186/225], Training Accuracy: 31.3760%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [187/225], Training Accuracy: 31.3586%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [188/225], Training Accuracy: 31.3414%, Training Loss: 0.6853%\n",
      "Epoch [95/100], Step [189/225], Training Accuracy: 31.3740%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [190/225], Training Accuracy: 31.3240%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [191/225], Training Accuracy: 31.2909%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [192/225], Training Accuracy: 31.2256%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [193/225], Training Accuracy: 31.2338%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [194/225], Training Accuracy: 31.2339%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [195/225], Training Accuracy: 31.2099%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [196/225], Training Accuracy: 31.1783%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [197/225], Training Accuracy: 31.2183%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [198/225], Training Accuracy: 31.2421%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [199/225], Training Accuracy: 31.2343%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [200/225], Training Accuracy: 31.2188%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [201/225], Training Accuracy: 31.2422%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [202/225], Training Accuracy: 31.2423%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [203/225], Training Accuracy: 31.2192%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [204/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [205/225], Training Accuracy: 31.2424%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [206/225], Training Accuracy: 31.2197%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [207/225], Training Accuracy: 31.2047%, Training Loss: 0.6852%\n",
      "Epoch [95/100], Step [208/225], Training Accuracy: 31.2350%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [209/225], Training Accuracy: 31.2724%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [210/225], Training Accuracy: 31.3095%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [211/225], Training Accuracy: 31.2796%, Training Loss: 0.6851%\n",
      "Epoch [95/100], Step [212/225], Training Accuracy: 31.3384%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [213/225], Training Accuracy: 31.3160%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [214/225], Training Accuracy: 31.3376%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [215/225], Training Accuracy: 31.2936%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [216/225], Training Accuracy: 31.2283%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [217/225], Training Accuracy: 31.2284%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [218/225], Training Accuracy: 31.1998%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [219/225], Training Accuracy: 31.2429%, Training Loss: 0.6850%\n",
      "Epoch [95/100], Step [220/225], Training Accuracy: 31.2713%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [221/225], Training Accuracy: 31.2571%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [222/225], Training Accuracy: 31.2711%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [223/225], Training Accuracy: 31.3131%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [224/225], Training Accuracy: 31.3128%, Training Loss: 0.6849%\n",
      "Epoch [95/100], Step [225/225], Training Accuracy: 31.2813%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6889%\n",
      "Epoch [96/100], Step [2/225], Training Accuracy: 34.3750%, Training Loss: 0.6832%\n",
      "Epoch [96/100], Step [3/225], Training Accuracy: 33.8542%, Training Loss: 0.6876%\n",
      "Epoch [96/100], Step [4/225], Training Accuracy: 33.2031%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [5/225], Training Accuracy: 34.0625%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [6/225], Training Accuracy: 32.8125%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [7/225], Training Accuracy: 32.5893%, Training Loss: 0.6836%\n",
      "Epoch [96/100], Step [8/225], Training Accuracy: 32.4219%, Training Loss: 0.6835%\n",
      "Epoch [96/100], Step [9/225], Training Accuracy: 32.2917%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [11/225], Training Accuracy: 31.5341%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [12/225], Training Accuracy: 30.9896%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [13/225], Training Accuracy: 30.7692%, Training Loss: 0.6847%\n",
      "Epoch [96/100], Step [14/225], Training Accuracy: 30.6920%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [15/225], Training Accuracy: 30.9375%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [16/225], Training Accuracy: 30.9570%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [17/225], Training Accuracy: 30.7904%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [18/225], Training Accuracy: 30.9028%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [19/225], Training Accuracy: 31.3322%, Training Loss: 0.6855%\n",
      "Epoch [96/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [22/225], Training Accuracy: 31.7472%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [23/225], Training Accuracy: 31.6576%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [24/225], Training Accuracy: 31.9661%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [25/225], Training Accuracy: 32.0000%, Training Loss: 0.6847%\n",
      "Epoch [96/100], Step [26/225], Training Accuracy: 32.3317%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [27/225], Training Accuracy: 32.1181%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [28/225], Training Accuracy: 31.9196%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [29/225], Training Accuracy: 32.2198%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [30/225], Training Accuracy: 32.1354%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [32/225], Training Accuracy: 32.2266%, Training Loss: 0.6839%\n",
      "Epoch [96/100], Step [33/225], Training Accuracy: 32.2917%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [34/225], Training Accuracy: 32.1691%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [35/225], Training Accuracy: 32.0982%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [36/225], Training Accuracy: 31.9444%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [37/225], Training Accuracy: 31.9679%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [38/225], Training Accuracy: 31.8257%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [39/225], Training Accuracy: 31.5304%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [40/225], Training Accuracy: 31.6797%, Training Loss: 0.6847%\n",
      "Epoch [96/100], Step [41/225], Training Accuracy: 31.7835%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [42/225], Training Accuracy: 31.6220%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [43/225], Training Accuracy: 31.7224%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [44/225], Training Accuracy: 31.7116%, Training Loss: 0.6847%\n",
      "Epoch [96/100], Step [45/225], Training Accuracy: 31.7708%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [46/225], Training Accuracy: 31.6916%, Training Loss: 0.6847%\n",
      "Epoch [96/100], Step [47/225], Training Accuracy: 31.6822%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [49/225], Training Accuracy: 31.8240%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [50/225], Training Accuracy: 31.8750%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [51/225], Training Accuracy: 32.0772%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [52/225], Training Accuracy: 32.0312%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [53/225], Training Accuracy: 31.9575%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [54/225], Training Accuracy: 31.8287%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [55/225], Training Accuracy: 31.9034%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [56/225], Training Accuracy: 31.9475%, Training Loss: 0.6844%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Step [57/225], Training Accuracy: 31.9353%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [58/225], Training Accuracy: 31.8157%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [59/225], Training Accuracy: 32.0975%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [60/225], Training Accuracy: 32.2396%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [61/225], Training Accuracy: 32.1465%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [62/225], Training Accuracy: 32.1573%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [63/225], Training Accuracy: 32.1677%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [64/225], Training Accuracy: 32.1533%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [65/225], Training Accuracy: 32.0673%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [66/225], Training Accuracy: 32.1496%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [67/225], Training Accuracy: 32.1129%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [68/225], Training Accuracy: 32.1691%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [69/225], Training Accuracy: 32.1558%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [70/225], Training Accuracy: 32.0982%, Training Loss: 0.6841%\n",
      "Epoch [96/100], Step [71/225], Training Accuracy: 32.1083%, Training Loss: 0.6840%\n",
      "Epoch [96/100], Step [72/225], Training Accuracy: 31.9010%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [73/225], Training Accuracy: 31.8279%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [74/225], Training Accuracy: 31.9046%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [75/225], Training Accuracy: 31.8333%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [76/225], Training Accuracy: 31.8051%, Training Loss: 0.6842%\n",
      "Epoch [96/100], Step [77/225], Training Accuracy: 31.7979%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [78/225], Training Accuracy: 31.8309%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [79/225], Training Accuracy: 31.7642%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [80/225], Training Accuracy: 31.7188%, Training Loss: 0.6843%\n",
      "Epoch [96/100], Step [81/225], Training Accuracy: 31.6551%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [82/225], Training Accuracy: 31.6692%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [83/225], Training Accuracy: 31.6077%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [84/225], Training Accuracy: 31.6220%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [86/225], Training Accuracy: 31.5952%, Training Loss: 0.6844%\n",
      "Epoch [96/100], Step [87/225], Training Accuracy: 31.6272%, Training Loss: 0.6845%\n",
      "Epoch [96/100], Step [88/225], Training Accuracy: 31.5696%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [89/225], Training Accuracy: 31.4782%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [90/225], Training Accuracy: 31.4062%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [91/225], Training Accuracy: 31.4560%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [92/225], Training Accuracy: 31.4368%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [93/225], Training Accuracy: 31.4516%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [94/225], Training Accuracy: 31.4993%, Training Loss: 0.6846%\n",
      "Epoch [96/100], Step [95/225], Training Accuracy: 31.3816%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [96/225], Training Accuracy: 31.4616%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [97/225], Training Accuracy: 31.4594%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [98/225], Training Accuracy: 31.4573%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [99/225], Training Accuracy: 31.5657%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [100/225], Training Accuracy: 31.5469%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [101/225], Training Accuracy: 31.6522%, Training Loss: 0.6847%\n",
      "Epoch [96/100], Step [102/225], Training Accuracy: 31.5257%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [103/225], Training Accuracy: 31.5989%, Training Loss: 0.6848%\n",
      "Epoch [96/100], Step [104/225], Training Accuracy: 31.5655%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [105/225], Training Accuracy: 31.5179%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [106/225], Training Accuracy: 31.5301%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [107/225], Training Accuracy: 31.4398%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [108/225], Training Accuracy: 31.4959%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [109/225], Training Accuracy: 31.3647%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [110/225], Training Accuracy: 31.3920%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [111/225], Training Accuracy: 31.2922%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [112/225], Training Accuracy: 31.3616%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [113/225], Training Accuracy: 31.3330%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [114/225], Training Accuracy: 31.3871%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [115/225], Training Accuracy: 31.3451%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [116/225], Training Accuracy: 31.3712%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [117/225], Training Accuracy: 31.3301%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [118/225], Training Accuracy: 31.3030%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [119/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [120/225], Training Accuracy: 31.2891%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [121/225], Training Accuracy: 31.3017%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [122/225], Training Accuracy: 31.2884%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [123/225], Training Accuracy: 31.3008%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [124/225], Training Accuracy: 31.3004%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [125/225], Training Accuracy: 31.2875%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [126/225], Training Accuracy: 31.2128%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [127/225], Training Accuracy: 31.1762%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [128/225], Training Accuracy: 31.1768%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [129/225], Training Accuracy: 31.2016%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [130/225], Training Accuracy: 31.1178%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [131/225], Training Accuracy: 31.0711%, Training Loss: 0.6855%\n",
      "Epoch [96/100], Step [132/225], Training Accuracy: 31.0369%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [133/225], Training Accuracy: 31.0620%, Training Loss: 0.6855%\n",
      "Epoch [96/100], Step [134/225], Training Accuracy: 31.0984%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [135/225], Training Accuracy: 31.0880%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [136/225], Training Accuracy: 31.0892%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [137/225], Training Accuracy: 31.1474%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [138/225], Training Accuracy: 31.1594%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [139/225], Training Accuracy: 31.1039%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [140/225], Training Accuracy: 31.0826%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [141/225], Training Accuracy: 31.0284%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [142/225], Training Accuracy: 31.0849%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [143/225], Training Accuracy: 31.0752%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [144/225], Training Accuracy: 31.0981%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [145/225], Training Accuracy: 31.1746%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [146/225], Training Accuracy: 31.2072%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [147/225], Training Accuracy: 31.2287%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [148/225], Training Accuracy: 31.1867%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [149/225], Training Accuracy: 31.2185%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [150/225], Training Accuracy: 31.2396%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [151/225], Training Accuracy: 31.2914%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [152/225], Training Accuracy: 31.3014%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [153/225], Training Accuracy: 31.2704%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [154/225], Training Accuracy: 31.2906%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [155/225], Training Accuracy: 31.2702%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [156/225], Training Accuracy: 31.2800%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [157/225], Training Accuracy: 31.2002%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [158/225], Training Accuracy: 31.1709%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [159/225], Training Accuracy: 31.2598%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [160/225], Training Accuracy: 31.2500%, Training Loss: 0.6854%\n",
      "Epoch [96/100], Step [161/225], Training Accuracy: 31.3082%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [162/225], Training Accuracy: 31.2693%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [163/225], Training Accuracy: 31.3171%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [164/225], Training Accuracy: 31.2976%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [165/225], Training Accuracy: 31.2311%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [166/225], Training Accuracy: 31.2218%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [167/225], Training Accuracy: 31.2687%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [168/225], Training Accuracy: 31.2128%, Training Loss: 0.6853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Step [169/225], Training Accuracy: 31.1298%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [170/225], Training Accuracy: 31.0938%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [171/225], Training Accuracy: 31.1129%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [172/225], Training Accuracy: 31.1410%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [173/225], Training Accuracy: 31.1055%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [174/225], Training Accuracy: 31.1063%, Training Loss: 0.6853%\n",
      "Epoch [96/100], Step [175/225], Training Accuracy: 31.1607%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [176/225], Training Accuracy: 31.1967%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [177/225], Training Accuracy: 31.1882%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [178/225], Training Accuracy: 31.1447%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [179/225], Training Accuracy: 31.1191%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [180/225], Training Accuracy: 31.1719%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [181/225], Training Accuracy: 31.1291%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [182/225], Training Accuracy: 31.1298%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [183/225], Training Accuracy: 31.1390%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [184/225], Training Accuracy: 31.1056%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [185/225], Training Accuracy: 31.0811%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [186/225], Training Accuracy: 31.1156%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [187/225], Training Accuracy: 31.1163%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [188/225], Training Accuracy: 31.0921%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [189/225], Training Accuracy: 31.1260%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [190/225], Training Accuracy: 31.0773%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [191/225], Training Accuracy: 31.0700%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [192/225], Training Accuracy: 31.0059%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [193/225], Training Accuracy: 31.0233%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [194/225], Training Accuracy: 31.0406%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [195/225], Training Accuracy: 31.0256%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [196/225], Training Accuracy: 30.9949%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [197/225], Training Accuracy: 31.0438%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [198/225], Training Accuracy: 31.0764%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [199/225], Training Accuracy: 31.0537%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [200/225], Training Accuracy: 31.0391%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [201/225], Training Accuracy: 31.0634%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [202/225], Training Accuracy: 31.0489%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [203/225], Training Accuracy: 31.0345%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [204/225], Training Accuracy: 31.0892%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [205/225], Training Accuracy: 31.0823%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [206/225], Training Accuracy: 31.0604%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [207/225], Training Accuracy: 31.0386%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [208/225], Training Accuracy: 31.0472%, Training Loss: 0.6852%\n",
      "Epoch [96/100], Step [209/225], Training Accuracy: 31.0855%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [210/225], Training Accuracy: 31.1161%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [211/225], Training Accuracy: 31.1093%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [212/225], Training Accuracy: 31.1763%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [213/225], Training Accuracy: 31.1546%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [214/225], Training Accuracy: 31.1551%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [215/225], Training Accuracy: 31.1337%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [216/225], Training Accuracy: 31.0619%, Training Loss: 0.6851%\n",
      "Epoch [96/100], Step [217/225], Training Accuracy: 31.0700%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [218/225], Training Accuracy: 31.0493%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [219/225], Training Accuracy: 31.1144%, Training Loss: 0.6850%\n",
      "Epoch [96/100], Step [220/225], Training Accuracy: 31.1364%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [221/225], Training Accuracy: 31.1298%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [222/225], Training Accuracy: 31.1444%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [223/225], Training Accuracy: 31.1939%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [224/225], Training Accuracy: 31.1872%, Training Loss: 0.6849%\n",
      "Epoch [96/100], Step [225/225], Training Accuracy: 31.1701%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6862%\n",
      "Epoch [97/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6814%\n",
      "Epoch [97/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6826%\n",
      "Epoch [97/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6843%\n",
      "Epoch [97/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6835%\n",
      "Epoch [97/100], Step [8/225], Training Accuracy: 31.4453%, Training Loss: 0.6833%\n",
      "Epoch [97/100], Step [9/225], Training Accuracy: 31.5972%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [10/225], Training Accuracy: 31.0938%, Training Loss: 0.6839%\n",
      "Epoch [97/100], Step [11/225], Training Accuracy: 30.9659%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [12/225], Training Accuracy: 30.3385%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [13/225], Training Accuracy: 30.0481%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [14/225], Training Accuracy: 30.2455%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [15/225], Training Accuracy: 30.8333%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [16/225], Training Accuracy: 30.7617%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [17/225], Training Accuracy: 30.6985%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6857%\n",
      "Epoch [97/100], Step [20/225], Training Accuracy: 31.6406%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [21/225], Training Accuracy: 31.6220%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [22/225], Training Accuracy: 31.6051%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [23/225], Training Accuracy: 31.3859%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [24/225], Training Accuracy: 31.5104%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [25/225], Training Accuracy: 31.8125%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [26/225], Training Accuracy: 32.1514%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [27/225], Training Accuracy: 31.8287%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [28/225], Training Accuracy: 31.7522%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [29/225], Training Accuracy: 32.1121%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [30/225], Training Accuracy: 32.0833%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [31/225], Training Accuracy: 31.9556%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [32/225], Training Accuracy: 32.2266%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [33/225], Training Accuracy: 32.3864%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [34/225], Training Accuracy: 32.3989%, Training Loss: 0.6840%\n",
      "Epoch [97/100], Step [35/225], Training Accuracy: 32.2768%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [36/225], Training Accuracy: 32.1615%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [37/225], Training Accuracy: 32.1791%, Training Loss: 0.6843%\n",
      "Epoch [97/100], Step [38/225], Training Accuracy: 32.0312%, Training Loss: 0.6843%\n",
      "Epoch [97/100], Step [39/225], Training Accuracy: 31.7708%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [40/225], Training Accuracy: 31.8359%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [41/225], Training Accuracy: 31.9360%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [42/225], Training Accuracy: 31.7708%, Training Loss: 0.6848%\n",
      "Epoch [97/100], Step [43/225], Training Accuracy: 31.9767%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100], Step [44/225], Training Accuracy: 31.9247%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [45/225], Training Accuracy: 31.8750%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [46/225], Training Accuracy: 31.7935%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [47/225], Training Accuracy: 31.6822%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [48/225], Training Accuracy: 31.8034%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [49/225], Training Accuracy: 31.7921%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [50/225], Training Accuracy: 31.8125%, Training Loss: 0.6847%\n",
      "Epoch [97/100], Step [51/225], Training Accuracy: 32.0159%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [52/225], Training Accuracy: 32.0012%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [53/225], Training Accuracy: 31.9281%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [54/225], Training Accuracy: 31.7998%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [55/225], Training Accuracy: 31.9034%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [56/225], Training Accuracy: 31.9196%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [57/225], Training Accuracy: 31.9353%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [58/225], Training Accuracy: 31.8696%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [59/225], Training Accuracy: 32.1504%, Training Loss: 0.6843%\n",
      "Epoch [97/100], Step [60/225], Training Accuracy: 32.2135%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [61/225], Training Accuracy: 32.1465%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [62/225], Training Accuracy: 32.2077%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [63/225], Training Accuracy: 32.2421%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [64/225], Training Accuracy: 32.2266%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [65/225], Training Accuracy: 32.1154%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [66/225], Training Accuracy: 32.2206%, Training Loss: 0.6842%\n",
      "Epoch [97/100], Step [67/225], Training Accuracy: 32.2062%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [68/225], Training Accuracy: 32.2610%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [69/225], Training Accuracy: 32.2464%, Training Loss: 0.6840%\n",
      "Epoch [97/100], Step [70/225], Training Accuracy: 32.1875%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [71/225], Training Accuracy: 32.2623%, Training Loss: 0.6841%\n",
      "Epoch [97/100], Step [72/225], Training Accuracy: 32.0095%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [73/225], Training Accuracy: 31.9349%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [74/225], Training Accuracy: 32.0312%, Training Loss: 0.6843%\n",
      "Epoch [97/100], Step [75/225], Training Accuracy: 32.0208%, Training Loss: 0.6843%\n",
      "Epoch [97/100], Step [76/225], Training Accuracy: 31.9901%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [77/225], Training Accuracy: 31.9399%, Training Loss: 0.6844%\n",
      "Epoch [97/100], Step [78/225], Training Accuracy: 31.9712%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [79/225], Training Accuracy: 31.8631%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [80/225], Training Accuracy: 31.7969%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [81/225], Training Accuracy: 31.7515%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [82/225], Training Accuracy: 31.7645%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [83/225], Training Accuracy: 31.6830%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [84/225], Training Accuracy: 31.7150%, Training Loss: 0.6845%\n",
      "Epoch [97/100], Step [85/225], Training Accuracy: 31.6912%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [86/225], Training Accuracy: 31.7769%, Training Loss: 0.6846%\n",
      "Epoch [97/100], Step [87/225], Training Accuracy: 31.8068%, Training Loss: 0.6847%\n",
      "Epoch [97/100], Step [88/225], Training Accuracy: 31.7294%, Training Loss: 0.6848%\n",
      "Epoch [97/100], Step [89/225], Training Accuracy: 31.6362%, Training Loss: 0.6848%\n",
      "Epoch [97/100], Step [90/225], Training Accuracy: 31.5451%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [91/225], Training Accuracy: 31.6106%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [92/225], Training Accuracy: 31.6067%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [93/225], Training Accuracy: 31.5860%, Training Loss: 0.6848%\n",
      "Epoch [97/100], Step [94/225], Training Accuracy: 31.6323%, Training Loss: 0.6848%\n",
      "Epoch [97/100], Step [95/225], Training Accuracy: 31.5132%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [96/225], Training Accuracy: 31.6243%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [97/225], Training Accuracy: 31.6366%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [98/225], Training Accuracy: 31.6486%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [99/225], Training Accuracy: 31.7866%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [100/225], Training Accuracy: 31.7500%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [101/225], Training Accuracy: 31.8533%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [102/225], Training Accuracy: 31.7555%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [103/225], Training Accuracy: 31.8265%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [104/225], Training Accuracy: 31.8059%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [105/225], Training Accuracy: 31.7262%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [106/225], Training Accuracy: 31.7217%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [107/225], Training Accuracy: 31.6297%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [108/225], Training Accuracy: 31.7274%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [109/225], Training Accuracy: 31.5797%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [110/225], Training Accuracy: 31.5767%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [111/225], Training Accuracy: 31.4611%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [112/225], Training Accuracy: 31.5709%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [113/225], Training Accuracy: 31.5404%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [114/225], Training Accuracy: 31.5789%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [115/225], Training Accuracy: 31.5489%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [116/225], Training Accuracy: 31.5194%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [117/225], Training Accuracy: 31.4637%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [118/225], Training Accuracy: 31.4486%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [119/225], Training Accuracy: 31.3944%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [120/225], Training Accuracy: 31.4193%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [121/225], Training Accuracy: 31.4179%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [122/225], Training Accuracy: 31.4037%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [123/225], Training Accuracy: 31.4405%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [124/225], Training Accuracy: 31.4264%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [126/225], Training Accuracy: 31.3616%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [127/225], Training Accuracy: 31.2992%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [128/225], Training Accuracy: 31.2744%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [129/225], Training Accuracy: 31.3227%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [131/225], Training Accuracy: 31.2381%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [132/225], Training Accuracy: 31.2382%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [133/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [134/225], Training Accuracy: 31.3083%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [135/225], Training Accuracy: 31.3194%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [136/225], Training Accuracy: 31.3534%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [137/225], Training Accuracy: 31.3755%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [138/225], Training Accuracy: 31.3519%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [139/225], Training Accuracy: 31.2950%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [140/225], Training Accuracy: 31.2835%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [141/225], Training Accuracy: 31.2389%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [142/225], Training Accuracy: 31.2940%, Training Loss: 0.6856%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100], Step [143/225], Training Accuracy: 31.2828%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [144/225], Training Accuracy: 31.2934%, Training Loss: 0.6856%\n",
      "Epoch [97/100], Step [145/225], Training Accuracy: 31.3685%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [146/225], Training Accuracy: 31.3998%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [147/225], Training Accuracy: 31.4094%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [148/225], Training Accuracy: 31.3661%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [149/225], Training Accuracy: 31.3863%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [150/225], Training Accuracy: 31.4167%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [151/225], Training Accuracy: 31.4570%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [152/225], Training Accuracy: 31.4556%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [153/225], Training Accuracy: 31.4338%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [154/225], Training Accuracy: 31.4326%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [155/225], Training Accuracy: 31.4214%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [156/225], Training Accuracy: 31.4403%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [157/225], Training Accuracy: 31.3396%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [158/225], Training Accuracy: 31.3192%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [159/225], Training Accuracy: 31.3876%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [160/225], Training Accuracy: 31.3574%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [161/225], Training Accuracy: 31.3859%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [162/225], Training Accuracy: 31.3465%, Training Loss: 0.6855%\n",
      "Epoch [97/100], Step [163/225], Training Accuracy: 31.4034%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [164/225], Training Accuracy: 31.3834%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [165/225], Training Accuracy: 31.3163%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [166/225], Training Accuracy: 31.3159%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [167/225], Training Accuracy: 31.3716%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [168/225], Training Accuracy: 31.3058%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [169/225], Training Accuracy: 31.2315%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [170/225], Training Accuracy: 31.1857%, Training Loss: 0.6854%\n",
      "Epoch [97/100], Step [171/225], Training Accuracy: 31.2043%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [172/225], Training Accuracy: 31.2137%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [173/225], Training Accuracy: 31.1868%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [174/225], Training Accuracy: 31.2141%, Training Loss: 0.6853%\n",
      "Epoch [97/100], Step [175/225], Training Accuracy: 31.2500%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [176/225], Training Accuracy: 31.2678%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [177/225], Training Accuracy: 31.2853%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [178/225], Training Accuracy: 31.2676%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [179/225], Training Accuracy: 31.2325%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [180/225], Training Accuracy: 31.2760%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [181/225], Training Accuracy: 31.2327%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [182/225], Training Accuracy: 31.2242%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [183/225], Training Accuracy: 31.2415%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [184/225], Training Accuracy: 31.1990%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [185/225], Training Accuracy: 31.1655%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [186/225], Training Accuracy: 31.1996%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [187/225], Training Accuracy: 31.1915%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [188/225], Training Accuracy: 31.1835%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [189/225], Training Accuracy: 31.2087%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [190/225], Training Accuracy: 31.1760%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [191/225], Training Accuracy: 31.1600%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [192/225], Training Accuracy: 31.0872%, Training Loss: 0.6852%\n",
      "Epoch [97/100], Step [193/225], Training Accuracy: 31.0881%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [194/225], Training Accuracy: 31.1050%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [195/225], Training Accuracy: 31.0978%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [196/225], Training Accuracy: 31.0826%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [197/225], Training Accuracy: 31.1310%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [198/225], Training Accuracy: 31.1553%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [199/225], Training Accuracy: 31.1401%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [200/225], Training Accuracy: 31.1172%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [201/225], Training Accuracy: 31.1256%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [202/225], Training Accuracy: 31.1340%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [203/225], Training Accuracy: 31.1115%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [204/225], Training Accuracy: 31.1504%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [205/225], Training Accuracy: 31.1509%, Training Loss: 0.6851%\n",
      "Epoch [97/100], Step [206/225], Training Accuracy: 31.1286%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [207/225], Training Accuracy: 31.1217%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [208/225], Training Accuracy: 31.1599%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [209/225], Training Accuracy: 31.1977%, Training Loss: 0.6850%\n",
      "Epoch [97/100], Step [210/225], Training Accuracy: 31.2277%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [211/225], Training Accuracy: 31.2204%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [212/225], Training Accuracy: 31.2795%, Training Loss: 0.6848%\n",
      "Epoch [97/100], Step [213/225], Training Accuracy: 31.2573%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [214/225], Training Accuracy: 31.2792%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [215/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [216/225], Training Accuracy: 31.1777%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [217/225], Training Accuracy: 31.1780%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [218/225], Training Accuracy: 31.1497%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [219/225], Training Accuracy: 31.2143%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [220/225], Training Accuracy: 31.2429%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [221/225], Training Accuracy: 31.2288%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [222/225], Training Accuracy: 31.2430%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [223/225], Training Accuracy: 31.2850%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [224/225], Training Accuracy: 31.2779%, Training Loss: 0.6849%\n",
      "Epoch [97/100], Step [225/225], Training Accuracy: 31.2674%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [1/225], Training Accuracy: 29.6875%, Training Loss: 0.6915%\n",
      "Epoch [98/100], Step [2/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [98/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6883%\n",
      "Epoch [98/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6857%\n",
      "Epoch [98/100], Step [5/225], Training Accuracy: 33.4375%, Training Loss: 0.6863%\n",
      "Epoch [98/100], Step [6/225], Training Accuracy: 32.5521%, Training Loss: 0.6859%\n",
      "Epoch [98/100], Step [7/225], Training Accuracy: 32.1429%, Training Loss: 0.6856%\n",
      "Epoch [98/100], Step [8/225], Training Accuracy: 31.8359%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [9/225], Training Accuracy: 31.9444%, Training Loss: 0.6859%\n",
      "Epoch [98/100], Step [10/225], Training Accuracy: 31.8750%, Training Loss: 0.6855%\n",
      "Epoch [98/100], Step [11/225], Training Accuracy: 31.8182%, Training Loss: 0.6855%\n",
      "Epoch [98/100], Step [12/225], Training Accuracy: 30.7292%, Training Loss: 0.6857%\n",
      "Epoch [98/100], Step [13/225], Training Accuracy: 30.6490%, Training Loss: 0.6856%\n",
      "Epoch [98/100], Step [14/225], Training Accuracy: 30.8036%, Training Loss: 0.6862%\n",
      "Epoch [98/100], Step [15/225], Training Accuracy: 31.1458%, Training Loss: 0.6862%\n",
      "Epoch [98/100], Step [16/225], Training Accuracy: 31.2500%, Training Loss: 0.6860%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Step [17/225], Training Accuracy: 30.8824%, Training Loss: 0.6860%\n",
      "Epoch [98/100], Step [18/225], Training Accuracy: 30.8160%, Training Loss: 0.6862%\n",
      "Epoch [98/100], Step [19/225], Training Accuracy: 31.4145%, Training Loss: 0.6864%\n",
      "Epoch [98/100], Step [20/225], Training Accuracy: 31.8750%, Training Loss: 0.6863%\n",
      "Epoch [98/100], Step [21/225], Training Accuracy: 31.8452%, Training Loss: 0.6861%\n",
      "Epoch [98/100], Step [22/225], Training Accuracy: 32.0312%, Training Loss: 0.6861%\n",
      "Epoch [98/100], Step [23/225], Training Accuracy: 31.7935%, Training Loss: 0.6859%\n",
      "Epoch [98/100], Step [24/225], Training Accuracy: 31.9010%, Training Loss: 0.6857%\n",
      "Epoch [98/100], Step [25/225], Training Accuracy: 32.0625%, Training Loss: 0.6854%\n",
      "Epoch [98/100], Step [26/225], Training Accuracy: 32.4519%, Training Loss: 0.6853%\n",
      "Epoch [98/100], Step [27/225], Training Accuracy: 32.1759%, Training Loss: 0.6852%\n",
      "Epoch [98/100], Step [28/225], Training Accuracy: 31.8638%, Training Loss: 0.6852%\n",
      "Epoch [98/100], Step [29/225], Training Accuracy: 32.2737%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [30/225], Training Accuracy: 32.2917%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [31/225], Training Accuracy: 32.2077%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [32/225], Training Accuracy: 32.3730%, Training Loss: 0.6846%\n",
      "Epoch [98/100], Step [33/225], Training Accuracy: 32.4337%, Training Loss: 0.6845%\n",
      "Epoch [98/100], Step [34/225], Training Accuracy: 32.3989%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [35/225], Training Accuracy: 32.2768%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [36/225], Training Accuracy: 32.1181%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [37/225], Training Accuracy: 32.0946%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [38/225], Training Accuracy: 31.9079%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [39/225], Training Accuracy: 31.6106%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [40/225], Training Accuracy: 31.6016%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [41/225], Training Accuracy: 31.7454%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [42/225], Training Accuracy: 31.6220%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [43/225], Training Accuracy: 31.7951%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [44/225], Training Accuracy: 31.7472%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [45/225], Training Accuracy: 31.8056%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [46/225], Training Accuracy: 31.6236%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6846%\n",
      "Epoch [98/100], Step [48/225], Training Accuracy: 31.7057%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [49/225], Training Accuracy: 31.7921%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [50/225], Training Accuracy: 31.8125%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [51/225], Training Accuracy: 31.9240%, Training Loss: 0.6842%\n",
      "Epoch [98/100], Step [52/225], Training Accuracy: 31.9411%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [53/225], Training Accuracy: 31.7807%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [54/225], Training Accuracy: 31.6840%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [55/225], Training Accuracy: 31.7330%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [56/225], Training Accuracy: 31.7801%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [57/225], Training Accuracy: 31.7982%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [58/225], Training Accuracy: 31.7349%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [59/225], Training Accuracy: 32.0710%, Training Loss: 0.6839%\n",
      "Epoch [98/100], Step [60/225], Training Accuracy: 32.1615%, Training Loss: 0.6839%\n",
      "Epoch [98/100], Step [61/225], Training Accuracy: 32.0441%, Training Loss: 0.6838%\n",
      "Epoch [98/100], Step [62/225], Training Accuracy: 32.0565%, Training Loss: 0.6839%\n",
      "Epoch [98/100], Step [63/225], Training Accuracy: 32.1429%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [64/225], Training Accuracy: 32.1289%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [65/225], Training Accuracy: 32.0433%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [66/225], Training Accuracy: 32.1259%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [67/225], Training Accuracy: 32.0896%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [68/225], Training Accuracy: 32.1461%, Training Loss: 0.6839%\n",
      "Epoch [98/100], Step [69/225], Training Accuracy: 32.1105%, Training Loss: 0.6838%\n",
      "Epoch [98/100], Step [70/225], Training Accuracy: 32.0536%, Training Loss: 0.6838%\n",
      "Epoch [98/100], Step [71/225], Training Accuracy: 32.0863%, Training Loss: 0.6837%\n",
      "Epoch [98/100], Step [72/225], Training Accuracy: 31.8793%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [73/225], Training Accuracy: 31.8065%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [74/225], Training Accuracy: 31.9046%, Training Loss: 0.6838%\n",
      "Epoch [98/100], Step [75/225], Training Accuracy: 31.8958%, Training Loss: 0.6838%\n",
      "Epoch [98/100], Step [76/225], Training Accuracy: 31.8668%, Training Loss: 0.6838%\n",
      "Epoch [98/100], Step [77/225], Training Accuracy: 31.7573%, Training Loss: 0.6839%\n",
      "Epoch [98/100], Step [78/225], Training Accuracy: 31.7508%, Training Loss: 0.6839%\n",
      "Epoch [98/100], Step [79/225], Training Accuracy: 31.6851%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [80/225], Training Accuracy: 31.6602%, Training Loss: 0.6840%\n",
      "Epoch [98/100], Step [81/225], Training Accuracy: 31.5586%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [82/225], Training Accuracy: 31.5739%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [83/225], Training Accuracy: 31.5136%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [84/225], Training Accuracy: 31.5662%, Training Loss: 0.6841%\n",
      "Epoch [98/100], Step [85/225], Training Accuracy: 31.5441%, Training Loss: 0.6842%\n",
      "Epoch [98/100], Step [86/225], Training Accuracy: 31.5952%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [87/225], Training Accuracy: 31.5553%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [88/225], Training Accuracy: 31.5163%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [89/225], Training Accuracy: 31.4256%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [90/225], Training Accuracy: 31.3542%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [91/225], Training Accuracy: 31.4560%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [92/225], Training Accuracy: 31.4708%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [93/225], Training Accuracy: 31.4684%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [94/225], Training Accuracy: 31.5326%, Training Loss: 0.6842%\n",
      "Epoch [98/100], Step [95/225], Training Accuracy: 31.4145%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [96/225], Training Accuracy: 31.4941%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [97/225], Training Accuracy: 31.5077%, Training Loss: 0.6845%\n",
      "Epoch [98/100], Step [98/225], Training Accuracy: 31.5210%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [99/225], Training Accuracy: 31.6130%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [100/225], Training Accuracy: 31.6094%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [101/225], Training Accuracy: 31.7296%, Training Loss: 0.6843%\n",
      "Epoch [98/100], Step [102/225], Training Accuracy: 31.6176%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [103/225], Training Accuracy: 31.6596%, Training Loss: 0.6844%\n",
      "Epoch [98/100], Step [104/225], Training Accuracy: 31.6406%, Training Loss: 0.6845%\n",
      "Epoch [98/100], Step [105/225], Training Accuracy: 31.5923%, Training Loss: 0.6845%\n",
      "Epoch [98/100], Step [106/225], Training Accuracy: 31.6185%, Training Loss: 0.6845%\n",
      "Epoch [98/100], Step [107/225], Training Accuracy: 31.5129%, Training Loss: 0.6846%\n",
      "Epoch [98/100], Step [108/225], Training Accuracy: 31.5828%, Training Loss: 0.6846%\n",
      "Epoch [98/100], Step [109/225], Training Accuracy: 31.4650%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [110/225], Training Accuracy: 31.4915%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [111/225], Training Accuracy: 31.3626%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [112/225], Training Accuracy: 31.4314%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [113/225], Training Accuracy: 31.4436%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [114/225], Training Accuracy: 31.5241%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [115/225], Training Accuracy: 31.5082%, Training Loss: 0.6847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Step [116/225], Training Accuracy: 31.5194%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [117/225], Training Accuracy: 31.4503%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [118/225], Training Accuracy: 31.4221%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [119/225], Training Accuracy: 31.3682%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [120/225], Training Accuracy: 31.3411%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [121/225], Training Accuracy: 31.3404%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [122/225], Training Accuracy: 31.3268%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [123/225], Training Accuracy: 31.3389%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [124/225], Training Accuracy: 31.3130%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [125/225], Training Accuracy: 31.3125%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [126/225], Training Accuracy: 31.2376%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [127/225], Training Accuracy: 31.2008%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [128/225], Training Accuracy: 31.2134%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [129/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [130/225], Training Accuracy: 31.1899%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [131/225], Training Accuracy: 31.1427%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [132/225], Training Accuracy: 31.1080%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [133/225], Training Accuracy: 31.1208%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [134/225], Training Accuracy: 31.1917%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [135/225], Training Accuracy: 31.2037%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [136/225], Training Accuracy: 31.2270%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [137/225], Training Accuracy: 31.2728%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [138/225], Training Accuracy: 31.2726%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [139/225], Training Accuracy: 31.2163%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [140/225], Training Accuracy: 31.2054%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [141/225], Training Accuracy: 31.1392%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [142/225], Training Accuracy: 31.2170%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [143/225], Training Accuracy: 31.2172%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [144/225], Training Accuracy: 31.2391%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [145/225], Training Accuracy: 31.3362%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [146/225], Training Accuracy: 31.3677%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [147/225], Training Accuracy: 31.3882%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [148/225], Training Accuracy: 31.3450%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [149/225], Training Accuracy: 31.3549%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [150/225], Training Accuracy: 31.3750%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [151/225], Training Accuracy: 31.4363%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [152/225], Training Accuracy: 31.4350%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [153/225], Training Accuracy: 31.4032%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [154/225], Training Accuracy: 31.4225%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [155/225], Training Accuracy: 31.3911%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [156/225], Training Accuracy: 31.3902%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [157/225], Training Accuracy: 31.3296%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [158/225], Training Accuracy: 31.2994%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [159/225], Training Accuracy: 31.3384%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [160/225], Training Accuracy: 31.3477%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [161/225], Training Accuracy: 31.3956%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [162/225], Training Accuracy: 31.3754%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [163/225], Training Accuracy: 31.4417%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [164/225], Training Accuracy: 31.4310%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [165/225], Training Accuracy: 31.3731%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [166/225], Training Accuracy: 31.3724%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [167/225], Training Accuracy: 31.4184%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [168/225], Training Accuracy: 31.3802%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [169/225], Training Accuracy: 31.3055%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [170/225], Training Accuracy: 31.2592%, Training Loss: 0.6851%\n",
      "Epoch [98/100], Step [171/225], Training Accuracy: 31.2774%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [172/225], Training Accuracy: 31.2863%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [173/225], Training Accuracy: 31.2771%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [174/225], Training Accuracy: 31.2859%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [175/225], Training Accuracy: 31.3214%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [176/225], Training Accuracy: 31.3388%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [177/225], Training Accuracy: 31.3294%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [178/225], Training Accuracy: 31.3027%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [179/225], Training Accuracy: 31.2675%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [180/225], Training Accuracy: 31.3108%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [181/225], Training Accuracy: 31.2759%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [182/225], Training Accuracy: 31.2586%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [183/225], Training Accuracy: 31.2756%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [184/225], Training Accuracy: 31.2585%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [185/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [98/100], Step [186/225], Training Accuracy: 31.2584%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [187/225], Training Accuracy: 31.2751%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [188/225], Training Accuracy: 31.2666%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [189/225], Training Accuracy: 31.2913%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [190/225], Training Accuracy: 31.2500%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [191/225], Training Accuracy: 31.2255%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [192/225], Training Accuracy: 31.1605%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [193/225], Training Accuracy: 31.1528%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [194/225], Training Accuracy: 31.1614%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [195/225], Training Accuracy: 31.1538%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [196/225], Training Accuracy: 31.1384%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [197/225], Training Accuracy: 31.1707%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [198/225], Training Accuracy: 31.1948%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [199/225], Training Accuracy: 31.1715%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [200/225], Training Accuracy: 31.1562%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [201/225], Training Accuracy: 31.1645%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [202/225], Training Accuracy: 31.1572%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [203/225], Training Accuracy: 31.1345%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [204/225], Training Accuracy: 31.1887%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [205/225], Training Accuracy: 31.1814%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [206/225], Training Accuracy: 31.1666%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [207/225], Training Accuracy: 31.1368%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [208/225], Training Accuracy: 31.1674%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [209/225], Training Accuracy: 31.2051%, Training Loss: 0.6849%\n",
      "Epoch [98/100], Step [210/225], Training Accuracy: 31.2351%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [211/225], Training Accuracy: 31.2278%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [212/225], Training Accuracy: 31.2795%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [213/225], Training Accuracy: 31.2720%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [214/225], Training Accuracy: 31.3011%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [215/225], Training Accuracy: 31.2791%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [216/225], Training Accuracy: 31.2138%, Training Loss: 0.6848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Step [217/225], Training Accuracy: 31.2140%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [218/225], Training Accuracy: 31.1855%, Training Loss: 0.6848%\n",
      "Epoch [98/100], Step [219/225], Training Accuracy: 31.2571%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [220/225], Training Accuracy: 31.2713%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [221/225], Training Accuracy: 31.2641%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [222/225], Training Accuracy: 31.2782%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [223/225], Training Accuracy: 31.3201%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [224/225], Training Accuracy: 31.3198%, Training Loss: 0.6847%\n",
      "Epoch [98/100], Step [225/225], Training Accuracy: 31.3021%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6875%\n",
      "Epoch [99/100], Step [2/225], Training Accuracy: 32.8125%, Training Loss: 0.6861%\n",
      "Epoch [99/100], Step [3/225], Training Accuracy: 31.7708%, Training Loss: 0.6881%\n",
      "Epoch [99/100], Step [4/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [5/225], Training Accuracy: 32.8125%, Training Loss: 0.6858%\n",
      "Epoch [99/100], Step [6/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [7/225], Training Accuracy: 31.9196%, Training Loss: 0.6839%\n",
      "Epoch [99/100], Step [8/225], Training Accuracy: 31.6406%, Training Loss: 0.6834%\n",
      "Epoch [99/100], Step [9/225], Training Accuracy: 31.4236%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [10/225], Training Accuracy: 30.7812%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [11/225], Training Accuracy: 30.3977%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [12/225], Training Accuracy: 29.8177%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [13/225], Training Accuracy: 29.5673%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [14/225], Training Accuracy: 29.9107%, Training Loss: 0.6861%\n",
      "Epoch [99/100], Step [15/225], Training Accuracy: 30.5208%, Training Loss: 0.6860%\n",
      "Epoch [99/100], Step [16/225], Training Accuracy: 30.4688%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [17/225], Training Accuracy: 30.3309%, Training Loss: 0.6856%\n",
      "Epoch [99/100], Step [18/225], Training Accuracy: 30.6424%, Training Loss: 0.6856%\n",
      "Epoch [99/100], Step [19/225], Training Accuracy: 31.1678%, Training Loss: 0.6856%\n",
      "Epoch [99/100], Step [20/225], Training Accuracy: 31.5625%, Training Loss: 0.6856%\n",
      "Epoch [99/100], Step [21/225], Training Accuracy: 31.5476%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [22/225], Training Accuracy: 31.6761%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [23/225], Training Accuracy: 31.6576%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [24/225], Training Accuracy: 31.7708%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [25/225], Training Accuracy: 31.9375%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [26/225], Training Accuracy: 32.3317%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [27/225], Training Accuracy: 32.0602%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [28/225], Training Accuracy: 32.0312%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [29/225], Training Accuracy: 32.3815%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [30/225], Training Accuracy: 32.3958%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [31/225], Training Accuracy: 32.2077%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [32/225], Training Accuracy: 32.4707%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [33/225], Training Accuracy: 32.6231%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [34/225], Training Accuracy: 32.5827%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [35/225], Training Accuracy: 32.4107%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [36/225], Training Accuracy: 32.1615%, Training Loss: 0.6845%\n",
      "Epoch [99/100], Step [37/225], Training Accuracy: 32.1368%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [38/225], Training Accuracy: 31.9901%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [39/225], Training Accuracy: 31.6506%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [40/225], Training Accuracy: 31.6797%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [41/225], Training Accuracy: 31.8216%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [42/225], Training Accuracy: 31.6964%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [43/225], Training Accuracy: 31.8677%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [44/225], Training Accuracy: 31.7827%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [45/225], Training Accuracy: 31.8056%, Training Loss: 0.6846%\n",
      "Epoch [99/100], Step [46/225], Training Accuracy: 31.6576%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [47/225], Training Accuracy: 31.5824%, Training Loss: 0.6846%\n",
      "Epoch [99/100], Step [48/225], Training Accuracy: 31.7383%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [49/225], Training Accuracy: 31.7602%, Training Loss: 0.6845%\n",
      "Epoch [99/100], Step [50/225], Training Accuracy: 31.7188%, Training Loss: 0.6846%\n",
      "Epoch [99/100], Step [51/225], Training Accuracy: 31.8934%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [52/225], Training Accuracy: 31.9111%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [53/225], Training Accuracy: 31.8396%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [54/225], Training Accuracy: 31.7130%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [55/225], Training Accuracy: 31.7898%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [56/225], Training Accuracy: 31.8080%, Training Loss: 0.6845%\n",
      "Epoch [99/100], Step [57/225], Training Accuracy: 31.7982%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [58/225], Training Accuracy: 31.6810%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [59/225], Training Accuracy: 32.0710%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [60/225], Training Accuracy: 32.1354%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [61/225], Training Accuracy: 32.0697%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [62/225], Training Accuracy: 32.1321%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [63/225], Training Accuracy: 32.1925%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [64/225], Training Accuracy: 32.2266%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [65/225], Training Accuracy: 32.1394%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [66/225], Training Accuracy: 32.1970%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [67/225], Training Accuracy: 32.1828%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [68/225], Training Accuracy: 32.2610%, Training Loss: 0.6841%\n",
      "Epoch [99/100], Step [69/225], Training Accuracy: 32.2464%, Training Loss: 0.6840%\n",
      "Epoch [99/100], Step [70/225], Training Accuracy: 32.2321%, Training Loss: 0.6840%\n",
      "Epoch [99/100], Step [71/225], Training Accuracy: 32.3063%, Training Loss: 0.6840%\n",
      "Epoch [99/100], Step [72/225], Training Accuracy: 32.0530%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [73/225], Training Accuracy: 31.9349%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [74/225], Training Accuracy: 32.0312%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [75/225], Training Accuracy: 31.9583%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [76/225], Training Accuracy: 31.9901%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [77/225], Training Accuracy: 31.8791%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [78/225], Training Accuracy: 31.8910%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [79/225], Training Accuracy: 31.8236%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [80/225], Training Accuracy: 31.7969%, Training Loss: 0.6842%\n",
      "Epoch [99/100], Step [81/225], Training Accuracy: 31.7130%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [82/225], Training Accuracy: 31.7264%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [83/225], Training Accuracy: 31.6453%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [84/225], Training Accuracy: 31.6964%, Training Loss: 0.6843%\n",
      "Epoch [99/100], Step [85/225], Training Accuracy: 31.6544%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [86/225], Training Accuracy: 31.6860%, Training Loss: 0.6844%\n",
      "Epoch [99/100], Step [87/225], Training Accuracy: 31.6810%, Training Loss: 0.6845%\n",
      "Epoch [99/100], Step [88/225], Training Accuracy: 31.6229%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [89/225], Training Accuracy: 31.4958%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [90/225], Training Accuracy: 31.4062%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [91/225], Training Accuracy: 31.4560%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [92/225], Training Accuracy: 31.4538%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [93/225], Training Accuracy: 31.4012%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [94/225], Training Accuracy: 31.4328%, Training Loss: 0.6847%\n",
      "Epoch [99/100], Step [95/225], Training Accuracy: 31.3487%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [96/225], Training Accuracy: 31.4941%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [97/225], Training Accuracy: 31.4916%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [98/225], Training Accuracy: 31.4892%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Step [99/225], Training Accuracy: 31.6130%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [100/225], Training Accuracy: 31.5625%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [101/225], Training Accuracy: 31.6677%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [102/225], Training Accuracy: 31.5717%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [103/225], Training Accuracy: 31.6292%, Training Loss: 0.6848%\n",
      "Epoch [99/100], Step [104/225], Training Accuracy: 31.5956%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [105/225], Training Accuracy: 31.5327%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [106/225], Training Accuracy: 31.5596%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [107/225], Training Accuracy: 31.4836%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [108/225], Training Accuracy: 31.5538%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [109/225], Training Accuracy: 31.4364%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [110/225], Training Accuracy: 31.4205%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [111/225], Training Accuracy: 31.3204%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [112/225], Training Accuracy: 31.3895%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [113/225], Training Accuracy: 31.4021%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [114/225], Training Accuracy: 31.4556%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [115/225], Training Accuracy: 31.4266%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [116/225], Training Accuracy: 31.4251%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [117/225], Training Accuracy: 31.3702%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [118/225], Training Accuracy: 31.3692%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [119/225], Training Accuracy: 31.3157%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [120/225], Training Accuracy: 31.3542%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [121/225], Training Accuracy: 31.3275%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [122/225], Training Accuracy: 31.3397%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [123/225], Training Accuracy: 31.3643%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [124/225], Training Accuracy: 31.3508%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [125/225], Training Accuracy: 31.3375%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [126/225], Training Accuracy: 31.2748%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [127/225], Training Accuracy: 31.2254%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [128/225], Training Accuracy: 31.2256%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [129/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [130/225], Training Accuracy: 31.1538%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [131/225], Training Accuracy: 31.1188%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [132/225], Training Accuracy: 31.0724%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [133/225], Training Accuracy: 31.0855%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [134/225], Training Accuracy: 31.1334%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [135/225], Training Accuracy: 31.1574%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [136/225], Training Accuracy: 31.1581%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [137/225], Training Accuracy: 31.1930%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [138/225], Training Accuracy: 31.2274%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [139/225], Training Accuracy: 31.1938%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [140/225], Training Accuracy: 31.1719%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [141/225], Training Accuracy: 31.1613%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [142/225], Training Accuracy: 31.2060%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [143/225], Training Accuracy: 31.2063%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [144/225], Training Accuracy: 31.2174%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [145/225], Training Accuracy: 31.2716%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [146/225], Training Accuracy: 31.3035%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [147/225], Training Accuracy: 31.3138%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [148/225], Training Accuracy: 31.2817%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [149/225], Training Accuracy: 31.3129%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [150/225], Training Accuracy: 31.3125%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [151/225], Training Accuracy: 31.3742%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [152/225], Training Accuracy: 31.3836%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [153/225], Training Accuracy: 31.3419%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [154/225], Training Accuracy: 31.3819%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [155/225], Training Accuracy: 31.3710%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [156/225], Training Accuracy: 31.3802%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [157/225], Training Accuracy: 31.3097%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [158/225], Training Accuracy: 31.2994%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [159/225], Training Accuracy: 31.3384%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [160/225], Training Accuracy: 31.3184%, Training Loss: 0.6855%\n",
      "Epoch [99/100], Step [161/225], Training Accuracy: 31.3762%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [162/225], Training Accuracy: 31.3657%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [163/225], Training Accuracy: 31.4225%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [164/225], Training Accuracy: 31.4405%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [165/225], Training Accuracy: 31.3826%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [166/225], Training Accuracy: 31.3630%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [167/225], Training Accuracy: 31.4091%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [168/225], Training Accuracy: 31.3616%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [169/225], Training Accuracy: 31.2777%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [170/225], Training Accuracy: 31.2224%, Training Loss: 0.6854%\n",
      "Epoch [99/100], Step [171/225], Training Accuracy: 31.2409%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [172/225], Training Accuracy: 31.2500%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [173/225], Training Accuracy: 31.2319%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [174/225], Training Accuracy: 31.2320%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [175/225], Training Accuracy: 31.2589%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [176/225], Training Accuracy: 31.2766%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [177/225], Training Accuracy: 31.2765%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [178/225], Training Accuracy: 31.2588%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [179/225], Training Accuracy: 31.2413%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [180/225], Training Accuracy: 31.2847%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [181/225], Training Accuracy: 31.2414%, Training Loss: 0.6853%\n",
      "Epoch [99/100], Step [182/225], Training Accuracy: 31.2328%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [183/225], Training Accuracy: 31.2415%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [184/225], Training Accuracy: 31.2160%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [185/225], Training Accuracy: 31.1740%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [186/225], Training Accuracy: 31.1912%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [187/225], Training Accuracy: 31.2166%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [188/225], Training Accuracy: 31.2001%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [189/225], Training Accuracy: 31.2169%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [190/225], Training Accuracy: 31.1760%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [191/225], Training Accuracy: 31.1437%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [192/225], Training Accuracy: 31.0791%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [193/225], Training Accuracy: 31.0638%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [194/225], Training Accuracy: 31.0809%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [195/225], Training Accuracy: 31.0657%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [196/225], Training Accuracy: 31.0348%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [197/225], Training Accuracy: 31.0596%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [198/225], Training Accuracy: 31.0843%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [199/225], Training Accuracy: 31.0694%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [200/225], Training Accuracy: 31.0547%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [201/225], Training Accuracy: 31.0712%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [202/225], Training Accuracy: 31.0644%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [203/225], Training Accuracy: 31.0576%, Training Loss: 0.6852%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Step [204/225], Training Accuracy: 31.1198%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [205/225], Training Accuracy: 31.1204%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [206/225], Training Accuracy: 31.0983%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [207/225], Training Accuracy: 31.0462%, Training Loss: 0.6852%\n",
      "Epoch [99/100], Step [208/225], Training Accuracy: 31.0847%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [209/225], Training Accuracy: 31.1229%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [210/225], Training Accuracy: 31.1533%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [211/225], Training Accuracy: 31.1241%, Training Loss: 0.6851%\n",
      "Epoch [99/100], Step [212/225], Training Accuracy: 31.1689%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [213/225], Training Accuracy: 31.1473%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [214/225], Training Accuracy: 31.1624%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [215/225], Training Accuracy: 31.1337%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [216/225], Training Accuracy: 31.0619%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [217/225], Training Accuracy: 31.0700%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [218/225], Training Accuracy: 31.0493%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [219/225], Training Accuracy: 31.1073%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [220/225], Training Accuracy: 31.1293%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [221/225], Training Accuracy: 31.1227%, Training Loss: 0.6850%\n",
      "Epoch [99/100], Step [222/225], Training Accuracy: 31.1444%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [223/225], Training Accuracy: 31.1729%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [224/225], Training Accuracy: 31.1733%, Training Loss: 0.6849%\n",
      "Epoch [99/100], Step [225/225], Training Accuracy: 31.1562%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [1/225], Training Accuracy: 34.3750%, Training Loss: 0.6934%\n",
      "Epoch [100/100], Step [2/225], Training Accuracy: 33.5938%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [3/225], Training Accuracy: 31.2500%, Training Loss: 0.6893%\n",
      "Epoch [100/100], Step [4/225], Training Accuracy: 31.6406%, Training Loss: 0.6861%\n",
      "Epoch [100/100], Step [5/225], Training Accuracy: 32.1875%, Training Loss: 0.6863%\n",
      "Epoch [100/100], Step [6/225], Training Accuracy: 31.5104%, Training Loss: 0.6862%\n",
      "Epoch [100/100], Step [7/225], Training Accuracy: 31.4732%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [8/225], Training Accuracy: 30.8594%, Training Loss: 0.6841%\n",
      "Epoch [100/100], Step [9/225], Training Accuracy: 30.5556%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [10/225], Training Accuracy: 30.1562%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [11/225], Training Accuracy: 29.8295%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [12/225], Training Accuracy: 29.1667%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [13/225], Training Accuracy: 29.0865%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [14/225], Training Accuracy: 29.2411%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [15/225], Training Accuracy: 29.7917%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [16/225], Training Accuracy: 29.6875%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [17/225], Training Accuracy: 29.6875%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [18/225], Training Accuracy: 29.8611%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [19/225], Training Accuracy: 30.2632%, Training Loss: 0.6859%\n",
      "Epoch [100/100], Step [20/225], Training Accuracy: 30.7031%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [21/225], Training Accuracy: 30.7292%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [22/225], Training Accuracy: 30.8239%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [23/225], Training Accuracy: 30.7745%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [24/225], Training Accuracy: 30.9896%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [25/225], Training Accuracy: 31.3125%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [26/225], Training Accuracy: 31.6106%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [27/225], Training Accuracy: 31.4236%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [28/225], Training Accuracy: 31.1942%, Training Loss: 0.6845%\n",
      "Epoch [100/100], Step [29/225], Training Accuracy: 31.6272%, Training Loss: 0.6844%\n",
      "Epoch [100/100], Step [30/225], Training Accuracy: 31.6667%, Training Loss: 0.6842%\n",
      "Epoch [100/100], Step [31/225], Training Accuracy: 31.4516%, Training Loss: 0.6843%\n",
      "Epoch [100/100], Step [32/225], Training Accuracy: 31.7871%, Training Loss: 0.6839%\n",
      "Epoch [100/100], Step [33/225], Training Accuracy: 31.9602%, Training Loss: 0.6839%\n",
      "Epoch [100/100], Step [34/225], Training Accuracy: 31.8474%, Training Loss: 0.6840%\n",
      "Epoch [100/100], Step [35/225], Training Accuracy: 31.6964%, Training Loss: 0.6842%\n",
      "Epoch [100/100], Step [36/225], Training Accuracy: 31.5104%, Training Loss: 0.6844%\n",
      "Epoch [100/100], Step [37/225], Training Accuracy: 31.5456%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [38/225], Training Accuracy: 31.3322%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [39/225], Training Accuracy: 31.0897%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [40/225], Training Accuracy: 31.1719%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [41/225], Training Accuracy: 31.1357%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [42/225], Training Accuracy: 30.9896%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [43/225], Training Accuracy: 31.2137%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [44/225], Training Accuracy: 31.2145%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [45/225], Training Accuracy: 31.2500%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [46/225], Training Accuracy: 31.2500%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [47/225], Training Accuracy: 31.1503%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [48/225], Training Accuracy: 31.3151%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [49/225], Training Accuracy: 31.3457%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [50/225], Training Accuracy: 31.3438%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [51/225], Training Accuracy: 31.4645%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [52/225], Training Accuracy: 31.5505%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [53/225], Training Accuracy: 31.4564%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [54/225], Training Accuracy: 31.3368%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [55/225], Training Accuracy: 31.4205%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [56/225], Training Accuracy: 31.4732%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [57/225], Training Accuracy: 31.4419%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [58/225], Training Accuracy: 31.3039%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [59/225], Training Accuracy: 31.7532%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [60/225], Training Accuracy: 31.8490%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [61/225], Training Accuracy: 31.7879%, Training Loss: 0.6845%\n",
      "Epoch [100/100], Step [62/225], Training Accuracy: 31.8800%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [63/225], Training Accuracy: 31.9692%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [64/225], Training Accuracy: 31.9580%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [65/225], Training Accuracy: 31.8750%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [66/225], Training Accuracy: 31.9129%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [67/225], Training Accuracy: 31.9030%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [68/225], Training Accuracy: 31.9623%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [69/225], Training Accuracy: 31.9973%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [70/225], Training Accuracy: 32.0312%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [71/225], Training Accuracy: 32.0423%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [72/225], Training Accuracy: 31.8576%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [73/225], Training Accuracy: 31.8065%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [74/225], Training Accuracy: 31.9046%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [75/225], Training Accuracy: 31.8958%, Training Loss: 0.6846%\n",
      "Epoch [100/100], Step [76/225], Training Accuracy: 31.8257%, Training Loss: 0.6847%\n",
      "Epoch [100/100], Step [77/225], Training Accuracy: 31.6964%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [78/225], Training Accuracy: 31.7508%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [79/225], Training Accuracy: 31.6851%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [80/225], Training Accuracy: 31.6602%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [81/225], Training Accuracy: 31.5586%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [82/225], Training Accuracy: 31.5739%, Training Loss: 0.6848%\n",
      "Epoch [100/100], Step [83/225], Training Accuracy: 31.5136%, Training Loss: 0.6849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Step [84/225], Training Accuracy: 31.5848%, Training Loss: 0.6849%\n",
      "Epoch [100/100], Step [85/225], Training Accuracy: 31.5993%, Training Loss: 0.6850%\n",
      "Epoch [100/100], Step [86/225], Training Accuracy: 31.6497%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [87/225], Training Accuracy: 31.6631%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [88/225], Training Accuracy: 31.6406%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [89/225], Training Accuracy: 31.5836%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [90/225], Training Accuracy: 31.4931%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [91/225], Training Accuracy: 31.5419%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [92/225], Training Accuracy: 31.5387%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [93/225], Training Accuracy: 31.5524%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [94/225], Training Accuracy: 31.6489%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [95/225], Training Accuracy: 31.5461%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [96/225], Training Accuracy: 31.6569%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [97/225], Training Accuracy: 31.6527%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [98/225], Training Accuracy: 31.6645%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [99/225], Training Accuracy: 31.7708%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [100/225], Training Accuracy: 31.7344%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [101/225], Training Accuracy: 31.8688%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [102/225], Training Accuracy: 31.7402%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [103/225], Training Accuracy: 31.7961%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [104/225], Training Accuracy: 31.7758%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [105/225], Training Accuracy: 31.7113%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [106/225], Training Accuracy: 31.7512%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [107/225], Training Accuracy: 31.6443%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [108/225], Training Accuracy: 31.6985%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [109/225], Training Accuracy: 31.5510%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [110/225], Training Accuracy: 31.5483%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [111/225], Training Accuracy: 31.4471%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [112/225], Training Accuracy: 31.5709%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [113/225], Training Accuracy: 31.5680%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [114/225], Training Accuracy: 31.6338%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [115/225], Training Accuracy: 31.5897%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [116/225], Training Accuracy: 31.5867%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [117/225], Training Accuracy: 31.5171%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [118/225], Training Accuracy: 31.4883%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [119/225], Training Accuracy: 31.4207%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [120/225], Training Accuracy: 31.4323%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [121/225], Training Accuracy: 31.4050%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [122/225], Training Accuracy: 31.4037%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [123/225], Training Accuracy: 31.4533%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [124/225], Training Accuracy: 31.4138%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [125/225], Training Accuracy: 31.4125%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [126/225], Training Accuracy: 31.3616%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [127/225], Training Accuracy: 31.2992%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [128/225], Training Accuracy: 31.2988%, Training Loss: 0.6857%\n",
      "Epoch [100/100], Step [129/225], Training Accuracy: 31.3227%, Training Loss: 0.6857%\n",
      "Epoch [100/100], Step [130/225], Training Accuracy: 31.2500%, Training Loss: 0.6857%\n",
      "Epoch [100/100], Step [131/225], Training Accuracy: 31.2023%, Training Loss: 0.6857%\n",
      "Epoch [100/100], Step [132/225], Training Accuracy: 31.1908%, Training Loss: 0.6857%\n",
      "Epoch [100/100], Step [133/225], Training Accuracy: 31.2148%, Training Loss: 0.6857%\n",
      "Epoch [100/100], Step [134/225], Training Accuracy: 31.2500%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [135/225], Training Accuracy: 31.2731%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [136/225], Training Accuracy: 31.2615%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [137/225], Training Accuracy: 31.2956%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [138/225], Training Accuracy: 31.3293%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [139/225], Training Accuracy: 31.3174%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [140/225], Training Accuracy: 31.2835%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [141/225], Training Accuracy: 31.2389%, Training Loss: 0.6856%\n",
      "Epoch [100/100], Step [142/225], Training Accuracy: 31.2830%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [143/225], Training Accuracy: 31.2719%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [144/225], Training Accuracy: 31.2500%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [145/225], Training Accuracy: 31.3039%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [146/225], Training Accuracy: 31.3142%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [147/225], Training Accuracy: 31.3350%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [148/225], Training Accuracy: 31.3239%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [149/225], Training Accuracy: 31.3549%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [150/225], Training Accuracy: 31.3854%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [151/225], Training Accuracy: 31.4466%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [152/225], Training Accuracy: 31.4248%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [153/225], Training Accuracy: 31.4134%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [154/225], Training Accuracy: 31.4326%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [155/225], Training Accuracy: 31.4214%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [156/225], Training Accuracy: 31.4403%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [157/225], Training Accuracy: 31.3495%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [158/225], Training Accuracy: 31.2994%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [159/225], Training Accuracy: 31.3483%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [160/225], Training Accuracy: 31.3086%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [161/225], Training Accuracy: 31.3470%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [162/225], Training Accuracy: 31.3272%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [163/225], Training Accuracy: 31.3746%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [164/225], Training Accuracy: 31.3643%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [165/225], Training Accuracy: 31.3163%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [166/225], Training Accuracy: 31.3159%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [167/225], Training Accuracy: 31.3529%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [168/225], Training Accuracy: 31.3151%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [169/225], Training Accuracy: 31.2315%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [170/225], Training Accuracy: 31.1857%, Training Loss: 0.6855%\n",
      "Epoch [100/100], Step [171/225], Training Accuracy: 31.2135%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [172/225], Training Accuracy: 31.2409%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [173/225], Training Accuracy: 31.2229%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [174/225], Training Accuracy: 31.2320%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [175/225], Training Accuracy: 31.2679%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [176/225], Training Accuracy: 31.2944%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [177/225], Training Accuracy: 31.2941%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [178/225], Training Accuracy: 31.3027%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [179/225], Training Accuracy: 31.2936%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [180/225], Training Accuracy: 31.3194%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [181/225], Training Accuracy: 31.2845%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [182/225], Training Accuracy: 31.2843%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [183/225], Training Accuracy: 31.2756%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [184/225], Training Accuracy: 31.2330%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [185/225], Training Accuracy: 31.1824%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [186/225], Training Accuracy: 31.2080%, Training Loss: 0.6854%\n",
      "Epoch [100/100], Step [187/225], Training Accuracy: 31.1999%, Training Loss: 0.6854%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Step [188/225], Training Accuracy: 31.1835%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [189/225], Training Accuracy: 31.2004%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [190/225], Training Accuracy: 31.1595%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [191/225], Training Accuracy: 31.1355%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [192/225], Training Accuracy: 31.0628%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [193/225], Training Accuracy: 31.0881%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [194/225], Training Accuracy: 31.1050%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [195/225], Training Accuracy: 31.0577%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [196/225], Training Accuracy: 31.0268%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [197/225], Training Accuracy: 31.0676%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [198/225], Training Accuracy: 31.0843%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [199/225], Training Accuracy: 31.0537%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [200/225], Training Accuracy: 31.0312%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [201/225], Training Accuracy: 31.0479%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [202/225], Training Accuracy: 31.0566%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [203/225], Training Accuracy: 31.0345%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [204/225], Training Accuracy: 31.0892%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [205/225], Training Accuracy: 31.0976%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [206/225], Training Accuracy: 31.0831%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [207/225], Training Accuracy: 31.0537%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [208/225], Training Accuracy: 31.0847%, Training Loss: 0.6853%\n",
      "Epoch [100/100], Step [209/225], Training Accuracy: 31.1379%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [210/225], Training Accuracy: 31.1830%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [211/225], Training Accuracy: 31.1759%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [212/225], Training Accuracy: 31.2279%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [213/225], Training Accuracy: 31.2060%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [214/225], Training Accuracy: 31.1989%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [215/225], Training Accuracy: 31.1628%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [216/225], Training Accuracy: 31.0981%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [217/225], Training Accuracy: 31.0916%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [218/225], Training Accuracy: 31.0636%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [219/225], Training Accuracy: 31.1216%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [220/225], Training Accuracy: 31.1364%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [221/225], Training Accuracy: 31.1227%, Training Loss: 0.6852%\n",
      "Epoch [100/100], Step [222/225], Training Accuracy: 31.1444%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [223/225], Training Accuracy: 31.1869%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [224/225], Training Accuracy: 31.1872%, Training Loss: 0.6851%\n",
      "Epoch [100/100], Step [225/225], Training Accuracy: 31.1492%, Training Loss: 0.6851%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "total_step = len(train_loader)\n",
    "for ep in range(total_epoch):\n",
    "    global_enc.train(), local_enc.train(), local_disc.train(), global_disc.train(), mine.train(), decomposer.train(), classifier.train()\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss = 0\n",
    "    for bidx, (batchx, batchy) in enumerate(train_loader):\n",
    "        batchx = batchx.to(device)\n",
    "        batchy = batchy.to(device)\n",
    "        # Reset gradient\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Feed input to enocders and then obtain local feature (relevant, irrelevant) and global feature\n",
    "        localf = local_enc(batchx) #[batch, d1, 1, t1]\n",
    "        rele, irre = decomposer(localf) #[batch, d1, 1, t1], #[batch, depth, 1, t1]\n",
    "        globalf = global_enc(rele) #[batch, d2]\n",
    "\n",
    "        # Feed the relevant feature to classifier\n",
    "        logits = classifier(globalf) #[batch, 4]\n",
    "        loss_class = cls_criterion(logits, batchy)\n",
    "\n",
    "        # To ensure good decomposition, estimate MI between relevant feature and irrelevant feature\n",
    "        rele_ = torch.reshape(rele, (rele.shape[0], -1)) #[batch, d1*t1]\n",
    "        irre_ = torch.reshape(irre, (irre.shape[0], -1)) #[batch, d1*t1]\n",
    "        ishuffle = torch.index_select(irre_, 0, torch.randperm(irre_.shape[0]).to(device))\n",
    "        djoint = mine(rele_, irre_) #[batch, 1]\n",
    "        dmarginal = mine(rele_, ishuffle) #[batch, 1]\n",
    "        loss_decomposition = - estimate_JSD_MI(djoint, dmarginal, True)\n",
    "\n",
    "        # Estimate global MI\n",
    "        gshuffle = torch.index_select(globalf, 0, torch.randperm(globalf.shape[0]).to(device)) #[batch, d2]\n",
    "        gjoint = global_disc(rele, globalf) #[batch, 1]\n",
    "        gmarginal = global_disc(rele, gshuffle) #[batch, 1]\n",
    "        loss_global_mi = estimate_JSD_MI(gjoint, gmarginal, True)\n",
    "\n",
    "        # Estimate local MI\n",
    "        ljoint = local_disc(rele, globalf)\n",
    "        lmarginal = local_disc(rele, gshuffle)\n",
    "        temp = estimate_JSD_MI(ljoint, lmarginal, False)\n",
    "        loss_local_mi = temp.mean()\n",
    "\n",
    "        loss_dim = - (loss_global_mi + loss_local_mi)\n",
    "\n",
    "        # All objective function\n",
    "        loss_all = alpha * loss_class + beta * loss_decomposition + gamma * loss_dim\n",
    "\n",
    "        loss_all.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        _, predicted = logits.max(1)\n",
    "        total += batchy.size(0)\n",
    "        correct += predicted.eq(batchy).sum().item()\n",
    "        running_loss += loss_all.item()\n",
    "        accu=100.*correct/total\n",
    "        train_loss = running_loss/(bidx+1)\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Training Accuracy: {:.4f}%, Training Loss: {:.4f}%'.format(ep+1, total_epoch, bidx+1, total_step, accu, train_loss))\n",
    "\n",
    "    scheduler.step()  # learning rate decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f570182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 30.628126737076155 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    global_enc.eval(), global_disc.eval(), local_enc.eval(), local_disc.eval(), mine.eval(), classifier.eval(), decomposer.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, Y in test_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        localf = local_enc(X)\n",
    "        rele, irre = decomposer(localf) \n",
    "        globalf = global_enc(rele)\n",
    "        logits = classifier(globalf) \n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += Y.size(0)\n",
    "        correct += (predicted == Y).sum().item()\n",
    "\n",
    "    print('Test Accuracy : {} %'.format(100 * correct / total))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c439973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04385cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce71586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
